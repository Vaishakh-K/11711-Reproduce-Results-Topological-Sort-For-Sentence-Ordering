1-8-28	crowdsourcing is cheap and fast , but suffers from the problem of low-quality data .	crowdsourcing has gained immense popularity in machine learning applications for obtaining large amounts of labeled data .	0	1	0	5.235318	-4.6587644	0
1-8-28	crowdsourcing has gained immense popularity in machine learning applications for obtaining large amounts of labeled data .	to address this fundamental challenge in crowdsourcing , we propose a simple payment mechanism to incentivize workers to answer only the questions that they are sure of and skip the rest .	1	0	2	-5.931033	5.168158	1
1-8-28	we show that surprisingly , under a mild and natural `` no-free-lunch '' requirement , this mechanism is the one and only incentive-compatible payment mechanism possible .	crowdsourcing has gained immense popularity in machine learning applications for obtaining large amounts of labeled data .	0	3	0	5.6208363	-5.025217	0
1-8-28	crowdsourcing has gained immense popularity in machine learning applications for obtaining large amounts of labeled data .	we also show that among all possible incentive-compatible mechanisms ( that may or may not satisfy no-free-lunch ) , our mechanism makes the smallest possible payment to spammers .	1	0	4	-5.8655777	5.1436844	1
1-8-28	crowdsourcing has gained immense popularity in machine learning applications for obtaining large amounts of labeled data .	interestingly , this unique mechanism takes a `` multiplicative '' form .	1	0	5	-5.848386	5.0994234	1
1-8-28	the simplicity of the mechanism is an added benefit .	crowdsourcing has gained immense popularity in machine learning applications for obtaining large amounts of labeled data .	0	6	0	5.558221	-4.9860954	0
1-8-28	in preliminary experiments involving over several hundred workers , we observe a significant reduction in the error rates under our unique mechanism for the same or lower monetary expenditure .	crowdsourcing has gained immense popularity in machine learning applications for obtaining large amounts of labeled data .	0	7	0	5.640739	-5.0578337	0
1-8-28	to address this fundamental challenge in crowdsourcing , we propose a simple payment mechanism to incentivize workers to answer only the questions that they are sure of and skip the rest .	crowdsourcing is cheap and fast , but suffers from the problem of low-quality data .	0	2	1	5.5528717	-4.896676	0
1-8-28	crowdsourcing is cheap and fast , but suffers from the problem of low-quality data .	we show that surprisingly , under a mild and natural `` no-free-lunch '' requirement , this mechanism is the one and only incentive-compatible payment mechanism possible .	1	1	3	-6.005455	5.104862	1
1-8-28	we also show that among all possible incentive-compatible mechanisms ( that may or may not satisfy no-free-lunch ) , our mechanism makes the smallest possible payment to spammers .	crowdsourcing is cheap and fast , but suffers from the problem of low-quality data .	0	4	1	5.6123977	-5.002885	0
1-8-28	crowdsourcing is cheap and fast , but suffers from the problem of low-quality data .	interestingly , this unique mechanism takes a `` multiplicative '' form .	1	1	5	-5.9460115	5.022108	1
1-8-28	the simplicity of the mechanism is an added benefit .	crowdsourcing is cheap and fast , but suffers from the problem of low-quality data .	0	6	1	5.601785	-4.9823055	0
1-8-28	crowdsourcing is cheap and fast , but suffers from the problem of low-quality data .	in preliminary experiments involving over several hundred workers , we observe a significant reduction in the error rates under our unique mechanism for the same or lower monetary expenditure .	1	1	7	-5.951768	5.049591	1
1-8-28	we show that surprisingly , under a mild and natural `` no-free-lunch '' requirement , this mechanism is the one and only incentive-compatible payment mechanism possible .	to address this fundamental challenge in crowdsourcing , we propose a simple payment mechanism to incentivize workers to answer only the questions that they are sure of and skip the rest .	0	3	2	5.501361	-4.8390565	0
1-8-28	we also show that among all possible incentive-compatible mechanisms ( that may or may not satisfy no-free-lunch ) , our mechanism makes the smallest possible payment to spammers .	to address this fundamental challenge in crowdsourcing , we propose a simple payment mechanism to incentivize workers to answer only the questions that they are sure of and skip the rest .	0	4	2	5.546342	-4.8852882	0
1-8-28	interestingly , this unique mechanism takes a `` multiplicative '' form .	to address this fundamental challenge in crowdsourcing , we propose a simple payment mechanism to incentivize workers to answer only the questions that they are sure of and skip the rest .	0	5	2	5.4307327	-4.73958	0
1-8-28	to address this fundamental challenge in crowdsourcing , we propose a simple payment mechanism to incentivize workers to answer only the questions that they are sure of and skip the rest .	the simplicity of the mechanism is an added benefit .	1	2	6	-5.9555674	5.1711845	1
1-8-28	to address this fundamental challenge in crowdsourcing , we propose a simple payment mechanism to incentivize workers to answer only the questions that they are sure of and skip the rest .	in preliminary experiments involving over several hundred workers , we observe a significant reduction in the error rates under our unique mechanism for the same or lower monetary expenditure .	1	2	7	-5.9822817	5.1968594	1
1-8-28	we show that surprisingly , under a mild and natural `` no-free-lunch '' requirement , this mechanism is the one and only incentive-compatible payment mechanism possible .	we also show that among all possible incentive-compatible mechanisms ( that may or may not satisfy no-free-lunch ) , our mechanism makes the smallest possible payment to spammers .	1	3	4	-4.891112	4.5095425	1
1-8-28	we show that surprisingly , under a mild and natural `` no-free-lunch '' requirement , this mechanism is the one and only incentive-compatible payment mechanism possible .	interestingly , this unique mechanism takes a `` multiplicative '' form .	1	3	5	-2.7434487	2.7116246	1
1-8-28	we show that surprisingly , under a mild and natural `` no-free-lunch '' requirement , this mechanism is the one and only incentive-compatible payment mechanism possible .	the simplicity of the mechanism is an added benefit .	1	3	6	-4.560089	4.199663	1
1-8-28	in preliminary experiments involving over several hundred workers , we observe a significant reduction in the error rates under our unique mechanism for the same or lower monetary expenditure .	we show that surprisingly , under a mild and natural `` no-free-lunch '' requirement , this mechanism is the one and only incentive-compatible payment mechanism possible .	0	7	3	3.4214382	-3.183395	0
1-8-28	interestingly , this unique mechanism takes a `` multiplicative '' form .	we also show that among all possible incentive-compatible mechanisms ( that may or may not satisfy no-free-lunch ) , our mechanism makes the smallest possible payment to spammers .	0	5	4	-3.273045	3.16494	1
1-8-28	we also show that among all possible incentive-compatible mechanisms ( that may or may not satisfy no-free-lunch ) , our mechanism makes the smallest possible payment to spammers .	the simplicity of the mechanism is an added benefit .	1	4	6	-2.082024	2.10287	1
1-8-28	we also show that among all possible incentive-compatible mechanisms ( that may or may not satisfy no-free-lunch ) , our mechanism makes the smallest possible payment to spammers .	in preliminary experiments involving over several hundred workers , we observe a significant reduction in the error rates under our unique mechanism for the same or lower monetary expenditure .	1	4	7	2.7711322	-2.6040564	0
1-8-28	the simplicity of the mechanism is an added benefit .	interestingly , this unique mechanism takes a `` multiplicative '' form .	0	6	5	2.3401437	-2.2543406	0
1-8-28	interestingly , this unique mechanism takes a `` multiplicative '' form .	in preliminary experiments involving over several hundred workers , we observe a significant reduction in the error rates under our unique mechanism for the same or lower monetary expenditure .	1	5	7	-2.9479864	2.8612015	1
1-8-28	in preliminary experiments involving over several hundred workers , we observe a significant reduction in the error rates under our unique mechanism for the same or lower monetary expenditure .	the simplicity of the mechanism is an added benefit .	0	7	6	-2.8192306	2.699562	1
2-9-36	convex potential minimisation is the de facto approach to binary classification .	however , long and servedio proved that under symmetric label noise ( sln ) , minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing .	1	0	1	-5.8700366	5.240938	1
2-9-36	convex potential minimisation is the de facto approach to binary classification .	this ostensibly shows that convex losses are not sln-robust .	1	0	2	-5.8981028	5.1010876	1
2-9-36	in this paper , we propose a convex , classification-calibrated loss and prove that it is sln-robust .	convex potential minimisation is the de facto approach to binary classification .	0	3	0	5.4242697	-4.8581924	0
2-9-36	the loss avoids the long and servedio result by virtue of being negatively unbounded .	convex potential minimisation is the de facto approach to binary classification .	0	4	0	5.514569	-4.9765053	0
2-9-36	convex potential minimisation is the de facto approach to binary classification .	the loss is a modification of the hinge loss , where one does not clamp at zero ; hence , we call it the unhinged loss .	1	0	5	-5.910402	5.207221	1
2-9-36	convex potential minimisation is the de facto approach to binary classification .	we show that the optimal unhinged solution is equivalent to that of a strongly regularised svm , and is the limiting solution for any convex potential ; this implies that strong 2 regularisation makes most standard learners sln-robust .	1	0	6	-5.9053936	5.138509	1
2-9-36	convex potential minimisation is the de facto approach to binary classification .	experiments confirm the unhinged loss ' sln-robustness is borne out in practice .	1	0	7	-5.920686	5.135858	1
2-9-36	so , with apologies to wilde , while the truth is rarely pure , it can be simple .	convex potential minimisation is the de facto approach to binary classification .	0	8	0	5.3402624	-4.827463	0
2-9-36	however , long and servedio proved that under symmetric label noise ( sln ) , minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing .	this ostensibly shows that convex losses are not sln-robust .	1	1	2	-5.9239492	5.130936	1
2-9-36	in this paper , we propose a convex , classification-calibrated loss and prove that it is sln-robust .	however , long and servedio proved that under symmetric label noise ( sln ) , minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing .	0	3	1	5.5933356	-4.9962854	0
2-9-36	however , long and servedio proved that under symmetric label noise ( sln ) , minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing .	the loss avoids the long and servedio result by virtue of being negatively unbounded .	1	1	4	-4.8592596	4.5281963	1
2-9-36	however , long and servedio proved that under symmetric label noise ( sln ) , minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing .	the loss is a modification of the hinge loss , where one does not clamp at zero ; hence , we call it the unhinged loss .	1	1	5	-5.617746	5.167014	1
2-9-36	we show that the optimal unhinged solution is equivalent to that of a strongly regularised svm , and is the limiting solution for any convex potential ; this implies that strong 2 regularisation makes most standard learners sln-robust .	however , long and servedio proved that under symmetric label noise ( sln ) , minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing .	0	6	1	5.6139655	-5.0065	0
2-9-36	experiments confirm the unhinged loss ' sln-robustness is borne out in practice .	however , long and servedio proved that under symmetric label noise ( sln ) , minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing .	0	7	1	5.5866213	-4.9713197	0
2-9-36	so , with apologies to wilde , while the truth is rarely pure , it can be simple .	however , long and servedio proved that under symmetric label noise ( sln ) , minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing .	0	8	1	4.2530227	-3.8538218	0
2-9-36	in this paper , we propose a convex , classification-calibrated loss and prove that it is sln-robust .	this ostensibly shows that convex losses are not sln-robust .	0	3	2	-2.9040923	2.9378536	1
2-9-36	the loss avoids the long and servedio result by virtue of being negatively unbounded .	this ostensibly shows that convex losses are not sln-robust .	0	4	2	1.7757568	-1.6328106	0
2-9-36	this ostensibly shows that convex losses are not sln-robust .	the loss is a modification of the hinge loss , where one does not clamp at zero ; hence , we call it the unhinged loss .	1	2	5	-0.20884174	0.39230052	1
2-9-36	we show that the optimal unhinged solution is equivalent to that of a strongly regularised svm , and is the limiting solution for any convex potential ; this implies that strong 2 regularisation makes most standard learners sln-robust .	this ostensibly shows that convex losses are not sln-robust .	0	6	2	2.9439769	-2.718661	0
2-9-36	experiments confirm the unhinged loss ' sln-robustness is borne out in practice .	this ostensibly shows that convex losses are not sln-robust .	0	7	2	4.4723577	-4.012656	0
2-9-36	so , with apologies to wilde , while the truth is rarely pure , it can be simple .	this ostensibly shows that convex losses are not sln-robust .	0	8	2	1.9359188	-1.8831804	0
2-9-36	in this paper , we propose a convex , classification-calibrated loss and prove that it is sln-robust .	the loss avoids the long and servedio result by virtue of being negatively unbounded .	1	3	4	-2.4907072	2.5159962	1
2-9-36	the loss is a modification of the hinge loss , where one does not clamp at zero ; hence , we call it the unhinged loss .	in this paper , we propose a convex , classification-calibrated loss and prove that it is sln-robust .	0	5	3	3.554288	-3.2430792	0
2-9-36	we show that the optimal unhinged solution is equivalent to that of a strongly regularised svm , and is the limiting solution for any convex potential ; this implies that strong 2 regularisation makes most standard learners sln-robust .	in this paper , we propose a convex , classification-calibrated loss and prove that it is sln-robust .	0	6	3	4.4079185	-3.9239466	0
2-9-36	experiments confirm the unhinged loss ' sln-robustness is borne out in practice .	in this paper , we propose a convex , classification-calibrated loss and prove that it is sln-robust .	0	7	3	5.4046817	-4.7837753	0
2-9-36	in this paper , we propose a convex , classification-calibrated loss and prove that it is sln-robust .	so , with apologies to wilde , while the truth is rarely pure , it can be simple .	1	3	8	0.9364092	-0.78643	0
2-9-36	the loss avoids the long and servedio result by virtue of being negatively unbounded .	the loss is a modification of the hinge loss , where one does not clamp at zero ; hence , we call it the unhinged loss .	1	4	5	1.81842	-1.728671	0
2-9-36	we show that the optimal unhinged solution is equivalent to that of a strongly regularised svm , and is the limiting solution for any convex potential ; this implies that strong 2 regularisation makes most standard learners sln-robust .	the loss avoids the long and servedio result by virtue of being negatively unbounded .	0	6	4	3.6204696	-3.4257429	0
2-9-36	the loss avoids the long and servedio result by virtue of being negatively unbounded .	experiments confirm the unhinged loss ' sln-robustness is borne out in practice .	1	4	7	-5.962473	5.1113873	1
2-9-36	so , with apologies to wilde , while the truth is rarely pure , it can be simple .	the loss avoids the long and servedio result by virtue of being negatively unbounded .	0	8	4	-0.19529888	0.2541925	1
2-9-36	we show that the optimal unhinged solution is equivalent to that of a strongly regularised svm , and is the limiting solution for any convex potential ; this implies that strong 2 regularisation makes most standard learners sln-robust .	the loss is a modification of the hinge loss , where one does not clamp at zero ; hence , we call it the unhinged loss .	0	6	5	4.5085773	-4.1634045	0
2-9-36	the loss is a modification of the hinge loss , where one does not clamp at zero ; hence , we call it the unhinged loss .	experiments confirm the unhinged loss ' sln-robustness is borne out in practice .	1	5	7	-5.9651318	5.0435104	1
2-9-36	so , with apologies to wilde , while the truth is rarely pure , it can be simple .	the loss is a modification of the hinge loss , where one does not clamp at zero ; hence , we call it the unhinged loss .	0	8	5	0.35611162	-0.3446533	0
2-9-36	experiments confirm the unhinged loss ' sln-robustness is borne out in practice .	we show that the optimal unhinged solution is equivalent to that of a strongly regularised svm , and is the limiting solution for any convex potential ; this implies that strong 2 regularisation makes most standard learners sln-robust .	0	7	6	4.669176	-4.271492	0
2-9-36	so , with apologies to wilde , while the truth is rarely pure , it can be simple .	we show that the optimal unhinged solution is equivalent to that of a strongly regularised svm , and is the limiting solution for any convex potential ; this implies that strong 2 regularisation makes most standard learners sln-robust .	0	8	6	-1.8720665	1.9813223	1
2-9-36	so , with apologies to wilde , while the truth is rarely pure , it can be simple .	experiments confirm the unhinged loss ' sln-robustness is borne out in practice .	0	8	7	-5.8956294	5.146702	1
3-7-21	one of the central questions in statistical learning theory is to determine the conditions under which agents can learn from experience .	this includes the necessary and sufficient conditions for generalization from a given finite training set to new observations .	1	0	1	-5.8324375	5.222742	1
3-7-21	one of the central questions in statistical learning theory is to determine the conditions under which agents can learn from experience .	in this paper , we prove that algorithmic stability in the inference process is equivalent to uniform generalization across all parametric loss functions .	1	0	2	-5.937873	5.2062807	1
3-7-21	we provide various interpretations of this result .	one of the central questions in statistical learning theory is to determine the conditions under which agents can learn from experience .	0	3	0	5.658414	-5.0865445	0
3-7-21	one of the central questions in statistical learning theory is to determine the conditions under which agents can learn from experience .	for instance , a relationship is proved between stability and data processing , which reveals that algorithmic stability can be improved by post-processing the inferred hypothesis or by augmenting training examples with artificial noise prior to learning .	1	0	4	-5.9255753	5.150942	1
3-7-21	in addition , we establish a relationship between algorithmic stability and the size of the observation space , which provides a formal justification for dimensionality reduction methods .	one of the central questions in statistical learning theory is to determine the conditions under which agents can learn from experience .	0	5	0	5.6396656	-5.028153	0
3-7-21	one of the central questions in statistical learning theory is to determine the conditions under which agents can learn from experience .	finally , we connect algorithmic stability to the size of the hypothesis space , which recovers the classical pac result that the size ( complexity ) of the hypothesis space should be controlled in order to improve algorithmic stability and improve generalization .	1	0	6	-5.911437	5.1492977	1
3-7-21	in this paper , we prove that algorithmic stability in the inference process is equivalent to uniform generalization across all parametric loss functions .	this includes the necessary and sufficient conditions for generalization from a given finite training set to new observations .	0	2	1	-2.0733304	2.2187111	1
3-7-21	this includes the necessary and sufficient conditions for generalization from a given finite training set to new observations .	we provide various interpretations of this result .	1	1	3	-2.882843	2.9784174	1
3-7-21	for instance , a relationship is proved between stability and data processing , which reveals that algorithmic stability can be improved by post-processing the inferred hypothesis or by augmenting training examples with artificial noise prior to learning .	this includes the necessary and sufficient conditions for generalization from a given finite training set to new observations .	0	4	1	4.676497	-4.2811956	0
3-7-21	in addition , we establish a relationship between algorithmic stability and the size of the observation space , which provides a formal justification for dimensionality reduction methods .	this includes the necessary and sufficient conditions for generalization from a given finite training set to new observations .	0	5	1	4.5899663	-4.1566453	0
3-7-21	this includes the necessary and sufficient conditions for generalization from a given finite training set to new observations .	finally , we connect algorithmic stability to the size of the hypothesis space , which recovers the classical pac result that the size ( complexity ) of the hypothesis space should be controlled in order to improve algorithmic stability and improve generalization .	1	1	6	-5.9847765	5.1749496	1
3-7-21	we provide various interpretations of this result .	in this paper , we prove that algorithmic stability in the inference process is equivalent to uniform generalization across all parametric loss functions .	0	3	2	4.3798246	-3.922632	0
3-7-21	for instance , a relationship is proved between stability and data processing , which reveals that algorithmic stability can be improved by post-processing the inferred hypothesis or by augmenting training examples with artificial noise prior to learning .	in this paper , we prove that algorithmic stability in the inference process is equivalent to uniform generalization across all parametric loss functions .	0	4	2	5.027878	-4.4727564	0
3-7-21	in addition , we establish a relationship between algorithmic stability and the size of the observation space , which provides a formal justification for dimensionality reduction methods .	in this paper , we prove that algorithmic stability in the inference process is equivalent to uniform generalization across all parametric loss functions .	0	5	2	5.1829033	-4.51681	0
3-7-21	finally , we connect algorithmic stability to the size of the hypothesis space , which recovers the classical pac result that the size ( complexity ) of the hypothesis space should be controlled in order to improve algorithmic stability and improve generalization .	in this paper , we prove that algorithmic stability in the inference process is equivalent to uniform generalization across all parametric loss functions .	0	6	2	5.3982697	-4.741397	0
3-7-21	we provide various interpretations of this result .	for instance , a relationship is proved between stability and data processing , which reveals that algorithmic stability can be improved by post-processing the inferred hypothesis or by augmenting training examples with artificial noise prior to learning .	1	3	4	-5.5622373	5.05505	1
3-7-21	in addition , we establish a relationship between algorithmic stability and the size of the observation space , which provides a formal justification for dimensionality reduction methods .	we provide various interpretations of this result .	0	5	3	2.9651477	-2.721624	0
3-7-21	finally , we connect algorithmic stability to the size of the hypothesis space , which recovers the classical pac result that the size ( complexity ) of the hypothesis space should be controlled in order to improve algorithmic stability and improve generalization .	we provide various interpretations of this result .	0	6	3	3.8934875	-3.5035596	0
3-7-21	in addition , we establish a relationship between algorithmic stability and the size of the observation space , which provides a formal justification for dimensionality reduction methods .	for instance , a relationship is proved between stability and data processing , which reveals that algorithmic stability can be improved by post-processing the inferred hypothesis or by augmenting training examples with artificial noise prior to learning .	0	5	4	-3.0846634	3.0407667	1
3-7-21	finally , we connect algorithmic stability to the size of the hypothesis space , which recovers the classical pac result that the size ( complexity ) of the hypothesis space should be controlled in order to improve algorithmic stability and improve generalization .	for instance , a relationship is proved between stability and data processing , which reveals that algorithmic stability can be improved by post-processing the inferred hypothesis or by augmenting training examples with artificial noise prior to learning .	0	6	4	3.3643217	-3.1434436	0
3-7-21	in addition , we establish a relationship between algorithmic stability and the size of the observation space , which provides a formal justification for dimensionality reduction methods .	finally , we connect algorithmic stability to the size of the hypothesis space , which recovers the classical pac result that the size ( complexity ) of the hypothesis space should be controlled in order to improve algorithmic stability and improve generalization .	1	5	6	-4.309733	4.0394187	1
4-5-10	we present an easily computable , closed form parametric expression for the conditional likelihood , in which hyperparameters are recursively updated as a function of the streaming data assuming conjugate priors .	we develop a sequential low-complexity inference procedure for dirichlet process mixtures of gaussians for online clustering and parameter estimation when the number of clusters are unknown a-priori .	0	1	0	4.647254	-4.101301	0
4-5-10	we develop a sequential low-complexity inference procedure for dirichlet process mixtures of gaussians for online clustering and parameter estimation when the number of clusters are unknown a-priori .	motivated by large-sample asymptotics , we propose a novel adaptive low-complexity design for the dirichlet process concentration parameter and show that the number of classes grow at most at a logarithmic rate .	1	0	2	-5.8003626	5.1945267	1
4-5-10	we develop a sequential low-complexity inference procedure for dirichlet process mixtures of gaussians for online clustering and parameter estimation when the number of clusters are unknown a-priori .	we further prove that in the large-sample limit , the conditional likelihood and data predictive distribution become asymptotically gaussian .	1	0	3	-5.9240637	5.2247057	1
4-5-10	we develop a sequential low-complexity inference procedure for dirichlet process mixtures of gaussians for online clustering and parameter estimation when the number of clusters are unknown a-priori .	we demonstrate through experiments on synthetic and real data sets that our approach is superior to other online state-of-the-art methods .	1	0	4	-6.009865	5.227561	1
4-5-10	we present an easily computable , closed form parametric expression for the conditional likelihood , in which hyperparameters are recursively updated as a function of the streaming data assuming conjugate priors .	motivated by large-sample asymptotics , we propose a novel adaptive low-complexity design for the dirichlet process concentration parameter and show that the number of classes grow at most at a logarithmic rate .	1	1	2	-0.8149199	1.0307987	1
4-5-10	we further prove that in the large-sample limit , the conditional likelihood and data predictive distribution become asymptotically gaussian .	we present an easily computable , closed form parametric expression for the conditional likelihood , in which hyperparameters are recursively updated as a function of the streaming data assuming conjugate priors .	0	3	1	3.50669	-3.2516053	0
4-5-10	we demonstrate through experiments on synthetic and real data sets that our approach is superior to other online state-of-the-art methods .	we present an easily computable , closed form parametric expression for the conditional likelihood , in which hyperparameters are recursively updated as a function of the streaming data assuming conjugate priors .	0	4	1	5.473914	-4.854418	0
4-5-10	motivated by large-sample asymptotics , we propose a novel adaptive low-complexity design for the dirichlet process concentration parameter and show that the number of classes grow at most at a logarithmic rate .	we further prove that in the large-sample limit , the conditional likelihood and data predictive distribution become asymptotically gaussian .	1	2	3	-1.7345589	1.9224664	1
4-5-10	we demonstrate through experiments on synthetic and real data sets that our approach is superior to other online state-of-the-art methods .	motivated by large-sample asymptotics , we propose a novel adaptive low-complexity design for the dirichlet process concentration parameter and show that the number of classes grow at most at a logarithmic rate .	0	4	2	5.4717216	-4.8494315	0
4-5-10	we further prove that in the large-sample limit , the conditional likelihood and data predictive distribution become asymptotically gaussian .	we demonstrate through experiments on synthetic and real data sets that our approach is superior to other online state-of-the-art methods .	1	3	4	-5.6242104	4.98519	1
5-7-21	the markov chain monte carlo procedures that are used are often discrete-time analogues of associated stochastic differential equations ( sdes ) .	monte carlo sampling for bayesian posterior inference is a common approach used in machine learning .	0	1	0	5.343622	-4.80171	0
5-7-21	these sdes are guaranteed to leave invariant the required posterior distribution .	monte carlo sampling for bayesian posterior inference is a common approach used in machine learning .	0	2	0	5.642563	-5.016318	0
5-7-21	an area of current research addresses the computational benefits of stochastic gradient methods in this setting .	monte carlo sampling for bayesian posterior inference is a common approach used in machine learning .	0	3	0	5.4700527	-4.8807254	0
5-7-21	existing techniques rely on estimating the variance or covariance of the subsampling error , and typically assume constant variance .	monte carlo sampling for bayesian posterior inference is a common approach used in machine learning .	0	4	0	5.6030555	-5.018131	0
5-7-21	in this article , we propose a covariance-controlled adaptive langevin thermostat that can effectively dissipate parameter-dependent noise while maintaining a desired target distribution .	monte carlo sampling for bayesian posterior inference is a common approach used in machine learning .	0	5	0	5.645316	-5.0502396	0
5-7-21	monte carlo sampling for bayesian posterior inference is a common approach used in machine learning .	the proposed method achieves a substantial speedup over popular alternative schemes for large-scale machine learning applications .	1	0	6	-5.977812	5.120648	1
5-7-21	these sdes are guaranteed to leave invariant the required posterior distribution .	the markov chain monte carlo procedures that are used are often discrete-time analogues of associated stochastic differential equations ( sdes ) .	0	2	1	5.6685796	-5.019613	0
5-7-21	the markov chain monte carlo procedures that are used are often discrete-time analogues of associated stochastic differential equations ( sdes ) .	an area of current research addresses the computational benefits of stochastic gradient methods in this setting .	1	1	3	-2.931759	3.0049052	1
5-7-21	the markov chain monte carlo procedures that are used are often discrete-time analogues of associated stochastic differential equations ( sdes ) .	existing techniques rely on estimating the variance or covariance of the subsampling error , and typically assume constant variance .	1	1	4	-3.4126391	3.3090277	1
5-7-21	the markov chain monte carlo procedures that are used are often discrete-time analogues of associated stochastic differential equations ( sdes ) .	in this article , we propose a covariance-controlled adaptive langevin thermostat that can effectively dissipate parameter-dependent noise while maintaining a desired target distribution .	1	1	5	-5.94791	5.2124233	1
5-7-21	the proposed method achieves a substantial speedup over popular alternative schemes for large-scale machine learning applications .	the markov chain monte carlo procedures that are used are often discrete-time analogues of associated stochastic differential equations ( sdes ) .	0	6	1	5.556709	-4.9536724	0
5-7-21	an area of current research addresses the computational benefits of stochastic gradient methods in this setting .	these sdes are guaranteed to leave invariant the required posterior distribution .	0	3	2	-5.410677	5.007178	1
5-7-21	these sdes are guaranteed to leave invariant the required posterior distribution .	existing techniques rely on estimating the variance or covariance of the subsampling error , and typically assume constant variance .	1	2	4	4.9931746	-4.423929	0
5-7-21	these sdes are guaranteed to leave invariant the required posterior distribution .	in this article , we propose a covariance-controlled adaptive langevin thermostat that can effectively dissipate parameter-dependent noise while maintaining a desired target distribution .	1	2	5	1.6441699	-1.3563385	0
5-7-21	these sdes are guaranteed to leave invariant the required posterior distribution .	the proposed method achieves a substantial speedup over popular alternative schemes for large-scale machine learning applications .	1	2	6	-5.935652	5.0023637	1
5-7-21	an area of current research addresses the computational benefits of stochastic gradient methods in this setting .	existing techniques rely on estimating the variance or covariance of the subsampling error , and typically assume constant variance .	1	3	4	-2.438806	2.5257359	1
5-7-21	in this article , we propose a covariance-controlled adaptive langevin thermostat that can effectively dissipate parameter-dependent noise while maintaining a desired target distribution .	an area of current research addresses the computational benefits of stochastic gradient methods in this setting .	0	5	3	4.7513456	-4.3151116	0
5-7-21	the proposed method achieves a substantial speedup over popular alternative schemes for large-scale machine learning applications .	an area of current research addresses the computational benefits of stochastic gradient methods in this setting .	0	6	3	5.4764457	-4.9414625	0
5-7-21	in this article , we propose a covariance-controlled adaptive langevin thermostat that can effectively dissipate parameter-dependent noise while maintaining a desired target distribution .	existing techniques rely on estimating the variance or covariance of the subsampling error , and typically assume constant variance .	0	5	4	5.5277886	-4.904401	0
5-7-21	the proposed method achieves a substantial speedup over popular alternative schemes for large-scale machine learning applications .	existing techniques rely on estimating the variance or covariance of the subsampling error , and typically assume constant variance .	0	6	4	5.6787252	-5.0773754	0
5-7-21	in this article , we propose a covariance-controlled adaptive langevin thermostat that can effectively dissipate parameter-dependent noise while maintaining a desired target distribution .	the proposed method achieves a substantial speedup over popular alternative schemes for large-scale machine learning applications .	1	5	6	-6.0097528	5.1794014	1
6-7-21	the proposed method is robust to extreme events in asset returns , and accommodates large portfolios under limited historical data .	we propose a robust portfolio optimization approach based on quantile statistics .	0	1	0	5.200731	-4.5975995	0
6-7-21	we propose a robust portfolio optimization approach based on quantile statistics .	specifically , we show that the risk of the estimated portfolio converges to the oracle optimal risk with parametric rate under weakly dependent asset returns .	1	0	2	-5.8710794	5.1623874	1
6-7-21	we propose a robust portfolio optimization approach based on quantile statistics .	the theory does not rely on higher order moment assumptions , thus allowing for heavy-tailed asset returns .	1	0	3	-5.6283274	5.093849	1
6-7-21	moreover , the rate of convergence quantifies that the size of the portfolio under management is allowed to scale exponentially with the sample size of the historical data .	we propose a robust portfolio optimization approach based on quantile statistics .	0	4	0	5.3462815	-4.72921	0
6-7-21	we propose a robust portfolio optimization approach based on quantile statistics .	the empirical effectiveness of the proposed method is demonstrated under both synthetic and real stock data .	1	0	5	-5.989353	5.118844	1
6-7-21	our work extends existing ones by achieving robustness in high dimensions , and by allowing serial dependence .	we propose a robust portfolio optimization approach based on quantile statistics .	0	6	0	4.2068567	-3.7658272	0
6-7-21	the proposed method is robust to extreme events in asset returns , and accommodates large portfolios under limited historical data .	specifically , we show that the risk of the estimated portfolio converges to the oracle optimal risk with parametric rate under weakly dependent asset returns .	1	1	2	-4.221114	3.9408526	1
6-7-21	the theory does not rely on higher order moment assumptions , thus allowing for heavy-tailed asset returns .	the proposed method is robust to extreme events in asset returns , and accommodates large portfolios under limited historical data .	0	3	1	-1.4116253	1.6186574	1
6-7-21	moreover , the rate of convergence quantifies that the size of the portfolio under management is allowed to scale exponentially with the sample size of the historical data .	the proposed method is robust to extreme events in asset returns , and accommodates large portfolios under limited historical data .	0	4	1	4.1365523	-3.8169408	0
6-7-21	the proposed method is robust to extreme events in asset returns , and accommodates large portfolios under limited historical data .	the empirical effectiveness of the proposed method is demonstrated under both synthetic and real stock data .	1	1	5	-5.7749486	5.1553774	1
6-7-21	the proposed method is robust to extreme events in asset returns , and accommodates large portfolios under limited historical data .	our work extends existing ones by achieving robustness in high dimensions , and by allowing serial dependence .	1	1	6	-1.2253964	1.3176222	1
6-7-21	the theory does not rely on higher order moment assumptions , thus allowing for heavy-tailed asset returns .	specifically , we show that the risk of the estimated portfolio converges to the oracle optimal risk with parametric rate under weakly dependent asset returns .	0	3	2	-0.24108939	0.49393097	1
6-7-21	specifically , we show that the risk of the estimated portfolio converges to the oracle optimal risk with parametric rate under weakly dependent asset returns .	moreover , the rate of convergence quantifies that the size of the portfolio under management is allowed to scale exponentially with the sample size of the historical data .	1	2	4	-3.954283	3.7366056	1
6-7-21	the empirical effectiveness of the proposed method is demonstrated under both synthetic and real stock data .	specifically , we show that the risk of the estimated portfolio converges to the oracle optimal risk with parametric rate under weakly dependent asset returns .	0	5	2	4.994257	-4.490447	0
6-7-21	specifically , we show that the risk of the estimated portfolio converges to the oracle optimal risk with parametric rate under weakly dependent asset returns .	our work extends existing ones by achieving robustness in high dimensions , and by allowing serial dependence .	1	2	6	3.2996469	-3.0620868	0
6-7-21	moreover , the rate of convergence quantifies that the size of the portfolio under management is allowed to scale exponentially with the sample size of the historical data .	the theory does not rely on higher order moment assumptions , thus allowing for heavy-tailed asset returns .	0	4	3	2.3051505	-2.1721084	0
6-7-21	the theory does not rely on higher order moment assumptions , thus allowing for heavy-tailed asset returns .	the empirical effectiveness of the proposed method is demonstrated under both synthetic and real stock data .	1	3	5	-5.798856	5.0910864	1
6-7-21	our work extends existing ones by achieving robustness in high dimensions , and by allowing serial dependence .	the theory does not rely on higher order moment assumptions , thus allowing for heavy-tailed asset returns .	0	6	3	-1.9159393	2.045651	1
6-7-21	the empirical effectiveness of the proposed method is demonstrated under both synthetic and real stock data .	moreover , the rate of convergence quantifies that the size of the portfolio under management is allowed to scale exponentially with the sample size of the historical data .	0	5	4	4.820241	-4.4328527	0
6-7-21	moreover , the rate of convergence quantifies that the size of the portfolio under management is allowed to scale exponentially with the sample size of the historical data .	our work extends existing ones by achieving robustness in high dimensions , and by allowing serial dependence .	1	4	6	2.9794903	-2.7591763	0
6-7-21	the empirical effectiveness of the proposed method is demonstrated under both synthetic and real stock data .	our work extends existing ones by achieving robustness in high dimensions , and by allowing serial dependence .	1	5	6	5.076058	-4.4988804	0
7-7-21	we study the problem of multiclass classification with an extremely large number of classes ( k ) , with the goal of obtaining train and test time complexity logarithmic in the number of classes .	we develop top-down tree construction approaches for constructing logarithmic depth trees .	1	0	1	-5.7054014	5.2040524	1
7-7-21	on the theoretical front , we formulate a new objective function , which is optimized at each node of the tree and creates dynamic partitions of the data which are both pure ( in terms of class labels ) and balanced .	we study the problem of multiclass classification with an extremely large number of classes ( k ) , with the goal of obtaining train and test time complexity logarithmic in the number of classes .	0	2	0	5.582434	-5.010638	0
7-7-21	we demonstrate that under favorable conditions , we can construct logarithmic depth trees that have leaves with low label entropy .	we study the problem of multiclass classification with an extremely large number of classes ( k ) , with the goal of obtaining train and test time complexity logarithmic in the number of classes .	0	3	0	5.6730766	-5.100781	0
7-7-21	we study the problem of multiclass classification with an extremely large number of classes ( k ) , with the goal of obtaining train and test time complexity logarithmic in the number of classes .	however , the objective function at the nodes is challenging to optimize computationally .	1	0	4	-5.7342463	5.136797	1
7-7-21	we study the problem of multiclass classification with an extremely large number of classes ( k ) , with the goal of obtaining train and test time complexity logarithmic in the number of classes .	we address the empirical problem with a new online decision tree construction procedure .	1	0	5	-5.911005	5.200223	1
7-7-21	experiments demonstrate that this online algorithm quickly achieves improvement in test error compared to more common logarithmic training time approaches , which makes it a plausible method in computationally constrained large-k applications .	we study the problem of multiclass classification with an extremely large number of classes ( k ) , with the goal of obtaining train and test time complexity logarithmic in the number of classes .	0	6	0	5.6359177	-5.050959	0
7-7-21	we develop top-down tree construction approaches for constructing logarithmic depth trees .	on the theoretical front , we formulate a new objective function , which is optimized at each node of the tree and creates dynamic partitions of the data which are both pure ( in terms of class labels ) and balanced .	1	1	2	-5.9907	5.2261763	1
7-7-21	we demonstrate that under favorable conditions , we can construct logarithmic depth trees that have leaves with low label entropy .	we develop top-down tree construction approaches for constructing logarithmic depth trees .	0	3	1	4.835133	-4.228459	0
7-7-21	we develop top-down tree construction approaches for constructing logarithmic depth trees .	however , the objective function at the nodes is challenging to optimize computationally .	1	1	4	3.1804278	-3.0296717	0
7-7-21	we develop top-down tree construction approaches for constructing logarithmic depth trees .	we address the empirical problem with a new online decision tree construction procedure .	1	1	5	-5.866964	4.9630165	1
7-7-21	we develop top-down tree construction approaches for constructing logarithmic depth trees .	experiments demonstrate that this online algorithm quickly achieves improvement in test error compared to more common logarithmic training time approaches , which makes it a plausible method in computationally constrained large-k applications .	1	1	6	-6.0111747	5.153915	1
7-7-21	on the theoretical front , we formulate a new objective function , which is optimized at each node of the tree and creates dynamic partitions of the data which are both pure ( in terms of class labels ) and balanced .	we demonstrate that under favorable conditions , we can construct logarithmic depth trees that have leaves with low label entropy .	1	2	3	2.341784	-2.088592	0
7-7-21	however , the objective function at the nodes is challenging to optimize computationally .	on the theoretical front , we formulate a new objective function , which is optimized at each node of the tree and creates dynamic partitions of the data which are both pure ( in terms of class labels ) and balanced .	0	4	2	-5.5019393	5.1018267	1
7-7-21	on the theoretical front , we formulate a new objective function , which is optimized at each node of the tree and creates dynamic partitions of the data which are both pure ( in terms of class labels ) and balanced .	we address the empirical problem with a new online decision tree construction procedure .	1	2	5	-0.4610389	0.8659677	1
7-7-21	on the theoretical front , we formulate a new objective function , which is optimized at each node of the tree and creates dynamic partitions of the data which are both pure ( in terms of class labels ) and balanced .	experiments demonstrate that this online algorithm quickly achieves improvement in test error compared to more common logarithmic training time approaches , which makes it a plausible method in computationally constrained large-k applications .	1	2	6	-5.9754796	5.124086	1
7-7-21	we demonstrate that under favorable conditions , we can construct logarithmic depth trees that have leaves with low label entropy .	however , the objective function at the nodes is challenging to optimize computationally .	1	3	4	5.1545267	-4.594495	0
7-7-21	we demonstrate that under favorable conditions , we can construct logarithmic depth trees that have leaves with low label entropy .	we address the empirical problem with a new online decision tree construction procedure .	1	3	5	-0.37825996	0.78444165	1
7-7-21	experiments demonstrate that this online algorithm quickly achieves improvement in test error compared to more common logarithmic training time approaches , which makes it a plausible method in computationally constrained large-k applications .	we demonstrate that under favorable conditions , we can construct logarithmic depth trees that have leaves with low label entropy .	0	6	3	5.1311064	-4.5531206	0
7-7-21	we address the empirical problem with a new online decision tree construction procedure .	however , the objective function at the nodes is challenging to optimize computationally .	0	5	4	5.162691	-4.49582	0
7-7-21	experiments demonstrate that this online algorithm quickly achieves improvement in test error compared to more common logarithmic training time approaches , which makes it a plausible method in computationally constrained large-k applications .	however , the objective function at the nodes is challenging to optimize computationally .	0	6	4	5.641732	-4.9805584	0
7-7-21	experiments demonstrate that this online algorithm quickly achieves improvement in test error compared to more common logarithmic training time approaches , which makes it a plausible method in computationally constrained large-k applications .	we address the empirical problem with a new online decision tree construction procedure .	0	6	5	5.5448117	-4.9166	0
8-3-3	we formulate this in terms of finding the closest ultrametric to a specified set of distances and solve it using an lp relaxation that leverages minimum cost perfect matching as a subroutine to efficiently explore the space of planar partitions .	we study the problem of hierarchical clustering on planar graphs .	0	1	0	5.5334735	-4.977564	0
8-3-3	we study the problem of hierarchical clustering on planar graphs .	we apply our algorithm to the problem of hierarchical image segmentation .	1	0	2	-5.9634075	5.110471	1
8-3-3	we apply our algorithm to the problem of hierarchical image segmentation .	we formulate this in terms of finding the closest ultrametric to a specified set of distances and solve it using an lp relaxation that leverages minimum cost perfect matching as a subroutine to efficiently explore the space of planar partitions .	0	2	1	4.9086504	-4.3540516	0
9-7-21	we propose an approach for retrieving a sequence of natural sentences for an image stream .	since general users often take a series of pictures on their special moments , it would better take into consideration of the whole image stream to produce natural language descriptions .	1	0	1	0.98626626	-0.76575994	0
9-7-21	while almost all previous studies have dealt with the relation between a single image and a single natural sentence , our work extends both input and output dimension to a sequence of images and a sequence of sentences .	we propose an approach for retrieving a sequence of natural sentences for an image stream .	0	2	0	1.5523436	-1.0850846	0
9-7-21	to this end , we design a multimodal architecture called coherence recurrent convolutional network ( crcn ) , which consists of convolutional neural networks , bidirectional recurrent neural networks , and an entity-based local coherence model .	we propose an approach for retrieving a sequence of natural sentences for an image stream .	0	3	0	4.6055365	-4.0842023	0
9-7-21	we propose an approach for retrieving a sequence of natural sentences for an image stream .	our approach directly learns from vast user-generated resource of blog posts as text-image parallel training data .	1	0	4	-5.931078	5.014366	1
9-7-21	we propose an approach for retrieving a sequence of natural sentences for an image stream .	we demonstrate that our approach outperforms other state-of-the-art candidate methods , using both quantitative measures ( e.g .	1	0	5	-6.0035872	5.1507754	1
9-7-21	bleu and top-k recall ) and user studies via amazon mechanical turk .	we propose an approach for retrieving a sequence of natural sentences for an image stream .	0	6	0	4.796587	-4.2783933	0
9-7-21	while almost all previous studies have dealt with the relation between a single image and a single natural sentence , our work extends both input and output dimension to a sequence of images and a sequence of sentences .	since general users often take a series of pictures on their special moments , it would better take into consideration of the whole image stream to produce natural language descriptions .	0	2	1	2.3428502	-2.0012684	0
9-7-21	since general users often take a series of pictures on their special moments , it would better take into consideration of the whole image stream to produce natural language descriptions .	to this end , we design a multimodal architecture called coherence recurrent convolutional network ( crcn ) , which consists of convolutional neural networks , bidirectional recurrent neural networks , and an entity-based local coherence model .	1	1	3	-5.868188	5.2157273	1
9-7-21	our approach directly learns from vast user-generated resource of blog posts as text-image parallel training data .	since general users often take a series of pictures on their special moments , it would better take into consideration of the whole image stream to produce natural language descriptions .	0	4	1	4.4575777	-4.045415	0
9-7-21	we demonstrate that our approach outperforms other state-of-the-art candidate methods , using both quantitative measures ( e.g .	since general users often take a series of pictures on their special moments , it would better take into consideration of the whole image stream to produce natural language descriptions .	0	5	1	5.498549	-4.945679	0
9-7-21	since general users often take a series of pictures on their special moments , it would better take into consideration of the whole image stream to produce natural language descriptions .	bleu and top-k recall ) and user studies via amazon mechanical turk .	1	1	6	-4.482657	4.2712793	1
9-7-21	[CLS] to this end, we design a multimodal architecture called coherence recurrent convolutional network ( crcn ), which consists of convolutional neural networks, bidirectional recurrent neural networks, and an entity - based local coherence	while almost all previous studies have dealt with the relation between a single image and a single natural sentence , our work extends both input and output dimension to a sequence of images and a sequence of sentences .	0	3	2	3.8473306	-3.4990933	0
9-7-21	our approach directly learns from vast user-generated resource of blog posts as text-image parallel training data .	while almost all previous studies have dealt with the relation between a single image and a single natural sentence , our work extends both input and output dimension to a sequence of images and a sequence of sentences .	0	4	2	4.227726	-3.7759485	0
9-7-21	while almost all previous studies have dealt with the relation between a single image and a single natural sentence , our work extends both input and output dimension to a sequence of images and a sequence of sentences .	we demonstrate that our approach outperforms other state-of-the-art candidate methods , using both quantitative measures ( e.g .	1	2	5	-6.008527	5.183756	1
9-7-21	while almost all previous studies have dealt with the relation between a single image and a single natural sentence , our work extends both input and output dimension to a sequence of images and a sequence of sentences .	bleu and top-k recall ) and user studies via amazon mechanical turk .	1	2	6	-4.923166	4.6099367	1
9-7-21	our approach directly learns from vast user-generated resource of blog posts as text-image parallel training data .	to this end , we design a multimodal architecture called coherence recurrent convolutional network ( crcn ) , which consists of convolutional neural networks , bidirectional recurrent neural networks , and an entity-based local coherence model .	0	4	3	2.2459986	-1.9436421	0
9-7-21	we demonstrate that our approach outperforms other state-of-the-art candidate methods , using both quantitative measures ( e.g .	to this end , we design a multimodal architecture called coherence recurrent convolutional network ( crcn ) , which consists of convolutional neural networks , bidirectional recurrent neural networks , and an entity-based local coherence model .	0	5	3	5.169345	-4.575241	0
9-7-21	to this end , we design a multimodal architecture called coherence recurrent convolutional network ( crcn ) , which consists of convolutional neural networks , bidirectional recurrent neural networks , and an entity-based local coherence model .	bleu and top-k recall ) and user studies via amazon mechanical turk .	1	3	6	-2.3696775	2.445972	1
9-7-21	our approach directly learns from vast user-generated resource of blog posts as text-image parallel training data .	we demonstrate that our approach outperforms other state-of-the-art candidate methods , using both quantitative measures ( e.g .	1	4	5	-5.6384315	5.1023045	1
9-7-21	bleu and top-k recall ) and user studies via amazon mechanical turk .	our approach directly learns from vast user-generated resource of blog posts as text-image parallel training data .	0	6	4	-1.8892612	2.1141644	1
9-7-21	we demonstrate that our approach outperforms other state-of-the-art candidate methods , using both quantitative measures ( e.g .	bleu and top-k recall ) and user studies via amazon mechanical turk .	1	5	6	-1.6021849	1.7570649	1
10-9-36	one of the most popular cc algorithms is kwikcluster : an algorithm that serially clusters neighborhoods of vertices , and obtains a 3-approximation ratio .	given a similarity graph between items , correlation clustering ( cc ) groups similar items together and dissimilar ones apart .	0	1	0	5.695332	-5.084323	0
10-9-36	given a similarity graph between items , correlation clustering ( cc ) groups similar items together and dissimilar ones apart .	unfortunately , in practice kwikcluster requires a large number of clustering rounds , a potential bottleneck for large graphs .	1	0	2	-5.138912	4.6683855	1
10-9-36	we present c4 and clusterwild ! , two algorithms for parallel correlation clustering that run in a polylogarithmic number of rounds , and provably achieve nearly linear speedups .	given a similarity graph between items , correlation clustering ( cc ) groups similar items together and dissimilar ones apart .	0	3	0	5.5206614	-4.8675833	0
10-9-36	c4 uses concurrency control to enforce serializability of a parallel clustering process , and guarantees a 3-approximation ratio .	given a similarity graph between items , correlation clustering ( cc ) groups similar items together and dissimilar ones apart .	0	4	0	5.641122	-5.0173035	0
10-9-36	given a similarity graph between items , correlation clustering ( cc ) groups similar items together and dissimilar ones apart .	clusterwild !	1	0	5	-5.578448	4.987873	1
10-9-36	given a similarity graph between items , correlation clustering ( cc ) groups similar items together and dissimilar ones apart .	is a coordination free algorithm that abandons consistency for the benefit of better scaling ; this leads to a provably small loss in the 3 approximation ratio .	1	0	6	-5.949179	5.229639	1
10-9-36	we demonstrate experimentally that both algorithms outperform the state of the art , both in terms of clustering accuracy and running time .	given a similarity graph between items , correlation clustering ( cc ) groups similar items together and dissimilar ones apart .	0	7	0	5.596677	-5.0012965	0
10-9-36	we show that our algorithms can cluster billion-edge graphs in under 5 seconds on 32 cores , while achieving a 15 speedup .	given a similarity graph between items , correlation clustering ( cc ) groups similar items together and dissimilar ones apart .	0	8	0	5.6034484	-4.9944415	0
10-9-36	unfortunately , in practice kwikcluster requires a large number of clustering rounds , a potential bottleneck for large graphs .	one of the most popular cc algorithms is kwikcluster : an algorithm that serially clusters neighborhoods of vertices , and obtains a 3-approximation ratio .	0	2	1	5.042924	-4.421169	0
10-9-36	one of the most popular cc algorithms is kwikcluster : an algorithm that serially clusters neighborhoods of vertices , and obtains a 3-approximation ratio .	we present c4 and clusterwild ! , two algorithms for parallel correlation clustering that run in a polylogarithmic number of rounds , and provably achieve nearly linear speedups .	1	1	3	-5.066279	4.747465	1
10-9-36	c4 uses concurrency control to enforce serializability of a parallel clustering process , and guarantees a 3-approximation ratio .	one of the most popular cc algorithms is kwikcluster : an algorithm that serially clusters neighborhoods of vertices , and obtains a 3-approximation ratio .	0	4	1	3.823642	-3.5133178	0
10-9-36	clusterwild !	one of the most popular cc algorithms is kwikcluster : an algorithm that serially clusters neighborhoods of vertices , and obtains a 3-approximation ratio .	0	5	1	0.5008965	-0.30602956	0
10-9-36	is a coordination free algorithm that abandons consistency for the benefit of better scaling ; this leads to a provably small loss in the 3 approximation ratio .	one of the most popular cc algorithms is kwikcluster : an algorithm that serially clusters neighborhoods of vertices , and obtains a 3-approximation ratio .	0	6	1	5.165004	-4.557052	0
10-9-36	one of the most popular cc algorithms is kwikcluster : an algorithm that serially clusters neighborhoods of vertices , and obtains a 3-approximation ratio .	we demonstrate experimentally that both algorithms outperform the state of the art , both in terms of clustering accuracy and running time .	1	1	7	-6.002093	5.206627	1
10-9-36	we show that our algorithms can cluster billion-edge graphs in under 5 seconds on 32 cores , while achieving a 15 speedup .	one of the most popular cc algorithms is kwikcluster : an algorithm that serially clusters neighborhoods of vertices , and obtains a 3-approximation ratio .	0	8	1	5.474952	-4.8278565	0
10-9-36	unfortunately , in practice kwikcluster requires a large number of clustering rounds , a potential bottleneck for large graphs .	we present c4 and clusterwild ! , two algorithms for parallel correlation clustering that run in a polylogarithmic number of rounds , and provably achieve nearly linear speedups .	1	2	3	-5.9501357	5.248811	1
10-9-36	c4 uses concurrency control to enforce serializability of a parallel clustering process , and guarantees a 3-approximation ratio .	unfortunately , in practice kwikcluster requires a large number of clustering rounds , a potential bottleneck for large graphs .	0	4	2	5.3618736	-4.790489	0
10-9-36	clusterwild !	unfortunately , in practice kwikcluster requires a large number of clustering rounds , a potential bottleneck for large graphs .	0	5	2	1.7124995	-1.6752356	0
10-9-36	is a coordination free algorithm that abandons consistency for the benefit of better scaling ; this leads to a provably small loss in the 3 approximation ratio .	unfortunately , in practice kwikcluster requires a large number of clustering rounds , a potential bottleneck for large graphs .	0	6	2	4.4134383	-3.9960032	0
10-9-36	we demonstrate experimentally that both algorithms outperform the state of the art , both in terms of clustering accuracy and running time .	unfortunately , in practice kwikcluster requires a large number of clustering rounds , a potential bottleneck for large graphs .	0	7	2	5.615983	-5.0131464	0
10-9-36	unfortunately , in practice kwikcluster requires a large number of clustering rounds , a potential bottleneck for large graphs .	we show that our algorithms can cluster billion-edge graphs in under 5 seconds on 32 cores , while achieving a 15 speedup .	1	2	8	-5.998225	5.171054	1
10-9-36	we present c4 and clusterwild ! , two algorithms for parallel correlation clustering that run in a polylogarithmic number of rounds , and provably achieve nearly linear speedups .	c4 uses concurrency control to enforce serializability of a parallel clustering process , and guarantees a 3-approximation ratio .	1	3	4	-2.8522773	2.7170603	1
10-9-36	clusterwild !	we present c4 and clusterwild ! , two algorithms for parallel correlation clustering that run in a polylogarithmic number of rounds , and provably achieve nearly linear speedups .	0	5	3	0.93512577	-0.59431785	0
10-9-36	we present c4 and clusterwild ! , two algorithms for parallel correlation clustering that run in a polylogarithmic number of rounds , and provably achieve nearly linear speedups .	is a coordination free algorithm that abandons consistency for the benefit of better scaling ; this leads to a provably small loss in the 3 approximation ratio .	1	3	6	-3.7646878	3.5464334	1
10-9-36	we present c4 and clusterwild ! , two algorithms for parallel correlation clustering that run in a polylogarithmic number of rounds , and provably achieve nearly linear speedups .	we demonstrate experimentally that both algorithms outperform the state of the art , both in terms of clustering accuracy and running time .	1	3	7	-5.9860353	5.2331686	1
10-9-36	we present c4 and clusterwild ! , two algorithms for parallel correlation clustering that run in a polylogarithmic number of rounds , and provably achieve nearly linear speedups .	we show that our algorithms can cluster billion-edge graphs in under 5 seconds on 32 cores , while achieving a 15 speedup .	1	3	8	-5.77108	5.198065	1
10-9-36	c4 uses concurrency control to enforce serializability of a parallel clustering process , and guarantees a 3-approximation ratio .	clusterwild !	1	4	5	2.9907634	-2.7157564	0
10-9-36	is a coordination free algorithm that abandons consistency for the benefit of better scaling ; this leads to a provably small loss in the 3 approximation ratio .	c4 uses concurrency control to enforce serializability of a parallel clustering process , and guarantees a 3-approximation ratio .	0	6	4	2.915845	-2.716972	0
10-9-36	we demonstrate experimentally that both algorithms outperform the state of the art , both in terms of clustering accuracy and running time .	c4 uses concurrency control to enforce serializability of a parallel clustering process , and guarantees a 3-approximation ratio .	0	7	4	5.065303	-4.584888	0
10-9-36	c4 uses concurrency control to enforce serializability of a parallel clustering process , and guarantees a 3-approximation ratio .	we show that our algorithms can cluster billion-edge graphs in under 5 seconds on 32 cores , while achieving a 15 speedup .	1	4	8	-4.695407	4.3603945	1
10-9-36	clusterwild !	is a coordination free algorithm that abandons consistency for the benefit of better scaling ; this leads to a provably small loss in the 3 approximation ratio .	1	5	6	-2.5238552	2.626364	1
10-9-36	clusterwild !	we demonstrate experimentally that both algorithms outperform the state of the art , both in terms of clustering accuracy and running time .	1	5	7	-4.647851	4.245865	1
10-9-36	we show that our algorithms can cluster billion-edge graphs in under 5 seconds on 32 cores , while achieving a 15 speedup .	clusterwild !	0	8	5	3.8175836	-3.4847784	0
10-9-36	is a coordination free algorithm that abandons consistency for the benefit of better scaling ; this leads to a provably small loss in the 3 approximation ratio .	we demonstrate experimentally that both algorithms outperform the state of the art , both in terms of clustering accuracy and running time .	1	6	7	-5.978055	5.136975	1
10-9-36	we show that our algorithms can cluster billion-edge graphs in under 5 seconds on 32 cores , while achieving a 15 speedup .	is a coordination free algorithm that abandons consistency for the benefit of better scaling ; this leads to a provably small loss in the 3 approximation ratio .	0	8	6	4.7234907	-4.223113	0
10-9-36	we demonstrate experimentally that both algorithms outperform the state of the art , both in terms of clustering accuracy and running time .	we show that our algorithms can cluster billion-edge graphs in under 5 seconds on 32 cores , while achieving a 15 speedup .	1	7	8	3.1305225	-2.9516015	0
11-8-28	state-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations .	advances like sppnet and fast r-cnn have reduced the running time of these detection networks , exposing region proposal computation as a bottleneck .	1	0	1	-4.1074524	3.8308253	1
11-8-28	in this work , we introduce a region proposal network ( rpn ) that shares full-image convolutional features with the detection network , thus enabling nearly cost-free region proposals .	state-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations .	0	2	0	5.267484	-4.6485786	0
11-8-28	an rpn is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position .	state-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations .	0	3	0	5.079455	-4.5225067	0
11-8-28	rpns are trained end-to-end to generate highquality region proposals , which are used by fast r-cnn for detection .	state-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations .	0	4	0	5.7024465	-5.0911264	0
11-8-28	state-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations .	with a simple alternating optimization , rpn and fast r-cnn can be trained to share convolutional features .	1	0	5	-5.926631	5.201643	1
11-8-28	for the very deep vgg-16 model , our detection system has a frame rate of 5fps ( including all steps ) on a gpu , while achieving state-of-the-art object detection accuracy on pascal voc 2007 ( 73.2 % map ) and 2012 ( 70.4 % map ) using 300 proposals per image .	state-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations .	0	6	0	5.6038165	-4.9577913	0
11-8-28	code is available at https : //github.com/shaoqingren/faster_rcnn .	state-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations .	0	7	0	5.549446	-4.962226	0
11-8-28	in this work , we introduce a region proposal network ( rpn ) that shares full-image convolutional features with the detection network , thus enabling nearly cost-free region proposals .	advances like sppnet and fast r-cnn have reduced the running time of these detection networks , exposing region proposal computation as a bottleneck .	0	2	1	5.2127633	-4.62216	0
11-8-28	advances like sppnet and fast r-cnn have reduced the running time of these detection networks , exposing region proposal computation as a bottleneck .	an rpn is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position .	1	1	3	-2.6445851	2.76822	1
11-8-28	advances like sppnet and fast r-cnn have reduced the running time of these detection networks , exposing region proposal computation as a bottleneck .	rpns are trained end-to-end to generate highquality region proposals , which are used by fast r-cnn for detection .	1	1	4	-5.890645	5.1733894	1
11-8-28	advances like sppnet and fast r-cnn have reduced the running time of these detection networks , exposing region proposal computation as a bottleneck .	with a simple alternating optimization , rpn and fast r-cnn can be trained to share convolutional features .	1	1	5	-5.9127417	5.1495705	1
11-8-28	[CLS] for the very deep vgg - 16 model, our detection system has a frame rate of 5fps ( including all steps ) on a gpu, while achieving state - of - the - art object detection accuracy on pascal voc 2007 ( 73. 2 % map ) and 2012 ( 70. 4 % map ) using 300 proposals per image	advances like sppnet and fast r-cnn have reduced the running time of these detection networks , exposing region proposal computation as a bottleneck .	0	6	1	5.665905	-4.9852266	0
11-8-28	advances like sppnet and fast r-cnn have reduced the running time of these detection networks , exposing region proposal computation as a bottleneck .	code is available at https : //github.com/shaoqingren/faster_rcnn .	1	1	7	-5.8771605	4.9070177	1
11-8-28	in this work , we introduce a region proposal network ( rpn ) that shares full-image convolutional features with the detection network , thus enabling nearly cost-free region proposals .	an rpn is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position .	1	2	3	-4.9239306	4.534622	1
11-8-28	in this work , we introduce a region proposal network ( rpn ) that shares full-image convolutional features with the detection network , thus enabling nearly cost-free region proposals .	rpns are trained end-to-end to generate highquality region proposals , which are used by fast r-cnn for detection .	1	2	4	-5.9080887	5.202895	1
11-8-28	in this work , we introduce a region proposal network ( rpn ) that shares full-image convolutional features with the detection network , thus enabling nearly cost-free region proposals .	with a simple alternating optimization , rpn and fast r-cnn can be trained to share convolutional features .	1	2	5	-5.8493605	5.194971	1
11-8-28	[CLS] for the very deep vgg - 16 model, our detection system has a frame rate of 5fps ( including all steps ) on a gpu, while achieving state - of - the - art object detection accuracy on pascal voc 2007 ( 73. 2 % map ) and 2012	in this work , we introduce a region proposal network ( rpn ) that shares full-image convolutional features with the detection network , thus enabling nearly cost-free region proposals .	0	6	2	5.0060697	-4.4418726	0
11-8-28	code is available at https : //github.com/shaoqingren/faster_rcnn .	in this work , we introduce a region proposal network ( rpn ) that shares full-image convolutional features with the detection network , thus enabling nearly cost-free region proposals .	0	7	2	5.176055	-4.5887685	0
11-8-28	rpns are trained end-to-end to generate highquality region proposals , which are used by fast r-cnn for detection .	an rpn is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position .	0	4	3	5.2446404	-4.5902953	0
11-8-28	with a simple alternating optimization , rpn and fast r-cnn can be trained to share convolutional features .	an rpn is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position .	0	5	3	5.0043583	-4.4012566	0
11-8-28	an rpn is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position .	[CLS] for the very deep vgg - 16 model, our detection system has a frame rate of 5fps ( including all steps ) on a gpu, while achieving state - of - the - art object detection accuracy on pascal voc 2007 ( 73. 2 % map ) and 2012 ( 70. 4 % map ) using 300 proposals per image	1	3	6	-5.912421	5.2248406	1
11-8-28	an rpn is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position .	code is available at https : //github.com/shaoqingren/faster_rcnn .	1	3	7	-5.9234204	4.966498	1
11-8-28	with a simple alternating optimization , rpn and fast r-cnn can be trained to share convolutional features .	rpns are trained end-to-end to generate highquality region proposals , which are used by fast r-cnn for detection .	0	5	4	-2.421408	2.485833	1
11-8-28	rpns are trained end-to-end to generate highquality region proposals , which are used by fast r-cnn for detection .	[CLS] for the very deep vgg - 16 model, our detection system has a frame rate of 5fps ( including all steps ) on a gpu, while achieving state - of - the - art object detection accuracy on pascal voc 2007 ( 73. 2 % map ) and 2012 ( 70. 4 % map ) using 300 proposals per	1	4	6	-4.7031574	4.339816	1
11-8-28	rpns are trained end-to-end to generate highquality region proposals , which are used by fast r-cnn for detection .	code is available at https : //github.com/shaoqingren/faster_rcnn .	1	4	7	-5.735064	4.8836374	1
11-8-28	for the very deep vgg-16 model , our detection system has a frame rate of 5fps ( including all steps ) on a gpu , while achieving state-of-the-art object detection accuracy on pascal voc 2007 ( 73.2 % map ) and 2012 ( 70.4 % map ) using 300 proposals per image .	with a simple alternating optimization , rpn and fast r-cnn can be trained to share convolutional features .	0	6	5	4.1006994	-3.8175855	0
11-8-28	code is available at https : //github.com/shaoqingren/faster_rcnn .	with a simple alternating optimization , rpn and fast r-cnn can be trained to share convolutional features .	0	7	5	4.9958324	-4.482291	0
11-8-28	code is available at https : //github.com/shaoqingren/faster_rcnn .	for the very deep vgg-16 model , our detection system has a frame rate of 5fps ( including all steps ) on a gpu , while achieving state-of-the-art object detection accuracy on pascal voc 2007 ( 73.2 % map ) and 2012 ( 70.4 % map ) using 300 proposals per image .	0	7	6	3.5827928	-3.3482502	0
12-6-15	this concept was shown to be useful for dimensionality reduction .	space-time is a profound concept in physics .	0	1	0	5.6057186	-5.046612	0
12-6-15	space-time is a profound concept in physics .	we present basic definitions with interesting counter-intuitions .	1	0	2	-5.9191055	5.058423	1
12-6-15	we give theoretical propositions to show that space-time is a more powerful representation than euclidean space .	space-time is a profound concept in physics .	0	3	0	5.6191473	-5.040175	0
12-6-15	we apply this concept to manifold learning for preserving local information .	space-time is a profound concept in physics .	0	4	0	5.649973	-5.0641975	0
12-6-15	space-time is a profound concept in physics .	empirical results on nonmetric datasets show that more information can be preserved in space-time .	1	0	5	-5.9395294	5.117219	1
12-6-15	we present basic definitions with interesting counter-intuitions .	this concept was shown to be useful for dimensionality reduction .	0	2	1	4.219571	-3.8321948	0
12-6-15	this concept was shown to be useful for dimensionality reduction .	we give theoretical propositions to show that space-time is a more powerful representation than euclidean space .	1	1	3	-3.8201623	3.6796994	1
12-6-15	this concept was shown to be useful for dimensionality reduction .	we apply this concept to manifold learning for preserving local information .	1	1	4	-5.338225	4.866073	1
12-6-15	empirical results on nonmetric datasets show that more information can be preserved in space-time .	this concept was shown to be useful for dimensionality reduction .	0	5	1	4.92295	-4.434311	0
12-6-15	we present basic definitions with interesting counter-intuitions .	we give theoretical propositions to show that space-time is a more powerful representation than euclidean space .	1	2	3	1.4098535	-1.0017354	0
12-6-15	we present basic definitions with interesting counter-intuitions .	we apply this concept to manifold learning for preserving local information .	1	2	4	0.17399916	0.0700806	0
12-6-15	we present basic definitions with interesting counter-intuitions .	empirical results on nonmetric datasets show that more information can be preserved in space-time .	1	2	5	-5.5902147	5.018601	1
12-6-15	we apply this concept to manifold learning for preserving local information .	we give theoretical propositions to show that space-time is a more powerful representation than euclidean space .	0	4	3	2.0358064	-1.8077587	0
12-6-15	we give theoretical propositions to show that space-time is a more powerful representation than euclidean space .	empirical results on nonmetric datasets show that more information can be preserved in space-time .	1	3	5	-5.83496	5.1141944	1
12-6-15	we apply this concept to manifold learning for preserving local information .	empirical results on nonmetric datasets show that more information can be preserved in space-time .	1	4	5	-5.0010424	4.5427017	1
13-2-1	with o ( r3 2 n log n ) random measurements of a positive semidefinite nxn matrix of rank r and condition number , our method is guaranteed to converge linearly to the global optimum .	we propose a simple , scalable , and fast gradient descent algorithm to optimize a nonconvex objective for the rank minimization problem and a closely related family of semidefinite programs .	0	1	0	5.2988873	-4.6069603	0
14-5-10	it models settings where there is uncertainty regarding which submodular function to optimize .	interactive submodular set cover is an interactive variant of submodular set cover over a hypothesis class of submodular functions , where the goal is to satisfy all sufficiently plausible submodular functions to a target threshold using as few ( cost-weighted ) actions as possible .	0	1	0	2.8724854	-2.724229	0
14-5-10	[CLS] interactive submodular set cover is an interactive variant of submodular set cover over a hypothesis class of submodular functions, where the goal is to satisfy all sufficiently plausible submodular functions to a target threshold using as few ( cost - weighted ) actions as possible	in this paper , we propose a new extension , which we call smooth interactive submodular set cover , that allows the target threshold to vary depending on the plausibility of each hypothesis .	1	0	2	-1.7925813	1.9843662	1
14-5-10	we present the first algorithm for this more general setting with theoretical guarantees on optimality .	interactive submodular set cover is an interactive variant of submodular set cover over a hypothesis class of submodular functions , where the goal is to satisfy all sufficiently plausible submodular functions to a target threshold using as few ( cost-weighted ) actions as possible .	0	3	0	3.4127505	-3.1566	0
14-5-10	[CLS] interactive submodular set cover is an interactive variant of submodular set cover over a hypothesis class of submodular functions, where the goal is to satisfy all sufficiently plausible submodular functions to a target threshold using as few ( cost - weighted )	we further show how to extend our approach to deal with realvalued functions , which yields new theoretical results for real-valued submodular set cover for both the interactive and non-interactive settings .	1	0	4	-5.964769	5.055934	1
14-5-10	in this paper , we propose a new extension , which we call smooth interactive submodular set cover , that allows the target threshold to vary depending on the plausibility of each hypothesis .	it models settings where there is uncertainty regarding which submodular function to optimize .	0	2	1	-2.8602734	2.8629205	1
14-5-10	it models settings where there is uncertainty regarding which submodular function to optimize .	we present the first algorithm for this more general setting with theoretical guarantees on optimality .	1	1	3	-4.1202908	3.945074	1
14-5-10	we further show how to extend our approach to deal with realvalued functions , which yields new theoretical results for real-valued submodular set cover for both the interactive and non-interactive settings .	it models settings where there is uncertainty regarding which submodular function to optimize .	0	4	1	4.7219815	-4.240468	0
14-5-10	in this paper , we propose a new extension , which we call smooth interactive submodular set cover , that allows the target threshold to vary depending on the plausibility of each hypothesis .	we present the first algorithm for this more general setting with theoretical guarantees on optimality .	1	2	3	-5.3931503	4.9524755	1
14-5-10	we further show how to extend our approach to deal with realvalued functions , which yields new theoretical results for real-valued submodular set cover for both the interactive and non-interactive settings .	in this paper , we propose a new extension , which we call smooth interactive submodular set cover , that allows the target threshold to vary depending on the plausibility of each hypothesis .	0	4	2	5.488051	-4.811095	0
14-5-10	we present the first algorithm for this more general setting with theoretical guarantees on optimality .	we further show how to extend our approach to deal with realvalued functions , which yields new theoretical results for real-valued submodular set cover for both the interactive and non-interactive settings .	1	3	4	-5.9237914	5.1039352	1
15-8-28	we propose a generative model for solving these problems of physical scene understanding from real-world videos and images .	humans demonstrate remarkable abilities to predict physical events in dynamic scenes , and to infer the physical properties of objects from static images .	0	1	0	5.3196864	-4.524133	0
15-8-28	humans demonstrate remarkable abilities to predict physical events in dynamic scenes , and to infer the physical properties of objects from static images .	at the core of our generative model is a 3d physics engine , operating on an object-based representation of physical properties , including mass , position , 3d shape , and friction .	1	0	2	-5.911669	5.205409	1
15-8-28	we can infer these latent properties using relatively brief runs of mcmc , which drive simulations in the physics engine to fit key features of visual observations .	humans demonstrate remarkable abilities to predict physical events in dynamic scenes , and to infer the physical properties of objects from static images .	0	3	0	5.7043004	-5.087057	0
15-8-28	humans demonstrate remarkable abilities to predict physical events in dynamic scenes , and to infer the physical properties of objects from static images .	we further explore directly mapping visual inputs to physical properties , inverting a part of the generative process using deep learning .	1	0	4	-6.0146813	5.1729865	1
15-8-28	humans demonstrate remarkable abilities to predict physical events in dynamic scenes , and to infer the physical properties of objects from static images .	we name our model galileo , and evaluate it on a video dataset with simple yet physically rich scenarios .	1	0	5	-6.0004416	5.193183	1
15-8-28	results show that galileo is able to infer the physical properties of objects and predict the outcome of a variety of physical events , with an accuracy comparable to human subjects .	humans demonstrate remarkable abilities to predict physical events in dynamic scenes , and to infer the physical properties of objects from static images .	0	6	0	5.6400557	-5.0699143	0
15-8-28	humans demonstrate remarkable abilities to predict physical events in dynamic scenes , and to infer the physical properties of objects from static images .	our study points towards an account of human vision with generative physical knowledge at its core , and various recognition models as helpers leading to efficient inference .	1	0	7	-5.9543266	5.12634	1
15-8-28	we propose a generative model for solving these problems of physical scene understanding from real-world videos and images .	at the core of our generative model is a 3d physics engine , operating on an object-based representation of physical properties , including mass , position , 3d shape , and friction .	1	1	2	-5.845749	5.232536	1
15-8-28	we propose a generative model for solving these problems of physical scene understanding from real-world videos and images .	we can infer these latent properties using relatively brief runs of mcmc , which drive simulations in the physics engine to fit key features of visual observations .	1	1	3	-5.873484	5.2135186	1
15-8-28	we further explore directly mapping visual inputs to physical properties , inverting a part of the generative process using deep learning .	we propose a generative model for solving these problems of physical scene understanding from real-world videos and images .	0	4	1	5.6485443	-4.997161	0
15-8-28	we propose a generative model for solving these problems of physical scene understanding from real-world videos and images .	we name our model galileo , and evaluate it on a video dataset with simple yet physically rich scenarios .	1	1	5	-5.9050264	5.187566	1
15-8-28	we propose a generative model for solving these problems of physical scene understanding from real-world videos and images .	results show that galileo is able to infer the physical properties of objects and predict the outcome of a variety of physical events , with an accuracy comparable to human subjects .	1	1	6	-5.9270573	5.2055025	1
15-8-28	our study points towards an account of human vision with generative physical knowledge at its core , and various recognition models as helpers leading to efficient inference .	we propose a generative model for solving these problems of physical scene understanding from real-world videos and images .	0	7	1	5.52293	-4.934824	0
15-8-28	at the core of our generative model is a 3d physics engine , operating on an object-based representation of physical properties , including mass , position , 3d shape , and friction .	we can infer these latent properties using relatively brief runs of mcmc , which drive simulations in the physics engine to fit key features of visual observations .	1	2	3	-6.0082636	5.2023277	1
15-8-28	we further explore directly mapping visual inputs to physical properties , inverting a part of the generative process using deep learning .	at the core of our generative model is a 3d physics engine , operating on an object-based representation of physical properties , including mass , position , 3d shape , and friction .	0	4	2	4.539459	-4.022232	0
15-8-28	we name our model galileo , and evaluate it on a video dataset with simple yet physically rich scenarios .	at the core of our generative model is a 3d physics engine , operating on an object-based representation of physical properties , including mass , position , 3d shape , and friction .	0	5	2	4.7553444	-4.197641	0
15-8-28	results show that galileo is able to infer the physical properties of objects and predict the outcome of a variety of physical events , with an accuracy comparable to human subjects .	at the core of our generative model is a 3d physics engine , operating on an object-based representation of physical properties , including mass , position , 3d shape , and friction .	0	6	2	5.284133	-4.7170305	0
15-8-28	our study points towards an account of human vision with generative physical knowledge at its core , and various recognition models as helpers leading to efficient inference .	at the core of our generative model is a 3d physics engine , operating on an object-based representation of physical properties , including mass , position , 3d shape , and friction .	0	7	2	2.3664584	-2.2365823	0
15-8-28	we further explore directly mapping visual inputs to physical properties , inverting a part of the generative process using deep learning .	we can infer these latent properties using relatively brief runs of mcmc , which drive simulations in the physics engine to fit key features of visual observations .	0	4	3	2.3592482	-2.2102728	0
15-8-28	we can infer these latent properties using relatively brief runs of mcmc , which drive simulations in the physics engine to fit key features of visual observations .	we name our model galileo , and evaluate it on a video dataset with simple yet physically rich scenarios .	1	3	5	-3.6975436	3.6416378	1
15-8-28	results show that galileo is able to infer the physical properties of objects and predict the outcome of a variety of physical events , with an accuracy comparable to human subjects .	we can infer these latent properties using relatively brief runs of mcmc , which drive simulations in the physics engine to fit key features of visual observations .	0	6	3	4.962416	-4.4688625	0
15-8-28	our study points towards an account of human vision with generative physical knowledge at its core , and various recognition models as helpers leading to efficient inference .	we can infer these latent properties using relatively brief runs of mcmc , which drive simulations in the physics engine to fit key features of visual observations .	0	7	3	3.4431267	-3.171616	0
15-8-28	we name our model galileo , and evaluate it on a video dataset with simple yet physically rich scenarios .	we further explore directly mapping visual inputs to physical properties , inverting a part of the generative process using deep learning .	0	5	4	-1.8772957	2.008809	1
15-8-28	results show that galileo is able to infer the physical properties of objects and predict the outcome of a variety of physical events , with an accuracy comparable to human subjects .	we further explore directly mapping visual inputs to physical properties , inverting a part of the generative process using deep learning .	0	6	4	4.258437	-3.8844643	0
15-8-28	we further explore directly mapping visual inputs to physical properties , inverting a part of the generative process using deep learning .	our study points towards an account of human vision with generative physical knowledge at its core , and various recognition models as helpers leading to efficient inference .	1	4	7	-2.3120484	2.2775872	1
15-8-28	we name our model galileo , and evaluate it on a video dataset with simple yet physically rich scenarios .	results show that galileo is able to infer the physical properties of objects and predict the outcome of a variety of physical events , with an accuracy comparable to human subjects .	1	5	6	-5.730101	5.103923	1
15-8-28	our study points towards an account of human vision with generative physical knowledge at its core , and various recognition models as helpers leading to efficient inference .	we name our model galileo , and evaluate it on a video dataset with simple yet physically rich scenarios .	0	7	5	-4.0199547	3.8568132	1
15-8-28	our study points towards an account of human vision with generative physical knowledge at its core , and various recognition models as helpers leading to efficient inference .	results show that galileo is able to infer the physical properties of objects and predict the outcome of a variety of physical events , with an accuracy comparable to human subjects .	0	7	6	-2.7842045	2.7261577	1
16-5-10	we introduce t-level auctions to interpolate between simple auctions , such as welfare maximization with reserve prices , and optimal auctions , thereby balancing the competing demands of expressivity and simplicity .	this paper develops a general approach , rooted in statistical learning theory , to learning an approximately revenue-maximizing auction from data .	0	1	0	5.47104	-4.927496	0
16-5-10	we prove that such auctions have small representation error , in the sense that for every product distribution f over bidders ' valuations , there exists a t-level auction with small t and expected revenue close to optimal .	this paper develops a general approach , rooted in statistical learning theory , to learning an approximately revenue-maximizing auction from data .	0	2	0	5.574666	-5.0317235	0
16-5-10	we show that the set of t-level auctions has modest pseudodimension ( for polynomial t ) and therefore leads to small learning error .	this paper develops a general approach , rooted in statistical learning theory , to learning an approximately revenue-maximizing auction from data .	0	3	0	5.69121	-5.062721	0
16-5-10	this paper develops a general approach , rooted in statistical learning theory , to learning an approximately revenue-maximizing auction from data .	one consequence of our results is that , in arbitrary single-parameter settings , one can learn a mechanism with expected revenue arbitrarily close to optimal from a polynomial number of samples .	1	0	4	-6.0197115	5.172673	1
16-5-10	we introduce t-level auctions to interpolate between simple auctions , such as welfare maximization with reserve prices , and optimal auctions , thereby balancing the competing demands of expressivity and simplicity .	we prove that such auctions have small representation error , in the sense that for every product distribution f over bidders ' valuations , there exists a t-level auction with small t and expected revenue close to optimal .	1	1	2	-4.5913453	4.2482977	1
16-5-10	we introduce t-level auctions to interpolate between simple auctions , such as welfare maximization with reserve prices , and optimal auctions , thereby balancing the competing demands of expressivity and simplicity .	we show that the set of t-level auctions has modest pseudodimension ( for polynomial t ) and therefore leads to small learning error .	1	1	3	-5.4202185	4.9874463	1
16-5-10	one consequence of our results is that , in arbitrary single-parameter settings , one can learn a mechanism with expected revenue arbitrarily close to optimal from a polynomial number of samples .	we introduce t-level auctions to interpolate between simple auctions , such as welfare maximization with reserve prices , and optimal auctions , thereby balancing the competing demands of expressivity and simplicity .	0	4	1	5.3480797	-4.735777	0
16-5-10	we show that the set of t-level auctions has modest pseudodimension ( for polynomial t ) and therefore leads to small learning error .	we prove that such auctions have small representation error , in the sense that for every product distribution f over bidders ' valuations , there exists a t-level auction with small t and expected revenue close to optimal .	0	3	2	2.9033146	-2.6254437	0
16-5-10	one consequence of our results is that , in arbitrary single-parameter settings , one can learn a mechanism with expected revenue arbitrarily close to optimal from a polynomial number of samples .	we prove that such auctions have small representation error , in the sense that for every product distribution f over bidders ' valuations , there exists a t-level auction with small t and expected revenue close to optimal .	0	4	2	5.335308	-4.742293	0
16-5-10	we show that the set of t-level auctions has modest pseudodimension ( for polynomial t ) and therefore leads to small learning error .	one consequence of our results is that , in arbitrary single-parameter settings , one can learn a mechanism with expected revenue arbitrarily close to optimal from a polynomial number of samples .	1	3	4	-5.8741927	5.0593033	1
17-5-10	this variability can arise from single-neuron stochasticity , neural dynamics on short time-scales , as well as from modulations of neural firing properties on long time-scales , often referred to as neural non-stationarity .	neural population activity often exhibits rich variability .	0	1	0	5.655403	-5.0922546	0
17-5-10	neural population activity often exhibits rich variability .	to better understand the nature of co-variability in neural circuits and their impact on cortical information processing , we introduce a hierarchical dynamics model that is able to capture both slow inter-trial modulations in firing rates as well as neural population dynamics .	1	0	2	-5.8959727	5.1395493	1
17-5-10	we derive a bayesian laplace propagation algorithm for joint inference of parameters and population states .	neural population activity often exhibits rich variability .	0	3	0	5.7310743	-5.1728606	0
17-5-10	neural population activity often exhibits rich variability .	on neural population recordings from primary visual cortex , we demonstrate that our model provides a better account of the structure of neural firing than stationary dynamics models .	1	0	4	-5.9401746	5.081806	1
17-5-10	[CLS] this variability can arise from single - neuron stochasticity, neural dynamics on short time - scales, as well as from modulations of neural firing properties on long time - scales, often referred to as neural non - stationarity.	[CLS] to better understand the nature of co - variability in neural circuits and their impact on cortical information processing, we introduce a hierarchical dynamics model that is able to capture both slow inter - trial modulations in firing rates as well as neural population dynamics	1	1	2	-4.3186893	3.984501	1
17-5-10	we derive a bayesian laplace propagation algorithm for joint inference of parameters and population states .	this variability can arise from single-neuron stochasticity , neural dynamics on short time-scales , as well as from modulations of neural firing properties on long time-scales , often referred to as neural non-stationarity .	0	3	1	3.9480581	-3.6065183	0
17-5-10	this variability can arise from single-neuron stochasticity , neural dynamics on short time-scales , as well as from modulations of neural firing properties on long time-scales , often referred to as neural non-stationarity .	on neural population recordings from primary visual cortex , we demonstrate that our model provides a better account of the structure of neural firing than stationary dynamics models .	1	1	4	-5.9369583	5.043568	1
17-5-10	to better understand the nature of co-variability in neural circuits and their impact on cortical information processing , we introduce a hierarchical dynamics model that is able to capture both slow inter-trial modulations in firing rates as well as neural population dynamics .	we derive a bayesian laplace propagation algorithm for joint inference of parameters and population states .	1	2	3	3.128934	-2.817717	0
17-5-10	to better understand the nature of co-variability in neural circuits and their impact on cortical information processing , we introduce a hierarchical dynamics model that is able to capture both slow inter-trial modulations in firing rates as well as neural population dynamics .	on neural population recordings from primary visual cortex , we demonstrate that our model provides a better account of the structure of neural firing than stationary dynamics models .	1	2	4	-5.688415	5.1686826	1
17-5-10	we derive a bayesian laplace propagation algorithm for joint inference of parameters and population states .	on neural population recordings from primary visual cortex , we demonstrate that our model provides a better account of the structure of neural firing than stationary dynamics models .	1	3	4	-5.9846935	5.1044664	1
18-4-6	we introduce the locally linear latent variable model ( ll-lvm ) , a probabilistic model for non-linear manifold discovery that describes a joint distribution over observations , their manifold coordinates and locally linear maps conditioned on a set of neighbourhood relationships .	the model allows straightforward variational optimisation of the posterior distribution on coordinates and locally linear maps from the latent space to the observation space given the data .	1	0	1	-5.786641	5.204404	1
18-4-6	thus , the ll-lvm encapsulates the local-geometry preserving intuitions that underlie non-probabilistic methods such as locally linear embedding ( lle ) .	we introduce the locally linear latent variable model ( ll-lvm ) , a probabilistic model for non-linear manifold discovery that describes a joint distribution over observations , their manifold coordinates and locally linear maps conditioned on a set of neighbourhood relationships .	0	2	0	5.5414605	-4.91582	0
18-4-6	[CLS] we introduce the locally linear latent variable model ( ll - lvm ), a probabilistic model for non - linear manifold discovery that describes a joint distribution over observations, their manifold coordinates and locally linear maps conditioned on a set of	[CLS] its probabilistic semantics make it easy to evaluate the quality of hypothesised neighbourhood relationships, select the intrinsic dimensionality of the manifold, construct out - of - sample extensions and to combine the manifold model with additional probabilistic models that	1	0	3	-5.5127726	5.046287	1
18-4-6	thus , the ll-lvm encapsulates the local-geometry preserving intuitions that underlie non-probabilistic methods such as locally linear embedding ( lle ) .	the model allows straightforward variational optimisation of the posterior distribution on coordinates and locally linear maps from the latent space to the observation space given the data .	0	2	1	-2.5178325	2.5905151	1
18-4-6	the model allows straightforward variational optimisation of the posterior distribution on coordinates and locally linear maps from the latent space to the observation space given the data .	its probabilistic semantics make it easy to evaluate the quality of hypothesised neighbourhood relationships , select the intrinsic dimensionality of the manifold , construct out-of-sample extensions and to combine the manifold model with additional probabilistic models that capture the structure of coordinates within the manifold .	1	1	3	-3.1427155	3.0492833	1
18-4-6	thus , the ll-lvm encapsulates the local-geometry preserving intuitions that underlie non-probabilistic methods such as locally linear embedding ( lle ) .	[CLS] its probabilistic semantics make it easy to evaluate the quality of hypothesised neighbourhood relationships, select the intrinsic dimensionality of the manifold, construct out - of - sample extensions and to combine the manifold model with additional probabilistic models that capture the structure of coordinates within the	1	2	3	-5.6252604	4.9109073	1
19-11-55	color constancy is the recovery of true surface color from observed color , and requires estimating the chromaticity of scene illumination to correct for the bias it induces .	in this paper , we show that the per-pixel color statistics of natural scenes -- without any spatial or semantic context -- can by themselves be a powerful cue for color constancy .	1	0	1	-5.0315895	4.608177	1
19-11-55	specifically , we describe an illuminant estimation method that is built around a `` classifier '' for identifying the true chromaticity of a pixel given its luminance ( absolute brightness across color channels ) .	color constancy is the recovery of true surface color from observed color , and requires estimating the chromaticity of scene illumination to correct for the bias it induces .	0	2	0	5.3734674	-4.7527885	0
19-11-55	color constancy is the recovery of true surface color from observed color , and requires estimating the chromaticity of scene illumination to correct for the bias it induces .	during inference , each pixel 's observed color restricts its true chromaticity to those values that can be explained by one of a candidate set of illuminants , and applying the classifier over these values yields a distribution over the corresponding illuminants .	1	0	3	-5.7878566	5.266545	1
19-11-55	a global estimate for the scene illuminant is computed through a simple aggregation of these distributions across all pixels .	color constancy is the recovery of true surface color from observed color , and requires estimating the chromaticity of scene illumination to correct for the bias it induces .	0	4	0	5.4334116	-4.8214326	0
19-11-55	we begin by simply defining the luminance-to-chromaticity classifier by computing empirical histograms over discretized chromaticity and luminance values from a training set of natural images .	color constancy is the recovery of true surface color from observed color , and requires estimating the chromaticity of scene illumination to correct for the bias it induces .	0	5	0	5.311846	-4.674672	0
19-11-55	these histograms reflect a preference for hues corresponding to smooth reflectance functions , and for achromatic colors in brighter pixels .	color constancy is the recovery of true surface color from observed color , and requires estimating the chromaticity of scene illumination to correct for the bias it induces .	0	6	0	5.309371	-4.722344	0
19-11-55	color constancy is the recovery of true surface color from observed color , and requires estimating the chromaticity of scene illumination to correct for the bias it induces .	despite its simplicity , the resulting estimation algorithm outperforms current state-of-the-art color constancy methods .	1	0	7	-6.0032163	5.2018147	1
19-11-55	color constancy is the recovery of true surface color from observed color , and requires estimating the chromaticity of scene illumination to correct for the bias it induces .	next , we propose a method to learn the luminanceto-chromaticity classifier `` end-to-end '' .	1	0	8	-5.915401	5.0281425	1
19-11-55	using stochastic gradient descent , we set chromaticity-luminance likelihoods to minimize errors in the final scene illuminant estimates on a training set .	color constancy is the recovery of true surface color from observed color , and requires estimating the chromaticity of scene illumination to correct for the bias it induces .	0	9	0	5.6074305	-4.943084	0
19-11-55	color constancy is the recovery of true surface color from observed color , and requires estimating the chromaticity of scene illumination to correct for the bias it induces .	this leads to further improvements in accuracy , most significantly in the tail of the error distribution .	1	0	10	-5.9384727	5.198085	1
19-11-55	specifically , we describe an illuminant estimation method that is built around a `` classifier '' for identifying the true chromaticity of a pixel given its luminance ( absolute brightness across color channels ) .	in this paper , we show that the per-pixel color statistics of natural scenes -- without any spatial or semantic context -- can by themselves be a powerful cue for color constancy .	0	2	1	5.4247823	-4.773898	0
19-11-55	in this paper , we show that the per-pixel color statistics of natural scenes -- without any spatial or semantic context -- can by themselves be a powerful cue for color constancy .	during inference , each pixel 's observed color restricts its true chromaticity to those values that can be explained by one of a candidate set of illuminants , and applying the classifier over these values yields a distribution over the corresponding illuminants .	1	1	3	-4.926399	4.592284	1
19-11-55	a global estimate for the scene illuminant is computed through a simple aggregation of these distributions across all pixels .	in this paper , we show that the per-pixel color statistics of natural scenes -- without any spatial or semantic context -- can by themselves be a powerful cue for color constancy .	0	4	1	5.2985115	-4.704096	0
19-11-55	we begin by simply defining the luminance-to-chromaticity classifier by computing empirical histograms over discretized chromaticity and luminance values from a training set of natural images .	in this paper , we show that the per-pixel color statistics of natural scenes -- without any spatial or semantic context -- can by themselves be a powerful cue for color constancy .	0	5	1	5.0468893	-4.431839	0
19-11-55	these histograms reflect a preference for hues corresponding to smooth reflectance functions , and for achromatic colors in brighter pixels .	in this paper , we show that the per-pixel color statistics of natural scenes -- without any spatial or semantic context -- can by themselves be a powerful cue for color constancy .	0	6	1	3.6146984	-3.3099105	0
19-11-55	in this paper , we show that the per-pixel color statistics of natural scenes -- without any spatial or semantic context -- can by themselves be a powerful cue for color constancy .	despite its simplicity , the resulting estimation algorithm outperforms current state-of-the-art color constancy methods .	1	1	7	-6.000674	5.2180576	1
19-11-55	in this paper , we show that the per-pixel color statistics of natural scenes -- without any spatial or semantic context -- can by themselves be a powerful cue for color constancy .	next , we propose a method to learn the luminanceto-chromaticity classifier `` end-to-end '' .	1	1	8	-5.961324	5.1124287	1
19-11-55	using stochastic gradient descent , we set chromaticity-luminance likelihoods to minimize errors in the final scene illuminant estimates on a training set .	in this paper , we show that the per-pixel color statistics of natural scenes -- without any spatial or semantic context -- can by themselves be a powerful cue for color constancy .	0	9	1	5.6494203	-4.9655933	0
19-11-55	in this paper , we show that the per-pixel color statistics of natural scenes -- without any spatial or semantic context -- can by themselves be a powerful cue for color constancy .	this leads to further improvements in accuracy , most significantly in the tail of the error distribution .	1	1	10	-5.9799676	5.2320786	1
19-11-55	[CLS] during inference, each pixel's observed color restricts its true chromaticity to those values that can be explained by one of a candidate set of illuminants, and applying the classifier over these values yields a distribution over the corresponding illuminants	specifically , we describe an illuminant estimation method that is built around a `` classifier '' for identifying the true chromaticity of a pixel given its luminance ( absolute brightness across color channels ) .	0	3	2	2.9516196	-2.658945	0
19-11-55	specifically , we describe an illuminant estimation method that is built around a `` classifier '' for identifying the true chromaticity of a pixel given its luminance ( absolute brightness across color channels ) .	a global estimate for the scene illuminant is computed through a simple aggregation of these distributions across all pixels .	1	2	4	-3.6340573	3.447651	1
19-11-55	specifically , we describe an illuminant estimation method that is built around a `` classifier '' for identifying the true chromaticity of a pixel given its luminance ( absolute brightness across color channels ) .	we begin by simply defining the luminance-to-chromaticity classifier by computing empirical histograms over discretized chromaticity and luminance values from a training set of natural images .	1	2	5	-3.5801322	3.3289738	1
19-11-55	these histograms reflect a preference for hues corresponding to smooth reflectance functions , and for achromatic colors in brighter pixels .	specifically , we describe an illuminant estimation method that is built around a `` classifier '' for identifying the true chromaticity of a pixel given its luminance ( absolute brightness across color channels ) .	0	6	2	-4.3269553	4.1063604	1
19-11-55	despite its simplicity , the resulting estimation algorithm outperforms current state-of-the-art color constancy methods .	specifically , we describe an illuminant estimation method that is built around a `` classifier '' for identifying the true chromaticity of a pixel given its luminance ( absolute brightness across color channels ) .	0	7	2	5.1974087	-4.590709	0
19-11-55	next , we propose a method to learn the luminanceto-chromaticity classifier `` end-to-end '' .	specifically , we describe an illuminant estimation method that is built around a `` classifier '' for identifying the true chromaticity of a pixel given its luminance ( absolute brightness across color channels ) .	0	8	2	3.7650738	-3.4218097	0
19-11-55	specifically , we describe an illuminant estimation method that is built around a `` classifier '' for identifying the true chromaticity of a pixel given its luminance ( absolute brightness across color channels ) .	using stochastic gradient descent , we set chromaticity-luminance likelihoods to minimize errors in the final scene illuminant estimates on a training set .	1	2	9	-5.3193016	4.876149	1
19-11-55	this leads to further improvements in accuracy , most significantly in the tail of the error distribution .	specifically , we describe an illuminant estimation method that is built around a `` classifier '' for identifying the true chromaticity of a pixel given its luminance ( absolute brightness across color channels ) .	0	10	2	4.3422194	-3.890285	0
19-11-55	a global estimate for the scene illuminant is computed through a simple aggregation of these distributions across all pixels .	during inference , each pixel 's observed color restricts its true chromaticity to those values that can be explained by one of a candidate set of illuminants , and applying the classifier over these values yields a distribution over the corresponding illuminants .	0	4	3	3.7250917	-3.4571588	0
19-11-55	[CLS] during inference, each pixel's observed color restricts its true chromaticity to those values that can be explained by one of a candidate set of illuminants, and applying the classifier over these values yields a distribution over the corresponding illuminants	we begin by simply defining the luminance-to-chromaticity classifier by computing empirical histograms over discretized chromaticity and luminance values from a training set of natural images .	1	3	5	2.943999	-2.7370453	0
19-11-55	during inference , each pixel 's observed color restricts its true chromaticity to those values that can be explained by one of a candidate set of illuminants , and applying the classifier over these values yields a distribution over the corresponding illuminants .	these histograms reflect a preference for hues corresponding to smooth reflectance functions , and for achromatic colors in brighter pixels .	1	3	6	2.6934872	-2.4193795	0
19-11-55	despite its simplicity , the resulting estimation algorithm outperforms current state-of-the-art color constancy methods .	during inference , each pixel 's observed color restricts its true chromaticity to those values that can be explained by one of a candidate set of illuminants , and applying the classifier over these values yields a distribution over the corresponding illuminants .	0	7	3	5.372486	-4.830457	0
19-11-55	during inference , each pixel 's observed color restricts its true chromaticity to those values that can be explained by one of a candidate set of illuminants , and applying the classifier over these values yields a distribution over the corresponding illuminants .	next , we propose a method to learn the luminanceto-chromaticity classifier `` end-to-end '' .	1	3	8	-5.0775003	4.4643927	1
19-11-55	during inference , each pixel 's observed color restricts its true chromaticity to those values that can be explained by one of a candidate set of illuminants , and applying the classifier over these values yields a distribution over the corresponding illuminants .	using stochastic gradient descent , we set chromaticity-luminance likelihoods to minimize errors in the final scene illuminant estimates on a training set .	1	3	9	-4.934611	4.649934	1
19-11-55	during inference , each pixel 's observed color restricts its true chromaticity to those values that can be explained by one of a candidate set of illuminants , and applying the classifier over these values yields a distribution over the corresponding illuminants .	this leads to further improvements in accuracy , most significantly in the tail of the error distribution .	1	3	10	-5.9626927	5.052152	1
19-11-55	a global estimate for the scene illuminant is computed through a simple aggregation of these distributions across all pixels .	we begin by simply defining the luminance-to-chromaticity classifier by computing empirical histograms over discretized chromaticity and luminance values from a training set of natural images .	1	4	5	3.9657927	-3.6279452	0
19-11-55	a global estimate for the scene illuminant is computed through a simple aggregation of these distributions across all pixels .	these histograms reflect a preference for hues corresponding to smooth reflectance functions , and for achromatic colors in brighter pixels .	1	4	6	3.2074034	-2.915153	0
19-11-55	a global estimate for the scene illuminant is computed through a simple aggregation of these distributions across all pixels .	despite its simplicity , the resulting estimation algorithm outperforms current state-of-the-art color constancy methods .	1	4	7	-5.9983997	5.182317	1
19-11-55	next , we propose a method to learn the luminanceto-chromaticity classifier `` end-to-end '' .	a global estimate for the scene illuminant is computed through a simple aggregation of these distributions across all pixels .	0	8	4	2.979351	-2.8129263	0
19-11-55	using stochastic gradient descent , we set chromaticity-luminance likelihoods to minimize errors in the final scene illuminant estimates on a training set .	a global estimate for the scene illuminant is computed through a simple aggregation of these distributions across all pixels .	0	9	4	3.135073	-2.8902297	0
19-11-55	this leads to further improvements in accuracy , most significantly in the tail of the error distribution .	a global estimate for the scene illuminant is computed through a simple aggregation of these distributions across all pixels .	0	10	4	4.7913113	-4.3325167	0
19-11-55	we begin by simply defining the luminance-to-chromaticity classifier by computing empirical histograms over discretized chromaticity and luminance values from a training set of natural images .	these histograms reflect a preference for hues corresponding to smooth reflectance functions , and for achromatic colors in brighter pixels .	1	5	6	-2.858901	2.8069746	1
19-11-55	we begin by simply defining the luminance-to-chromaticity classifier by computing empirical histograms over discretized chromaticity and luminance values from a training set of natural images .	despite its simplicity , the resulting estimation algorithm outperforms current state-of-the-art color constancy methods .	1	5	7	-5.945893	5.2313547	1
19-11-55	we begin by simply defining the luminance-to-chromaticity classifier by computing empirical histograms over discretized chromaticity and luminance values from a training set of natural images .	next , we propose a method to learn the luminanceto-chromaticity classifier `` end-to-end '' .	1	5	8	-5.8119206	5.2009263	1
19-11-55	using stochastic gradient descent , we set chromaticity-luminance likelihoods to minimize errors in the final scene illuminant estimates on a training set .	we begin by simply defining the luminance-to-chromaticity classifier by computing empirical histograms over discretized chromaticity and luminance values from a training set of natural images .	0	9	5	4.435981	-3.954361	0
19-11-55	we begin by simply defining the luminance-to-chromaticity classifier by computing empirical histograms over discretized chromaticity and luminance values from a training set of natural images .	this leads to further improvements in accuracy , most significantly in the tail of the error distribution .	1	5	10	-5.9706516	5.2008076	1
19-11-55	despite its simplicity , the resulting estimation algorithm outperforms current state-of-the-art color constancy methods .	these histograms reflect a preference for hues corresponding to smooth reflectance functions , and for achromatic colors in brighter pixels .	0	7	6	5.2593174	-4.7068176	0
19-11-55	next , we propose a method to learn the luminanceto-chromaticity classifier `` end-to-end '' .	these histograms reflect a preference for hues corresponding to smooth reflectance functions , and for achromatic colors in brighter pixels .	0	8	6	3.990375	-3.635036	0
19-11-55	using stochastic gradient descent , we set chromaticity-luminance likelihoods to minimize errors in the final scene illuminant estimates on a training set .	these histograms reflect a preference for hues corresponding to smooth reflectance functions , and for achromatic colors in brighter pixels .	0	9	6	4.0873847	-3.6573029	0
19-11-55	this leads to further improvements in accuracy , most significantly in the tail of the error distribution .	these histograms reflect a preference for hues corresponding to smooth reflectance functions , and for achromatic colors in brighter pixels .	0	10	6	5.1232314	-4.535912	0
19-11-55	despite its simplicity , the resulting estimation algorithm outperforms current state-of-the-art color constancy methods .	next , we propose a method to learn the luminanceto-chromaticity classifier `` end-to-end '' .	1	7	8	4.0440416	-3.7297325	0
19-11-55	using stochastic gradient descent , we set chromaticity-luminance likelihoods to minimize errors in the final scene illuminant estimates on a training set .	despite its simplicity , the resulting estimation algorithm outperforms current state-of-the-art color constancy methods .	0	9	7	-5.7793226	5.1607504	1
19-11-55	this leads to further improvements in accuracy , most significantly in the tail of the error distribution .	despite its simplicity , the resulting estimation algorithm outperforms current state-of-the-art color constancy methods .	0	10	7	-4.1232953	3.8182447	1
19-11-55	next , we propose a method to learn the luminanceto-chromaticity classifier `` end-to-end '' .	using stochastic gradient descent , we set chromaticity-luminance likelihoods to minimize errors in the final scene illuminant estimates on a training set .	1	8	9	2.3525515	-2.1122754	0
19-11-55	next , we propose a method to learn the luminanceto-chromaticity classifier `` end-to-end '' .	this leads to further improvements in accuracy , most significantly in the tail of the error distribution .	1	8	10	-2.6730902	2.7513957	1
19-11-55	using stochastic gradient descent , we set chromaticity-luminance likelihoods to minimize errors in the final scene illuminant estimates on a training set .	this leads to further improvements in accuracy , most significantly in the tail of the error distribution .	1	9	10	-5.6115494	5.0205493	1
20-6-15	we show that the maximum-likelihood ( ml ) estimate of models derived from luce 's choice axiom ( e.g. , the plackett-luce model ) can be expressed as the stationary distribution of a markov chain .	this conveys insight into several recently proposed spectral inference algorithms .	1	0	1	-5.726366	4.8082566	1
20-6-15	we take advantage of this perspective and formulate a new spectral algorithm that is significantly more accurate than previous ones for the plackett-luce model .	we show that the maximum-likelihood ( ml ) estimate of models derived from luce 's choice axiom ( e.g. , the plackett-luce model ) can be expressed as the stationary distribution of a markov chain .	0	2	0	5.358289	-4.6960697	0
20-6-15	with a simple adaptation , this algorithm can be used iteratively , producing a sequence of estimates that converges to the ml estimate .	we show that the maximum-likelihood ( ml ) estimate of models derived from luce 's choice axiom ( e.g. , the plackett-luce model ) can be expressed as the stationary distribution of a markov chain .	0	3	0	5.6704936	-5.086568	0
20-6-15	the ml version runs faster than competing approaches on a benchmark of five datasets .	we show that the maximum-likelihood ( ml ) estimate of models derived from luce 's choice axiom ( e.g. , the plackett-luce model ) can be expressed as the stationary distribution of a markov chain .	0	4	0	5.6097846	-5.0449114	0
20-6-15	our algorithms are easy to implement , making them relevant for practitioners at large .	we show that the maximum-likelihood ( ml ) estimate of models derived from luce 's choice axiom ( e.g. , the plackett-luce model ) can be expressed as the stationary distribution of a markov chain .	0	5	0	5.27649	-4.6500645	0
20-6-15	we take advantage of this perspective and formulate a new spectral algorithm that is significantly more accurate than previous ones for the plackett-luce model .	this conveys insight into several recently proposed spectral inference algorithms .	0	2	1	-3.7146091	3.5018349	1
20-6-15	this conveys insight into several recently proposed spectral inference algorithms .	with a simple adaptation , this algorithm can be used iteratively , producing a sequence of estimates that converges to the ml estimate .	1	1	3	-0.7422416	0.85516435	1
20-6-15	this conveys insight into several recently proposed spectral inference algorithms .	the ml version runs faster than competing approaches on a benchmark of five datasets .	1	1	4	2.1649015	-2.057194	0
20-6-15	our algorithms are easy to implement , making them relevant for practitioners at large .	this conveys insight into several recently proposed spectral inference algorithms .	0	5	1	-1.7909949	1.9072381	1
20-6-15	with a simple adaptation , this algorithm can be used iteratively , producing a sequence of estimates that converges to the ml estimate .	we take advantage of this perspective and formulate a new spectral algorithm that is significantly more accurate than previous ones for the plackett-luce model .	0	3	2	3.388442	-3.109304	0
20-6-15	we take advantage of this perspective and formulate a new spectral algorithm that is significantly more accurate than previous ones for the plackett-luce model .	the ml version runs faster than competing approaches on a benchmark of five datasets .	1	2	4	-1.8317913	1.958475	1
20-6-15	our algorithms are easy to implement , making them relevant for practitioners at large .	we take advantage of this perspective and formulate a new spectral algorithm that is significantly more accurate than previous ones for the plackett-luce model .	0	5	2	3.9713602	-3.5872104	0
20-6-15	the ml version runs faster than competing approaches on a benchmark of five datasets .	with a simple adaptation , this algorithm can be used iteratively , producing a sequence of estimates that converges to the ml estimate .	0	4	3	-1.2362866	1.5220873	1
20-6-15	our algorithms are easy to implement , making them relevant for practitioners at large .	with a simple adaptation , this algorithm can be used iteratively , producing a sequence of estimates that converges to the ml estimate .	0	5	3	2.2236423	-2.048804	0
20-6-15	the ml version runs faster than competing approaches on a benchmark of five datasets .	our algorithms are easy to implement , making them relevant for practitioners at large .	1	4	5	-3.903107	3.6363943	1
21-6-15	in deterministic optimization , line searches are a standard tool ensuring stability and efficiency .	where only stochastic gradients are available , no direct equivalent has so far been formulated , because uncertain gradients do not allow for a strict sequence of decisions collapsing the search space .	1	0	1	-5.92599	5.2300344	1
21-6-15	in deterministic optimization , line searches are a standard tool ensuring stability and efficiency .	we construct a probabilistic line search by combining the structure of existing deterministic methods with notions from bayesian optimization .	1	0	2	-5.934738	5.194644	1
21-6-15	our method retains a gaussian process surrogate of the univariate optimization objective , and uses a probabilistic belief over the wolfe conditions to monitor the descent .	in deterministic optimization , line searches are a standard tool ensuring stability and efficiency .	0	3	0	5.6270647	-5.0122213	0
21-6-15	in deterministic optimization , line searches are a standard tool ensuring stability and efficiency .	the algorithm has very low computational cost , and no user-controlled parameters .	1	0	4	-5.926483	5.211824	1
21-6-15	experiments show that it effectively removes the need to define a learning rate for stochastic gradient descent .	in deterministic optimization , line searches are a standard tool ensuring stability and efficiency .	0	5	0	5.668474	-5.0472116	0
21-6-15	where only stochastic gradients are available , no direct equivalent has so far been formulated , because uncertain gradients do not allow for a strict sequence of decisions collapsing the search space .	we construct a probabilistic line search by combining the structure of existing deterministic methods with notions from bayesian optimization .	1	1	2	-2.4037237	2.543489	1
21-6-15	where only stochastic gradients are available , no direct equivalent has so far been formulated , because uncertain gradients do not allow for a strict sequence of decisions collapsing the search space .	our method retains a gaussian process surrogate of the univariate optimization objective , and uses a probabilistic belief over the wolfe conditions to monitor the descent .	1	1	3	-4.4009957	4.2362556	1
21-6-15	the algorithm has very low computational cost , and no user-controlled parameters .	where only stochastic gradients are available , no direct equivalent has so far been formulated , because uncertain gradients do not allow for a strict sequence of decisions collapsing the search space .	0	4	1	1.8368275	-1.7158877	0
21-6-15	experiments show that it effectively removes the need to define a learning rate for stochastic gradient descent .	where only stochastic gradients are available , no direct equivalent has so far been formulated , because uncertain gradients do not allow for a strict sequence of decisions collapsing the search space .	0	5	1	5.350198	-4.7676754	0
21-6-15	we construct a probabilistic line search by combining the structure of existing deterministic methods with notions from bayesian optimization .	our method retains a gaussian process surrogate of the univariate optimization objective , and uses a probabilistic belief over the wolfe conditions to monitor the descent .	1	2	3	-2.9665952	2.9281588	1
21-6-15	we construct a probabilistic line search by combining the structure of existing deterministic methods with notions from bayesian optimization .	the algorithm has very low computational cost , and no user-controlled parameters .	1	2	4	-4.755807	4.4449987	1
21-6-15	experiments show that it effectively removes the need to define a learning rate for stochastic gradient descent .	we construct a probabilistic line search by combining the structure of existing deterministic methods with notions from bayesian optimization .	0	5	2	5.3353205	-4.735173	0
21-6-15	the algorithm has very low computational cost , and no user-controlled parameters .	our method retains a gaussian process surrogate of the univariate optimization objective , and uses a probabilistic belief over the wolfe conditions to monitor the descent .	0	4	3	-1.322907	1.4816538	1
21-6-15	our method retains a gaussian process surrogate of the univariate optimization objective , and uses a probabilistic belief over the wolfe conditions to monitor the descent .	experiments show that it effectively removes the need to define a learning rate for stochastic gradient descent .	1	3	5	-5.7531986	5.092431	1
21-6-15	experiments show that it effectively removes the need to define a learning rate for stochastic gradient descent .	the algorithm has very low computational cost , and no user-controlled parameters .	0	5	4	4.753146	-4.298775	0
22-4-6	in this paper , we discuss the limitations of standard deep learning approaches and show that some of these limitations can be overcome by learning how to grow the complexity of a model in a structured way .	despite the recent achievements in machine learning , we are still very far from achieving real artificial intelligence .	0	1	0	5.4176865	-4.8537607	0
22-4-6	specifically , we study the simplest sequence prediction problems that are beyond the scope of what is learnable with standard recurrent networks , algorithmically generated sequences which can only be learned by models which have the capacity to count and to memorize sequences .	despite the recent achievements in machine learning , we are still very far from achieving real artificial intelligence .	0	2	0	5.2273903	-4.671376	0
22-4-6	we show that some basic algorithms can be learned from sequential data using a recurrent network associated with a trainable memory .	despite the recent achievements in machine learning , we are still very far from achieving real artificial intelligence .	0	3	0	5.46342	-4.9211073	0
22-4-6	specifically , we study the simplest sequence prediction problems that are beyond the scope of what is learnable with standard recurrent networks , algorithmically generated sequences which can only be learned by models which have the capacity to count and to memorize sequences .	in this paper , we discuss the limitations of standard deep learning approaches and show that some of these limitations can be overcome by learning how to grow the complexity of a model in a structured way .	0	2	1	3.6521227	-3.3884115	0
22-4-6	in this paper , we discuss the limitations of standard deep learning approaches and show that some of these limitations can be overcome by learning how to grow the complexity of a model in a structured way .	we show that some basic algorithms can be learned from sequential data using a recurrent network associated with a trainable memory .	1	1	3	-5.0217013	4.7868543	1
22-4-6	specifically , we study the simplest sequence prediction problems that are beyond the scope of what is learnable with standard recurrent networks , algorithmically generated sequences which can only be learned by models which have the capacity to count and to memorize sequences .	we show that some basic algorithms can be learned from sequential data using a recurrent network associated with a trainable memory .	1	2	3	-0.21477672	0.48406538	1
23-9-36	humans have the remarkable ability to follow the gaze of other people to identify what they are looking at .	following eye gaze , or gaze-following , is an important ability that allows us to understand what other people are thinking , the actions they are performing , and even predict what they might do next .	1	0	1	-3.7496326	3.6044414	1
23-9-36	despite the importance of this topic , this problem has only been studied in limited scenarios within the computer vision community .	humans have the remarkable ability to follow the gaze of other people to identify what they are looking at .	0	2	0	5.654317	-4.9569473	0
23-9-36	humans have the remarkable ability to follow the gaze of other people to identify what they are looking at .	in this paper , we propose a deep neural networkbased approach for gaze-following and a new benchmark dataset , gazefollow , for thorough evaluation .	1	0	3	-5.8750515	5.1958585	1
23-9-36	humans have the remarkable ability to follow the gaze of other people to identify what they are looking at .	given an image and the location of a head , our approach follows the gaze of the person and identifies the object being looked at .	1	0	4	-5.821018	5.2736416	1
23-9-36	humans have the remarkable ability to follow the gaze of other people to identify what they are looking at .	our deep network is able to discover how to extract head pose and gaze orientation , and to select objects in the scene that are in the predicted line of sight and likely to be looked at ( such as televisions , balls and food ) .	1	0	5	-5.9437485	5.1724973	1
23-9-36	the quantitative evaluation shows that our approach produces reliable results , even when viewing only the back of the head .	humans have the remarkable ability to follow the gaze of other people to identify what they are looking at .	0	6	0	5.697361	-5.0956216	0
23-9-36	humans have the remarkable ability to follow the gaze of other people to identify what they are looking at .	while our method outperforms several baseline approaches , we are still far from reaching human performance on this task .	1	0	7	-6.002026	5.180129	1
23-9-36	overall , we believe that gazefollowing is a challenging and important problem that deserves more attention from the community .	humans have the remarkable ability to follow the gaze of other people to identify what they are looking at .	0	8	0	5.650709	-5.038035	0
23-9-36	following eye gaze , or gaze-following , is an important ability that allows us to understand what other people are thinking , the actions they are performing , and even predict what they might do next .	despite the importance of this topic , this problem has only been studied in limited scenarios within the computer vision community .	1	1	2	-5.9711323	5.198515	1
23-9-36	following eye gaze , or gaze-following , is an important ability that allows us to understand what other people are thinking , the actions they are performing , and even predict what they might do next .	in this paper , we propose a deep neural networkbased approach for gaze-following and a new benchmark dataset , gazefollow , for thorough evaluation .	1	1	3	-5.8961277	5.1443453	1
23-9-36	following eye gaze , or gaze-following , is an important ability that allows us to understand what other people are thinking , the actions they are performing , and even predict what they might do next .	given an image and the location of a head , our approach follows the gaze of the person and identifies the object being looked at .	1	1	4	-5.6148014	5.1378584	1
23-9-36	our deep network is able to discover how to extract head pose and gaze orientation , and to select objects in the scene that are in the predicted line of sight and likely to be looked at ( such as televisions , balls and food ) .	following eye gaze , or gaze-following , is an important ability that allows us to understand what other people are thinking , the actions they are performing , and even predict what they might do next .	0	5	1	5.5964622	-5.0305185	0
23-9-36	the quantitative evaluation shows that our approach produces reliable results , even when viewing only the back of the head .	following eye gaze , or gaze-following , is an important ability that allows us to understand what other people are thinking , the actions they are performing , and even predict what they might do next .	0	6	1	5.6071815	-5.0624	0
23-9-36	following eye gaze , or gaze-following , is an important ability that allows us to understand what other people are thinking , the actions they are performing , and even predict what they might do next .	while our method outperforms several baseline approaches , we are still far from reaching human performance on this task .	1	1	7	-5.9987	5.204028	1
23-9-36	following eye gaze , or gaze-following , is an important ability that allows us to understand what other people are thinking , the actions they are performing , and even predict what they might do next .	overall , we believe that gazefollowing is a challenging and important problem that deserves more attention from the community .	1	1	8	-5.972038	5.197445	1
23-9-36	despite the importance of this topic , this problem has only been studied in limited scenarios within the computer vision community .	in this paper , we propose a deep neural networkbased approach for gaze-following and a new benchmark dataset , gazefollow , for thorough evaluation .	1	2	3	-5.8126383	5.2582917	1
23-9-36	given an image and the location of a head , our approach follows the gaze of the person and identifies the object being looked at .	despite the importance of this topic , this problem has only been studied in limited scenarios within the computer vision community .	0	4	2	3.7002125	-3.342648	0
23-9-36	our deep network is able to discover how to extract head pose and gaze orientation , and to select objects in the scene that are in the predicted line of sight and likely to be looked at ( such as televisions , balls and food ) .	despite the importance of this topic , this problem has only been studied in limited scenarios within the computer vision community .	0	5	2	5.4850187	-4.9227233	0
23-9-36	the quantitative evaluation shows that our approach produces reliable results , even when viewing only the back of the head .	despite the importance of this topic , this problem has only been studied in limited scenarios within the computer vision community .	0	6	2	5.555637	-4.9868813	0
23-9-36	despite the importance of this topic , this problem has only been studied in limited scenarios within the computer vision community .	while our method outperforms several baseline approaches , we are still far from reaching human performance on this task .	1	2	7	-5.9180474	5.2069497	1
23-9-36	despite the importance of this topic , this problem has only been studied in limited scenarios within the computer vision community .	overall , we believe that gazefollowing is a challenging and important problem that deserves more attention from the community .	1	2	8	-5.6435204	5.114942	1
23-9-36	given an image and the location of a head , our approach follows the gaze of the person and identifies the object being looked at .	in this paper , we propose a deep neural networkbased approach for gaze-following and a new benchmark dataset , gazefollow , for thorough evaluation .	0	4	3	3.1102939	-2.5875797	0
23-9-36	in this paper , we propose a deep neural networkbased approach for gaze-following and a new benchmark dataset , gazefollow , for thorough evaluation .	our deep network is able to discover how to extract head pose and gaze orientation , and to select objects in the scene that are in the predicted line of sight and likely to be looked at ( such as televisions , balls and food ) .	1	3	5	-5.906328	5.2034144	1
23-9-36	in this paper , we propose a deep neural networkbased approach for gaze-following and a new benchmark dataset , gazefollow , for thorough evaluation .	the quantitative evaluation shows that our approach produces reliable results , even when viewing only the back of the head .	1	3	6	-5.9895234	5.0978594	1
23-9-36	in this paper , we propose a deep neural networkbased approach for gaze-following and a new benchmark dataset , gazefollow , for thorough evaluation .	while our method outperforms several baseline approaches , we are still far from reaching human performance on this task .	1	3	7	-3.8913543	3.7002609	1
23-9-36	in this paper , we propose a deep neural networkbased approach for gaze-following and a new benchmark dataset , gazefollow , for thorough evaluation .	overall , we believe that gazefollowing is a challenging and important problem that deserves more attention from the community .	1	3	8	-5.4615116	4.8602777	1
23-9-36	our deep network is able to discover how to extract head pose and gaze orientation , and to select objects in the scene that are in the predicted line of sight and likely to be looked at ( such as televisions , balls and food ) .	given an image and the location of a head , our approach follows the gaze of the person and identifies the object being looked at .	0	5	4	5.443841	-4.791256	0
23-9-36	given an image and the location of a head , our approach follows the gaze of the person and identifies the object being looked at .	the quantitative evaluation shows that our approach produces reliable results , even when viewing only the back of the head .	1	4	6	-5.982285	5.067628	1
23-9-36	while our method outperforms several baseline approaches , we are still far from reaching human performance on this task .	given an image and the location of a head , our approach follows the gaze of the person and identifies the object being looked at .	0	7	4	5.2124424	-4.5650835	0
23-9-36	given an image and the location of a head , our approach follows the gaze of the person and identifies the object being looked at .	overall , we believe that gazefollowing is a challenging and important problem that deserves more attention from the community .	1	4	8	-4.497876	4.1576586	1
23-9-36	the quantitative evaluation shows that our approach produces reliable results , even when viewing only the back of the head .	our deep network is able to discover how to extract head pose and gaze orientation , and to select objects in the scene that are in the predicted line of sight and likely to be looked at ( such as televisions , balls and food ) .	0	6	5	4.3332863	-4.0023932	0
23-9-36	while our method outperforms several baseline approaches , we are still far from reaching human performance on this task .	our deep network is able to discover how to extract head pose and gaze orientation , and to select objects in the scene that are in the predicted line of sight and likely to be looked at ( such as televisions , balls and food ) .	0	7	5	-2.0878987	2.2132814	1
23-9-36	overall , we believe that gazefollowing is a challenging and important problem that deserves more attention from the community .	our deep network is able to discover how to extract head pose and gaze orientation , and to select objects in the scene that are in the predicted line of sight and likely to be looked at ( such as televisions , balls and food ) .	0	8	5	0.11843148	-0.04021515	0
23-9-36	while our method outperforms several baseline approaches , we are still far from reaching human performance on this task .	the quantitative evaluation shows that our approach produces reliable results , even when viewing only the back of the head .	0	7	6	-5.3868694	4.9737926	1
23-9-36	overall , we believe that gazefollowing is a challenging and important problem that deserves more attention from the community .	the quantitative evaluation shows that our approach produces reliable results , even when viewing only the back of the head .	0	8	6	-3.4324975	3.2837653	1
23-9-36	while our method outperforms several baseline approaches , we are still far from reaching human performance on this task .	overall , we believe that gazefollowing is a challenging and important problem that deserves more attention from the community .	1	7	8	-3.142554	3.1429043	1
24-5-10	given a multi-armed bandit problem it may be desirable to achieve a smallerthan-usual worst-case regret for some special actions .	i show that the price for such unbalanced worst-case regret guarantees is rather high .	1	0	1	-5.8427143	5.2461767	1
24-5-10	specifically , if an algorithm enjoys a worst-case regret of b with respect to some action , then there must exist another action for which the worst-case regret is at least ( nk/b ) , where n is the horizon and k the number of actions .	given a multi-armed bandit problem it may be desirable to achieve a smallerthan-usual worst-case regret for some special actions .	0	2	0	5.105652	-4.562374	0
24-5-10	i also give upper bounds in both the stochastic and adversarial settings showing that this result can not be improved .	given a multi-armed bandit problem it may be desirable to achieve a smallerthan-usual worst-case regret for some special actions .	0	3	0	5.5886226	-5.0255938	0
24-5-10	for the stochastic case the pareto regret frontier is characterised exactly up to constant factors .	given a multi-armed bandit problem it may be desirable to achieve a smallerthan-usual worst-case regret for some special actions .	0	4	0	5.408348	-4.8630133	0
24-5-10	i show that the price for such unbalanced worst-case regret guarantees is rather high .	specifically , if an algorithm enjoys a worst-case regret of b with respect to some action , then there must exist another action for which the worst-case regret is at least ( nk/b ) , where n is the horizon and k the number of actions .	1	1	2	4.6809835	-4.2320137	0
24-5-10	i also give upper bounds in both the stochastic and adversarial settings showing that this result can not be improved .	i show that the price for such unbalanced worst-case regret guarantees is rather high .	0	3	1	3.406816	-3.2210565	0
24-5-10	i show that the price for such unbalanced worst-case regret guarantees is rather high .	for the stochastic case the pareto regret frontier is characterised exactly up to constant factors .	1	1	4	0.585061	-0.42048815	0
24-5-10	i also give upper bounds in both the stochastic and adversarial settings showing that this result can not be improved .	specifically , if an algorithm enjoys a worst-case regret of b with respect to some action , then there must exist another action for which the worst-case regret is at least ( nk/b ) , where n is the horizon and k the number of actions .	0	3	2	5.3588357	-4.773986	0
24-5-10	for the stochastic case the pareto regret frontier is characterised exactly up to constant factors .	specifically , if an algorithm enjoys a worst-case regret of b with respect to some action , then there must exist another action for which the worst-case regret is at least ( nk/b ) , where n is the horizon and k the number of actions .	0	4	2	4.0642667	-3.7196012	0
24-5-10	i also give upper bounds in both the stochastic and adversarial settings showing that this result can not be improved .	for the stochastic case the pareto regret frontier is characterised exactly up to constant factors .	1	3	4	3.6098068	-3.4140224	0
25-9-36	gaussians variables with mean 0 and variance 1 and the hypothesis that there is a planted principal submatrix b of dimension l for which all upper triangular variables are i.i.d .	we consider the following detection problem : given a realization of a symmetric matrix x of dimension n , distinguish between the hypothesis that all upper triangular variables are i.i.d .	0	1	0	5.3168902	-4.740856	0
25-9-36	we consider the following detection problem : given a realization of a symmetric matrix x of dimension n , distinguish between the hypothesis that all upper triangular variables are i.i.d .	gaussians with mean 1 and variance 1 , whereas all other upper triangular elements of x not in b are i.i.d .	1	0	2	-5.7698326	5.2183714	1
25-9-36	gaussians variables with mean 0 and variance 1 .	we consider the following detection problem : given a realization of a symmetric matrix x of dimension n , distinguish between the hypothesis that all upper triangular variables are i.i.d .	0	3	0	5.583207	-4.980934	0
25-9-36	we consider the following detection problem : given a realization of a symmetric matrix x of dimension n , distinguish between the hypothesis that all upper triangular variables are i.i.d .	we refer to this as the `gaussian hidden clique problem ' .	1	0	4	-5.648834	5.1133757	1
25-9-36	when l = ( 1 + ) n ( > 0 ) , it is possible to solve this detection problem with probability 1 - on ( 1 ) by computing the spectrum of x and considering the largest eigenvalue of x .	we consider the following detection problem : given a realization of a symmetric matrix x of dimension n , distinguish between the hypothesis that all upper triangular variables are i.i.d .	0	5	0	5.5646973	-4.974717	0
25-9-36	we prove that when l < ( 1 - ) n no algorithm that examines only the eigenvalues of x can detect the existence of a hidden gaussian clique , with error probability vanishing as n .	we consider the following detection problem : given a realization of a symmetric matrix x of dimension n , distinguish between the hypothesis that all upper triangular variables are i.i.d .	0	6	0	5.599452	-4.9743605	0
25-9-36	the result above is an immediate consequence of a more general result on rank-one perturbations of k-dimensional gaussian tensors .	we consider the following detection problem : given a realization of a symmetric matrix x of dimension n , distinguish between the hypothesis that all upper triangular variables are i.i.d .	0	7	0	5.6069536	-5.0223794	0
25-9-36	we consider the following detection problem : given a realization of a symmetric matrix x of dimension n , distinguish between the hypothesis that all upper triangular variables are i.i.d .	in this context we establish a lower bound on the critical signal-to-noise ratio below which a rank-one signal can not be detected .	1	0	8	-5.9883566	5.1869955	1
25-9-36	gaussians variables with mean 0 and variance 1 and the hypothesis that there is a planted principal submatrix b of dimension l for which all upper triangular variables are i.i.d .	gaussians with mean 1 and variance 1 , whereas all other upper triangular elements of x not in b are i.i.d .	1	1	2	-3.2029703	3.0423894	1
25-9-36	gaussians variables with mean 0 and variance 1 .	gaussians variables with mean 0 and variance 1 and the hypothesis that there is a planted principal submatrix b of dimension l for which all upper triangular variables are i.i.d .	0	3	1	2.9836142	-2.6614478	0
25-9-36	we refer to this as the `gaussian hidden clique problem ' .	gaussians variables with mean 0 and variance 1 and the hypothesis that there is a planted principal submatrix b of dimension l for which all upper triangular variables are i.i.d .	0	4	1	2.325712	-2.143432	0
25-9-36	gaussians variables with mean 0 and variance 1 and the hypothesis that there is a planted principal submatrix b of dimension l for which all upper triangular variables are i.i.d .	when l = ( 1 + ) n ( > 0 ) , it is possible to solve this detection problem with probability 1 - on ( 1 ) by computing the spectrum of x and considering the largest eigenvalue of x .	1	1	5	-2.777919	2.8383226	1
25-9-36	gaussians variables with mean 0 and variance 1 and the hypothesis that there is a planted principal submatrix b of dimension l for which all upper triangular variables are i.i.d .	we prove that when l < ( 1 - ) n no algorithm that examines only the eigenvalues of x can detect the existence of a hidden gaussian clique , with error probability vanishing as n .	1	1	6	-3.9746768	3.7307134	1
25-9-36	gaussians variables with mean 0 and variance 1 and the hypothesis that there is a planted principal submatrix b of dimension l for which all upper triangular variables are i.i.d .	the result above is an immediate consequence of a more general result on rank-one perturbations of k-dimensional gaussian tensors .	1	1	7	-5.8955593	5.09776	1
25-9-36	gaussians variables with mean 0 and variance 1 and the hypothesis that there is a planted principal submatrix b of dimension l for which all upper triangular variables are i.i.d .	in this context we establish a lower bound on the critical signal-to-noise ratio below which a rank-one signal can not be detected .	1	1	8	-2.3707252	2.542657	1
25-9-36	gaussians with mean 1 and variance 1 , whereas all other upper triangular elements of x not in b are i.i.d .	gaussians variables with mean 0 and variance 1 .	1	2	3	-2.5650396	2.6233006	1
25-9-36	we refer to this as the `gaussian hidden clique problem ' .	gaussians with mean 1 and variance 1 , whereas all other upper triangular elements of x not in b are i.i.d .	0	4	2	-1.4383954	1.568681	1
25-9-36	when l = ( 1 + ) n ( > 0 ) , it is possible to solve this detection problem with probability 1 - on ( 1 ) by computing the spectrum of x and considering the largest eigenvalue of x .	gaussians with mean 1 and variance 1 , whereas all other upper triangular elements of x not in b are i.i.d .	0	5	2	-0.9014698	1.0744301	1
25-9-36	gaussians with mean 1 and variance 1 , whereas all other upper triangular elements of x not in b are i.i.d .	we prove that when l < ( 1 - ) n no algorithm that examines only the eigenvalues of x can detect the existence of a hidden gaussian clique , with error probability vanishing as n .	1	2	6	-1.9804286	2.129444	1
25-9-36	the result above is an immediate consequence of a more general result on rank-one perturbations of k-dimensional gaussian tensors .	gaussians with mean 1 and variance 1 , whereas all other upper triangular elements of x not in b are i.i.d .	0	7	2	5.0295906	-4.494208	0
25-9-36	gaussians with mean 1 and variance 1 , whereas all other upper triangular elements of x not in b are i.i.d .	in this context we establish a lower bound on the critical signal-to-noise ratio below which a rank-one signal can not be detected .	1	2	8	-2.0958552	2.2731922	1
25-9-36	gaussians variables with mean 0 and variance 1 .	we refer to this as the `gaussian hidden clique problem ' .	1	3	4	3.3741617	-3.0385098	0
25-9-36	gaussians variables with mean 0 and variance 1 .	when l = ( 1 + ) n ( > 0 ) , it is possible to solve this detection problem with probability 1 - on ( 1 ) by computing the spectrum of x and considering the largest eigenvalue of x .	1	3	5	2.8111916	-2.5717282	0
25-9-36	we prove that when l < ( 1 - ) n no algorithm that examines only the eigenvalues of x can detect the existence of a hidden gaussian clique , with error probability vanishing as n .	gaussians variables with mean 0 and variance 1 .	0	6	3	-0.49161363	0.7989899	1
25-9-36	the result above is an immediate consequence of a more general result on rank-one perturbations of k-dimensional gaussian tensors .	gaussians variables with mean 0 and variance 1 .	0	7	3	4.547547	-4.1332903	0
25-9-36	in this context we establish a lower bound on the critical signal-to-noise ratio below which a rank-one signal can not be detected .	gaussians variables with mean 0 and variance 1 .	0	8	3	-1.0018178	1.283344	1
25-9-36	we refer to this as the `gaussian hidden clique problem ' .	when l = ( 1 + ) n ( > 0 ) , it is possible to solve this detection problem with probability 1 - on ( 1 ) by computing the spectrum of x and considering the largest eigenvalue of x .	1	4	5	2.4272707	-2.2400813	0
25-9-36	we refer to this as the `gaussian hidden clique problem ' .	we prove that when l < ( 1 - ) n no algorithm that examines only the eigenvalues of x can detect the existence of a hidden gaussian clique , with error probability vanishing as n .	1	4	6	-1.9907024	2.1099994	1
25-9-36	the result above is an immediate consequence of a more general result on rank-one perturbations of k-dimensional gaussian tensors .	we refer to this as the `gaussian hidden clique problem ' .	0	7	4	5.142565	-4.5232286	0
25-9-36	in this context we establish a lower bound on the critical signal-to-noise ratio below which a rank-one signal can not be detected .	we refer to this as the `gaussian hidden clique problem ' .	0	8	4	3.1396375	-2.8871431	0
25-9-36	we prove that when l < ( 1 - ) n no algorithm that examines only the eigenvalues of x can detect the existence of a hidden gaussian clique , with error probability vanishing as n .	when l = ( 1 + ) n ( > 0 ) , it is possible to solve this detection problem with probability 1 - on ( 1 ) by computing the spectrum of x and considering the largest eigenvalue of x .	0	6	5	2.5134048	-2.3908944	0
25-9-36	the result above is an immediate consequence of a more general result on rank-one perturbations of k-dimensional gaussian tensors .	when l = ( 1 + ) n ( > 0 ) , it is possible to solve this detection problem with probability 1 - on ( 1 ) by computing the spectrum of x and considering the largest eigenvalue of x .	0	7	5	5.334972	-4.7437215	0
25-9-36	when l = ( 1 + ) n ( > 0 ) , it is possible to solve this detection problem with probability 1 - on ( 1 ) by computing the spectrum of x and considering the largest eigenvalue of x .	in this context we establish a lower bound on the critical signal-to-noise ratio below which a rank-one signal can not be detected .	1	5	8	-5.3838015	4.921347	1
25-9-36	the result above is an immediate consequence of a more general result on rank-one perturbations of k-dimensional gaussian tensors .	we prove that when l < ( 1 - ) n no algorithm that examines only the eigenvalues of x can detect the existence of a hidden gaussian clique , with error probability vanishing as n .	0	7	6	5.1037073	-4.53319	0
25-9-36	in this context we establish a lower bound on the critical signal-to-noise ratio below which a rank-one signal can not be detected .	we prove that when l < ( 1 - ) n no algorithm that examines only the eigenvalues of x can detect the existence of a hidden gaussian clique , with error probability vanishing as n .	0	8	6	1.8897855	-1.6191292	0
25-9-36	the result above is an immediate consequence of a more general result on rank-one perturbations of k-dimensional gaussian tensors .	in this context we establish a lower bound on the critical signal-to-noise ratio below which a rank-one signal can not be detected .	1	7	8	3.892302	-3.5525815	0
26-5-10	the reasoning is sound : a reduction in variance due to more rapid sampling can outweigh the bias introduced .	to improve the efficiency of monte carlo estimation , practitioners are turning to biased markov chain monte carlo procedures that trade off asymptotic exactness for computational speed .	0	1	0	5.3877897	-4.820679	0
26-5-10	to improve the efficiency of monte carlo estimation , practitioners are turning to biased markov chain monte carlo procedures that trade off asymptotic exactness for computational speed .	however , the inexactness creates new challenges for sampler and parameter selection , since standard measures of sample quality like effective sample size do not account for asymptotic bias .	1	0	2	-5.057352	4.6986895	1
26-5-10	to improve the efficiency of monte carlo estimation , practitioners are turning to biased markov chain monte carlo procedures that trade off asymptotic exactness for computational speed .	to address these challenges , we introduce a new computable quality measure based on stein 's method that quantifies the maximum discrepancy between sample and target expectations over a large class of test functions .	1	0	3	-5.8434687	5.241991	1
26-5-10	we use our tool to compare exact , biased , and deterministic sample sequences and illustrate applications to hyperparameter selection , convergence rate assessment , and quantifying bias-variance tradeoffs in posterior inference .	to improve the efficiency of monte carlo estimation , practitioners are turning to biased markov chain monte carlo procedures that trade off asymptotic exactness for computational speed .	0	4	0	5.5049696	-4.9240007	0
26-5-10	the reasoning is sound : a reduction in variance due to more rapid sampling can outweigh the bias introduced .	however , the inexactness creates new challenges for sampler and parameter selection , since standard measures of sample quality like effective sample size do not account for asymptotic bias .	1	1	2	5.0916886	-4.5892663	0
26-5-10	the reasoning is sound : a reduction in variance due to more rapid sampling can outweigh the bias introduced .	to address these challenges , we introduce a new computable quality measure based on stein 's method that quantifies the maximum discrepancy between sample and target expectations over a large class of test functions .	1	1	3	4.7901363	-4.282031	0
26-5-10	we use our tool to compare exact , biased , and deterministic sample sequences and illustrate applications to hyperparameter selection , convergence rate assessment , and quantifying bias-variance tradeoffs in posterior inference .	the reasoning is sound : a reduction in variance due to more rapid sampling can outweigh the bias introduced .	0	4	1	3.4170775	-3.2505474	0
26-5-10	however , the inexactness creates new challenges for sampler and parameter selection , since standard measures of sample quality like effective sample size do not account for asymptotic bias .	to address these challenges , we introduce a new computable quality measure based on stein 's method that quantifies the maximum discrepancy between sample and target expectations over a large class of test functions .	1	2	3	-5.3944182	5.002652	1
26-5-10	however , the inexactness creates new challenges for sampler and parameter selection , since standard measures of sample quality like effective sample size do not account for asymptotic bias .	we use our tool to compare exact , biased , and deterministic sample sequences and illustrate applications to hyperparameter selection , convergence rate assessment , and quantifying bias-variance tradeoffs in posterior inference .	1	2	4	-5.9679756	5.1200304	1
26-5-10	to address these challenges , we introduce a new computable quality measure based on stein 's method that quantifies the maximum discrepancy between sample and target expectations over a large class of test functions .	we use our tool to compare exact , biased , and deterministic sample sequences and illustrate applications to hyperparameter selection , convergence rate assessment , and quantifying bias-variance tradeoffs in posterior inference .	1	3	4	-5.9849405	5.170375	1
27-6-15	super resolving a low-resolution video is usually handled by either single-image super-resolution ( sr ) or multi-frame sr. single-image sr deals with each video frame independently , and ignores intrinsic temporal dependency of video frames which actually plays a very important role in video super-resolution .	multi-frame sr generally extracts motion information , e.g. , optical flow , to model the temporal dependency , which often shows high computational cost .	1	0	1	-5.836846	5.1784563	1
27-6-15	[CLS] super resolving a low - resolution video is usually handled by either single - image super - resolution ( sr ) or multi - frame sr. single - image sr deals with each video frame independently, and ignores intrinsic temporal dependency of video frames which actually plays a very important	considering that recurrent neural networks ( rnns ) can model long-term contextual information of temporal sequences well , we propose a bidirectional recurrent convolutional network for efficient multi-frame sr .	1	0	2	-6.0111413	5.1963496	1
27-6-15	[CLS] super resolving a low - resolution video is usually handled by either single - image super - resolution ( sr ) or multi - frame sr. single - image sr deals with each video frame independently, and ignores intrinsic temporal dependency of video frames which actually plays	[CLS] different from vanilla rnns, 1 ) the commonly - used recurrent full connections are replaced with weight - sharing convolutional connections and 2 ) conditional convolutional connections from previous input layers to the current hidden layer are added	1	0	3	-5.622386	5.0995507	1
27-6-15	super resolving a low-resolution video is usually handled by either single-image super-resolution ( sr ) or multi-frame sr. single-image sr deals with each video frame independently , and ignores intrinsic temporal dependency of video frames which actually plays a very important role in video super-resolution .	with the powerful temporal dependency modelling , our model can super resolve videos with complex motions and achieve state-of-the-art performance .	1	0	4	-6.0241375	5.1009226	1
27-6-15	super resolving a low-resolution video is usually handled by either single-image super-resolution ( sr ) or multi-frame sr. single-image sr deals with each video frame independently , and ignores intrinsic temporal dependency of video frames which actually plays a very important role in video super-resolution .	due to the cheap convolution operations , our model has a low computational complexity and runs orders of magnitude faster than other multi-frame methods .	1	0	5	-6.040193	5.2127075	1
27-6-15	multi-frame sr generally extracts motion information , e.g. , optical flow , to model the temporal dependency , which often shows high computational cost .	considering that recurrent neural networks ( rnns ) can model long-term contextual information of temporal sequences well , we propose a bidirectional recurrent convolutional network for efficient multi-frame sr .	1	1	2	-4.4670706	4.1526623	1
27-6-15	multi-frame sr generally extracts motion information , e.g. , optical flow , to model the temporal dependency , which often shows high computational cost .	different from vanilla rnns , 1 ) the commonly-used recurrent full connections are replaced with weight-sharing convolutional connections and 2 ) conditional convolutional connections from previous input layers to the current hidden layer are added for enhancing visual-temporal dependency modelling .	1	1	3	-5.972432	5.1393785	1
27-6-15	multi-frame sr generally extracts motion information , e.g. , optical flow , to model the temporal dependency , which often shows high computational cost .	with the powerful temporal dependency modelling , our model can super resolve videos with complex motions and achieve state-of-the-art performance .	1	1	4	-5.92873	5.004907	1
27-6-15	multi-frame sr generally extracts motion information , e.g. , optical flow , to model the temporal dependency , which often shows high computational cost .	due to the cheap convolution operations , our model has a low computational complexity and runs orders of magnitude faster than other multi-frame methods .	1	1	5	-5.959486	5.184049	1
27-6-15	[CLS] different from vanilla rnns, 1 ) the commonly - used recurrent full connections are replaced with weight - sharing convolutional connections and 2 ) conditional convolutional connections from previous input layers to the current hidden layer are added for enhancing visual	considering that recurrent neural networks ( rnns ) can model long-term contextual information of temporal sequences well , we propose a bidirectional recurrent convolutional network for efficient multi-frame sr .	0	3	2	5.111849	-4.5078077	0
27-6-15	considering that recurrent neural networks ( rnns ) can model long-term contextual information of temporal sequences well , we propose a bidirectional recurrent convolutional network for efficient multi-frame sr .	with the powerful temporal dependency modelling , our model can super resolve videos with complex motions and achieve state-of-the-art performance .	1	2	4	-5.9553623	5.1622086	1
27-6-15	considering that recurrent neural networks ( rnns ) can model long-term contextual information of temporal sequences well , we propose a bidirectional recurrent convolutional network for efficient multi-frame sr .	due to the cheap convolution operations , our model has a low computational complexity and runs orders of magnitude faster than other multi-frame methods .	1	2	5	-5.7707357	5.172577	1
27-6-15	different from vanilla rnns , 1 ) the commonly-used recurrent full connections are replaced with weight-sharing convolutional connections and 2 ) conditional convolutional connections from previous input layers to the current hidden layer are added for enhancing visual-temporal dependency modelling .	with the powerful temporal dependency modelling , our model can super resolve videos with complex motions and achieve state-of-the-art performance .	1	3	4	-4.9998837	4.52686	1
27-6-15	due to the cheap convolution operations , our model has a low computational complexity and runs orders of magnitude faster than other multi-frame methods .	different from vanilla rnns , 1 ) the commonly-used recurrent full connections are replaced with weight-sharing convolutional connections and 2 ) conditional convolutional connections from previous input layers to the current hidden layer are added for enhancing visual-temporal dependency modelling .	0	5	3	2.7939405	-2.673847	0
27-6-15	due to the cheap convolution operations , our model has a low computational complexity and runs orders of magnitude faster than other multi-frame methods .	with the powerful temporal dependency modelling , our model can super resolve videos with complex motions and achieve state-of-the-art performance .	0	5	4	-3.666918	3.490868	1
28-8-28	expectation propagation is a very popular algorithm for variational inference , but comes with few theoretical guarantees .	in this article , we prove that the approximation errors made by ep can be bounded .	1	0	1	-5.9464707	5.1573915	1
28-8-28	expectation propagation is a very popular algorithm for variational inference , but comes with few theoretical guarantees .	our bounds have an asymptotic interpretation in the number n of datapoints , which allows us to study ep 's convergence with respect to the true posterior .	1	0	2	-5.973162	5.2057967	1
28-8-28	in particular , we show that ep converges at a rate of o ( n-2 ) for the mean , up to an order of magnitude faster than the traditional gaussian approximation at the mode .	expectation propagation is a very popular algorithm for variational inference , but comes with few theoretical guarantees .	0	3	0	5.5854197	-4.985752	0
28-8-28	expectation propagation is a very popular algorithm for variational inference , but comes with few theoretical guarantees .	we also give similar asymptotic expansions for moments of order 2 to 4 , as well as excess kullback-leibler cost ( defined as the additional kl cost incurred by using ep rather than the ideal gaussian approximation ) .	1	0	4	-5.9199996	5.1609683	1
28-8-28	all these expansions highlight the superior convergence properties of ep .	expectation propagation is a very popular algorithm for variational inference , but comes with few theoretical guarantees .	0	5	0	5.49289	-4.8578935	0
28-8-28	our approach for deriving those results is likely applicable to many similar approximate inference methods .	expectation propagation is a very popular algorithm for variational inference , but comes with few theoretical guarantees .	0	6	0	5.6168857	-5.0039263	0
28-8-28	in addition , we introduce bounds on the moments of log-concave distributions that may be of independent interest .	expectation propagation is a very popular algorithm for variational inference , but comes with few theoretical guarantees .	0	7	0	5.5529804	-4.9125423	0
28-8-28	our bounds have an asymptotic interpretation in the number n of datapoints , which allows us to study ep 's convergence with respect to the true posterior .	in this article , we prove that the approximation errors made by ep can be bounded .	0	2	1	4.973999	-4.362721	0
28-8-28	in particular , we show that ep converges at a rate of o ( n-2 ) for the mean , up to an order of magnitude faster than the traditional gaussian approximation at the mode .	in this article , we prove that the approximation errors made by ep can be bounded .	0	3	1	3.8097334	-3.457536	0
28-8-28	we also give similar asymptotic expansions for moments of order 2 to 4 , as well as excess kullback-leibler cost ( defined as the additional kl cost incurred by using ep rather than the ideal gaussian approximation ) .	in this article , we prove that the approximation errors made by ep can be bounded .	0	4	1	4.8070507	-4.258602	0
28-8-28	all these expansions highlight the superior convergence properties of ep .	in this article , we prove that the approximation errors made by ep can be bounded .	0	5	1	-3.3079808	3.270717	1
28-8-28	in this article , we prove that the approximation errors made by ep can be bounded .	our approach for deriving those results is likely applicable to many similar approximate inference methods .	1	1	6	-5.935501	5.00745	1
28-8-28	in this article , we prove that the approximation errors made by ep can be bounded .	in addition , we introduce bounds on the moments of log-concave distributions that may be of independent interest .	1	1	7	-5.6472206	4.986726	1
28-8-28	in particular , we show that ep converges at a rate of o ( n-2 ) for the mean , up to an order of magnitude faster than the traditional gaussian approximation at the mode .	our bounds have an asymptotic interpretation in the number n of datapoints , which allows us to study ep 's convergence with respect to the true posterior .	0	3	2	4.1799135	-3.810325	0
28-8-28	our bounds have an asymptotic interpretation in the number n of datapoints , which allows us to study ep 's convergence with respect to the true posterior .	we also give similar asymptotic expansions for moments of order 2 to 4 , as well as excess kullback-leibler cost ( defined as the additional kl cost incurred by using ep rather than the ideal gaussian approximation ) .	1	2	4	-5.934502	5.1437483	1
28-8-28	our bounds have an asymptotic interpretation in the number n of datapoints , which allows us to study ep 's convergence with respect to the true posterior .	all these expansions highlight the superior convergence properties of ep .	1	2	5	-0.9783336	1.2068835	1
28-8-28	our bounds have an asymptotic interpretation in the number n of datapoints , which allows us to study ep 's convergence with respect to the true posterior .	our approach for deriving those results is likely applicable to many similar approximate inference methods .	1	2	6	-5.835927	4.91717	1
28-8-28	our bounds have an asymptotic interpretation in the number n of datapoints , which allows us to study ep 's convergence with respect to the true posterior .	in addition , we introduce bounds on the moments of log-concave distributions that may be of independent interest .	1	2	7	-3.8656688	3.766059	1
28-8-28	we also give similar asymptotic expansions for moments of order 2 to 4 , as well as excess kullback-leibler cost ( defined as the additional kl cost incurred by using ep rather than the ideal gaussian approximation ) .	in particular , we show that ep converges at a rate of o ( n-2 ) for the mean , up to an order of magnitude faster than the traditional gaussian approximation at the mode .	0	4	3	2.9711392	-2.8321962	0
28-8-28	in particular , we show that ep converges at a rate of o ( n-2 ) for the mean , up to an order of magnitude faster than the traditional gaussian approximation at the mode .	all these expansions highlight the superior convergence properties of ep .	1	3	5	1.6520338	-1.5099818	0
28-8-28	in particular , we show that ep converges at a rate of o ( n-2 ) for the mean , up to an order of magnitude faster than the traditional gaussian approximation at the mode .	our approach for deriving those results is likely applicable to many similar approximate inference methods .	1	3	6	-5.4051304	4.83507	1
28-8-28	in particular , we show that ep converges at a rate of o ( n-2 ) for the mean , up to an order of magnitude faster than the traditional gaussian approximation at the mode .	in addition , we introduce bounds on the moments of log-concave distributions that may be of independent interest .	1	3	7	-2.7844489	2.8227606	1
28-8-28	we also give similar asymptotic expansions for moments of order 2 to 4 , as well as excess kullback-leibler cost ( defined as the additional kl cost incurred by using ep rather than the ideal gaussian approximation ) .	all these expansions highlight the superior convergence properties of ep .	1	4	5	3.1606517	-2.9881802	0
28-8-28	we also give similar asymptotic expansions for moments of order 2 to 4 , as well as excess kullback-leibler cost ( defined as the additional kl cost incurred by using ep rather than the ideal gaussian approximation ) .	our approach for deriving those results is likely applicable to many similar approximate inference methods .	1	4	6	-4.2103457	3.9335494	1
28-8-28	we also give similar asymptotic expansions for moments of order 2 to 4 , as well as excess kullback-leibler cost ( defined as the additional kl cost incurred by using ep rather than the ideal gaussian approximation ) .	in addition , we introduce bounds on the moments of log-concave distributions that may be of independent interest .	1	4	7	1.2868263	-0.89336693	0
28-8-28	our approach for deriving those results is likely applicable to many similar approximate inference methods .	all these expansions highlight the superior convergence properties of ep .	0	6	5	5.0215464	-4.496051	0
28-8-28	all these expansions highlight the superior convergence properties of ep .	in addition , we introduce bounds on the moments of log-concave distributions that may be of independent interest .	1	5	7	-5.527283	4.94824	1
28-8-28	our approach for deriving those results is likely applicable to many similar approximate inference methods .	in addition , we introduce bounds on the moments of log-concave distributions that may be of independent interest .	1	6	7	2.771707	-2.6193018	0
29-8-28	nonlinear embedding algorithms such as stochastic neighbor embedding do dimensionality reduction by optimizing an objective function involving similarities between pairs of input patterns .	the result is a low-dimensional projection of each input pattern .	1	0	1	-5.9213066	5.200243	1
29-8-28	a common way to define an out-of-sample mapping is to optimize the objective directly over a parametric mapping of the inputs , such as a neural net .	nonlinear embedding algorithms such as stochastic neighbor embedding do dimensionality reduction by optimizing an objective function involving similarities between pairs of input patterns .	0	2	0	5.465019	-4.8974824	0
29-8-28	nonlinear embedding algorithms such as stochastic neighbor embedding do dimensionality reduction by optimizing an objective function involving similarities between pairs of input patterns .	this can be done using the chain rule and a nonlinear optimizer , but is very slow , because the objective involves a quadratic number of terms each dependent on the entire mapping 's parameters .	1	0	3	-5.9634743	5.1928854	1
29-8-28	using the method of auxiliary coordinates , we derive a training algorithm that works by alternating steps that train an auxiliary embedding with steps that train the mapping .	nonlinear embedding algorithms such as stochastic neighbor embedding do dimensionality reduction by optimizing an objective function involving similarities between pairs of input patterns .	0	4	0	5.198289	-4.594487	0
29-8-28	this has two advantages : 1 ) the algorithm is universal in that a specific learning algorithm for any choice of embedding and mapping can be constructed by simply reusing existing algorithms for the embedding and for the mapping .	nonlinear embedding algorithms such as stochastic neighbor embedding do dimensionality reduction by optimizing an objective function involving similarities between pairs of input patterns .	0	5	0	5.6029663	-4.955925	0
29-8-28	a user can then try possible mappings and embeddings with less effort .	nonlinear embedding algorithms such as stochastic neighbor embedding do dimensionality reduction by optimizing an objective function involving similarities between pairs of input patterns .	0	6	0	5.4565964	-4.8706903	0
29-8-28	2 ) the algorithm is fast , and it can reuse n -body methods developed for nonlinear embeddings , yielding linear-time iterations .	nonlinear embedding algorithms such as stochastic neighbor embedding do dimensionality reduction by optimizing an objective function involving similarities between pairs of input patterns .	0	7	0	5.6728926	-5.098446	0
29-8-28	the result is a low-dimensional projection of each input pattern .	a common way to define an out-of-sample mapping is to optimize the objective directly over a parametric mapping of the inputs , such as a neural net .	1	1	2	5.3016257	-4.7138443	0
29-8-28	the result is a low-dimensional projection of each input pattern .	this can be done using the chain rule and a nonlinear optimizer , but is very slow , because the objective involves a quadratic number of terms each dependent on the entire mapping 's parameters .	1	1	3	0.8890202	-0.57757306	0
29-8-28	using the method of auxiliary coordinates , we derive a training algorithm that works by alternating steps that train an auxiliary embedding with steps that train the mapping .	the result is a low-dimensional projection of each input pattern .	0	4	1	-1.2578852	1.4970013	1
29-8-28	the result is a low-dimensional projection of each input pattern .	this has two advantages : 1 ) the algorithm is universal in that a specific learning algorithm for any choice of embedding and mapping can be constructed by simply reusing existing algorithms for the embedding and for the mapping .	1	1	5	-4.182046	3.9297411	1
29-8-28	a user can then try possible mappings and embeddings with less effort .	the result is a low-dimensional projection of each input pattern .	0	6	1	0.3810678	-0.22694008	0
29-8-28	the result is a low-dimensional projection of each input pattern .	2 ) the algorithm is fast , and it can reuse n -body methods developed for nonlinear embeddings , yielding linear-time iterations .	1	1	7	-3.878575	3.6685905	1
29-8-28	this can be done using the chain rule and a nonlinear optimizer , but is very slow , because the objective involves a quadratic number of terms each dependent on the entire mapping 's parameters .	a common way to define an out-of-sample mapping is to optimize the objective directly over a parametric mapping of the inputs , such as a neural net .	0	3	2	4.9551077	-4.3898115	0
29-8-28	using the method of auxiliary coordinates , we derive a training algorithm that works by alternating steps that train an auxiliary embedding with steps that train the mapping .	a common way to define an out-of-sample mapping is to optimize the objective directly over a parametric mapping of the inputs , such as a neural net .	0	4	2	5.424834	-4.745286	0
29-8-28	this has two advantages : 1 ) the algorithm is universal in that a specific learning algorithm for any choice of embedding and mapping can be constructed by simply reusing existing algorithms for the embedding and for the mapping .	a common way to define an out-of-sample mapping is to optimize the objective directly over a parametric mapping of the inputs , such as a neural net .	0	5	2	5.6006207	-4.926175	0
29-8-28	a common way to define an out-of-sample mapping is to optimize the objective directly over a parametric mapping of the inputs , such as a neural net .	a user can then try possible mappings and embeddings with less effort .	1	2	6	-5.8607283	5.210134	1
29-8-28	2 ) the algorithm is fast , and it can reuse n -body methods developed for nonlinear embeddings , yielding linear-time iterations .	a common way to define an out-of-sample mapping is to optimize the objective directly over a parametric mapping of the inputs , such as a neural net .	0	7	2	5.645302	-5.040324	0
29-8-28	using the method of auxiliary coordinates , we derive a training algorithm that works by alternating steps that train an auxiliary embedding with steps that train the mapping .	this can be done using the chain rule and a nonlinear optimizer , but is very slow , because the objective involves a quadratic number of terms each dependent on the entire mapping 's parameters .	0	4	3	3.351077	-3.157587	0
29-8-28	this can be done using the chain rule and a nonlinear optimizer , but is very slow , because the objective involves a quadratic number of terms each dependent on the entire mapping 's parameters .	this has two advantages : 1 ) the algorithm is universal in that a specific learning algorithm for any choice of embedding and mapping can be constructed by simply reusing existing algorithms for the embedding and for the mapping .	1	3	5	-5.254877	4.8403153	1
29-8-28	a user can then try possible mappings and embeddings with less effort .	this can be done using the chain rule and a nonlinear optimizer , but is very slow , because the objective involves a quadratic number of terms each dependent on the entire mapping 's parameters .	0	6	3	-0.44464058	0.63545257	1
29-8-28	2 ) the algorithm is fast , and it can reuse n -body methods developed for nonlinear embeddings , yielding linear-time iterations .	this can be done using the chain rule and a nonlinear optimizer , but is very slow , because the objective involves a quadratic number of terms each dependent on the entire mapping 's parameters .	0	7	3	4.667388	-4.2351866	0
29-8-28	this has two advantages : 1 ) the algorithm is universal in that a specific learning algorithm for any choice of embedding and mapping can be constructed by simply reusing existing algorithms for the embedding and for the mapping .	using the method of auxiliary coordinates , we derive a training algorithm that works by alternating steps that train an auxiliary embedding with steps that train the mapping .	0	5	4	3.706623	-3.3258097	0
29-8-28	using the method of auxiliary coordinates , we derive a training algorithm that works by alternating steps that train an auxiliary embedding with steps that train the mapping .	a user can then try possible mappings and embeddings with less effort .	1	4	6	-1.2758684	1.4492633	1
29-8-28	using the method of auxiliary coordinates , we derive a training algorithm that works by alternating steps that train an auxiliary embedding with steps that train the mapping .	2 ) the algorithm is fast , and it can reuse n -body methods developed for nonlinear embeddings , yielding linear-time iterations .	1	4	7	-5.7248936	5.1278634	1
29-8-28	this has two advantages : 1 ) the algorithm is universal in that a specific learning algorithm for any choice of embedding and mapping can be constructed by simply reusing existing algorithms for the embedding and for the mapping .	a user can then try possible mappings and embeddings with less effort .	1	5	6	1.6570013	-1.4260198	0
29-8-28	2 ) the algorithm is fast , and it can reuse n -body methods developed for nonlinear embeddings , yielding linear-time iterations .	this has two advantages : 1 ) the algorithm is universal in that a specific learning algorithm for any choice of embedding and mapping can be constructed by simply reusing existing algorithms for the embedding and for the mapping .	0	7	5	4.089451	-3.7734883	0
29-8-28	a user can then try possible mappings and embeddings with less effort .	2 ) the algorithm is fast , and it can reuse n -body methods developed for nonlinear embeddings , yielding linear-time iterations .	1	6	7	-4.371029	4.07904	1
30-5-10	here we introduce a new model of natural textures based on the feature spaces of convolutional neural networks optimised for object recognition .	samples from the model are of high perceptual quality demonstrating the generative power of neural networks trained in a purely discriminative fashion .	1	0	1	-5.942155	5.1579895	1
30-5-10	here we introduce a new model of natural textures based on the feature spaces of convolutional neural networks optimised for object recognition .	within the model , textures are represented by the correlations between feature maps in several layers of the network .	1	0	2	-5.7016773	5.1622105	1
30-5-10	we show that across layers the texture representations increasingly capture the statistical properties of natural images while making object information more and more explicit .	here we introduce a new model of natural textures based on the feature spaces of convolutional neural networks optimised for object recognition .	0	3	0	3.8889713	-3.5015192	0
30-5-10	here we introduce a new model of natural textures based on the feature spaces of convolutional neural networks optimised for object recognition .	the model provides a new tool to generate stimuli for neuroscience and might offer insights into the deep representations learned by convolutional neural networks .	1	0	4	-5.9548483	5.1590986	1
30-5-10	samples from the model are of high perceptual quality demonstrating the generative power of neural networks trained in a purely discriminative fashion .	within the model , textures are represented by the correlations between feature maps in several layers of the network .	1	1	2	4.1861773	-3.7847128	0
30-5-10	samples from the model are of high perceptual quality demonstrating the generative power of neural networks trained in a purely discriminative fashion .	we show that across layers the texture representations increasingly capture the statistical properties of natural images while making object information more and more explicit .	1	1	3	4.401437	-3.939996	0
30-5-10	the model provides a new tool to generate stimuli for neuroscience and might offer insights into the deep representations learned by convolutional neural networks .	samples from the model are of high perceptual quality demonstrating the generative power of neural networks trained in a purely discriminative fashion .	0	4	1	2.7780068	-2.630187	0
30-5-10	we show that across layers the texture representations increasingly capture the statistical properties of natural images while making object information more and more explicit .	within the model , textures are represented by the correlations between feature maps in several layers of the network .	0	3	2	4.564972	-4.0892754	0
30-5-10	the model provides a new tool to generate stimuli for neuroscience and might offer insights into the deep representations learned by convolutional neural networks .	within the model , textures are represented by the correlations between feature maps in several layers of the network .	0	4	2	4.830081	-4.3148775	0
30-5-10	we show that across layers the texture representations increasingly capture the statistical properties of natural images while making object information more and more explicit .	the model provides a new tool to generate stimuli for neuroscience and might offer insights into the deep representations learned by convolutional neural networks .	1	3	4	-5.9299765	5.0499897	1
31-6-15	whereas distributed estimation of sample mean statistics has been the subject of a good deal of attention , computation of u statistics , relying on more expensive averaging over pairs of observations , is a less investigated area .	efficient and robust algorithms for decentralized estimation in networks are essential to many distributed systems .	0	1	0	5.3223805	-4.703721	0
31-6-15	efficient and robust algorithms for decentralized estimation in networks are essential to many distributed systems .	yet , such data functionals are essential to describe global properties of a statistical population , with important examples including area under the curve , empirical variance , gini mean difference and within-cluster point scatter .	1	0	2	-5.9100266	5.201874	1
31-6-15	efficient and robust algorithms for decentralized estimation in networks are essential to many distributed systems .	this paper proposes new synchronous and asynchronous randomized gossip algorithms which simultaneously propagate data across the network and maintain local estimates of the u -statistic of interest .	1	0	3	-5.869272	5.207627	1
31-6-15	we establish convergence rate bounds of o ( 1/t ) and o ( log t/t ) for the synchronous and asynchronous cases respectively , where t is the number of iterations , with explicit data and network dependent terms .	efficient and robust algorithms for decentralized estimation in networks are essential to many distributed systems .	0	4	0	5.719946	-5.1306458	0
31-6-15	beyond favorable comparisons in terms of rate analysis , numerical experiments provide empirical evidence the proposed algorithms surpasses the previously introduced approach .	efficient and robust algorithms for decentralized estimation in networks are essential to many distributed systems .	0	5	0	5.610689	-5.0112557	0
31-6-15	yet , such data functionals are essential to describe global properties of a statistical population , with important examples including area under the curve , empirical variance , gini mean difference and within-cluster point scatter .	whereas distributed estimation of sample mean statistics has been the subject of a good deal of attention , computation of u statistics , relying on more expensive averaging over pairs of observations , is a less investigated area .	0	2	1	4.283118	-3.826488	0
31-6-15	whereas distributed estimation of sample mean statistics has been the subject of a good deal of attention , computation of u statistics , relying on more expensive averaging over pairs of observations , is a less investigated area .	this paper proposes new synchronous and asynchronous randomized gossip algorithms which simultaneously propagate data across the network and maintain local estimates of the u -statistic of interest .	1	1	3	-5.8068023	5.2278347	1
31-6-15	we establish convergence rate bounds of o ( 1/t ) and o ( log t/t ) for the synchronous and asynchronous cases respectively , where t is the number of iterations , with explicit data and network dependent terms .	whereas distributed estimation of sample mean statistics has been the subject of a good deal of attention , computation of u statistics , relying on more expensive averaging over pairs of observations , is a less investigated area .	0	4	1	5.601452	-4.9475408	0
31-6-15	beyond favorable comparisons in terms of rate analysis , numerical experiments provide empirical evidence the proposed algorithms surpasses the previously introduced approach .	whereas distributed estimation of sample mean statistics has been the subject of a good deal of attention , computation of u statistics , relying on more expensive averaging over pairs of observations , is a less investigated area .	0	5	1	5.57483	-4.9014854	0
31-6-15	this paper proposes new synchronous and asynchronous randomized gossip algorithms which simultaneously propagate data across the network and maintain local estimates of the u -statistic of interest .	yet , such data functionals are essential to describe global properties of a statistical population , with important examples including area under the curve , empirical variance , gini mean difference and within-cluster point scatter .	0	3	2	4.886978	-4.3169537	0
31-6-15	yet , such data functionals are essential to describe global properties of a statistical population , with important examples including area under the curve , empirical variance , gini mean difference and within-cluster point scatter .	we establish convergence rate bounds of o ( 1/t ) and o ( log t/t ) for the synchronous and asynchronous cases respectively , where t is the number of iterations , with explicit data and network dependent terms .	1	2	4	-5.8183327	5.2636695	1
31-6-15	beyond favorable comparisons in terms of rate analysis , numerical experiments provide empirical evidence the proposed algorithms surpasses the previously introduced approach .	yet , such data functionals are essential to describe global properties of a statistical population , with important examples including area under the curve , empirical variance , gini mean difference and within-cluster point scatter .	0	5	2	5.4400797	-4.767737	0
31-6-15	we establish convergence rate bounds of o ( 1/t ) and o ( log t/t ) for the synchronous and asynchronous cases respectively , where t is the number of iterations , with explicit data and network dependent terms .	this paper proposes new synchronous and asynchronous randomized gossip algorithms which simultaneously propagate data across the network and maintain local estimates of the u -statistic of interest .	0	4	3	5.435868	-4.7755127	0
31-6-15	beyond favorable comparisons in terms of rate analysis , numerical experiments provide empirical evidence the proposed algorithms surpasses the previously introduced approach .	this paper proposes new synchronous and asynchronous randomized gossip algorithms which simultaneously propagate data across the network and maintain local estimates of the u -statistic of interest .	0	5	3	5.500571	-4.7933254	0
31-6-15	we establish convergence rate bounds of o ( 1/t ) and o ( log t/t ) for the synchronous and asynchronous cases respectively , where t is the number of iterations , with explicit data and network dependent terms .	beyond favorable comparisons in terms of rate analysis , numerical experiments provide empirical evidence the proposed algorithms surpasses the previously introduced approach .	1	4	5	-5.7109866	5.043992	1
32-6-15	this paper presents a methodology for creating streaming , distributed inference algorithms for bayesian nonparametric ( bnp ) models .	in the proposed framework , processing nodes receive a sequence of data minibatches , compute a variational posterior for each , and make asynchronous streaming updates to a central model .	1	0	1	-5.916601	5.2140713	1
32-6-15	in contrast to previous algorithms , the proposed framework is truly streaming , distributed , asynchronous , learning-rate-free , and truncation-free .	this paper presents a methodology for creating streaming , distributed inference algorithms for bayesian nonparametric ( bnp ) models .	0	2	0	5.6446524	-5.041196	0
32-6-15	this paper presents a methodology for creating streaming , distributed inference algorithms for bayesian nonparametric ( bnp ) models .	the key challenge in developing the framework , arising from the fact that bnp models do not impose an inherent ordering on their components , is finding the correspondence between minibatch and central bnp posterior components before performing each update .	1	0	3	-5.8971367	5.1585417	1
32-6-15	this paper presents a methodology for creating streaming , distributed inference algorithms for bayesian nonparametric ( bnp ) models .	to address this , the paper develops a combinatorial optimization problem over component correspondences , and provides an efficient solution technique .	1	0	4	-5.9856286	5.213334	1
32-6-15	the paper concludes with an application of the methodology to the dp mixture model , with experimental results demonstrating its practical scalability and performance .	this paper presents a methodology for creating streaming , distributed inference algorithms for bayesian nonparametric ( bnp ) models .	0	5	0	5.6173205	-5.035436	0
32-6-15	in the proposed framework , processing nodes receive a sequence of data minibatches , compute a variational posterior for each , and make asynchronous streaming updates to a central model .	in contrast to previous algorithms , the proposed framework is truly streaming , distributed , asynchronous , learning-rate-free , and truncation-free .	1	1	2	-1.5529782	1.7259325	1
32-6-15	in the proposed framework , processing nodes receive a sequence of data minibatches , compute a variational posterior for each , and make asynchronous streaming updates to a central model .	the key challenge in developing the framework , arising from the fact that bnp models do not impose an inherent ordering on their components , is finding the correspondence between minibatch and central bnp posterior components before performing each update .	1	1	3	-3.6743493	3.4837615	1
32-6-15	to address this , the paper develops a combinatorial optimization problem over component correspondences , and provides an efficient solution technique .	in the proposed framework , processing nodes receive a sequence of data minibatches , compute a variational posterior for each , and make asynchronous streaming updates to a central model .	0	4	1	2.8137598	-2.57254	0
32-6-15	in the proposed framework , processing nodes receive a sequence of data minibatches , compute a variational posterior for each , and make asynchronous streaming updates to a central model .	the paper concludes with an application of the methodology to the dp mixture model , with experimental results demonstrating its practical scalability and performance .	1	1	5	-5.9411554	5.0590763	1
32-6-15	in contrast to previous algorithms , the proposed framework is truly streaming , distributed , asynchronous , learning-rate-free , and truncation-free .	the key challenge in developing the framework , arising from the fact that bnp models do not impose an inherent ordering on their components , is finding the correspondence between minibatch and central bnp posterior components before performing each update .	1	2	3	-1.8994633	1.9823848	1
32-6-15	to address this , the paper develops a combinatorial optimization problem over component correspondences , and provides an efficient solution technique .	in contrast to previous algorithms , the proposed framework is truly streaming , distributed , asynchronous , learning-rate-free , and truncation-free .	0	4	2	1.6576287	-1.5400486	0
32-6-15	the paper concludes with an application of the methodology to the dp mixture model , with experimental results demonstrating its practical scalability and performance .	in contrast to previous algorithms , the proposed framework is truly streaming , distributed , asynchronous , learning-rate-free , and truncation-free .	0	5	2	5.0219626	-4.508868	0
32-6-15	the key challenge in developing the framework , arising from the fact that bnp models do not impose an inherent ordering on their components , is finding the correspondence between minibatch and central bnp posterior components before performing each update .	to address this , the paper develops a combinatorial optimization problem over component correspondences , and provides an efficient solution technique .	1	3	4	-5.616939	5.027251	1
32-6-15	the key challenge in developing the framework , arising from the fact that bnp models do not impose an inherent ordering on their components , is finding the correspondence between minibatch and central bnp posterior components before performing each update .	the paper concludes with an application of the methodology to the dp mixture model , with experimental results demonstrating its practical scalability and performance .	1	3	5	-5.991272	5.1421895	1
32-6-15	the paper concludes with an application of the methodology to the dp mixture model , with experimental results demonstrating its practical scalability and performance .	to address this , the paper develops a combinatorial optimization problem over component correspondences , and provides an efficient solution technique .	0	5	4	4.593552	-4.106257	0
33-6-15	in this paper , we investigate whether we can extract these biases and transfer them into a machine recognition system .	although the human visual system can recognize many concepts under challenging conditions , it still has some biases .	0	1	0	5.675788	-5.0743504	0
33-6-15	we introduce a novel method that , inspired by well-known tools in human psychophysics , estimates the biases that the human visual system might use for recognition , but in computer vision feature spaces .	although the human visual system can recognize many concepts under challenging conditions , it still has some biases .	0	2	0	5.507054	-4.8179836	0
33-6-15	our experiments are surprising , and suggest that classifiers from the human visual system can be transferred into a machine with some success .	although the human visual system can recognize many concepts under challenging conditions , it still has some biases .	0	3	0	5.626716	-5.006019	0
33-6-15	although the human visual system can recognize many concepts under challenging conditions , it still has some biases .	since these classifiers seem to capture favorable biases in the human visual system , we further present an svm formulation that constrains the orientation of the svm hyperplane to agree with the bias from human visual system .	1	0	4	-5.831628	5.033457	1
33-6-15	our results suggest that transferring this human bias into machines may help object recognition systems generalize across datasets and perform better when very little training data is available .	although the human visual system can recognize many concepts under challenging conditions , it still has some biases .	0	5	0	5.631201	-5.0021625	0
33-6-15	in this paper , we investigate whether we can extract these biases and transfer them into a machine recognition system .	we introduce a novel method that , inspired by well-known tools in human psychophysics , estimates the biases that the human visual system might use for recognition , but in computer vision feature spaces .	1	1	2	-4.2338815	4.0078297	1
33-6-15	our experiments are surprising , and suggest that classifiers from the human visual system can be transferred into a machine with some success .	in this paper , we investigate whether we can extract these biases and transfer them into a machine recognition system .	0	3	1	5.493084	-4.9268847	0
33-6-15	in this paper , we investigate whether we can extract these biases and transfer them into a machine recognition system .	since these classifiers seem to capture favorable biases in the human visual system , we further present an svm formulation that constrains the orientation of the svm hyperplane to agree with the bias from human visual system .	1	1	4	-5.98545	5.1704226	1
33-6-15	our results suggest that transferring this human bias into machines may help object recognition systems generalize across datasets and perform better when very little training data is available .	in this paper , we investigate whether we can extract these biases and transfer them into a machine recognition system .	0	5	1	5.5663457	-4.9582777	0
33-6-15	our experiments are surprising , and suggest that classifiers from the human visual system can be transferred into a machine with some success .	we introduce a novel method that , inspired by well-known tools in human psychophysics , estimates the biases that the human visual system might use for recognition , but in computer vision feature spaces .	0	3	2	5.4761868	-4.922449	0
33-6-15	we introduce a novel method that , inspired by well-known tools in human psychophysics , estimates the biases that the human visual system might use for recognition , but in computer vision feature spaces .	since these classifiers seem to capture favorable biases in the human visual system , we further present an svm formulation that constrains the orientation of the svm hyperplane to agree with the bias from human visual system .	1	2	4	-5.915872	5.21426	1
33-6-15	our results suggest that transferring this human bias into machines may help object recognition systems generalize across datasets and perform better when very little training data is available .	we introduce a novel method that , inspired by well-known tools in human psychophysics , estimates the biases that the human visual system might use for recognition , but in computer vision feature spaces .	0	5	2	5.5899153	-4.9295835	0
33-6-15	our experiments are surprising , and suggest that classifiers from the human visual system can be transferred into a machine with some success .	since these classifiers seem to capture favorable biases in the human visual system , we further present an svm formulation that constrains the orientation of the svm hyperplane to agree with the bias from human visual system .	1	3	4	-5.2241063	4.80877	1
33-6-15	our experiments are surprising , and suggest that classifiers from the human visual system can be transferred into a machine with some success .	our results suggest that transferring this human bias into machines may help object recognition systems generalize across datasets and perform better when very little training data is available .	1	3	5	-5.462909	4.8744664	1
33-6-15	since these classifiers seem to capture favorable biases in the human visual system , we further present an svm formulation that constrains the orientation of the svm hyperplane to agree with the bias from human visual system .	our results suggest that transferring this human bias into machines may help object recognition systems generalize across datasets and perform better when very little training data is available .	1	4	5	-3.9493499	3.6311054	1
34-8-28	maximum a-posteriori ( map ) inference is an important task for many applications .	although the standard formulation gives rise to a hard combinatorial optimization problem , several effective approximations have been proposed and studied in recent years .	1	0	1	-5.9538374	5.1373453	1
34-8-28	we focus on linear programming ( lp ) relaxations , which have achieved state-of-the-art performance in many applications .	maximum a-posteriori ( map ) inference is an important task for many applications .	0	2	0	5.5823636	-4.944794	0
34-8-28	however , optimization of the resulting program is in general challenging due to non-smoothness and complex non-separable constraints .	maximum a-posteriori ( map ) inference is an important task for many applications .	0	3	0	5.6853724	-5.076455	0
34-8-28	therefore , in this work we study the benefits of augmenting the objective function of the relaxation with strong convexity .	maximum a-posteriori ( map ) inference is an important task for many applications .	0	4	0	5.651683	-4.9910593	0
34-8-28	maximum a-posteriori ( map ) inference is an important task for many applications .	specifically , we introduce strong convexity by adding a quadratic term to the lp relaxation objective .	1	0	5	-5.9476256	5.101904	1
34-8-28	we provide theoretical guarantees for the resulting programs , bounding the difference between their optimal value and the original optimum .	maximum a-posteriori ( map ) inference is an important task for many applications .	0	6	0	5.7441187	-5.132839	0
34-8-28	further , we propose suitable optimization algorithms and analyze their convergence .	maximum a-posteriori ( map ) inference is an important task for many applications .	0	7	0	5.7001133	-5.068754	0
34-8-28	we focus on linear programming ( lp ) relaxations , which have achieved state-of-the-art performance in many applications .	although the standard formulation gives rise to a hard combinatorial optimization problem , several effective approximations have been proposed and studied in recent years .	0	2	1	-3.467415	3.4348507	1
34-8-28	however , optimization of the resulting program is in general challenging due to non-smoothness and complex non-separable constraints .	although the standard formulation gives rise to a hard combinatorial optimization problem , several effective approximations have been proposed and studied in recent years .	0	3	1	2.7536955	-2.5569305	0
34-8-28	therefore , in this work we study the benefits of augmenting the objective function of the relaxation with strong convexity .	although the standard formulation gives rise to a hard combinatorial optimization problem , several effective approximations have been proposed and studied in recent years .	0	4	1	4.678357	-4.187414	0
34-8-28	although the standard formulation gives rise to a hard combinatorial optimization problem , several effective approximations have been proposed and studied in recent years .	specifically , we introduce strong convexity by adding a quadratic term to the lp relaxation objective .	1	1	5	-5.8584647	5.145116	1
34-8-28	we provide theoretical guarantees for the resulting programs , bounding the difference between their optimal value and the original optimum .	although the standard formulation gives rise to a hard combinatorial optimization problem , several effective approximations have been proposed and studied in recent years .	0	6	1	4.938368	-4.424774	0
34-8-28	further , we propose suitable optimization algorithms and analyze their convergence .	although the standard formulation gives rise to a hard combinatorial optimization problem , several effective approximations have been proposed and studied in recent years .	0	7	1	5.3506184	-4.744165	0
34-8-28	we focus on linear programming ( lp ) relaxations , which have achieved state-of-the-art performance in many applications .	however , optimization of the resulting program is in general challenging due to non-smoothness and complex non-separable constraints .	1	2	3	-4.8692427	4.5139523	1
34-8-28	we focus on linear programming ( lp ) relaxations , which have achieved state-of-the-art performance in many applications .	therefore , in this work we study the benefits of augmenting the objective function of the relaxation with strong convexity .	1	2	4	-5.9396133	5.1616573	1
34-8-28	we focus on linear programming ( lp ) relaxations , which have achieved state-of-the-art performance in many applications .	specifically , we introduce strong convexity by adding a quadratic term to the lp relaxation objective .	1	2	5	-5.9664774	5.1538873	1
34-8-28	we provide theoretical guarantees for the resulting programs , bounding the difference between their optimal value and the original optimum .	we focus on linear programming ( lp ) relaxations , which have achieved state-of-the-art performance in many applications .	0	6	2	5.5197353	-4.966589	0
34-8-28	further , we propose suitable optimization algorithms and analyze their convergence .	we focus on linear programming ( lp ) relaxations , which have achieved state-of-the-art performance in many applications .	0	7	2	5.4671416	-4.8723235	0
34-8-28	therefore , in this work we study the benefits of augmenting the objective function of the relaxation with strong convexity .	however , optimization of the resulting program is in general challenging due to non-smoothness and complex non-separable constraints .	0	4	3	4.000791	-3.6675718	0
34-8-28	however , optimization of the resulting program is in general challenging due to non-smoothness and complex non-separable constraints .	specifically , we introduce strong convexity by adding a quadratic term to the lp relaxation objective .	1	3	5	-5.823227	5.196207	1
34-8-28	however , optimization of the resulting program is in general challenging due to non-smoothness and complex non-separable constraints .	we provide theoretical guarantees for the resulting programs , bounding the difference between their optimal value and the original optimum .	1	3	6	-5.8903627	5.2348204	1
34-8-28	further , we propose suitable optimization algorithms and analyze their convergence .	however , optimization of the resulting program is in general challenging due to non-smoothness and complex non-separable constraints .	0	7	3	4.939665	-4.3463316	0
34-8-28	therefore , in this work we study the benefits of augmenting the objective function of the relaxation with strong convexity .	specifically , we introduce strong convexity by adding a quadratic term to the lp relaxation objective .	1	4	5	-3.0579376	3.067927	1
34-8-28	therefore , in this work we study the benefits of augmenting the objective function of the relaxation with strong convexity .	we provide theoretical guarantees for the resulting programs , bounding the difference between their optimal value and the original optimum .	1	4	6	-3.9589987	3.8376746	1
34-8-28	therefore , in this work we study the benefits of augmenting the objective function of the relaxation with strong convexity .	further , we propose suitable optimization algorithms and analyze their convergence .	1	4	7	-5.43984	4.8878384	1
34-8-28	we provide theoretical guarantees for the resulting programs , bounding the difference between their optimal value and the original optimum .	specifically , we introduce strong convexity by adding a quadratic term to the lp relaxation objective .	0	6	5	1.5633962	-1.3173923	0
34-8-28	specifically , we introduce strong convexity by adding a quadratic term to the lp relaxation objective .	further , we propose suitable optimization algorithms and analyze their convergence .	1	5	7	-4.928646	4.518304	1
34-8-28	we provide theoretical guarantees for the resulting programs , bounding the difference between their optimal value and the original optimum .	further , we propose suitable optimization algorithms and analyze their convergence .	1	6	7	-5.026045	4.628686	1
35-6-15	two algorithms are proposed that instead seek to minimize regret with respect to the copeland winner , which , unlike the condorcet winner , is guaranteed to exist .	a version of the dueling bandit problem is addressed in which a condorcet winner may not exist .	0	1	0	4.42986	-4.042807	0
35-6-15	a version of the dueling bandit problem is addressed in which a condorcet winner may not exist .	the first , copeland confidence bound ( ccb ) , is designed for small numbers of arms , while the second , scalable copeland bandits ( scb ) , works better for large-scale problems .	1	0	2	-5.903259	5.252364	1
35-6-15	we provide theoretical results bounding the regret accumulated by ccb and scb , both substantially improving existing results .	a version of the dueling bandit problem is addressed in which a condorcet winner may not exist .	0	3	0	5.376219	-4.783716	0
35-6-15	such existing results either offer bounds of the form o ( k log t ) but require restrictive assumptions , or offer bounds of the form o ( k 2 log t ) without requiring such assumptions .	a version of the dueling bandit problem is addressed in which a condorcet winner may not exist .	0	4	0	5.0188546	-4.4731855	0
35-6-15	our results offer the best of both worlds : o ( k log t ) bounds without restrictive assumptions .	a version of the dueling bandit problem is addressed in which a condorcet winner may not exist .	0	5	0	5.469861	-4.9511623	0
35-6-15	two algorithms are proposed that instead seek to minimize regret with respect to the copeland winner , which , unlike the condorcet winner , is guaranteed to exist .	the first , copeland confidence bound ( ccb ) , is designed for small numbers of arms , while the second , scalable copeland bandits ( scb ) , works better for large-scale problems .	1	1	2	-5.8560987	5.289398	1
35-6-15	two algorithms are proposed that instead seek to minimize regret with respect to the copeland winner , which , unlike the condorcet winner , is guaranteed to exist .	we provide theoretical results bounding the regret accumulated by ccb and scb , both substantially improving existing results .	1	1	3	-5.8319483	5.1682987	1
35-6-15	two algorithms are proposed that instead seek to minimize regret with respect to the copeland winner , which , unlike the condorcet winner , is guaranteed to exist .	such existing results either offer bounds of the form o ( k log t ) but require restrictive assumptions , or offer bounds of the form o ( k 2 log t ) without requiring such assumptions .	1	1	4	0.84846896	-0.4752613	0
35-6-15	two algorithms are proposed that instead seek to minimize regret with respect to the copeland winner , which , unlike the condorcet winner , is guaranteed to exist .	our results offer the best of both worlds : o ( k log t ) bounds without restrictive assumptions .	1	1	5	-5.950695	5.1438775	1
35-6-15	we provide theoretical results bounding the regret accumulated by ccb and scb , both substantially improving existing results .	the first , copeland confidence bound ( ccb ) , is designed for small numbers of arms , while the second , scalable copeland bandits ( scb ) , works better for large-scale problems .	0	3	2	1.802461	-1.4418929	0
35-6-15	such existing results either offer bounds of the form o ( k log t ) but require restrictive assumptions , or offer bounds of the form o ( k 2 log t ) without requiring such assumptions .	the first , copeland confidence bound ( ccb ) , is designed for small numbers of arms , while the second , scalable copeland bandits ( scb ) , works better for large-scale problems .	0	4	2	-5.859949	5.2359896	1
35-6-15	the first , copeland confidence bound ( ccb ) , is designed for small numbers of arms , while the second , scalable copeland bandits ( scb ) , works better for large-scale problems .	our results offer the best of both worlds : o ( k log t ) bounds without restrictive assumptions .	1	2	5	-3.9092731	3.6695316	1
35-6-15	we provide theoretical results bounding the regret accumulated by ccb and scb , both substantially improving existing results .	such existing results either offer bounds of the form o ( k log t ) but require restrictive assumptions , or offer bounds of the form o ( k 2 log t ) without requiring such assumptions .	1	3	4	4.355292	-3.9080014	0
35-6-15	our results offer the best of both worlds : o ( k log t ) bounds without restrictive assumptions .	we provide theoretical results bounding the regret accumulated by ccb and scb , both substantially improving existing results .	0	5	3	3.4392228	-3.2366168	0
35-6-15	such existing results either offer bounds of the form o ( k log t ) but require restrictive assumptions , or offer bounds of the form o ( k 2 log t ) without requiring such assumptions .	our results offer the best of both worlds : o ( k log t ) bounds without restrictive assumptions .	1	4	5	-5.998797	5.0970554	1
36-6-15	we introduce the concept of coverage risk as an error measure for density ridge estimation .	the coverage risk generalizes the mean integrated square error to set estimation .	1	0	1	-5.4421177	4.9831414	1
36-6-15	we introduce the concept of coverage risk as an error measure for density ridge estimation .	we propose two risk estimators for the coverage risk and we show that we can select tuning parameters by minimizing the estimated risk .	1	0	2	-5.8972282	5.2270465	1
36-6-15	we study the rate of convergence for coverage risk and prove consistency of the risk estimators .	we introduce the concept of coverage risk as an error measure for density ridge estimation .	0	3	0	2.9110086	-2.6601748	0
36-6-15	we introduce the concept of coverage risk as an error measure for density ridge estimation .	we apply our method to three simulated datasets and to cosmology data .	1	0	4	-5.943293	5.2345285	1
36-6-15	in all the examples , the proposed method successfully recover the underlying density structure .	we introduce the concept of coverage risk as an error measure for density ridge estimation .	0	5	0	5.3435707	-4.738867	0
36-6-15	we propose two risk estimators for the coverage risk and we show that we can select tuning parameters by minimizing the estimated risk .	the coverage risk generalizes the mean integrated square error to set estimation .	0	2	1	4.257573	-3.8730419	0
36-6-15	the coverage risk generalizes the mean integrated square error to set estimation .	we study the rate of convergence for coverage risk and prove consistency of the risk estimators .	1	1	3	-3.3832304	3.2926638	1
36-6-15	the coverage risk generalizes the mean integrated square error to set estimation .	we apply our method to three simulated datasets and to cosmology data .	1	1	4	-5.9365606	5.1694517	1
36-6-15	in all the examples , the proposed method successfully recover the underlying density structure .	the coverage risk generalizes the mean integrated square error to set estimation .	0	5	1	4.981437	-4.511058	0
36-6-15	we propose two risk estimators for the coverage risk and we show that we can select tuning parameters by minimizing the estimated risk .	we study the rate of convergence for coverage risk and prove consistency of the risk estimators .	1	2	3	3.5246155	-3.2043428	0
36-6-15	we propose two risk estimators for the coverage risk and we show that we can select tuning parameters by minimizing the estimated risk .	we apply our method to three simulated datasets and to cosmology data .	1	2	4	-5.152186	4.7437963	1
36-6-15	we propose two risk estimators for the coverage risk and we show that we can select tuning parameters by minimizing the estimated risk .	in all the examples , the proposed method successfully recover the underlying density structure .	1	2	5	-5.1829886	4.6482325	1
36-6-15	we apply our method to three simulated datasets and to cosmology data .	we study the rate of convergence for coverage risk and prove consistency of the risk estimators .	0	4	3	5.445386	-4.8583326	0
36-6-15	we study the rate of convergence for coverage risk and prove consistency of the risk estimators .	in all the examples , the proposed method successfully recover the underlying density structure .	1	3	5	-6.0226583	5.158676	1
36-6-15	we apply our method to three simulated datasets and to cosmology data .	in all the examples , the proposed method successfully recover the underlying density structure .	1	4	5	-2.8268368	2.7124186	1
37-6-15	class ambiguity is typical in image classification problems with a large number of classes .	when classes are difficult to discriminate , it makes sense to allow k guesses and evaluate classifiers based on the top-k error instead of the standard zero-one loss .	1	0	1	-5.8758535	5.22262	1
37-6-15	we propose top-k multiclass svm as a direct method to optimize for top-k performance .	class ambiguity is typical in image classification problems with a large number of classes .	0	2	0	5.4929476	-4.9102697	0
37-6-15	our generalization of the well-known multiclass svm is based on a tight convex upper bound of the top-k error .	class ambiguity is typical in image classification problems with a large number of classes .	0	3	0	5.458064	-4.831501	0
37-6-15	class ambiguity is typical in image classification problems with a large number of classes .	we propose a fast optimization scheme based on an efficient projection onto the top-k simplex , which is of its own interest .	1	0	4	-5.96141	5.1245112	1
37-6-15	experiments on five datasets show consistent improvements in top-k accuracy compared to various baselines .	class ambiguity is typical in image classification problems with a large number of classes .	0	5	0	5.7034097	-5.146722	0
37-6-15	we propose top-k multiclass svm as a direct method to optimize for top-k performance .	when classes are difficult to discriminate , it makes sense to allow k guesses and evaluate classifiers based on the top-k error instead of the standard zero-one loss .	0	2	1	2.5766141	-2.3493814	0
37-6-15	our generalization of the well-known multiclass svm is based on a tight convex upper bound of the top-k error .	when classes are difficult to discriminate , it makes sense to allow k guesses and evaluate classifiers based on the top-k error instead of the standard zero-one loss .	0	3	1	2.9985857	-2.8308377	0
37-6-15	when classes are difficult to discriminate , it makes sense to allow k guesses and evaluate classifiers based on the top-k error instead of the standard zero-one loss .	we propose a fast optimization scheme based on an efficient projection onto the top-k simplex , which is of its own interest .	1	1	4	-6.001762	5.159686	1
37-6-15	experiments on five datasets show consistent improvements in top-k accuracy compared to various baselines .	when classes are difficult to discriminate , it makes sense to allow k guesses and evaluate classifiers based on the top-k error instead of the standard zero-one loss .	0	5	1	5.5740995	-4.9961834	0
37-6-15	we propose top-k multiclass svm as a direct method to optimize for top-k performance .	our generalization of the well-known multiclass svm is based on a tight convex upper bound of the top-k error .	1	2	3	-3.9670541	3.7784667	1
37-6-15	we propose top-k multiclass svm as a direct method to optimize for top-k performance .	we propose a fast optimization scheme based on an efficient projection onto the top-k simplex , which is of its own interest .	1	2	4	-5.6792207	5.1360874	1
37-6-15	we propose top-k multiclass svm as a direct method to optimize for top-k performance .	experiments on five datasets show consistent improvements in top-k accuracy compared to various baselines .	1	2	5	-5.9332314	5.1993513	1
37-6-15	our generalization of the well-known multiclass svm is based on a tight convex upper bound of the top-k error .	we propose a fast optimization scheme based on an efficient projection onto the top-k simplex , which is of its own interest .	1	3	4	-5.39989	4.9320607	1
37-6-15	our generalization of the well-known multiclass svm is based on a tight convex upper bound of the top-k error .	experiments on five datasets show consistent improvements in top-k accuracy compared to various baselines .	1	3	5	-5.931871	5.22095	1
37-6-15	experiments on five datasets show consistent improvements in top-k accuracy compared to various baselines .	we propose a fast optimization scheme based on an efficient projection onto the top-k simplex , which is of its own interest .	0	5	4	5.1739836	-4.6617723	0
38-4-6	we propose the -return as an alternative to the -return currently used by the td ( ) family of algorithms .	the benefit of the -return is that it accounts for the correlation of different length returns .	1	0	1	-5.6442204	5.099147	1
38-4-6	because it is difficult to compute exactly , we suggest one way of approximating the -return .	we propose the -return as an alternative to the -return currently used by the td ( ) family of algorithms .	0	2	0	1.1673298	-0.7467867	0
38-4-6	we propose the -return as an alternative to the -return currently used by the td ( ) family of algorithms .	we provide empirical studies that suggest that it is superior to the -return and -return for a variety of problems .	1	0	3	-5.91474	5.1286783	1
38-4-6	because it is difficult to compute exactly , we suggest one way of approximating the -return .	the benefit of the -return is that it accounts for the correlation of different length returns .	0	2	1	-4.972974	4.573244	1
38-4-6	we provide empirical studies that suggest that it is superior to the -return and -return for a variety of problems .	the benefit of the -return is that it accounts for the correlation of different length returns .	0	3	1	3.2733397	-3.1863093	0
38-4-6	we provide empirical studies that suggest that it is superior to the -return and -return for a variety of problems .	because it is difficult to compute exactly , we suggest one way of approximating the -return .	0	3	2	4.2684517	-3.8495955	0
39-8-28	it yields potentially useful data representations as superposition of disjoint parts , while it has been shown to work well for clustering tasks where traditional methods underperform .	orthogonal nonnegative matrix factorization ( onmf ) aims to approximate a nonnegative matrix as the product of two k-dimensional nonnegative factors , one of which has orthonormal columns .	0	1	0	5.50712	-4.927668	0
39-8-28	existing algorithms rely mostly on heuristics , which despite their good empirical performance , lack provable performance guarantees .	orthogonal nonnegative matrix factorization ( onmf ) aims to approximate a nonnegative matrix as the product of two k-dimensional nonnegative factors , one of which has orthonormal columns .	0	2	0	5.494935	-4.9130473	0
39-8-28	orthogonal nonnegative matrix factorization ( onmf ) aims to approximate a nonnegative matrix as the product of two k-dimensional nonnegative factors , one of which has orthonormal columns .	we present a new onmf algorithm with provable approximation guarantees .	1	0	3	-5.9486237	5.1371765	1
39-8-28	for any constant dimension k , we obtain an additive eptas without any assumptions on the input .	orthogonal nonnegative matrix factorization ( onmf ) aims to approximate a nonnegative matrix as the product of two k-dimensional nonnegative factors , one of which has orthonormal columns .	0	4	0	5.4237776	-4.796785	0
39-8-28	our algorithm relies on a novel approximation to the related nonnegative principal component analysis ( nnpca ) problem ; given an arbitrary data matrix , nnpca seeks k nonnegative components that jointly capture most of the variance .	orthogonal nonnegative matrix factorization ( onmf ) aims to approximate a nonnegative matrix as the product of two k-dimensional nonnegative factors , one of which has orthonormal columns .	0	5	0	5.0740414	-4.49132	0
39-8-28	orthogonal nonnegative matrix factorization ( onmf ) aims to approximate a nonnegative matrix as the product of two k-dimensional nonnegative factors , one of which has orthonormal columns .	our nnpca algorithm is of independent interest and generalizes previous work that could only obtain guarantees for a single component .	1	0	6	-5.988449	5.110546	1
39-8-28	we evaluate our algorithms on several real and synthetic datasets and show that their performance matches or outperforms the state of the art .	orthogonal nonnegative matrix factorization ( onmf ) aims to approximate a nonnegative matrix as the product of two k-dimensional nonnegative factors , one of which has orthonormal columns .	0	7	0	5.668828	-5.1112704	0
39-8-28	existing algorithms rely mostly on heuristics , which despite their good empirical performance , lack provable performance guarantees .	it yields potentially useful data representations as superposition of disjoint parts , while it has been shown to work well for clustering tasks where traditional methods underperform .	0	2	1	-4.9975386	4.5906897	1
39-8-28	we present a new onmf algorithm with provable approximation guarantees .	it yields potentially useful data representations as superposition of disjoint parts , while it has been shown to work well for clustering tasks where traditional methods underperform .	0	3	1	-2.3130512	2.408201	1
39-8-28	it yields potentially useful data representations as superposition of disjoint parts , while it has been shown to work well for clustering tasks where traditional methods underperform .	for any constant dimension k , we obtain an additive eptas without any assumptions on the input .	1	1	4	3.0707092	-2.8902082	0
39-8-28	it yields potentially useful data representations as superposition of disjoint parts , while it has been shown to work well for clustering tasks where traditional methods underperform .	our algorithm relies on a novel approximation to the related nonnegative principal component analysis ( nnpca ) problem ; given an arbitrary data matrix , nnpca seeks k nonnegative components that jointly capture most of the variance .	1	1	5	3.150003	-2.9491892	0
39-8-28	it yields potentially useful data representations as superposition of disjoint parts , while it has been shown to work well for clustering tasks where traditional methods underperform .	our nnpca algorithm is of independent interest and generalizes previous work that could only obtain guarantees for a single component .	1	1	6	-1.5802395	1.6123514	1
39-8-28	we evaluate our algorithms on several real and synthetic datasets and show that their performance matches or outperforms the state of the art .	it yields potentially useful data representations as superposition of disjoint parts , while it has been shown to work well for clustering tasks where traditional methods underperform .	0	7	1	4.9961653	-4.527841	0
39-8-28	we present a new onmf algorithm with provable approximation guarantees .	existing algorithms rely mostly on heuristics , which despite their good empirical performance , lack provable performance guarantees .	0	3	2	5.2846866	-4.685299	0
39-8-28	existing algorithms rely mostly on heuristics , which despite their good empirical performance , lack provable performance guarantees .	for any constant dimension k , we obtain an additive eptas without any assumptions on the input .	1	2	4	-3.997693	3.8836343	1
39-8-28	existing algorithms rely mostly on heuristics , which despite their good empirical performance , lack provable performance guarantees .	our algorithm relies on a novel approximation to the related nonnegative principal component analysis ( nnpca ) problem ; given an arbitrary data matrix , nnpca seeks k nonnegative components that jointly capture most of the variance .	1	2	5	-5.5265894	5.0614033	1
39-8-28	existing algorithms rely mostly on heuristics , which despite their good empirical performance , lack provable performance guarantees .	our nnpca algorithm is of independent interest and generalizes previous work that could only obtain guarantees for a single component .	1	2	6	-5.907955	5.0716677	1
39-8-28	we evaluate our algorithms on several real and synthetic datasets and show that their performance matches or outperforms the state of the art .	existing algorithms rely mostly on heuristics , which despite their good empirical performance , lack provable performance guarantees .	0	7	2	5.638376	-5.058709	0
39-8-28	we present a new onmf algorithm with provable approximation guarantees .	for any constant dimension k , we obtain an additive eptas without any assumptions on the input .	1	3	4	2.9354773	-2.7579174	0
39-8-28	our algorithm relies on a novel approximation to the related nonnegative principal component analysis ( nnpca ) problem ; given an arbitrary data matrix , nnpca seeks k nonnegative components that jointly capture most of the variance .	we present a new onmf algorithm with provable approximation guarantees .	0	5	3	-2.301177	2.4274302	1
39-8-28	we present a new onmf algorithm with provable approximation guarantees .	our nnpca algorithm is of independent interest and generalizes previous work that could only obtain guarantees for a single component .	1	3	6	-4.813058	4.3733497	1
39-8-28	we evaluate our algorithms on several real and synthetic datasets and show that their performance matches or outperforms the state of the art .	we present a new onmf algorithm with provable approximation guarantees .	0	7	3	5.576519	-4.954799	0
39-8-28	our algorithm relies on a novel approximation to the related nonnegative principal component analysis ( nnpca ) problem ; given an arbitrary data matrix , nnpca seeks k nonnegative components that jointly capture most of the variance .	for any constant dimension k , we obtain an additive eptas without any assumptions on the input .	0	5	4	-4.7732687	4.454735	1
39-8-28	for any constant dimension k , we obtain an additive eptas without any assumptions on the input .	our nnpca algorithm is of independent interest and generalizes previous work that could only obtain guarantees for a single component .	1	4	6	-4.0323634	3.76262	1
39-8-28	we evaluate our algorithms on several real and synthetic datasets and show that their performance matches or outperforms the state of the art .	for any constant dimension k , we obtain an additive eptas without any assumptions on the input .	0	7	4	5.542872	-4.951029	0
39-8-28	our algorithm relies on a novel approximation to the related nonnegative principal component analysis ( nnpca ) problem ; given an arbitrary data matrix , nnpca seeks k nonnegative components that jointly capture most of the variance .	our nnpca algorithm is of independent interest and generalizes previous work that could only obtain guarantees for a single component .	1	5	6	-5.909111	5.2032585	1
39-8-28	our algorithm relies on a novel approximation to the related nonnegative principal component analysis ( nnpca ) problem ; given an arbitrary data matrix , nnpca seeks k nonnegative components that jointly capture most of the variance .	we evaluate our algorithms on several real and synthetic datasets and show that their performance matches or outperforms the state of the art .	1	5	7	-6.0058074	5.204928	1
39-8-28	our nnpca algorithm is of independent interest and generalizes previous work that could only obtain guarantees for a single component .	we evaluate our algorithms on several real and synthetic datasets and show that their performance matches or outperforms the state of the art .	1	6	7	-5.762465	5.1516366	1
40-6-15	the greedy algorithm is extensively studied in the field of combinatorial optimization for decades .	in this paper , we address the online learning problem when the input to the greedy algorithm is stochastic with unknown parameters that have to be learned over time .	1	0	1	-5.874172	5.2591476	1
40-6-15	we first propose the greedy regret and -quasi greedy regret as learning metrics comparing with the performance of offline greedy algorithm .	the greedy algorithm is extensively studied in the field of combinatorial optimization for decades .	0	2	0	5.2761974	-4.6701713	0
40-6-15	we then propose two online greedy learning algorithms with semi-bandit feedbacks , which use multi-armed bandit and pure exploration bandit policies at each level of greedy learning , one for each of the regret metrics respectively .	the greedy algorithm is extensively studied in the field of combinatorial optimization for decades .	0	3	0	3.7704558	-3.4473972	0
40-6-15	both algorithms achieve o ( log t ) problem-dependent regret bound ( t being the time horizon ) for a general class of combinatorial structures and reward functions that allow greedy solutions .	the greedy algorithm is extensively studied in the field of combinatorial optimization for decades .	0	4	0	5.3263826	-4.698968	0
40-6-15	we further show that the bound is tight in t and other problem instance parameters .	the greedy algorithm is extensively studied in the field of combinatorial optimization for decades .	0	5	0	5.4846325	-4.9502873	0
40-6-15	we first propose the greedy regret and -quasi greedy regret as learning metrics comparing with the performance of offline greedy algorithm .	in this paper , we address the online learning problem when the input to the greedy algorithm is stochastic with unknown parameters that have to be learned over time .	0	2	1	4.8961525	-4.345696	0
40-6-15	we then propose two online greedy learning algorithms with semi-bandit feedbacks , which use multi-armed bandit and pure exploration bandit policies at each level of greedy learning , one for each of the regret metrics respectively .	in this paper , we address the online learning problem when the input to the greedy algorithm is stochastic with unknown parameters that have to be learned over time .	0	3	1	5.2251825	-4.646087	0
40-6-15	in this paper , we address the online learning problem when the input to the greedy algorithm is stochastic with unknown parameters that have to be learned over time .	both algorithms achieve o ( log t ) problem-dependent regret bound ( t being the time horizon ) for a general class of combinatorial structures and reward functions that allow greedy solutions .	1	1	4	-5.824822	5.232281	1
40-6-15	we further show that the bound is tight in t and other problem instance parameters .	in this paper , we address the online learning problem when the input to the greedy algorithm is stochastic with unknown parameters that have to be learned over time .	0	5	1	5.6104736	-5.010561	0
40-6-15	we first propose the greedy regret and -quasi greedy regret as learning metrics comparing with the performance of offline greedy algorithm .	we then propose two online greedy learning algorithms with semi-bandit feedbacks , which use multi-armed bandit and pure exploration bandit policies at each level of greedy learning , one for each of the regret metrics respectively .	1	2	3	-3.71427	3.5189314	1
40-6-15	we first propose the greedy regret and -quasi greedy regret as learning metrics comparing with the performance of offline greedy algorithm .	both algorithms achieve o ( log t ) problem-dependent regret bound ( t being the time horizon ) for a general class of combinatorial structures and reward functions that allow greedy solutions .	1	2	4	-4.100865	3.7297397	1
40-6-15	we first propose the greedy regret and -quasi greedy regret as learning metrics comparing with the performance of offline greedy algorithm .	we further show that the bound is tight in t and other problem instance parameters .	1	2	5	-3.3055942	3.1692348	1
40-6-15	we then propose two online greedy learning algorithms with semi-bandit feedbacks , which use multi-armed bandit and pure exploration bandit policies at each level of greedy learning , one for each of the regret metrics respectively .	both algorithms achieve o ( log t ) problem-dependent regret bound ( t being the time horizon ) for a general class of combinatorial structures and reward functions that allow greedy solutions .	1	3	4	-5.2202735	4.795517	1
40-6-15	we then propose two online greedy learning algorithms with semi-bandit feedbacks , which use multi-armed bandit and pure exploration bandit policies at each level of greedy learning , one for each of the regret metrics respectively .	we further show that the bound is tight in t and other problem instance parameters .	1	3	5	-0.679098	0.8899287	1
40-6-15	both algorithms achieve o ( log t ) problem-dependent regret bound ( t being the time horizon ) for a general class of combinatorial structures and reward functions that allow greedy solutions .	we further show that the bound is tight in t and other problem instance parameters .	1	4	5	-5.026635	4.6312294	1
41-7-21	we proffer a new , efficient deep structured model learning scheme , in which we show how deep convolutional neural networks ( cnns ) can be used to directly estimate the messages in message passing inference for structured prediction with conditional random fields ( crfs ) .	deep structured output learning shows great promise in tasks like semantic image segmentation .	0	1	0	3.4581194	-3.1649091	0
41-7-21	with such cnn message estimators , we obviate the need to learn or evaluate potential functions for message calculation .	deep structured output learning shows great promise in tasks like semantic image segmentation .	0	2	0	3.1302335	-2.9140015	0
41-7-21	this confers significant efficiency for learning , since otherwise when performing structured learning for a crf with cnn potentials it is necessary to undertake expensive inference for every stochastic gradient iteration .	deep structured output learning shows great promise in tasks like semantic image segmentation .	0	3	0	5.3375835	-4.7445416	0
41-7-21	deep structured output learning shows great promise in tasks like semantic image segmentation .	the network output dimension of message estimators is the same as the number of classes , rather than exponentially growing in the order of the potentials .	1	0	4	-4.781164	4.487412	1
41-7-21	hence it is more scalable for cases that involve a large number of classes .	deep structured output learning shows great promise in tasks like semantic image segmentation .	0	5	0	5.5851345	-4.9585667	0
41-7-21	deep structured output learning shows great promise in tasks like semantic image segmentation .	we apply our method to semantic image segmentation and achieve impressive performance , which demonstrates the effectiveness and usefulness of our cnn message learning method .	1	0	6	-5.9485025	5.068964	1
41-7-21	with such cnn message estimators , we obviate the need to learn or evaluate potential functions for message calculation .	we proffer a new , efficient deep structured model learning scheme , in which we show how deep convolutional neural networks ( cnns ) can be used to directly estimate the messages in message passing inference for structured prediction with conditional random fields ( crfs ) .	0	2	1	5.076968	-4.5029297	0
41-7-21	we proffer a new , efficient deep structured model learning scheme , in which we show how deep convolutional neural networks ( cnns ) can be used to directly estimate the messages in message passing inference for structured prediction with conditional random fields ( crfs ) .	this confers significant efficiency for learning , since otherwise when performing structured learning for a crf with cnn potentials it is necessary to undertake expensive inference for every stochastic gradient iteration .	1	1	3	-5.6830416	5.0322123	1
41-7-21	the network output dimension of message estimators is the same as the number of classes , rather than exponentially growing in the order of the potentials .	we proffer a new , efficient deep structured model learning scheme , in which we show how deep convolutional neural networks ( cnns ) can be used to directly estimate the messages in message passing inference for structured prediction with conditional random fields ( crfs ) .	0	4	1	3.4526725	-3.0511901	0
41-7-21	we proffer a new , efficient deep structured model learning scheme , in which we show how deep convolutional neural networks ( cnns ) can be used to directly estimate the messages in message passing inference for structured prediction with conditional random fields ( crfs ) .	hence it is more scalable for cases that involve a large number of classes .	1	1	5	-5.6194897	5.0762644	1
41-7-21	we proffer a new , efficient deep structured model learning scheme , in which we show how deep convolutional neural networks ( cnns ) can be used to directly estimate the messages in message passing inference for structured prediction with conditional random fields ( crfs ) .	we apply our method to semantic image segmentation and achieve impressive performance , which demonstrates the effectiveness and usefulness of our cnn message learning method .	1	1	6	-5.9723444	5.18861	1
41-7-21	with such cnn message estimators , we obviate the need to learn or evaluate potential functions for message calculation .	this confers significant efficiency for learning , since otherwise when performing structured learning for a crf with cnn potentials it is necessary to undertake expensive inference for every stochastic gradient iteration .	1	2	3	-2.7555964	2.8235717	1
41-7-21	with such cnn message estimators , we obviate the need to learn or evaluate potential functions for message calculation .	the network output dimension of message estimators is the same as the number of classes , rather than exponentially growing in the order of the potentials .	1	2	4	-2.186048	2.258183	1
41-7-21	with such cnn message estimators , we obviate the need to learn or evaluate potential functions for message calculation .	hence it is more scalable for cases that involve a large number of classes .	1	2	5	-5.4893484	5.031747	1
41-7-21	with such cnn message estimators , we obviate the need to learn or evaluate potential functions for message calculation .	we apply our method to semantic image segmentation and achieve impressive performance , which demonstrates the effectiveness and usefulness of our cnn message learning method .	1	2	6	-5.960615	5.1397038	1
41-7-21	this confers significant efficiency for learning , since otherwise when performing structured learning for a crf with cnn potentials it is necessary to undertake expensive inference for every stochastic gradient iteration .	the network output dimension of message estimators is the same as the number of classes , rather than exponentially growing in the order of the potentials .	1	3	4	4.8645144	-4.3594093	0
41-7-21	hence it is more scalable for cases that involve a large number of classes .	this confers significant efficiency for learning , since otherwise when performing structured learning for a crf with cnn potentials it is necessary to undertake expensive inference for every stochastic gradient iteration .	0	5	3	4.3328166	-3.9888759	0
41-7-21	we apply our method to semantic image segmentation and achieve impressive performance , which demonstrates the effectiveness and usefulness of our cnn message learning method .	this confers significant efficiency for learning , since otherwise when performing structured learning for a crf with cnn potentials it is necessary to undertake expensive inference for every stochastic gradient iteration .	0	6	3	5.284398	-4.711636	0
41-7-21	the network output dimension of message estimators is the same as the number of classes , rather than exponentially growing in the order of the potentials .	hence it is more scalable for cases that involve a large number of classes .	1	4	5	-5.9542294	5.1534615	1
41-7-21	we apply our method to semantic image segmentation and achieve impressive performance , which demonstrates the effectiveness and usefulness of our cnn message learning method .	the network output dimension of message estimators is the same as the number of classes , rather than exponentially growing in the order of the potentials .	0	6	4	5.459377	-4.849393	0
41-7-21	we apply our method to semantic image segmentation and achieve impressive performance , which demonstrates the effectiveness and usefulness of our cnn message learning method .	hence it is more scalable for cases that involve a large number of classes .	0	6	5	4.234706	-3.9151895	0
42-6-15	we propose that inherent stochasticity enables synaptic plasticity to carry out probabilistic inference by sampling from a posterior distribution of synaptic parameters .	we reexamine in this article the conceptual and mathematical framework for understanding the organization of plasticity in spiking neural networks .	0	1	0	4.0280848	-3.677898	0
42-6-15	this view provides a viable alternative to existing models that propose convergence of synaptic weights to maximum likelihood parameters .	we reexamine in this article the conceptual and mathematical framework for understanding the organization of plasticity in spiking neural networks .	0	2	0	-0.2681001	0.40640125	1
42-6-15	we reexamine in this article the conceptual and mathematical framework for understanding the organization of plasticity in spiking neural networks .	it explains how priors on weight distributions and connection probabilities can be merged optimally with learned experience .	1	0	3	-4.524521	4.2681065	1
42-6-15	we reexamine in this article the conceptual and mathematical framework for understanding the organization of plasticity in spiking neural networks .	in simulations we show that our model for synaptic plasticity allows spiking neural networks to compensate continuously for unforeseen disturbances .	1	0	4	-5.9335194	5.1413174	1
42-6-15	we reexamine in this article the conceptual and mathematical framework for understanding the organization of plasticity in spiking neural networks .	furthermore it provides a normative mathematical framework to better understand the permanent variability and rewiring observed in brain networks .	1	0	5	-5.8095884	4.9903345	1
42-6-15	this view provides a viable alternative to existing models that propose convergence of synaptic weights to maximum likelihood parameters .	we propose that inherent stochasticity enables synaptic plasticity to carry out probabilistic inference by sampling from a posterior distribution of synaptic parameters .	0	2	1	-2.8198235	2.9082086	1
42-6-15	we propose that inherent stochasticity enables synaptic plasticity to carry out probabilistic inference by sampling from a posterior distribution of synaptic parameters .	it explains how priors on weight distributions and connection probabilities can be merged optimally with learned experience .	1	1	3	-4.3693485	4.1101723	1
42-6-15	in simulations we show that our model for synaptic plasticity allows spiking neural networks to compensate continuously for unforeseen disturbances .	we propose that inherent stochasticity enables synaptic plasticity to carry out probabilistic inference by sampling from a posterior distribution of synaptic parameters .	0	4	1	4.537659	-4.110353	0
42-6-15	furthermore it provides a normative mathematical framework to better understand the permanent variability and rewiring observed in brain networks .	we propose that inherent stochasticity enables synaptic plasticity to carry out probabilistic inference by sampling from a posterior distribution of synaptic parameters .	0	5	1	4.957464	-4.4344473	0
42-6-15	it explains how priors on weight distributions and connection probabilities can be merged optimally with learned experience .	this view provides a viable alternative to existing models that propose convergence of synaptic weights to maximum likelihood parameters .	0	3	2	1.6039536	-1.4287778	0
42-6-15	this view provides a viable alternative to existing models that propose convergence of synaptic weights to maximum likelihood parameters .	in simulations we show that our model for synaptic plasticity allows spiking neural networks to compensate continuously for unforeseen disturbances .	1	2	4	-4.6639166	4.3558764	1
42-6-15	furthermore it provides a normative mathematical framework to better understand the permanent variability and rewiring observed in brain networks .	this view provides a viable alternative to existing models that propose convergence of synaptic weights to maximum likelihood parameters .	0	5	2	5.065834	-4.5345297	0
42-6-15	in simulations we show that our model for synaptic plasticity allows spiking neural networks to compensate continuously for unforeseen disturbances .	it explains how priors on weight distributions and connection probabilities can be merged optimally with learned experience .	0	4	3	2.325943	-2.1191301	0
42-6-15	it explains how priors on weight distributions and connection probabilities can be merged optimally with learned experience .	furthermore it provides a normative mathematical framework to better understand the permanent variability and rewiring observed in brain networks .	1	3	5	-5.874646	5.1296024	1
42-6-15	in simulations we show that our model for synaptic plasticity allows spiking neural networks to compensate continuously for unforeseen disturbances .	furthermore it provides a normative mathematical framework to better understand the permanent variability and rewiring observed in brain networks .	1	4	5	-5.6061335	5.0179443	1
43-9-36	however , solving the nonconvex and nonsmooth optimization problems remains a big challenge .	nonconvex and nonsmooth problems have recently received considerable attention in signal/image processing , statistics and machine learning .	0	1	0	5.4347477	-4.849703	0
43-9-36	nonconvex and nonsmooth problems have recently received considerable attention in signal/image processing , statistics and machine learning .	accelerated proximal gradient ( apg ) is an excellent method for convex programming .	1	0	2	-4.6792035	4.4301977	1
43-9-36	however , it is still unknown whether the usual apg can ensure the convergence to a critical point in nonconvex programming .	nonconvex and nonsmooth problems have recently received considerable attention in signal/image processing , statistics and machine learning .	0	3	0	5.5633273	-4.981438	0
43-9-36	in this paper , we extend apg for general nonconvex and nonsmooth programs by introducing a monitor that satisfies the sufficient descent property .	nonconvex and nonsmooth problems have recently received considerable attention in signal/image processing , statistics and machine learning .	0	4	0	5.558036	-4.937441	0
43-9-36	accordingly , we propose a monotone apg and a nonmonotone apg .	nonconvex and nonsmooth problems have recently received considerable attention in signal/image processing , statistics and machine learning .	0	5	0	5.5930066	-4.9964724	0
43-9-36	the latter waives the requirement on monotonic reduction of the objective function and needs less computation in each iteration .	nonconvex and nonsmooth problems have recently received considerable attention in signal/image processing , statistics and machine learning .	0	6	0	5.596177	-5.056852	0
43-9-36	to the best of our knowledge , we are the first to provide apg-type algorithms for general nonconvex and nonsmooth problems ensuring that every accumulation point is a critical point , and the convergence rates remain o k12 when the problems are convex , in which k is the number of iterations .	nonconvex and nonsmooth problems have recently received considerable attention in signal/image processing , statistics and machine learning .	0	7	0	5.612348	-5.0382886	0
43-9-36	numerical results testify to the advantage of our algorithms in speed .	nonconvex and nonsmooth problems have recently received considerable attention in signal/image processing , statistics and machine learning .	0	8	0	5.584835	-5.0635085	0
43-9-36	however , solving the nonconvex and nonsmooth optimization problems remains a big challenge .	accelerated proximal gradient ( apg ) is an excellent method for convex programming .	1	1	2	4.891531	-4.3806725	0
43-9-36	however , it is still unknown whether the usual apg can ensure the convergence to a critical point in nonconvex programming .	however , solving the nonconvex and nonsmooth optimization problems remains a big challenge .	0	3	1	2.4878714	-2.325108	0
43-9-36	however , solving the nonconvex and nonsmooth optimization problems remains a big challenge .	in this paper , we extend apg for general nonconvex and nonsmooth programs by introducing a monitor that satisfies the sufficient descent property .	1	1	4	-5.870549	5.2082987	1
43-9-36	accordingly , we propose a monotone apg and a nonmonotone apg .	however , solving the nonconvex and nonsmooth optimization problems remains a big challenge .	0	5	1	5.05764	-4.5037985	0
43-9-36	the latter waives the requirement on monotonic reduction of the objective function and needs less computation in each iteration .	however , solving the nonconvex and nonsmooth optimization problems remains a big challenge .	0	6	1	5.158327	-4.6217546	0
43-9-36	however , solving the nonconvex and nonsmooth optimization problems remains a big challenge .	to the best of our knowledge , we are the first to provide apg-type algorithms for general nonconvex and nonsmooth problems ensuring that every accumulation point is a critical point , and the convergence rates remain o k12 when the problems are convex , in which k is the number of iterations .	1	1	7	-5.9789586	5.1594195	1
43-9-36	however , solving the nonconvex and nonsmooth optimization problems remains a big challenge .	numerical results testify to the advantage of our algorithms in speed .	1	1	8	-5.92984	5.08804	1
43-9-36	however , it is still unknown whether the usual apg can ensure the convergence to a critical point in nonconvex programming .	accelerated proximal gradient ( apg ) is an excellent method for convex programming .	0	3	2	5.5941286	-5.0449467	0
43-9-36	accelerated proximal gradient ( apg ) is an excellent method for convex programming .	in this paper , we extend apg for general nonconvex and nonsmooth programs by introducing a monitor that satisfies the sufficient descent property .	1	2	4	-5.959913	5.121926	1
43-9-36	accelerated proximal gradient ( apg ) is an excellent method for convex programming .	accordingly , we propose a monotone apg and a nonmonotone apg .	1	2	5	-5.964431	5.1101494	1
43-9-36	accelerated proximal gradient ( apg ) is an excellent method for convex programming .	the latter waives the requirement on monotonic reduction of the objective function and needs less computation in each iteration .	1	2	6	-5.842463	5.230255	1
43-9-36	accelerated proximal gradient ( apg ) is an excellent method for convex programming .	to the best of our knowledge , we are the first to provide apg-type algorithms for general nonconvex and nonsmooth problems ensuring that every accumulation point is a critical point , and the convergence rates remain o k12 when the problems are convex , in which k is the number of iterations .	1	2	7	-6.001302	5.129939	1
43-9-36	accelerated proximal gradient ( apg ) is an excellent method for convex programming .	numerical results testify to the advantage of our algorithms in speed .	1	2	8	-5.9984603	5.1954775	1
43-9-36	however , it is still unknown whether the usual apg can ensure the convergence to a critical point in nonconvex programming .	in this paper , we extend apg for general nonconvex and nonsmooth programs by introducing a monitor that satisfies the sufficient descent property .	1	3	4	-5.813039	5.2683997	1
43-9-36	accordingly , we propose a monotone apg and a nonmonotone apg .	however , it is still unknown whether the usual apg can ensure the convergence to a critical point in nonconvex programming .	0	5	3	5.364723	-4.797931	0
43-9-36	the latter waives the requirement on monotonic reduction of the objective function and needs less computation in each iteration .	however , it is still unknown whether the usual apg can ensure the convergence to a critical point in nonconvex programming .	0	6	3	5.052857	-4.5635996	0
43-9-36	to the best of our knowledge , we are the first to provide apg-type algorithms for general nonconvex and nonsmooth problems ensuring that every accumulation point is a critical point , and the convergence rates remain o k12 when the problems are convex , in which k is the number of iterations .	however , it is still unknown whether the usual apg can ensure the convergence to a critical point in nonconvex programming .	0	7	3	5.2017307	-4.5828476	0
43-9-36	numerical results testify to the advantage of our algorithms in speed .	however , it is still unknown whether the usual apg can ensure the convergence to a critical point in nonconvex programming .	0	8	3	5.557584	-4.9806066	0
43-9-36	in this paper , we extend apg for general nonconvex and nonsmooth programs by introducing a monitor that satisfies the sufficient descent property .	accordingly , we propose a monotone apg and a nonmonotone apg .	1	4	5	-3.515376	3.4141977	1
43-9-36	the latter waives the requirement on monotonic reduction of the objective function and needs less computation in each iteration .	in this paper , we extend apg for general nonconvex and nonsmooth programs by introducing a monitor that satisfies the sufficient descent property .	0	6	4	2.568962	-2.4337904	0
43-9-36	in this paper , we extend apg for general nonconvex and nonsmooth programs by introducing a monitor that satisfies the sufficient descent property .	to the best of our knowledge , we are the first to provide apg-type algorithms for general nonconvex and nonsmooth problems ensuring that every accumulation point is a critical point , and the convergence rates remain o k12 when the problems are convex , in which k is the number of iterations .	1	4	7	-5.9009857	5.218203	1
43-9-36	numerical results testify to the advantage of our algorithms in speed .	in this paper , we extend apg for general nonconvex and nonsmooth programs by introducing a monitor that satisfies the sufficient descent property .	0	8	4	5.4371996	-4.825783	0
43-9-36	the latter waives the requirement on monotonic reduction of the objective function and needs less computation in each iteration .	accordingly , we propose a monotone apg and a nonmonotone apg .	0	6	5	2.181081	-2.0341918	0
43-9-36	to the best of our knowledge , we are the first to provide apg-type algorithms for general nonconvex and nonsmooth problems ensuring that every accumulation point is a critical point , and the convergence rates remain o k12 when the problems are convex , in which k is the number of iterations .	accordingly , we propose a monotone apg and a nonmonotone apg .	0	7	5	4.7088075	-4.154957	0
43-9-36	accordingly , we propose a monotone apg and a nonmonotone apg .	numerical results testify to the advantage of our algorithms in speed .	1	5	8	-5.844469	4.8906393	1
43-9-36	the latter waives the requirement on monotonic reduction of the objective function and needs less computation in each iteration .	to the best of our knowledge , we are the first to provide apg-type algorithms for general nonconvex and nonsmooth problems ensuring that every accumulation point is a critical point , and the convergence rates remain o k12 when the problems are convex , in which k is the number of iterations .	1	6	7	-5.022518	4.625268	1
43-9-36	the latter waives the requirement on monotonic reduction of the objective function and needs less computation in each iteration .	numerical results testify to the advantage of our algorithms in speed .	1	6	8	-5.953648	5.0704107	1
43-9-36	numerical results testify to the advantage of our algorithms in speed .	to the best of our knowledge , we are the first to provide apg-type algorithms for general nonconvex and nonsmooth problems ensuring that every accumulation point is a critical point , and the convergence rates remain o k12 when the problems are convex , in which k is the number of iterations .	0	8	7	4.8923736	-4.35208	0
44-5-10	we show that for a wide class of optimization problems , if the sketch is close ( in the spectral norm ) to the original data matrix , then one can recover a near optimal solution to the optimization problem by using the sketch .	we study how well one can recover sparse principal components of a data matrix using a sketch formed from a few of its elements .	0	1	0	5.6385775	-5.0288787	0
44-5-10	we study how well one can recover sparse principal components of a data matrix using a sketch formed from a few of its elements .	in particular , we use this approach to obtain sparse principal components and show that for m data points in n dimensions , o ( -2 k max { m , n } ) elements gives an -additive approximation to the sparse pca problem ( k is the stable rank of the data matrix ) .	1	0	2	-5.953663	5.2257843	1
44-5-10	we demonstrate our algorithms extensively on image , text , biological and financial data .	we study how well one can recover sparse principal components of a data matrix using a sketch formed from a few of its elements .	0	3	0	5.582771	-4.999043	0
44-5-10	we study how well one can recover sparse principal components of a data matrix using a sketch formed from a few of its elements .	the results show that not only are we able to recover the sparse pcas from the incomplete data , but by using our sparse sketch , the running time drops by a factor of five or more .	1	0	4	-5.8971214	5.1632366	1
44-5-10	we show that for a wide class of optimization problems , if the sketch is close ( in the spectral norm ) to the original data matrix , then one can recover a near optimal solution to the optimization problem by using the sketch .	[CLS] in particular, we use this approach to obtain sparse principal components and show that for m data points in n dimensions, o ( - 2 k max { m, n } ) elements gives an - additive approximation to the sparse pca problem ( k is the stable	1	1	2	-2.8089645	2.8579023	1
44-5-10	we demonstrate our algorithms extensively on image , text , biological and financial data .	we show that for a wide class of optimization problems , if the sketch is close ( in the spectral norm ) to the original data matrix , then one can recover a near optimal solution to the optimization problem by using the sketch .	0	3	1	5.257461	-4.6676655	0
44-5-10	we show that for a wide class of optimization problems , if the sketch is close ( in the spectral norm ) to the original data matrix , then one can recover a near optimal solution to the optimization problem by using the sketch .	the results show that not only are we able to recover the sparse pcas from the incomplete data , but by using our sparse sketch , the running time drops by a factor of five or more .	1	1	4	-5.7881055	5.2014904	1
44-5-10	we demonstrate our algorithms extensively on image , text , biological and financial data .	in particular , we use this approach to obtain sparse principal components and show that for m data points in n dimensions , o ( -2 k max { m , n } ) elements gives an -additive approximation to the sparse pca problem ( k is the stable rank of the data matrix ) .	0	3	2	5.2697372	-4.67987	0
44-5-10	[CLS] in particular, we use this approach to obtain sparse principal components and show that for m data points in n dimensions, o ( - 2 k max { m, n } ) elements gives an - additive approximation to the sparse pca problem ( k is the stable rank of the data matrix )	the results show that not only are we able to recover the sparse pcas from the incomplete data , but by using our sparse sketch , the running time drops by a factor of five or more .	1	2	4	-5.8333445	5.127283	1
44-5-10	the results show that not only are we able to recover the sparse pcas from the incomplete data , but by using our sparse sketch , the running time drops by a factor of five or more .	we demonstrate our algorithms extensively on image , text , biological and financial data .	0	4	3	2.6790483	-2.4878187	0
45-4-6	our estimators are derived from the von mises expansion and are based on the theory of influence functions , which appear in the semiparametric statistics literature .	we propose and analyse estimators for statistical functionals of one or more distributions under nonparametric assumptions .	0	1	0	5.136764	-4.53741	0
45-4-6	we propose and analyse estimators for statistical functionals of one or more distributions under nonparametric assumptions .	we show that estimators based either on data-splitting or a leave-one-out technique enjoy fast rates of convergence and other favorable theoretical properties .	1	0	2	-5.999418	5.1769385	1
45-4-6	we apply this framework to derive estimators for several popular information theoretic quantities , and via empirical evaluation , show the advantage of this approach over existing estimators .	we propose and analyse estimators for statistical functionals of one or more distributions under nonparametric assumptions .	0	3	0	5.266622	-4.5960608	0
45-4-6	our estimators are derived from the von mises expansion and are based on the theory of influence functions , which appear in the semiparametric statistics literature .	we show that estimators based either on data-splitting or a leave-one-out technique enjoy fast rates of convergence and other favorable theoretical properties .	1	1	2	-4.8101077	4.456888	1
45-4-6	our estimators are derived from the von mises expansion and are based on the theory of influence functions , which appear in the semiparametric statistics literature .	we apply this framework to derive estimators for several popular information theoretic quantities , and via empirical evaluation , show the advantage of this approach over existing estimators .	1	1	3	-5.6074486	5.0238066	1
45-4-6	we apply this framework to derive estimators for several popular information theoretic quantities , and via empirical evaluation , show the advantage of this approach over existing estimators .	we show that estimators based either on data-splitting or a leave-one-out technique enjoy fast rates of convergence and other favorable theoretical properties .	0	3	2	3.5986304	-3.3431773	0
46-4-6	selecting a good column ( or row ) subset of massive data matrices has found many applications in data analysis and machine learning .	we propose a new adaptive sampling algorithm that can be used to improve any relative-error column selection algorithm .	1	0	1	-5.966502	5.1793756	1
46-4-6	our algorithm delivers a tighter theoretical bound on the approximation error which we also demonstrate empirically using two well known relative-error column subset selection algorithms .	selecting a good column ( or row ) subset of massive data matrices has found many applications in data analysis and machine learning .	0	2	0	5.6923203	-5.0835075	0
46-4-6	our experimental results on synthetic and real-world data show that our algorithm outperforms non-adaptive sampling as well as prior adaptive sampling approaches .	selecting a good column ( or row ) subset of massive data matrices has found many applications in data analysis and machine learning .	0	3	0	5.665129	-5.060261	0
46-4-6	our algorithm delivers a tighter theoretical bound on the approximation error which we also demonstrate empirically using two well known relative-error column subset selection algorithms .	we propose a new adaptive sampling algorithm that can be used to improve any relative-error column selection algorithm .	0	2	1	5.5393934	-4.9010186	0
46-4-6	our experimental results on synthetic and real-world data show that our algorithm outperforms non-adaptive sampling as well as prior adaptive sampling approaches .	we propose a new adaptive sampling algorithm that can be used to improve any relative-error column selection algorithm .	0	3	1	5.5593576	-4.938427	0
46-4-6	our experimental results on synthetic and real-world data show that our algorithm outperforms non-adaptive sampling as well as prior adaptive sampling approaches .	our algorithm delivers a tighter theoretical bound on the approximation error which we also demonstrate empirically using two well known relative-error column subset selection algorithms .	0	3	2	3.4345574	-3.2064838	0
47-7-21	recent years have witnessed the superiority of non-convex sparse learning formulations over their convex counterparts in both theory and practice .	however , due to the non-convexity and non-smoothness of the regularizer , how to efficiently solve the non-convex optimization problem for large-scale data is still quite challenging .	1	0	1	-5.755574	5.168999	1
47-7-21	recent years have witnessed the superiority of non-convex sparse learning formulations over their convex counterparts in both theory and practice .	in this paper , we propose an efficient hybrid optimization algorithm for non-convex regularized problems ( honor ) .	1	0	2	-5.9788013	5.2016563	1
47-7-21	recent years have witnessed the superiority of non-convex sparse learning formulations over their convex counterparts in both theory and practice .	specifically , we develop a hybrid scheme which effectively integrates a quasi-newton ( qn ) step and a gradient descent ( gd ) step .	1	0	3	-5.9689264	5.1077757	1
47-7-21	recent years have witnessed the superiority of non-convex sparse learning formulations over their convex counterparts in both theory and practice .	our contributions are as follows : ( 1 ) honor incorporates the second-order information to greatly speed up the convergence , while it avoids solving a regularized quadratic programming and only involves matrixvector multiplications without explicitly forming the inverse hessian matrix .	1	0	4	-5.9909034	5.1715565	1
47-7-21	recent years have witnessed the superiority of non-convex sparse learning formulations over their convex counterparts in both theory and practice .	( 2 ) we establish a rigorous convergence analysis for honor , which shows that convergence is guaranteed even for non-convex problems , while it is typically challenging to analyze the convergence for non-convex problems .	1	0	5	-5.959846	5.096038	1
47-7-21	recent years have witnessed the superiority of non-convex sparse learning formulations over their convex counterparts in both theory and practice .	( 3 ) we conduct empirical studies on large-scale data sets and results demonstrate that honor converges significantly faster than state-of-the-art algorithms .	1	0	6	-5.9770093	5.1126204	1
47-7-21	in this paper , we propose an efficient hybrid optimization algorithm for non-convex regularized problems ( honor ) .	however , due to the non-convexity and non-smoothness of the regularizer , how to efficiently solve the non-convex optimization problem for large-scale data is still quite challenging .	0	2	1	4.624469	-4.1378455	0
47-7-21	however , due to the non-convexity and non-smoothness of the regularizer , how to efficiently solve the non-convex optimization problem for large-scale data is still quite challenging .	specifically , we develop a hybrid scheme which effectively integrates a quasi-newton ( qn ) step and a gradient descent ( gd ) step .	1	1	3	-5.9544044	5.209112	1
47-7-21	our contributions are as follows : ( 1 ) honor incorporates the second-order information to greatly speed up the convergence , while it avoids solving a regularized quadratic programming and only involves matrixvector multiplications without explicitly forming the inverse hessian matrix .	however , due to the non-convexity and non-smoothness of the regularizer , how to efficiently solve the non-convex optimization problem for large-scale data is still quite challenging .	0	4	1	4.6371727	-4.1427507	0
47-7-21	( 2 ) we establish a rigorous convergence analysis for honor , which shows that convergence is guaranteed even for non-convex problems , while it is typically challenging to analyze the convergence for non-convex problems .	however , due to the non-convexity and non-smoothness of the regularizer , how to efficiently solve the non-convex optimization problem for large-scale data is still quite challenging .	0	5	1	5.3128953	-4.7909217	0
47-7-21	( 3 ) we conduct empirical studies on large-scale data sets and results demonstrate that honor converges significantly faster than state-of-the-art algorithms .	however , due to the non-convexity and non-smoothness of the regularizer , how to efficiently solve the non-convex optimization problem for large-scale data is still quite challenging .	0	6	1	5.6041183	-4.976499	0
47-7-21	in this paper , we propose an efficient hybrid optimization algorithm for non-convex regularized problems ( honor ) .	specifically , we develop a hybrid scheme which effectively integrates a quasi-newton ( qn ) step and a gradient descent ( gd ) step .	1	2	3	-5.8513174	5.2448683	1
47-7-21	our contributions are as follows : ( 1 ) honor incorporates the second-order information to greatly speed up the convergence , while it avoids solving a regularized quadratic programming and only involves matrixvector multiplications without explicitly forming the inverse hessian matrix .	in this paper , we propose an efficient hybrid optimization algorithm for non-convex regularized problems ( honor ) .	0	4	2	5.5878177	-4.933795	0
47-7-21	in this paper , we propose an efficient hybrid optimization algorithm for non-convex regularized problems ( honor ) .	( 2 ) we establish a rigorous convergence analysis for honor , which shows that convergence is guaranteed even for non-convex problems , while it is typically challenging to analyze the convergence for non-convex problems .	1	2	5	-5.908158	5.229462	1
47-7-21	in this paper , we propose an efficient hybrid optimization algorithm for non-convex regularized problems ( honor ) .	( 3 ) we conduct empirical studies on large-scale data sets and results demonstrate that honor converges significantly faster than state-of-the-art algorithms .	1	2	6	-5.9379025	5.1932325	1
47-7-21	our contributions are as follows : ( 1 ) honor incorporates the second-order information to greatly speed up the convergence , while it avoids solving a regularized quadratic programming and only involves matrixvector multiplications without explicitly forming the inverse hessian matrix .	specifically , we develop a hybrid scheme which effectively integrates a quasi-newton ( qn ) step and a gradient descent ( gd ) step .	0	4	3	-4.0949974	3.9419737	1
47-7-21	specifically , we develop a hybrid scheme which effectively integrates a quasi-newton ( qn ) step and a gradient descent ( gd ) step .	( 2 ) we establish a rigorous convergence analysis for honor , which shows that convergence is guaranteed even for non-convex problems , while it is typically challenging to analyze the convergence for non-convex problems .	1	3	5	2.1708934	-1.8855071	0
47-7-21	( 3 ) we conduct empirical studies on large-scale data sets and results demonstrate that honor converges significantly faster than state-of-the-art algorithms .	specifically , we develop a hybrid scheme which effectively integrates a quasi-newton ( qn ) step and a gradient descent ( gd ) step .	0	6	3	5.151017	-4.56389	0
47-7-21	our contributions are as follows : ( 1 ) honor incorporates the second-order information to greatly speed up the convergence , while it avoids solving a regularized quadratic programming and only involves matrixvector multiplications without explicitly forming the inverse hessian matrix .	( 2 ) we establish a rigorous convergence analysis for honor , which shows that convergence is guaranteed even for non-convex problems , while it is typically challenging to analyze the convergence for non-convex problems .	1	4	5	-5.9685946	5.230419	1
47-7-21	( 3 ) we conduct empirical studies on large-scale data sets and results demonstrate that honor converges significantly faster than state-of-the-art algorithms .	our contributions are as follows : ( 1 ) honor incorporates the second-order information to greatly speed up the convergence , while it avoids solving a regularized quadratic programming and only involves matrixvector multiplications without explicitly forming the inverse hessian matrix .	0	6	4	5.3911824	-4.7623672	0
47-7-21	( 3 ) we conduct empirical studies on large-scale data sets and results demonstrate that honor converges significantly faster than state-of-the-art algorithms .	( 2 ) we establish a rigorous convergence analysis for honor , which shows that convergence is guaranteed even for non-convex problems , while it is typically challenging to analyze the convergence for non-convex problems .	0	6	5	5.326905	-4.726945	0
48-5-10	our method exploits stereo imagery to place proposals in the form of 3d bounding boxes .	the goal of this paper is to generate high-quality 3d object proposals in the context of autonomous driving .	0	1	0	5.4072094	-4.864219	0
48-5-10	the goal of this paper is to generate high-quality 3d object proposals in the context of autonomous driving .	we formulate the problem as minimizing an energy function encoding object size priors , ground plane as well as several depth informed features that reason about free space , point cloud densities and distance to the ground .	1	0	2	-5.4007635	4.937495	1
48-5-10	our experiments show significant performance gains over existing rgb and rgb-d object proposal methods on the challenging kitti benchmark .	the goal of this paper is to generate high-quality 3d object proposals in the context of autonomous driving .	0	3	0	5.582313	-5.043951	0
48-5-10	the goal of this paper is to generate high-quality 3d object proposals in the context of autonomous driving .	combined with convolutional neural net ( cnn ) scoring , our approach outperforms all existing results on all three kitti object classes .	1	0	4	-6.016413	5.2229366	1
48-5-10	we formulate the problem as minimizing an energy function encoding object size priors , ground plane as well as several depth informed features that reason about free space , point cloud densities and distance to the ground .	our method exploits stereo imagery to place proposals in the form of 3d bounding boxes .	0	2	1	-3.0969281	3.0060763	1
48-5-10	our method exploits stereo imagery to place proposals in the form of 3d bounding boxes .	our experiments show significant performance gains over existing rgb and rgb-d object proposal methods on the challenging kitti benchmark .	1	1	3	-5.601431	5.0360413	1
48-5-10	our method exploits stereo imagery to place proposals in the form of 3d bounding boxes .	combined with convolutional neural net ( cnn ) scoring , our approach outperforms all existing results on all three kitti object classes .	1	1	4	-5.714764	5.1610165	1
48-5-10	we formulate the problem as minimizing an energy function encoding object size priors , ground plane as well as several depth informed features that reason about free space , point cloud densities and distance to the ground .	our experiments show significant performance gains over existing rgb and rgb-d object proposal methods on the challenging kitti benchmark .	1	2	3	-6.021591	5.179383	1
48-5-10	combined with convolutional neural net ( cnn ) scoring , our approach outperforms all existing results on all three kitti object classes .	we formulate the problem as minimizing an energy function encoding object size priors , ground plane as well as several depth informed features that reason about free space , point cloud densities and distance to the ground .	0	4	2	5.370247	-4.7828665	0
48-5-10	combined with convolutional neural net ( cnn ) scoring , our approach outperforms all existing results on all three kitti object classes .	our experiments show significant performance gains over existing rgb and rgb-d object proposal methods on the challenging kitti benchmark .	0	4	3	2.3771648	-2.2120106	0
49-9-36	we study contextual bandits with budget and time constraints , referred to as constrained contextual bandits .	the time and budget constraints significantly complicate the exploration and exploitation tradeoff because they introduce complex coupling among contexts over time .	1	0	1	-5.803026	5.0758734	1
49-9-36	we study contextual bandits with budget and time constraints , referred to as constrained contextual bandits .	to gain insight , we first study unit-cost systems with known context distribution .	1	0	2	-6.0141945	5.1597967	1
49-9-36	when the expected rewards are known , we develop an approximation of the oracle , referred to adaptive-linear-programming ( alp ) , which achieves near-optimality and only requires the ordering of expected rewards .	we study contextual bandits with budget and time constraints , referred to as constrained contextual bandits .	0	3	0	5.6576176	-5.0475526	0
49-9-36	with these highly desirable features , we then combine alp with the upper-confidence-bound ( ucb ) method in the general case where the expected rewards are unknown a priori .	we study contextual bandits with budget and time constraints , referred to as constrained contextual bandits .	0	4	0	5.6491804	-5.017089	0
49-9-36	we show that the proposed ucb-alp algorithm achieves logarithmic regret except for certain boundary cases .	we study contextual bandits with budget and time constraints , referred to as constrained contextual bandits .	0	5	0	5.5988398	-5.014383	0
49-9-36	we study contextual bandits with budget and time constraints , referred to as constrained contextual bandits .	further , we design algorithms and obtain similar regret bounds for more general systems with unknown context distribution and heterogeneous costs .	1	0	6	-5.98891	5.195576	1
49-9-36	to the best of our knowledge , this is the first work that shows how to achieve logarithmic regret in constrained contextual bandits .	we study contextual bandits with budget and time constraints , referred to as constrained contextual bandits .	0	7	0	5.6315484	-4.9981146	0
49-9-36	moreover , this work also sheds light on the study of computationally efficient algorithms for general constrained contextual bandits .	we study contextual bandits with budget and time constraints , referred to as constrained contextual bandits .	0	8	0	5.5816236	-4.9595413	0
49-9-36	the time and budget constraints significantly complicate the exploration and exploitation tradeoff because they introduce complex coupling among contexts over time .	to gain insight , we first study unit-cost systems with known context distribution .	1	1	2	-4.656572	4.466239	1
49-9-36	when the expected rewards are known , we develop an approximation of the oracle , referred to adaptive-linear-programming ( alp ) , which achieves near-optimality and only requires the ordering of expected rewards .	the time and budget constraints significantly complicate the exploration and exploitation tradeoff because they introduce complex coupling among contexts over time .	0	3	1	5.127147	-4.5170436	0
49-9-36	with these highly desirable features , we then combine alp with the upper-confidence-bound ( ucb ) method in the general case where the expected rewards are unknown a priori .	the time and budget constraints significantly complicate the exploration and exploitation tradeoff because they introduce complex coupling among contexts over time .	0	4	1	4.889289	-4.3736706	0
49-9-36	the time and budget constraints significantly complicate the exploration and exploitation tradeoff because they introduce complex coupling among contexts over time .	we show that the proposed ucb-alp algorithm achieves logarithmic regret except for certain boundary cases .	1	1	5	-5.9810066	5.114812	1
49-9-36	further , we design algorithms and obtain similar regret bounds for more general systems with unknown context distribution and heterogeneous costs .	the time and budget constraints significantly complicate the exploration and exploitation tradeoff because they introduce complex coupling among contexts over time .	0	6	1	5.211699	-4.7085056	0
49-9-36	the time and budget constraints significantly complicate the exploration and exploitation tradeoff because they introduce complex coupling among contexts over time .	to the best of our knowledge , this is the first work that shows how to achieve logarithmic regret in constrained contextual bandits .	1	1	7	-5.962043	5.10293	1
49-9-36	moreover , this work also sheds light on the study of computationally efficient algorithms for general constrained contextual bandits .	the time and budget constraints significantly complicate the exploration and exploitation tradeoff because they introduce complex coupling among contexts over time .	0	8	1	5.119143	-4.610928	0
49-9-36	to gain insight , we first study unit-cost systems with known context distribution .	when the expected rewards are known , we develop an approximation of the oracle , referred to adaptive-linear-programming ( alp ) , which achieves near-optimality and only requires the ordering of expected rewards .	1	2	3	-5.6166077	5.1202826	1
49-9-36	to gain insight , we first study unit-cost systems with known context distribution .	with these highly desirable features , we then combine alp with the upper-confidence-bound ( ucb ) method in the general case where the expected rewards are unknown a priori .	1	2	4	-5.8485374	5.1950846	1
49-9-36	we show that the proposed ucb-alp algorithm achieves logarithmic regret except for certain boundary cases .	to gain insight , we first study unit-cost systems with known context distribution .	0	5	2	5.0639777	-4.488185	0
49-9-36	further , we design algorithms and obtain similar regret bounds for more general systems with unknown context distribution and heterogeneous costs .	to gain insight , we first study unit-cost systems with known context distribution .	0	6	2	5.2561207	-4.5904055	0
49-9-36	to the best of our knowledge , this is the first work that shows how to achieve logarithmic regret in constrained contextual bandits .	to gain insight , we first study unit-cost systems with known context distribution .	0	7	2	4.9176903	-4.3097587	0
49-9-36	to gain insight , we first study unit-cost systems with known context distribution .	moreover , this work also sheds light on the study of computationally efficient algorithms for general constrained contextual bandits .	1	2	8	-5.719184	4.976669	1
49-9-36	with these highly desirable features , we then combine alp with the upper-confidence-bound ( ucb ) method in the general case where the expected rewards are unknown a priori .	when the expected rewards are known , we develop an approximation of the oracle , referred to adaptive-linear-programming ( alp ) , which achieves near-optimality and only requires the ordering of expected rewards .	0	4	3	5.2101574	-4.556141	0
49-9-36	when the expected rewards are known , we develop an approximation of the oracle , referred to adaptive-linear-programming ( alp ) , which achieves near-optimality and only requires the ordering of expected rewards .	we show that the proposed ucb-alp algorithm achieves logarithmic regret except for certain boundary cases .	1	3	5	-5.9703493	5.1164036	1
49-9-36	when the expected rewards are known , we develop an approximation of the oracle , referred to adaptive-linear-programming ( alp ) , which achieves near-optimality and only requires the ordering of expected rewards .	further , we design algorithms and obtain similar regret bounds for more general systems with unknown context distribution and heterogeneous costs .	1	3	6	-4.2434998	4.027807	1
49-9-36	to the best of our knowledge , this is the first work that shows how to achieve logarithmic regret in constrained contextual bandits .	when the expected rewards are known , we develop an approximation of the oracle , referred to adaptive-linear-programming ( alp ) , which achieves near-optimality and only requires the ordering of expected rewards .	0	7	3	3.8503392	-3.511107	0
49-9-36	moreover , this work also sheds light on the study of computationally efficient algorithms for general constrained contextual bandits .	when the expected rewards are known , we develop an approximation of the oracle , referred to adaptive-linear-programming ( alp ) , which achieves near-optimality and only requires the ordering of expected rewards .	0	8	3	3.773384	-3.5765603	0
49-9-36	we show that the proposed ucb-alp algorithm achieves logarithmic regret except for certain boundary cases .	with these highly desirable features , we then combine alp with the upper-confidence-bound ( ucb ) method in the general case where the expected rewards are unknown a priori .	0	5	4	5.386242	-4.785145	0
49-9-36	further , we design algorithms and obtain similar regret bounds for more general systems with unknown context distribution and heterogeneous costs .	with these highly desirable features , we then combine alp with the upper-confidence-bound ( ucb ) method in the general case where the expected rewards are unknown a priori .	0	6	4	3.015843	-2.8331313	0
49-9-36	with these highly desirable features , we then combine alp with the upper-confidence-bound ( ucb ) method in the general case where the expected rewards are unknown a priori .	to the best of our knowledge , this is the first work that shows how to achieve logarithmic regret in constrained contextual bandits .	1	4	7	-5.107056	4.700754	1
49-9-36	with these highly desirable features , we then combine alp with the upper-confidence-bound ( ucb ) method in the general case where the expected rewards are unknown a priori .	moreover , this work also sheds light on the study of computationally efficient algorithms for general constrained contextual bandits .	1	4	8	-4.8733754	4.502625	1
49-9-36	further , we design algorithms and obtain similar regret bounds for more general systems with unknown context distribution and heterogeneous costs .	we show that the proposed ucb-alp algorithm achieves logarithmic regret except for certain boundary cases .	0	6	5	-3.1080859	3.0329013	1
49-9-36	we show that the proposed ucb-alp algorithm achieves logarithmic regret except for certain boundary cases .	to the best of our knowledge , this is the first work that shows how to achieve logarithmic regret in constrained contextual bandits .	1	5	7	-2.8179357	2.8108778	1
49-9-36	moreover , this work also sheds light on the study of computationally efficient algorithms for general constrained contextual bandits .	we show that the proposed ucb-alp algorithm achieves logarithmic regret except for certain boundary cases .	0	8	5	2.9682884	-2.8610964	0
49-9-36	further , we design algorithms and obtain similar regret bounds for more general systems with unknown context distribution and heterogeneous costs .	to the best of our knowledge , this is the first work that shows how to achieve logarithmic regret in constrained contextual bandits .	1	6	7	-3.3723793	3.2837496	1
49-9-36	moreover , this work also sheds light on the study of computationally efficient algorithms for general constrained contextual bandits .	further , we design algorithms and obtain similar regret bounds for more general systems with unknown context distribution and heterogeneous costs .	0	8	6	3.5366552	-3.3560643	0
49-9-36	to the best of our knowledge , this is the first work that shows how to achieve logarithmic regret in constrained contextual bandits .	moreover , this work also sheds light on the study of computationally efficient algorithms for general constrained contextual bandits .	1	7	8	-4.3212852	4.066309	1
50-5-10	at the same time , models of this class are very demanding in terms of computational resources .	deep neural networks currently demonstrate state-of-the-art performance in several domains .	0	1	0	5.4563856	-4.8590612	0
50-5-10	deep neural networks currently demonstrate state-of-the-art performance in several domains .	in particular , a large amount of memory is required by commonly used fully-connected layers , making it hard to use the models on low-end devices and stopping the further increase of the model size .	1	0	2	-5.8359833	5.2404428	1
50-5-10	in this paper we convert the dense weight matrices of the fully-connected layers to the tensor train format such that the number of parameters is reduced by a huge factor and at the same time the expressive power of the layer is preserved .	deep neural networks currently demonstrate state-of-the-art performance in several domains .	0	3	0	5.7471824	-5.111663	0
50-5-10	in particular , for the very deep vgg networks we report the compression factor of the dense weight matrix of a fully-connected layer up to 200000 times leading to the compression factor of the whole network up to 7 times .	deep neural networks currently demonstrate state-of-the-art performance in several domains .	0	4	0	5.6368413	-4.9770513	0
50-5-10	in particular , a large amount of memory is required by commonly used fully-connected layers , making it hard to use the models on low-end devices and stopping the further increase of the model size .	at the same time , models of this class are very demanding in terms of computational resources .	0	2	1	4.08253	-3.6254535	0
50-5-10	in this paper we convert the dense weight matrices of the fully-connected layers to the tensor train format such that the number of parameters is reduced by a huge factor and at the same time the expressive power of the layer is preserved .	at the same time , models of this class are very demanding in terms of computational resources .	0	3	1	5.3544884	-4.6894846	0
50-5-10	at the same time , models of this class are very demanding in terms of computational resources .	in particular , for the very deep vgg networks we report the compression factor of the dense weight matrix of a fully-connected layer up to 200000 times leading to the compression factor of the whole network up to 7 times .	1	1	4	-5.925613	5.210435	1
50-5-10	in this paper we convert the dense weight matrices of the fully-connected layers to the tensor train format such that the number of parameters is reduced by a huge factor and at the same time the expressive power of the layer is preserved .	in particular , a large amount of memory is required by commonly used fully-connected layers , making it hard to use the models on low-end devices and stopping the further increase of the model size .	0	3	2	3.9316235	-3.6276474	0
50-5-10	in particular , for the very deep vgg networks we report the compression factor of the dense weight matrix of a fully-connected layer up to 200000 times leading to the compression factor of the whole network up to 7 times .	in particular , a large amount of memory is required by commonly used fully-connected layers , making it hard to use the models on low-end devices and stopping the further increase of the model size .	0	4	2	4.388222	-3.962503	0
50-5-10	in this paper we convert the dense weight matrices of the fully-connected layers to the tensor train format such that the number of parameters is reduced by a huge factor and at the same time the expressive power of the layer is preserved .	in particular , for the very deep vgg networks we report the compression factor of the dense weight matrix of a fully-connected layer up to 200000 times leading to the compression factor of the whole network up to 7 times .	1	3	4	-5.7451296	5.1039495	1
51-8-28	the modern scale of data has brought new challenges to bayesian inference .	in particular , conventional mcmc algorithms are computationally very expensive for large data sets .	1	0	1	-5.4190903	4.958903	1
51-8-28	a promising approach to solve this problem is embarrassingly parallel mcmc ( ep-mcmc ) , which first partitions the data into multiple subsets and runs independent sampling algorithms on each subset .	the modern scale of data has brought new challenges to bayesian inference .	0	2	0	5.5588136	-4.980321	0
51-8-28	the subset posterior draws are then aggregated via some combining rules to obtain the final approximation .	the modern scale of data has brought new challenges to bayesian inference .	0	3	0	5.6324863	-5.0348234	0
51-8-28	the modern scale of data has brought new challenges to bayesian inference .	existing ep-mcmc algorithms are limited by approximation accuracy and difficulty in resampling .	1	0	4	-5.9022546	5.2108126	1
51-8-28	in this article , we propose a new ep-mcmc algorithm part that solves these problems .	the modern scale of data has brought new challenges to bayesian inference .	0	5	0	5.6602244	-5.0295057	0
51-8-28	the modern scale of data has brought new challenges to bayesian inference .	the new algorithm applies random partition trees to combine the subset posterior draws , which is distribution-free , easy to resample from and can adapt to multiple scales .	1	0	6	-5.921665	5.1889353	1
51-8-28	the modern scale of data has brought new challenges to bayesian inference .	we provide theoretical justification and extensive experiments illustrating empirical performance .	1	0	7	-5.935468	5.0645003	1
51-8-28	in particular , conventional mcmc algorithms are computationally very expensive for large data sets .	a promising approach to solve this problem is embarrassingly parallel mcmc ( ep-mcmc ) , which first partitions the data into multiple subsets and runs independent sampling algorithms on each subset .	1	1	2	-2.2707348	2.3642063	1
51-8-28	the subset posterior draws are then aggregated via some combining rules to obtain the final approximation .	in particular , conventional mcmc algorithms are computationally very expensive for large data sets .	0	3	1	4.5979614	-4.131036	0
51-8-28	existing ep-mcmc algorithms are limited by approximation accuracy and difficulty in resampling .	in particular , conventional mcmc algorithms are computationally very expensive for large data sets .	0	4	1	3.299748	-3.0611176	0
51-8-28	in this article , we propose a new ep-mcmc algorithm part that solves these problems .	in particular , conventional mcmc algorithms are computationally very expensive for large data sets .	0	5	1	5.242123	-4.5875826	0
51-8-28	in particular , conventional mcmc algorithms are computationally very expensive for large data sets .	the new algorithm applies random partition trees to combine the subset posterior draws , which is distribution-free , easy to resample from and can adapt to multiple scales .	1	1	6	-5.782619	5.237419	1
51-8-28	in particular , conventional mcmc algorithms are computationally very expensive for large data sets .	we provide theoretical justification and extensive experiments illustrating empirical performance .	1	1	7	-5.959648	5.1133747	1
51-8-28	a promising approach to solve this problem is embarrassingly parallel mcmc ( ep-mcmc ) , which first partitions the data into multiple subsets and runs independent sampling algorithms on each subset .	the subset posterior draws are then aggregated via some combining rules to obtain the final approximation .	1	2	3	-5.631626	5.1589365	1
51-8-28	a promising approach to solve this problem is embarrassingly parallel mcmc ( ep-mcmc ) , which first partitions the data into multiple subsets and runs independent sampling algorithms on each subset .	existing ep-mcmc algorithms are limited by approximation accuracy and difficulty in resampling .	1	2	4	-5.8314877	5.131543	1
51-8-28	a promising approach to solve this problem is embarrassingly parallel mcmc ( ep-mcmc ) , which first partitions the data into multiple subsets and runs independent sampling algorithms on each subset .	in this article , we propose a new ep-mcmc algorithm part that solves these problems .	1	2	5	-5.8773327	4.9278316	1
51-8-28	a promising approach to solve this problem is embarrassingly parallel mcmc ( ep-mcmc ) , which first partitions the data into multiple subsets and runs independent sampling algorithms on each subset .	the new algorithm applies random partition trees to combine the subset posterior draws , which is distribution-free , easy to resample from and can adapt to multiple scales .	1	2	6	-5.952379	5.2319326	1
51-8-28	a promising approach to solve this problem is embarrassingly parallel mcmc ( ep-mcmc ) , which first partitions the data into multiple subsets and runs independent sampling algorithms on each subset .	we provide theoretical justification and extensive experiments illustrating empirical performance .	1	2	7	-5.94416	5.1562347	1
51-8-28	existing ep-mcmc algorithms are limited by approximation accuracy and difficulty in resampling .	the subset posterior draws are then aggregated via some combining rules to obtain the final approximation .	0	4	3	-5.555937	5.103467	1
51-8-28	the subset posterior draws are then aggregated via some combining rules to obtain the final approximation .	in this article , we propose a new ep-mcmc algorithm part that solves these problems .	1	3	5	2.3822126	-2.0781305	0
51-8-28	the subset posterior draws are then aggregated via some combining rules to obtain the final approximation .	the new algorithm applies random partition trees to combine the subset posterior draws , which is distribution-free , easy to resample from and can adapt to multiple scales .	1	3	6	0.5581037	-0.22604294	0
51-8-28	we provide theoretical justification and extensive experiments illustrating empirical performance .	the subset posterior draws are then aggregated via some combining rules to obtain the final approximation .	0	7	3	5.384969	-4.827008	0
51-8-28	existing ep-mcmc algorithms are limited by approximation accuracy and difficulty in resampling .	in this article , we propose a new ep-mcmc algorithm part that solves these problems .	1	4	5	-5.8581486	5.179401	1
51-8-28	the new algorithm applies random partition trees to combine the subset posterior draws , which is distribution-free , easy to resample from and can adapt to multiple scales .	existing ep-mcmc algorithms are limited by approximation accuracy and difficulty in resampling .	0	6	4	4.917499	-4.4379644	0
51-8-28	we provide theoretical justification and extensive experiments illustrating empirical performance .	existing ep-mcmc algorithms are limited by approximation accuracy and difficulty in resampling .	0	7	4	5.5064526	-4.8856564	0
51-8-28	the new algorithm applies random partition trees to combine the subset posterior draws , which is distribution-free , easy to resample from and can adapt to multiple scales .	in this article , we propose a new ep-mcmc algorithm part that solves these problems .	0	6	5	4.8376718	-4.405036	0
51-8-28	in this article , we propose a new ep-mcmc algorithm part that solves these problems .	we provide theoretical justification and extensive experiments illustrating empirical performance .	1	5	7	-5.949977	5.1145077	1
51-8-28	we provide theoretical justification and extensive experiments illustrating empirical performance .	the new algorithm applies random partition trees to combine the subset posterior draws , which is distribution-free , easy to resample from and can adapt to multiple scales .	0	7	6	5.206409	-4.638858	0
52-5-10	we develop a shared response model for aggregating multi-subject fmri data that accounts for different functional topographies among anatomically aligned datasets .	multi-subject fmri data is critical for evaluating the generality and validity of findings across subjects , and its effective utilization helps improve analysis sensitivity .	0	1	0	3.7292361	-3.3665829	0
52-5-10	multi-subject fmri data is critical for evaluating the generality and validity of findings across subjects , and its effective utilization helps improve analysis sensitivity .	our model demonstrates improved sensitivity in identifying a shared response for a variety of datasets and anatomical brain regions of interest .	1	0	2	-2.844249	2.8063354	1
52-5-10	multi-subject fmri data is critical for evaluating the generality and validity of findings across subjects , and its effective utilization helps improve analysis sensitivity .	furthermore , by removing the identified shared response , it allows improved detection of group differences .	1	0	3	0.23725173	-0.07353124	0
52-5-10	multi-subject fmri data is critical for evaluating the generality and validity of findings across subjects , and its effective utilization helps improve analysis sensitivity .	the ability to identify what is shared and what is not shared opens the model to a wide range of multi-subject fmri studies .	1	0	4	-4.320447	4.15254	1
52-5-10	our model demonstrates improved sensitivity in identifying a shared response for a variety of datasets and anatomical brain regions of interest .	we develop a shared response model for aggregating multi-subject fmri data that accounts for different functional topographies among anatomically aligned datasets .	0	2	1	5.331445	-4.682248	0
52-5-10	furthermore , by removing the identified shared response , it allows improved detection of group differences .	we develop a shared response model for aggregating multi-subject fmri data that accounts for different functional topographies among anatomically aligned datasets .	0	3	1	3.712161	-3.3306758	0
52-5-10	we develop a shared response model for aggregating multi-subject fmri data that accounts for different functional topographies among anatomically aligned datasets .	the ability to identify what is shared and what is not shared opens the model to a wide range of multi-subject fmri studies .	1	1	4	-5.986805	5.1709414	1
52-5-10	furthermore , by removing the identified shared response , it allows improved detection of group differences .	our model demonstrates improved sensitivity in identifying a shared response for a variety of datasets and anatomical brain regions of interest .	0	3	2	-3.8393157	3.5831857	1
52-5-10	our model demonstrates improved sensitivity in identifying a shared response for a variety of datasets and anatomical brain regions of interest .	the ability to identify what is shared and what is not shared opens the model to a wide range of multi-subject fmri studies .	1	2	4	-2.0813272	2.1623313	1
52-5-10	the ability to identify what is shared and what is not shared opens the model to a wide range of multi-subject fmri studies .	furthermore , by removing the identified shared response , it allows improved detection of group differences .	0	4	3	3.0113547	-2.8656816	0
53-6-15	we develop a latent variable model and an efficient spectral algorithm motivated by the recent emergence of very large data sets of chromatin marks from multiple human cell types .	a natural model for chromatin data in one cell type is a hidden markov model ( hmm ) ; we model the relationship between multiple cell types by connecting their hidden states by a fixed tree of known structure .	1	0	1	2.8234262	-2.4083729	0
53-6-15	the main challenge with learning parameters of such models is that iterative methods such as em are very slow , while naive spectral methods result in time and space complexity exponential in the number of cell types .	we develop a latent variable model and an efficient spectral algorithm motivated by the recent emergence of very large data sets of chromatin marks from multiple human cell types .	0	2	0	-5.621477	5.031346	1
53-6-15	we exploit properties of the tree structure of the hidden states to provide spectral algorithms that are more computationally efficient for current biological datasets .	we develop a latent variable model and an efficient spectral algorithm motivated by the recent emergence of very large data sets of chromatin marks from multiple human cell types .	0	3	0	4.37206	-3.8570201	0
53-6-15	we provide sample complexity bounds for our algorithm and evaluate it experimentally on biological data from nine human cell types .	we develop a latent variable model and an efficient spectral algorithm motivated by the recent emergence of very large data sets of chromatin marks from multiple human cell types .	0	4	0	5.3770423	-4.744945	0
53-6-15	finally , we show that beyond our specific model , some of our algorithmic ideas can be applied to other graphical models .	we develop a latent variable model and an efficient spectral algorithm motivated by the recent emergence of very large data sets of chromatin marks from multiple human cell types .	0	5	0	5.288541	-4.6420894	0
53-6-15	a natural model for chromatin data in one cell type is a hidden markov model ( hmm ) ; we model the relationship between multiple cell types by connecting their hidden states by a fixed tree of known structure .	the main challenge with learning parameters of such models is that iterative methods such as em are very slow , while naive spectral methods result in time and space complexity exponential in the number of cell types .	1	1	2	-2.3767567	2.5106595	1
53-6-15	we exploit properties of the tree structure of the hidden states to provide spectral algorithms that are more computationally efficient for current biological datasets .	a natural model for chromatin data in one cell type is a hidden markov model ( hmm ) ; we model the relationship between multiple cell types by connecting their hidden states by a fixed tree of known structure .	0	3	1	5.6578026	-4.954785	0
53-6-15	we provide sample complexity bounds for our algorithm and evaluate it experimentally on biological data from nine human cell types .	a natural model for chromatin data in one cell type is a hidden markov model ( hmm ) ; we model the relationship between multiple cell types by connecting their hidden states by a fixed tree of known structure .	0	4	1	5.5386233	-4.879481	0
53-6-15	a natural model for chromatin data in one cell type is a hidden markov model ( hmm ) ; we model the relationship between multiple cell types by connecting their hidden states by a fixed tree of known structure .	finally , we show that beyond our specific model , some of our algorithmic ideas can be applied to other graphical models .	1	1	5	-5.8875957	4.9586396	1
53-6-15	we exploit properties of the tree structure of the hidden states to provide spectral algorithms that are more computationally efficient for current biological datasets .	the main challenge with learning parameters of such models is that iterative methods such as em are very slow , while naive spectral methods result in time and space complexity exponential in the number of cell types .	0	3	2	4.714815	-4.178878	0
53-6-15	the main challenge with learning parameters of such models is that iterative methods such as em are very slow , while naive spectral methods result in time and space complexity exponential in the number of cell types .	we provide sample complexity bounds for our algorithm and evaluate it experimentally on biological data from nine human cell types .	1	2	4	-5.957346	5.1006556	1
53-6-15	the main challenge with learning parameters of such models is that iterative methods such as em are very slow , while naive spectral methods result in time and space complexity exponential in the number of cell types .	finally , we show that beyond our specific model , some of our algorithmic ideas can be applied to other graphical models .	1	2	5	-5.8833027	4.96782	1
53-6-15	we exploit properties of the tree structure of the hidden states to provide spectral algorithms that are more computationally efficient for current biological datasets .	we provide sample complexity bounds for our algorithm and evaluate it experimentally on biological data from nine human cell types .	1	3	4	-5.6648207	5.133113	1
53-6-15	we exploit properties of the tree structure of the hidden states to provide spectral algorithms that are more computationally efficient for current biological datasets .	finally , we show that beyond our specific model , some of our algorithmic ideas can be applied to other graphical models .	1	3	5	-5.890205	5.1645403	1
53-6-15	finally , we show that beyond our specific model , some of our algorithmic ideas can be applied to other graphical models .	we provide sample complexity bounds for our algorithm and evaluate it experimentally on biological data from nine human cell types .	0	5	4	3.6818044	-3.4397268	0
54-4-6	our formalization in the context of infinite-horizon and finitely-nested interactive pomdps ( i-pomdp ) is distinct from em formulations for pomdps and cooperative multiagent planning frameworks .	this paper provides the first formalization of self-interested planning in multiagent settings using expectation-maximization ( em ) .	0	1	0	5.6360044	-4.9945626	0
54-4-6	this paper provides the first formalization of self-interested planning in multiagent settings using expectation-maximization ( em ) .	we exploit the graphical model structure specific to i-pomdps , and present a new approach based on block-coordinate descent for further speed up .	1	0	2	-5.996301	5.242588	1
54-4-6	forward filtering-backward sampling - a combination of exact filtering with sampling - is explored to exploit problem structure .	this paper provides the first formalization of self-interested planning in multiagent settings using expectation-maximization ( em ) .	0	3	0	5.4724927	-4.941739	0
54-4-6	we exploit the graphical model structure specific to i-pomdps , and present a new approach based on block-coordinate descent for further speed up .	our formalization in the context of infinite-horizon and finitely-nested interactive pomdps ( i-pomdp ) is distinct from em formulations for pomdps and cooperative multiagent planning frameworks .	0	2	1	5.261919	-4.5666623	0
54-4-6	forward filtering-backward sampling - a combination of exact filtering with sampling - is explored to exploit problem structure .	our formalization in the context of infinite-horizon and finitely-nested interactive pomdps ( i-pomdp ) is distinct from em formulations for pomdps and cooperative multiagent planning frameworks .	0	3	1	-0.27680716	0.58768773	1
54-4-6	forward filtering-backward sampling - a combination of exact filtering with sampling - is explored to exploit problem structure .	we exploit the graphical model structure specific to i-pomdps , and present a new approach based on block-coordinate descent for further speed up .	0	3	2	-5.4537315	4.9016485	1
55-10-45	mixture modeling is a general technique for making any simple model more expressive through weighted combination .	this generality and simplicity in part explains the success of the expectation maximization ( em ) algorithm , in which updates are easy to derive for a wide class of mixture models .	1	0	1	-5.953079	5.1204925	1
55-10-45	however , the likelihood of a mixture model is non-convex , so em has no known global convergence guarantees .	mixture modeling is a general technique for making any simple model more expressive through weighted combination .	0	2	0	5.455334	-4.8026133	0
55-10-45	mixture modeling is a general technique for making any simple model more expressive through weighted combination .	recently , method of moments approaches offer global guarantees for some mixture models , but they do not extend easily to the range of mixture models that exist .	1	0	3	-5.9539566	5.096386	1
55-10-45	in this work , we present polymom , an unifying framework based on method of moments in which estimation procedures are easily derivable , just as in em .	mixture modeling is a general technique for making any simple model more expressive through weighted combination .	0	4	0	5.0322247	-4.3630996	0
55-10-45	polymom is applicable when the moments of a single mixture component are polynomials of the parameters .	mixture modeling is a general technique for making any simple model more expressive through weighted combination .	0	5	0	5.3261952	-4.705926	0
55-10-45	mixture modeling is a general technique for making any simple model more expressive through weighted combination .	our key observation is that the moments of the mixture model are a mixture of these polynomials , which allows us to cast estimation as a generalized moment problem .	1	0	6	-5.951146	5.1205177	1
55-10-45	we solve its relaxations using semidefinite optimization , and then extract parameters using ideas from computer algebra .	mixture modeling is a general technique for making any simple model more expressive through weighted combination .	0	7	0	5.460974	-4.83718	0
55-10-45	mixture modeling is a general technique for making any simple model more expressive through weighted combination .	this framework allows us to draw insights and apply tools from convex optimization , computer algebra and the theory of moments to study problems in statistical estimation .	1	0	8	-6.0190105	5.157425	1
55-10-45	mixture modeling is a general technique for making any simple model more expressive through weighted combination .	simulations show good empirical performance on several models .	1	0	9	-5.9938087	5.1254754	1
55-10-45	however , the likelihood of a mixture model is non-convex , so em has no known global convergence guarantees .	this generality and simplicity in part explains the success of the expectation maximization ( em ) algorithm , in which updates are easy to derive for a wide class of mixture models .	0	2	1	3.273192	-3.032445	0
55-10-45	recently , method of moments approaches offer global guarantees for some mixture models , but they do not extend easily to the range of mixture models that exist .	this generality and simplicity in part explains the success of the expectation maximization ( em ) algorithm , in which updates are easy to derive for a wide class of mixture models .	0	3	1	-5.9628844	5.179998	1
55-10-45	in this work , we present polymom , an unifying framework based on method of moments in which estimation procedures are easily derivable , just as in em .	this generality and simplicity in part explains the success of the expectation maximization ( em ) algorithm , in which updates are easy to derive for a wide class of mixture models .	0	4	1	3.3307014	-3.011416	0
55-10-45	this generality and simplicity in part explains the success of the expectation maximization ( em ) algorithm , in which updates are easy to derive for a wide class of mixture models .	polymom is applicable when the moments of a single mixture component are polynomials of the parameters .	1	1	5	4.0324273	-3.7425666	0
55-10-45	our key observation is that the moments of the mixture model are a mixture of these polynomials , which allows us to cast estimation as a generalized moment problem .	this generality and simplicity in part explains the success of the expectation maximization ( em ) algorithm , in which updates are easy to derive for a wide class of mixture models .	0	6	1	-5.011909	4.600507	1
55-10-45	this generality and simplicity in part explains the success of the expectation maximization ( em ) algorithm , in which updates are easy to derive for a wide class of mixture models .	we solve its relaxations using semidefinite optimization , and then extract parameters using ideas from computer algebra .	1	1	7	2.0759408	-1.8817387	0
55-10-45	this generality and simplicity in part explains the success of the expectation maximization ( em ) algorithm , in which updates are easy to derive for a wide class of mixture models .	this framework allows us to draw insights and apply tools from convex optimization , computer algebra and the theory of moments to study problems in statistical estimation .	1	1	8	2.5106044	-2.3209114	0
55-10-45	simulations show good empirical performance on several models .	this generality and simplicity in part explains the success of the expectation maximization ( em ) algorithm , in which updates are easy to derive for a wide class of mixture models .	0	9	1	2.2300563	-2.1488624	0
55-10-45	however , the likelihood of a mixture model is non-convex , so em has no known global convergence guarantees .	recently , method of moments approaches offer global guarantees for some mixture models , but they do not extend easily to the range of mixture models that exist .	1	2	3	4.381504	-3.9019907	0
55-10-45	however , the likelihood of a mixture model is non-convex , so em has no known global convergence guarantees .	in this work , we present polymom , an unifying framework based on method of moments in which estimation procedures are easily derivable , just as in em .	1	2	4	-4.0425506	3.826344	1
55-10-45	however , the likelihood of a mixture model is non-convex , so em has no known global convergence guarantees .	polymom is applicable when the moments of a single mixture component are polynomials of the parameters .	1	2	5	-2.3558269	2.4319718	1
55-10-45	however , the likelihood of a mixture model is non-convex , so em has no known global convergence guarantees .	our key observation is that the moments of the mixture model are a mixture of these polynomials , which allows us to cast estimation as a generalized moment problem .	1	2	6	-5.2622604	4.9010315	1
55-10-45	we solve its relaxations using semidefinite optimization , and then extract parameters using ideas from computer algebra .	however , the likelihood of a mixture model is non-convex , so em has no known global convergence guarantees .	0	7	2	4.3602343	-3.9720488	0
55-10-45	this framework allows us to draw insights and apply tools from convex optimization , computer algebra and the theory of moments to study problems in statistical estimation .	however , the likelihood of a mixture model is non-convex , so em has no known global convergence guarantees .	0	8	2	4.6027055	-4.1323824	0
55-10-45	simulations show good empirical performance on several models .	however , the likelihood of a mixture model is non-convex , so em has no known global convergence guarantees .	0	9	2	5.0291877	-4.53897	0
55-10-45	recently , method of moments approaches offer global guarantees for some mixture models , but they do not extend easily to the range of mixture models that exist .	in this work , we present polymom , an unifying framework based on method of moments in which estimation procedures are easily derivable , just as in em .	1	3	4	-5.826139	5.283455	1
55-10-45	recently , method of moments approaches offer global guarantees for some mixture models , but they do not extend easily to the range of mixture models that exist .	polymom is applicable when the moments of a single mixture component are polynomials of the parameters .	1	3	5	-3.5022302	3.3864071	1
55-10-45	our key observation is that the moments of the mixture model are a mixture of these polynomials , which allows us to cast estimation as a generalized moment problem .	recently , method of moments approaches offer global guarantees for some mixture models , but they do not extend easily to the range of mixture models that exist .	0	6	3	5.2449813	-4.6561713	0
55-10-45	we solve its relaxations using semidefinite optimization , and then extract parameters using ideas from computer algebra .	recently , method of moments approaches offer global guarantees for some mixture models , but they do not extend easily to the range of mixture models that exist .	0	7	3	5.529618	-4.9405456	0
55-10-45	recently , method of moments approaches offer global guarantees for some mixture models , but they do not extend easily to the range of mixture models that exist .	this framework allows us to draw insights and apply tools from convex optimization , computer algebra and the theory of moments to study problems in statistical estimation .	1	3	8	-5.963645	5.181939	1
55-10-45	recently , method of moments approaches offer global guarantees for some mixture models , but they do not extend easily to the range of mixture models that exist .	simulations show good empirical performance on several models .	1	3	9	-5.9745326	5.1532903	1
55-10-45	in this work , we present polymom , an unifying framework based on method of moments in which estimation procedures are easily derivable , just as in em .	polymom is applicable when the moments of a single mixture component are polynomials of the parameters .	1	4	5	-5.0994844	4.6825285	1
55-10-45	in this work , we present polymom , an unifying framework based on method of moments in which estimation procedures are easily derivable , just as in em .	our key observation is that the moments of the mixture model are a mixture of these polynomials , which allows us to cast estimation as a generalized moment problem .	1	4	6	-5.35339	4.9494405	1
55-10-45	we solve its relaxations using semidefinite optimization , and then extract parameters using ideas from computer algebra .	in this work , we present polymom , an unifying framework based on method of moments in which estimation procedures are easily derivable , just as in em .	0	7	4	4.9993563	-4.4009647	0
55-10-45	this framework allows us to draw insights and apply tools from convex optimization , computer algebra and the theory of moments to study problems in statistical estimation .	in this work , we present polymom , an unifying framework based on method of moments in which estimation procedures are easily derivable , just as in em .	0	8	4	5.198889	-4.5000315	0
55-10-45	in this work , we present polymom , an unifying framework based on method of moments in which estimation procedures are easily derivable , just as in em .	simulations show good empirical performance on several models .	1	4	9	-5.963249	5.12817	1
55-10-45	our key observation is that the moments of the mixture model are a mixture of these polynomials , which allows us to cast estimation as a generalized moment problem .	polymom is applicable when the moments of a single mixture component are polynomials of the parameters .	0	6	5	4.9858713	-4.4196877	0
55-10-45	polymom is applicable when the moments of a single mixture component are polynomials of the parameters .	we solve its relaxations using semidefinite optimization , and then extract parameters using ideas from computer algebra .	1	5	7	-4.0819736	3.8251858	1
55-10-45	polymom is applicable when the moments of a single mixture component are polynomials of the parameters .	this framework allows us to draw insights and apply tools from convex optimization , computer algebra and the theory of moments to study problems in statistical estimation .	1	5	8	-3.6684337	3.5585308	1
55-10-45	polymom is applicable when the moments of a single mixture component are polynomials of the parameters .	simulations show good empirical performance on several models .	1	5	9	-5.6861773	5.09366	1
55-10-45	our key observation is that the moments of the mixture model are a mixture of these polynomials , which allows us to cast estimation as a generalized moment problem .	we solve its relaxations using semidefinite optimization , and then extract parameters using ideas from computer algebra .	1	6	7	-4.2957673	4.0661697	1
55-10-45	this framework allows us to draw insights and apply tools from convex optimization , computer algebra and the theory of moments to study problems in statistical estimation .	our key observation is that the moments of the mixture model are a mixture of these polynomials , which allows us to cast estimation as a generalized moment problem .	0	8	6	3.1473079	-2.945926	0
55-10-45	our key observation is that the moments of the mixture model are a mixture of these polynomials , which allows us to cast estimation as a generalized moment problem .	simulations show good empirical performance on several models .	1	6	9	-5.8516603	5.082947	1
55-10-45	this framework allows us to draw insights and apply tools from convex optimization , computer algebra and the theory of moments to study problems in statistical estimation .	we solve its relaxations using semidefinite optimization , and then extract parameters using ideas from computer algebra .	0	8	7	-2.4865313	2.517633	1
55-10-45	we solve its relaxations using semidefinite optimization , and then extract parameters using ideas from computer algebra .	simulations show good empirical performance on several models .	1	7	9	-5.4647517	4.9356365	1
55-10-45	this framework allows us to draw insights and apply tools from convex optimization , computer algebra and the theory of moments to study problems in statistical estimation .	simulations show good empirical performance on several models .	1	8	9	-5.4031878	4.8749156	1
56-6-15	however , its convergence rate is known to be slow ( sublinear ) when the solution lies at the boundary .	the frank-wolfe ( fw ) optimization algorithm has lately re-gained popularity thanks in particular to its ability to nicely handle the structured constraints appearing in machine learning applications .	0	1	0	4.8141975	-4.2884297	0
56-6-15	a simple lessknown fix is to add the possibility to take `away steps ' during optimization , an operation that importantly does not require a feasibility oracle .	the frank-wolfe ( fw ) optimization algorithm has lately re-gained popularity thanks in particular to its ability to nicely handle the structured constraints appearing in machine learning applications .	0	2	0	5.408451	-4.8118997	0
56-6-15	the frank-wolfe ( fw ) optimization algorithm has lately re-gained popularity thanks in particular to its ability to nicely handle the structured constraints appearing in machine learning applications .	[CLS] in this paper, we highlight and clarify several variants of the frank - wolfe optimization algorithm that have been successfully applied in practice : away - steps fw, pairwise fw, fully - corrective fw and wolfe's minimum norm point algorithm, and prove for the first time that they all enjoy global linear	1	0	3	-5.939593	5.1851773	1
56-6-15	the frank-wolfe ( fw ) optimization algorithm has lately re-gained popularity thanks in particular to its ability to nicely handle the structured constraints appearing in machine learning applications .	the constant in the convergence rate has an elegant interpretation as the product of the ( classical ) condition number of the function with a novel geometric quantity that plays the role of a `condition number ' of the constraint set .	1	0	4	-4.5671906	4.2544484	1
56-6-15	the frank-wolfe ( fw ) optimization algorithm has lately re-gained popularity thanks in particular to its ability to nicely handle the structured constraints appearing in machine learning applications .	we provide pointers to where these algorithms have made a difference in practice , in particular with the flow polytope , the marginal polytope and the base polytope for submodular optimization .	1	0	5	-5.952931	5.205085	1
56-6-15	a simple lessknown fix is to add the possibility to take `away steps ' during optimization , an operation that importantly does not require a feasibility oracle .	however , its convergence rate is known to be slow ( sublinear ) when the solution lies at the boundary .	0	2	1	4.694138	-4.1187315	0
56-6-15	however , its convergence rate is known to be slow ( sublinear ) when the solution lies at the boundary .	[CLS] in this paper, we highlight and clarify several variants of the frank - wolfe optimization algorithm that have been successfully applied in practice : away - steps fw, pairwise fw, fully - corrective fw and wolfe's minimum norm point algorithm, and prove for the first time that they all enjoy global linear convergence, under a weaker condition than strong convexity of	1	1	3	-5.9374495	5.0155787	1
56-6-15	the constant in the convergence rate has an elegant interpretation as the product of the ( classical ) condition number of the function with a novel geometric quantity that plays the role of a `condition number ' of the constraint set .	however , its convergence rate is known to be slow ( sublinear ) when the solution lies at the boundary .	0	4	1	4.1114454	-3.717873	0
56-6-15	we provide pointers to where these algorithms have made a difference in practice , in particular with the flow polytope , the marginal polytope and the base polytope for submodular optimization .	however , its convergence rate is known to be slow ( sublinear ) when the solution lies at the boundary .	0	5	1	5.3200665	-4.7096868	0
56-6-15	[CLS] in this paper, we highlight and clarify several variants of the frank - wolfe optimization algorithm that have been successfully applied in practice : away - steps fw, pairwise fw, fully - corrective fw and wolfe's minimum norm point algorithm, and prove for the first time that they all enjoy global linear convergence,	a simple lessknown fix is to add the possibility to take `away steps ' during optimization , an operation that importantly does not require a feasibility oracle .	0	3	2	0.6124534	-0.5145746	0
56-6-15	a simple lessknown fix is to add the possibility to take `away steps ' during optimization , an operation that importantly does not require a feasibility oracle .	the constant in the convergence rate has an elegant interpretation as the product of the ( classical ) condition number of the function with a novel geometric quantity that plays the role of a `condition number ' of the constraint set .	1	2	4	4.862014	-4.2552557	0
56-6-15	a simple lessknown fix is to add the possibility to take `away steps ' during optimization , an operation that importantly does not require a feasibility oracle .	we provide pointers to where these algorithms have made a difference in practice , in particular with the flow polytope , the marginal polytope and the base polytope for submodular optimization .	1	2	5	-4.6987567	4.264304	1
56-6-15	[CLS] in this paper, we highlight and clarify several variants of the frank - wolfe optimization algorithm that have been successfully applied in practice : away - steps fw, pairwise fw, fully - corrective fw and wolfe's minimum norm point algorithm, and prove for	the constant in the convergence rate has an elegant interpretation as the product of the ( classical ) condition number of the function with a novel geometric quantity that plays the role of a `condition number ' of the constraint set .	1	3	4	4.904922	-4.399684	0
56-6-15	[CLS] in this paper, we highlight and clarify several variants of the frank - wolfe optimization algorithm that have been successfully applied in practice : away - steps fw, pairwise fw, fully - corrective fw and wolfe's minimum norm point algorithm, and prove for the	we provide pointers to where these algorithms have made a difference in practice , in particular with the flow polytope , the marginal polytope and the base polytope for submodular optimization .	1	3	5	-4.8721137	4.462762	1
56-6-15	we provide pointers to where these algorithms have made a difference in practice , in particular with the flow polytope , the marginal polytope and the base polytope for submodular optimization .	the constant in the convergence rate has an elegant interpretation as the product of the ( classical ) condition number of the function with a novel geometric quantity that plays the role of a `condition number ' of the constraint set .	0	5	4	5.295204	-4.7388096	0
57-7-21	though effectively modeling student knowledge would have high educational impact , the task has many inherent challenges .	knowledge tracing -- where a machine models the knowledge of a student as they interact with coursework -- is a well established problem in computer supported education .	0	1	0	5.4830427	-4.8741837	0
57-7-21	in this paper we explore the utility of using recurrent neural networks ( rnns ) to model student learning .	knowledge tracing -- where a machine models the knowledge of a student as they interact with coursework -- is a well established problem in computer supported education .	0	2	0	5.6478033	-5.0087495	0
57-7-21	the rnn family of models have important advantages over previous methods in that they do not require the explicit encoding of human domain knowledge , and can capture more complex representations of student knowledge .	knowledge tracing -- where a machine models the knowledge of a student as they interact with coursework -- is a well established problem in computer supported education .	0	3	0	5.64758	-5.0100546	0
57-7-21	using neural networks results in substantial improvements in prediction performance on a range of knowledge tracing datasets .	knowledge tracing -- where a machine models the knowledge of a student as they interact with coursework -- is a well established problem in computer supported education .	0	4	0	5.676793	-5.093678	0
57-7-21	knowledge tracing -- where a machine models the knowledge of a student as they interact with coursework -- is a well established problem in computer supported education .	moreover the learned model can be used for intelligent curriculum design and allows straightforward interpretation and discovery of structure in student tasks .	1	0	5	-5.8892884	5.140394	1
57-7-21	knowledge tracing -- where a machine models the knowledge of a student as they interact with coursework -- is a well established problem in computer supported education .	these results suggest a promising new line of research for knowledge tracing and an exemplary application task for rnns .	1	0	6	-5.871623	5.1134653	1
57-7-21	in this paper we explore the utility of using recurrent neural networks ( rnns ) to model student learning .	though effectively modeling student knowledge would have high educational impact , the task has many inherent challenges .	0	2	1	4.269149	-3.7263873	0
57-7-21	though effectively modeling student knowledge would have high educational impact , the task has many inherent challenges .	the rnn family of models have important advantages over previous methods in that they do not require the explicit encoding of human domain knowledge , and can capture more complex representations of student knowledge .	1	1	3	-4.44895	4.203088	1
57-7-21	though effectively modeling student knowledge would have high educational impact , the task has many inherent challenges .	using neural networks results in substantial improvements in prediction performance on a range of knowledge tracing datasets .	1	1	4	-5.9073234	5.164966	1
57-7-21	though effectively modeling student knowledge would have high educational impact , the task has many inherent challenges .	moreover the learned model can be used for intelligent curriculum design and allows straightforward interpretation and discovery of structure in student tasks .	1	1	5	-6.015911	5.1667275	1
57-7-21	though effectively modeling student knowledge would have high educational impact , the task has many inherent challenges .	these results suggest a promising new line of research for knowledge tracing and an exemplary application task for rnns .	1	1	6	-5.7944517	4.8389716	1
57-7-21	in this paper we explore the utility of using recurrent neural networks ( rnns ) to model student learning .	the rnn family of models have important advantages over previous methods in that they do not require the explicit encoding of human domain knowledge , and can capture more complex representations of student knowledge .	1	2	3	-5.8594513	5.2183714	1
57-7-21	using neural networks results in substantial improvements in prediction performance on a range of knowledge tracing datasets .	in this paper we explore the utility of using recurrent neural networks ( rnns ) to model student learning .	0	4	2	4.4114966	-3.908281	0
57-7-21	moreover the learned model can be used for intelligent curriculum design and allows straightforward interpretation and discovery of structure in student tasks .	in this paper we explore the utility of using recurrent neural networks ( rnns ) to model student learning .	0	5	2	5.439373	-4.7953463	0
57-7-21	these results suggest a promising new line of research for knowledge tracing and an exemplary application task for rnns .	in this paper we explore the utility of using recurrent neural networks ( rnns ) to model student learning .	0	6	2	5.5876613	-4.943619	0
57-7-21	using neural networks results in substantial improvements in prediction performance on a range of knowledge tracing datasets .	the rnn family of models have important advantages over previous methods in that they do not require the explicit encoding of human domain knowledge , and can capture more complex representations of student knowledge .	0	4	3	4.7663994	-4.3372273	0
57-7-21	moreover the learned model can be used for intelligent curriculum design and allows straightforward interpretation and discovery of structure in student tasks .	the rnn family of models have important advantages over previous methods in that they do not require the explicit encoding of human domain knowledge , and can capture more complex representations of student knowledge .	0	5	3	4.940028	-4.39798	0
57-7-21	these results suggest a promising new line of research for knowledge tracing and an exemplary application task for rnns .	the rnn family of models have important advantages over previous methods in that they do not require the explicit encoding of human domain knowledge , and can capture more complex representations of student knowledge .	0	6	3	5.357179	-4.8031435	0
57-7-21	moreover the learned model can be used for intelligent curriculum design and allows straightforward interpretation and discovery of structure in student tasks .	using neural networks results in substantial improvements in prediction performance on a range of knowledge tracing datasets .	0	5	4	-3.1057153	3.0078802	1
57-7-21	these results suggest a promising new line of research for knowledge tracing and an exemplary application task for rnns .	using neural networks results in substantial improvements in prediction performance on a range of knowledge tracing datasets .	0	6	4	4.580093	-4.090199	0
57-7-21	moreover the learned model can be used for intelligent curriculum design and allows straightforward interpretation and discovery of structure in student tasks .	these results suggest a promising new line of research for knowledge tracing and an exemplary application task for rnns .	1	5	6	-5.1100745	4.625985	1
58-4-6	we consider moment matching techniques for estimation in latent dirichlet allocation ( lda ) .	by drawing explicit links between lda and discrete versions of independent component analysis ( ica ) , we first derive a new set of cumulantbased tensors , with an improved sample complexity .	1	0	1	-5.822321	5.2524867	1
58-4-6	moreover , we reuse standard ica techniques such as joint diagonalization of tensors to improve over existing methods based on the tensor power method .	we consider moment matching techniques for estimation in latent dirichlet allocation ( lda ) .	0	2	0	5.517123	-4.882753	0
58-4-6	in an extensive set of experiments on both synthetic and real datasets , we show that our new combination of tensors and orthogonal joint diagonalization techniques outperforms existing moment matching methods .	we consider moment matching techniques for estimation in latent dirichlet allocation ( lda ) .	0	3	0	5.5385256	-4.94528	0
58-4-6	moreover , we reuse standard ica techniques such as joint diagonalization of tensors to improve over existing methods based on the tensor power method .	by drawing explicit links between lda and discrete versions of independent component analysis ( ica ) , we first derive a new set of cumulantbased tensors , with an improved sample complexity .	0	2	1	5.428837	-4.769043	0
58-4-6	in an extensive set of experiments on both synthetic and real datasets , we show that our new combination of tensors and orthogonal joint diagonalization techniques outperforms existing moment matching methods .	by drawing explicit links between lda and discrete versions of independent component analysis ( ica ) , we first derive a new set of cumulantbased tensors , with an improved sample complexity .	0	3	1	5.500542	-4.831729	0
58-4-6	in an extensive set of experiments on both synthetic and real datasets , we show that our new combination of tensors and orthogonal joint diagonalization techniques outperforms existing moment matching methods .	moreover , we reuse standard ica techniques such as joint diagonalization of tensors to improve over existing methods based on the tensor power method .	0	3	2	4.1227007	-3.818931	0
59-9-36	we propose a robust and efficient approach to the problem of compressive phase retrieval in which the goal is to reconstruct a sparse vector from the magnitude of a number of its linear measurements .	the proposed framework relies on constrained sensing vectors and a two-stage reconstruction method that consists of two standard convex programs that are solved sequentially .	1	0	1	-5.912362	5.236119	1
59-9-36	we propose a robust and efficient approach to the problem of compressive phase retrieval in which the goal is to reconstruct a sparse vector from the magnitude of a number of its linear measurements .	in recent years , various methods are proposed for compressive phase retrieval , but they have suboptimal sample complexity or lack robustness guarantees .	1	0	2	-2.2261362	2.418797	1
59-9-36	we propose a robust and efficient approach to the problem of compressive phase retrieval in which the goal is to reconstruct a sparse vector from the magnitude of a number of its linear measurements .	the main obstacle has been that there is no straightforward convex relaxations for the type of structure in the target .	1	0	3	-4.9183655	4.42503	1
59-9-36	we propose a robust and efficient approach to the problem of compressive phase retrieval in which the goal is to reconstruct a sparse vector from the magnitude of a number of its linear measurements .	given a set of underdetermined measurements , there is a standard framework for recovering a sparse matrix , and a standard framework for recovering a low-rank matrix .	1	0	4	-2.0071301	2.3144188	1
59-9-36	however , a general , efficient method for recovering a jointly sparse and low-rank matrix has remained elusive .	we propose a robust and efficient approach to the problem of compressive phase retrieval in which the goal is to reconstruct a sparse vector from the magnitude of a number of its linear measurements .	0	5	0	-0.08096262	0.34911907	1
59-9-36	we propose a robust and efficient approach to the problem of compressive phase retrieval in which the goal is to reconstruct a sparse vector from the magnitude of a number of its linear measurements .	deviating from the models with generic measurements , in this paper we show that if the sensing vectors are chosen at random from an incoherent subspace , then the low-rank and sparse structures of the target signal can be effectively decoupled .	1	0	6	-5.635667	5.074775	1
59-9-36	[CLS] we show that a recovery algorithm that consists of a low - rank recovery stage followed by a sparse recovery stage will produce an accurate estimate of the target when the number of measurements is o ( k log kd ), where k and d denote the sparsity level and the dimension of the input	we propose a robust and efficient approach to the problem of compressive phase retrieval in which the goal is to reconstruct a sparse vector from the magnitude of a number of its linear measurements .	0	7	0	5.638726	-5.0623603	0
59-9-36	we propose a robust and efficient approach to the problem of compressive phase retrieval in which the goal is to reconstruct a sparse vector from the magnitude of a number of its linear measurements .	we also evaluate the algorithm through numerical simulation .	1	0	8	-5.963872	5.1424236	1
59-9-36	the proposed framework relies on constrained sensing vectors and a two-stage reconstruction method that consists of two standard convex programs that are solved sequentially .	in recent years , various methods are proposed for compressive phase retrieval , but they have suboptimal sample complexity or lack robustness guarantees .	1	1	2	5.530135	-4.9150963	0
59-9-36	the proposed framework relies on constrained sensing vectors and a two-stage reconstruction method that consists of two standard convex programs that are solved sequentially .	the main obstacle has been that there is no straightforward convex relaxations for the type of structure in the target .	1	1	3	4.919136	-4.332444	0
59-9-36	given a set of underdetermined measurements , there is a standard framework for recovering a sparse matrix , and a standard framework for recovering a low-rank matrix .	the proposed framework relies on constrained sensing vectors and a two-stage reconstruction method that consists of two standard convex programs that are solved sequentially .	0	4	1	-5.88626	5.2133727	1
59-9-36	however , a general , efficient method for recovering a jointly sparse and low-rank matrix has remained elusive .	the proposed framework relies on constrained sensing vectors and a two-stage reconstruction method that consists of two standard convex programs that are solved sequentially .	0	5	1	-5.9080887	5.227399	1
59-9-36	deviating from the models with generic measurements , in this paper we show that if the sensing vectors are chosen at random from an incoherent subspace , then the low-rank and sparse structures of the target signal can be effectively decoupled .	the proposed framework relies on constrained sensing vectors and a two-stage reconstruction method that consists of two standard convex programs that are solved sequentially .	0	6	1	-5.864205	5.2088423	1
59-9-36	we show that a recovery algorithm that consists of a low-rank recovery stage followed by a sparse recovery stage will produce an accurate estimate of the target when the number of measurements is o ( k log kd ) , where k and d denote the sparsity level and the dimension of the input signal .	the proposed framework relies on constrained sensing vectors and a two-stage reconstruction method that consists of two standard convex programs that are solved sequentially .	0	7	1	-1.4402031	1.6184487	1
59-9-36	we also evaluate the algorithm through numerical simulation .	the proposed framework relies on constrained sensing vectors and a two-stage reconstruction method that consists of two standard convex programs that are solved sequentially .	0	8	1	5.3106527	-4.7591763	0
59-9-36	the main obstacle has been that there is no straightforward convex relaxations for the type of structure in the target .	in recent years , various methods are proposed for compressive phase retrieval , but they have suboptimal sample complexity or lack robustness guarantees .	0	3	2	4.610902	-4.124394	0
59-9-36	in recent years , various methods are proposed for compressive phase retrieval , but they have suboptimal sample complexity or lack robustness guarantees .	given a set of underdetermined measurements , there is a standard framework for recovering a sparse matrix , and a standard framework for recovering a low-rank matrix .	1	2	4	4.7869997	-4.2339892	0
59-9-36	however , a general , efficient method for recovering a jointly sparse and low-rank matrix has remained elusive .	in recent years , various methods are proposed for compressive phase retrieval , but they have suboptimal sample complexity or lack robustness guarantees .	0	5	2	-3.3164873	3.2082255	1
59-9-36	deviating from the models with generic measurements , in this paper we show that if the sensing vectors are chosen at random from an incoherent subspace , then the low-rank and sparse structures of the target signal can be effectively decoupled .	in recent years , various methods are proposed for compressive phase retrieval , but they have suboptimal sample complexity or lack robustness guarantees .	0	6	2	4.9701777	-4.4467916	0
59-9-36	in recent years , various methods are proposed for compressive phase retrieval , but they have suboptimal sample complexity or lack robustness guarantees .	we show that a recovery algorithm that consists of a low-rank recovery stage followed by a sparse recovery stage will produce an accurate estimate of the target when the number of measurements is o ( k log kd ) , where k and d denote the sparsity level and the dimension of the input signal .	1	2	7	-5.898841	5.17777	1
59-9-36	we also evaluate the algorithm through numerical simulation .	in recent years , various methods are proposed for compressive phase retrieval , but they have suboptimal sample complexity or lack robustness guarantees .	0	8	2	5.682976	-5.0860567	0
59-9-36	the main obstacle has been that there is no straightforward convex relaxations for the type of structure in the target .	given a set of underdetermined measurements , there is a standard framework for recovering a sparse matrix , and a standard framework for recovering a low-rank matrix .	1	3	4	5.3813486	-4.823295	0
59-9-36	however , a general , efficient method for recovering a jointly sparse and low-rank matrix has remained elusive .	the main obstacle has been that there is no straightforward convex relaxations for the type of structure in the target .	0	5	3	-5.8608236	5.250675	1
59-9-36	deviating from the models with generic measurements , in this paper we show that if the sensing vectors are chosen at random from an incoherent subspace , then the low-rank and sparse structures of the target signal can be effectively decoupled .	the main obstacle has been that there is no straightforward convex relaxations for the type of structure in the target .	0	6	3	-0.9887372	1.1681228	1
59-9-36	we show that a recovery algorithm that consists of a low-rank recovery stage followed by a sparse recovery stage will produce an accurate estimate of the target when the number of measurements is o ( k log kd ) , where k and d denote the sparsity level and the dimension of the input signal .	the main obstacle has been that there is no straightforward convex relaxations for the type of structure in the target .	0	7	3	4.4562817	-3.9330711	0
59-9-36	the main obstacle has been that there is no straightforward convex relaxations for the type of structure in the target .	we also evaluate the algorithm through numerical simulation .	1	3	8	-5.974278	5.138835	1
59-9-36	however , a general , efficient method for recovering a jointly sparse and low-rank matrix has remained elusive .	given a set of underdetermined measurements , there is a standard framework for recovering a sparse matrix , and a standard framework for recovering a low-rank matrix .	0	5	4	2.463744	-2.0243685	0
59-9-36	given a set of underdetermined measurements , there is a standard framework for recovering a sparse matrix , and a standard framework for recovering a low-rank matrix .	deviating from the models with generic measurements , in this paper we show that if the sensing vectors are chosen at random from an incoherent subspace , then the low-rank and sparse structures of the target signal can be effectively decoupled .	1	4	6	-5.816074	5.2050924	1
59-9-36	given a set of underdetermined measurements , there is a standard framework for recovering a sparse matrix , and a standard framework for recovering a low-rank matrix .	we show that a recovery algorithm that consists of a low-rank recovery stage followed by a sparse recovery stage will produce an accurate estimate of the target when the number of measurements is o ( k log kd ) , where k and d denote the sparsity level and the dimension of the input signal .	1	4	7	-5.9207044	5.2235627	1
59-9-36	given a set of underdetermined measurements , there is a standard framework for recovering a sparse matrix , and a standard framework for recovering a low-rank matrix .	we also evaluate the algorithm through numerical simulation .	1	4	8	-5.9864597	5.118419	1
59-9-36	however , a general , efficient method for recovering a jointly sparse and low-rank matrix has remained elusive .	deviating from the models with generic measurements , in this paper we show that if the sensing vectors are chosen at random from an incoherent subspace , then the low-rank and sparse structures of the target signal can be effectively decoupled .	1	5	6	-5.8743324	5.269682	1
59-9-36	we show that a recovery algorithm that consists of a low-rank recovery stage followed by a sparse recovery stage will produce an accurate estimate of the target when the number of measurements is o ( k log kd ) , where k and d denote the sparsity level and the dimension of the input signal .	however , a general , efficient method for recovering a jointly sparse and low-rank matrix has remained elusive .	0	7	5	5.6554093	-5.0320845	0
59-9-36	we also evaluate the algorithm through numerical simulation .	however , a general , efficient method for recovering a jointly sparse and low-rank matrix has remained elusive .	0	8	5	5.689803	-5.1032	0
59-9-36	[CLS] we show that a recovery algorithm that consists of a low - rank recovery stage followed by a sparse recovery stage will produce an accurate estimate of the target when the number of measurements is o ( k log kd ), where k and d denote the	[CLS] deviating from the models with generic measurements, in this paper we show that if the sensing vectors are chosen at random from an incoherent subspace, then the low - rank and sparse structures of the target signal can be effectively decoup	0	7	6	5.1649814	-4.513129	0
59-9-36	deviating from the models with generic measurements , in this paper we show that if the sensing vectors are chosen at random from an incoherent subspace , then the low-rank and sparse structures of the target signal can be effectively decoupled .	we also evaluate the algorithm through numerical simulation .	1	6	8	-5.98724	5.1423917	1
59-9-36	we also evaluate the algorithm through numerical simulation .	we show that a recovery algorithm that consists of a low-rank recovery stage followed by a sparse recovery stage will produce an accurate estimate of the target when the number of measurements is o ( k log kd ) , where k and d denote the sparsity level and the dimension of the input signal .	0	8	7	5.5135813	-4.9067883	0
60-5-10	we introduce a globally-convergent algorithm for optimizing the tree-reweighted ( trw ) variational objective over the marginal polytope .	the algorithm is based on the conditional gradient method ( frank-wolfe ) and moves pseudomarginals within the marginal polytope through repeated maximum a posteriori ( map ) calls .	1	0	1	-5.625226	5.117005	1
60-5-10	this modular structure enables us to leverage black-box map solvers ( both exact and approximate ) for variational inference , and obtains more accurate results than tree-reweighted algorithms that optimize over the local consistency relaxation .	we introduce a globally-convergent algorithm for optimizing the tree-reweighted ( trw ) variational objective over the marginal polytope .	0	2	0	5.02914	-4.4221544	0
60-5-10	we introduce a globally-convergent algorithm for optimizing the tree-reweighted ( trw ) variational objective over the marginal polytope .	theoretically , we bound the sub-optimality for the proposed algorithm despite the trw objective having unbounded gradients at the boundary of the marginal polytope .	1	0	3	-5.907694	5.209749	1
60-5-10	we introduce a globally-convergent algorithm for optimizing the tree-reweighted ( trw ) variational objective over the marginal polytope .	empirically , we demonstrate the increased quality of results found by tightening the relaxation over the marginal polytope as well as the spanning tree polytope on synthetic and real-world instances .	1	0	4	-5.9465365	5.137883	1
60-5-10	the algorithm is based on the conditional gradient method ( frank-wolfe ) and moves pseudomarginals within the marginal polytope through repeated maximum a posteriori ( map ) calls .	this modular structure enables us to leverage black-box map solvers ( both exact and approximate ) for variational inference , and obtains more accurate results than tree-reweighted algorithms that optimize over the local consistency relaxation .	1	1	2	-5.8809133	5.1034684	1
60-5-10	theoretically , we bound the sub-optimality for the proposed algorithm despite the trw objective having unbounded gradients at the boundary of the marginal polytope .	the algorithm is based on the conditional gradient method ( frank-wolfe ) and moves pseudomarginals within the marginal polytope through repeated maximum a posteriori ( map ) calls .	0	3	1	4.279557	-3.9273663	0
60-5-10	empirically , we demonstrate the increased quality of results found by tightening the relaxation over the marginal polytope as well as the spanning tree polytope on synthetic and real-world instances .	the algorithm is based on the conditional gradient method ( frank-wolfe ) and moves pseudomarginals within the marginal polytope through repeated maximum a posteriori ( map ) calls .	0	4	1	5.2912054	-4.7168636	0
60-5-10	this modular structure enables us to leverage black-box map solvers ( both exact and approximate ) for variational inference , and obtains more accurate results than tree-reweighted algorithms that optimize over the local consistency relaxation .	theoretically , we bound the sub-optimality for the proposed algorithm despite the trw objective having unbounded gradients at the boundary of the marginal polytope .	1	2	3	3.5288227	-3.304188	0
60-5-10	this modular structure enables us to leverage black-box map solvers ( both exact and approximate ) for variational inference , and obtains more accurate results than tree-reweighted algorithms that optimize over the local consistency relaxation .	empirically , we demonstrate the increased quality of results found by tightening the relaxation over the marginal polytope as well as the spanning tree polytope on synthetic and real-world instances .	1	2	4	-2.7557988	2.7213616	1
60-5-10	empirically , we demonstrate the increased quality of results found by tightening the relaxation over the marginal polytope as well as the spanning tree polytope on synthetic and real-world instances .	theoretically , we bound the sub-optimality for the proposed algorithm despite the trw objective having unbounded gradients at the boundary of the marginal polytope .	0	4	3	4.9864883	-4.4334235	0
61-3-3	we present data-dependent learning bounds for the general scenario of nonstationary non-mixing stochastic processes .	our learning guarantees are expressed in terms of a data-dependent measure of sequential complexity and a discrepancy measure that can be estimated from data under some mild assumptions .	1	0	1	-5.880374	5.228855	1
61-3-3	we use our learning bounds to devise new algorithms for non-stationary time series forecasting for which we report some preliminary experimental results .	we present data-dependent learning bounds for the general scenario of nonstationary non-mixing stochastic processes .	0	2	0	5.640151	-5.01515	0
61-3-3	our learning guarantees are expressed in terms of a data-dependent measure of sequential complexity and a discrepancy measure that can be estimated from data under some mild assumptions .	we use our learning bounds to devise new algorithms for non-stationary time series forecasting for which we report some preliminary experimental results .	1	1	2	-5.355961	4.8318644	1
62-7-21	spectral embedding based on the singular value decomposition ( svd ) is a widely used `` preprocessing '' step in many learning tasks , typically leading to dimensionality reduction by projecting onto a number of dominant singular vectors and rescaling the coordinate axes ( by a predefined function of the singular value ) .	however , the number of such vectors required to capture problem structure grows with problem size , and even partial svd computation becomes a bottleneck .	1	0	1	-5.860558	5.2122803	1
62-7-21	in this paper , we propose a low-complexity compressive spectral embedding algorithm , which employs random projections and finite order polynomial expansions to compute approximations to svd-based embedding .	[CLS] spectral embedding based on the singular value decomposition ( svd ) is a widely used ` ` preprocessing'' step in many learning tasks, typically leading to dimensionality reduction by projecting onto a number of dominant singular vectors and rescaling the coordinate axes ( by	0	2	0	5.588119	-4.920806	0
62-7-21	[CLS] spectral embedding based on the singular value decomposition ( svd ) is a widely used ` ` preprocessing'' step in many learning tasks, typically leading to dimensionality reduction by projecting onto a number of dominant singular vectors and res	[CLS] for an mxn matrix with t non - zeros, its time complexity is o ( ( t + m + n ) log ( m + n ) ), and the embedding dimension is o ( log ( m + n ) )	1	0	3	-3.9083436	3.6658432	1
62-7-21	to the best of our knowledge , this is the first work to circumvent this dependence on the number of singular vectors for general svd-based embeddings .	[CLS] spectral embedding based on the singular value decomposition ( svd ) is a widely used ` ` preprocessing'' step in many learning tasks, typically leading to dimensionality reduction by projecting onto a number of dominant singular vectors and rescaling the coordinate axes ( by a predefined	0	4	0	5.594696	-4.976306	0
62-7-21	[CLS] spectral embedding based on the singular value decomposition ( svd ) is a widely used ` ` preprocessing'' step in many learning tasks, typically leading to dimensionality reduction by projecting onto a number of dominant singular vectors and res	[CLS] the key to sidestepping the svd is the observation that, for downstream inference tasks such as clustering and classification, we are only interested in using the resulting embedding to evaluate pairwise similarity metrics derived from the 2 - norm	1	0	5	-5.959939	5.1537514	1
62-7-21	[CLS] spectral embedding based on the singular value decomposition ( svd ) is a widely used ` ` preprocessing'' step in many learning tasks, typically leading to dimensionality reduction by projecting onto a number of dominant singular vectors and rescaling the coordinate axes ( by a predefined function of the singular	our numerical results on network datasets demonstrate the efficacy of the proposed method , and motivate further exploration of its application to large-scale inference tasks .	1	0	6	-5.951747	5.1235056	1
62-7-21	in this paper , we propose a low-complexity compressive spectral embedding algorithm , which employs random projections and finite order polynomial expansions to compute approximations to svd-based embedding .	however , the number of such vectors required to capture problem structure grows with problem size , and even partial svd computation becomes a bottleneck .	0	2	1	5.537986	-4.9046216	0
62-7-21	for an mxn matrix with t non-zeros , its time complexity is o ( ( t + m + n ) log ( m + n ) ) , and the embedding dimension is o ( log ( m + n ) ) , both of which are independent of the number of singular vectors whose effect we wish to capture .	however , the number of such vectors required to capture problem structure grows with problem size , and even partial svd computation becomes a bottleneck .	0	3	1	2.9050992	-2.7415843	0
62-7-21	however , the number of such vectors required to capture problem structure grows with problem size , and even partial svd computation becomes a bottleneck .	to the best of our knowledge , this is the first work to circumvent this dependence on the number of singular vectors for general svd-based embeddings .	1	1	4	-5.9556537	5.0253663	1
62-7-21	[CLS] the key to sidestepping the svd is the observation that, for downstream inference tasks such as clustering and classification, we are only interested in using the resulting embedding to evaluate pairwise similarity metrics derived from the 2 - norm, rather than capturing the effect of the underlying matrix on arbitrary vectors as a partial svd tries to	however , the number of such vectors required to capture problem structure grows with problem size , and even partial svd computation becomes a bottleneck .	0	5	1	4.754945	-4.1978083	0
62-7-21	our numerical results on network datasets demonstrate the efficacy of the proposed method , and motivate further exploration of its application to large-scale inference tasks .	however , the number of such vectors required to capture problem structure grows with problem size , and even partial svd computation becomes a bottleneck .	0	6	1	5.674684	-5.0369883	0
62-7-21	in this paper , we propose a low-complexity compressive spectral embedding algorithm , which employs random projections and finite order polynomial expansions to compute approximations to svd-based embedding .	[CLS] for an mxn matrix with t non - zeros, its time complexity is o ( ( t + m + n ) log ( m + n ) ), and the embedding dimension is o ( log ( m + n ) ), both of which are independent of	1	2	3	2.1382253	-1.9472812	0
62-7-21	in this paper , we propose a low-complexity compressive spectral embedding algorithm , which employs random projections and finite order polynomial expansions to compute approximations to svd-based embedding .	to the best of our knowledge , this is the first work to circumvent this dependence on the number of singular vectors for general svd-based embeddings .	1	2	4	-5.8116283	5.192122	1
62-7-21	in this paper , we propose a low-complexity compressive spectral embedding algorithm , which employs random projections and finite order polynomial expansions to compute approximations to svd-based embedding .	[CLS] the key to sidestepping the svd is the observation that, for downstream inference tasks such as clustering and classification, we are only interested in using the resulting embedding to evaluate pairwise similarity metrics derived from the 2 - norm, rather than capturing the effect of	1	2	5	-4.389757	4.0833635	1
62-7-21	our numerical results on network datasets demonstrate the efficacy of the proposed method , and motivate further exploration of its application to large-scale inference tasks .	in this paper , we propose a low-complexity compressive spectral embedding algorithm , which employs random projections and finite order polynomial expansions to compute approximations to svd-based embedding .	0	6	2	5.563282	-4.9203324	0
62-7-21	to the best of our knowledge , this is the first work to circumvent this dependence on the number of singular vectors for general svd-based embeddings .	[CLS] for an mxn matrix with t non - zeros, its time complexity is o ( ( t + m + n ) log ( m + n ) ), and the embedding dimension is o ( log ( m + n ) ), both of which are independent of the number of singular vectors	0	4	3	5.012942	-4.4645944	0
62-7-21	[CLS] for an mxn matrix with t non - zeros, its time complexity is o ( ( t + m + n ) log ( m + n ) ), and the embedding dimension is o ( log ( m + n ) )	[CLS] the key to sidestepping the svd is the observation that, for downstream inference tasks such as clustering and classification, we are only interested in using the resulting embedding to evaluate pairwise similarity metrics derived from the 2 - norm	1	3	5	-5.6254873	5.0282326	1
62-7-21	[CLS] for an mxn matrix with t non - zeros, its time complexity is o ( ( t + m + n ) log ( m + n ) ), and the embedding dimension is o ( log ( m + n ) ), both of which are independent of the number of singular vectors whose effect we wish	our numerical results on network datasets demonstrate the efficacy of the proposed method , and motivate further exploration of its application to large-scale inference tasks .	1	3	6	-5.7621355	4.845639	1
62-7-21	[CLS] the key to sidestepping the svd is the observation that, for downstream inference tasks such as clustering and classification, we are only interested in using the resulting embedding to evaluate pairwise similarity metrics derived from the 2 - norm, rather than capturing the effect of the underlying matrix on arbitrary	to the best of our knowledge , this is the first work to circumvent this dependence on the number of singular vectors for general svd-based embeddings .	0	5	4	-5.5213304	5.027133	1
62-7-21	our numerical results on network datasets demonstrate the efficacy of the proposed method , and motivate further exploration of its application to large-scale inference tasks .	to the best of our knowledge , this is the first work to circumvent this dependence on the number of singular vectors for general svd-based embeddings .	0	6	4	4.435773	-3.99715	0
62-7-21	our numerical results on network datasets demonstrate the efficacy of the proposed method , and motivate further exploration of its application to large-scale inference tasks .	[CLS] the key to sidestepping the svd is the observation that, for downstream inference tasks such as clustering and classification, we are only interested in using the resulting embedding to evaluate pairwise similarity metrics derived from the 2 - norm, rather than capturing the effect of the underlying matrix on arbitrary vectors as a partial	0	6	5	5.3584642	-4.7612567	0
63-6-15	compared with convex relaxation , nonconvex optimization exhibits superior empirical performance for large scale instances of low rank matrix estimation .	we study the estimation of low rank matrices via nonconvex optimization .	0	1	0	5.5395594	-4.9700246	0
63-6-15	however , the understanding of its theoretical guarantees are limited .	we study the estimation of low rank matrices via nonconvex optimization .	0	2	0	5.3296337	-4.809558	0
63-6-15	we study the estimation of low rank matrices via nonconvex optimization .	in this paper , we define the notion of projected oracle divergence based on which we establish sufficient conditions for the success of nonconvex optimization .	1	0	3	-5.7702203	5.115098	1
63-6-15	we illustrate the consequences of this general framework for matrix sensing .	we study the estimation of low rank matrices via nonconvex optimization .	0	4	0	5.541036	-4.950104	0
63-6-15	in particular , we prove that a broad class of nonconvex optimization algorithms , including alternating minimization and gradient-type methods , geometrically converge to the global optimum and exactly recover the true low rank matrices under standard conditions .	we study the estimation of low rank matrices via nonconvex optimization .	0	5	0	5.6266613	-5.0384455	0
63-6-15	compared with convex relaxation , nonconvex optimization exhibits superior empirical performance for large scale instances of low rank matrix estimation .	however , the understanding of its theoretical guarantees are limited .	1	1	2	3.2285767	-3.0225954	0
63-6-15	in this paper , we define the notion of projected oracle divergence based on which we establish sufficient conditions for the success of nonconvex optimization .	compared with convex relaxation , nonconvex optimization exhibits superior empirical performance for large scale instances of low rank matrix estimation .	0	3	1	-1.9610218	2.0902255	1
63-6-15	compared with convex relaxation , nonconvex optimization exhibits superior empirical performance for large scale instances of low rank matrix estimation .	we illustrate the consequences of this general framework for matrix sensing .	1	1	4	-5.438697	4.890729	1
63-6-15	in particular , we prove that a broad class of nonconvex optimization algorithms , including alternating minimization and gradient-type methods , geometrically converge to the global optimum and exactly recover the true low rank matrices under standard conditions .	compared with convex relaxation , nonconvex optimization exhibits superior empirical performance for large scale instances of low rank matrix estimation .	0	5	1	-4.7748923	4.438016	1
63-6-15	in this paper , we define the notion of projected oracle divergence based on which we establish sufficient conditions for the success of nonconvex optimization .	however , the understanding of its theoretical guarantees are limited .	0	3	2	5.217132	-4.606579	0
63-6-15	we illustrate the consequences of this general framework for matrix sensing .	however , the understanding of its theoretical guarantees are limited .	0	4	2	5.531233	-4.8424554	0
63-6-15	however , the understanding of its theoretical guarantees are limited .	in particular , we prove that a broad class of nonconvex optimization algorithms , including alternating minimization and gradient-type methods , geometrically converge to the global optimum and exactly recover the true low rank matrices under standard conditions .	1	2	5	-5.951875	5.2511883	1
63-6-15	we illustrate the consequences of this general framework for matrix sensing .	in this paper , we define the notion of projected oracle divergence based on which we establish sufficient conditions for the success of nonconvex optimization .	0	4	3	4.8995223	-4.2926054	0
63-6-15	in particular , we prove that a broad class of nonconvex optimization algorithms , including alternating minimization and gradient-type methods , geometrically converge to the global optimum and exactly recover the true low rank matrices under standard conditions .	in this paper , we define the notion of projected oracle divergence based on which we establish sufficient conditions for the success of nonconvex optimization .	0	5	3	4.8491383	-4.3120384	0
63-6-15	we illustrate the consequences of this general framework for matrix sensing .	in particular , we prove that a broad class of nonconvex optimization algorithms , including alternating minimization and gradient-type methods , geometrically converge to the global optimum and exactly recover the true low rank matrices under standard conditions .	1	4	5	4.329808	-3.9573543	0
64-9-36	deriving variational inference algorithms requires tedious model-specific calculations ; this makes it difficult for non-experts to use .	variational inference is a scalable technique for approximate bayesian inference .	0	1	0	5.3134623	-4.700965	0
64-9-36	we propose an automatic variational inference algorithm , automatic differentiation variational inference ( ) ; we implement it in stan ( code available ) , a probabilistic programming system .	variational inference is a scalable technique for approximate bayesian inference .	0	2	0	5.355525	-4.749711	0
64-9-36	variational inference is a scalable technique for approximate bayesian inference .	in the user provides a bayesian model and a dataset , nothing else .	1	0	3	-5.838702	5.183324	1
64-9-36	variational inference is a scalable technique for approximate bayesian inference .	we make no conjugacy assumptions and support a broad class of models .	1	0	4	-6.0078773	5.1995173	1
64-9-36	variational inference is a scalable technique for approximate bayesian inference .	the algorithm automatically determines an appropriate variational family and optimizes the variational objective .	1	0	5	-5.881121	5.1402674	1
64-9-36	we compare to sampling across hierarchical generalized linear models , nonconjugate matrix factorization , and a mixture model .	variational inference is a scalable technique for approximate bayesian inference .	0	6	0	5.43862	-4.8734465	0
64-9-36	we train the mixture model on a quarter million images .	variational inference is a scalable technique for approximate bayesian inference .	0	7	0	5.4625506	-4.900124	0
64-9-36	with we can use variational inference on any model we write in stan .	variational inference is a scalable technique for approximate bayesian inference .	0	8	0	5.2414513	-4.674968	0
64-9-36	we propose an automatic variational inference algorithm , automatic differentiation variational inference ( ) ; we implement it in stan ( code available ) , a probabilistic programming system .	deriving variational inference algorithms requires tedious model-specific calculations ; this makes it difficult for non-experts to use .	0	2	1	4.994637	-4.4309673	0
64-9-36	deriving variational inference algorithms requires tedious model-specific calculations ; this makes it difficult for non-experts to use .	in the user provides a bayesian model and a dataset , nothing else .	1	1	3	-4.419292	4.228422	1
64-9-36	we make no conjugacy assumptions and support a broad class of models .	deriving variational inference algorithms requires tedious model-specific calculations ; this makes it difficult for non-experts to use .	0	4	1	5.074276	-4.505204	0
64-9-36	the algorithm automatically determines an appropriate variational family and optimizes the variational objective .	deriving variational inference algorithms requires tedious model-specific calculations ; this makes it difficult for non-experts to use .	0	5	1	5.45483	-4.8532047	0
64-9-36	we compare to sampling across hierarchical generalized linear models , nonconjugate matrix factorization , and a mixture model .	deriving variational inference algorithms requires tedious model-specific calculations ; this makes it difficult for non-experts to use .	0	6	1	5.5602612	-4.9189024	0
64-9-36	deriving variational inference algorithms requires tedious model-specific calculations ; this makes it difficult for non-experts to use .	we train the mixture model on a quarter million images .	1	1	7	-5.8919806	5.0570374	1
64-9-36	with we can use variational inference on any model we write in stan .	deriving variational inference algorithms requires tedious model-specific calculations ; this makes it difficult for non-experts to use .	0	8	1	4.891515	-4.330883	0
64-9-36	we propose an automatic variational inference algorithm , automatic differentiation variational inference ( ) ; we implement it in stan ( code available ) , a probabilistic programming system .	in the user provides a bayesian model and a dataset , nothing else .	1	2	3	2.5437088	-2.425991	0
64-9-36	we make no conjugacy assumptions and support a broad class of models .	we propose an automatic variational inference algorithm , automatic differentiation variational inference ( ) ; we implement it in stan ( code available ) , a probabilistic programming system .	0	4	2	-0.015506871	0.17809775	1
64-9-36	we propose an automatic variational inference algorithm , automatic differentiation variational inference ( ) ; we implement it in stan ( code available ) , a probabilistic programming system .	the algorithm automatically determines an appropriate variational family and optimizes the variational objective .	1	2	5	-4.9056697	4.5021496	1
64-9-36	we propose an automatic variational inference algorithm , automatic differentiation variational inference ( ) ; we implement it in stan ( code available ) , a probabilistic programming system .	we compare to sampling across hierarchical generalized linear models , nonconjugate matrix factorization , and a mixture model .	1	2	6	-5.2011766	4.742419	1
64-9-36	we train the mixture model on a quarter million images .	we propose an automatic variational inference algorithm , automatic differentiation variational inference ( ) ; we implement it in stan ( code available ) , a probabilistic programming system .	0	7	2	4.053603	-3.6507874	0
64-9-36	we propose an automatic variational inference algorithm , automatic differentiation variational inference ( ) ; we implement it in stan ( code available ) , a probabilistic programming system .	with we can use variational inference on any model we write in stan .	1	2	8	-4.3333755	4.025207	1
64-9-36	we make no conjugacy assumptions and support a broad class of models .	in the user provides a bayesian model and a dataset , nothing else .	0	4	3	3.04393	-2.8705096	0
64-9-36	the algorithm automatically determines an appropriate variational family and optimizes the variational objective .	in the user provides a bayesian model and a dataset , nothing else .	0	5	3	2.9668152	-2.773279	0
64-9-36	we compare to sampling across hierarchical generalized linear models , nonconjugate matrix factorization , and a mixture model .	in the user provides a bayesian model and a dataset , nothing else .	0	6	3	4.779598	-4.3365	0
64-9-36	in the user provides a bayesian model and a dataset , nothing else .	we train the mixture model on a quarter million images .	1	3	7	-5.5957174	4.862525	1
64-9-36	in the user provides a bayesian model and a dataset , nothing else .	with we can use variational inference on any model we write in stan .	1	3	8	-5.2935762	4.59825	1
64-9-36	the algorithm automatically determines an appropriate variational family and optimizes the variational objective .	we make no conjugacy assumptions and support a broad class of models .	0	5	4	-1.7358794	1.8367171	1
64-9-36	we compare to sampling across hierarchical generalized linear models , nonconjugate matrix factorization , and a mixture model .	we make no conjugacy assumptions and support a broad class of models .	0	6	4	4.7471004	-4.3278313	0
64-9-36	we make no conjugacy assumptions and support a broad class of models .	we train the mixture model on a quarter million images .	1	4	7	-2.8696146	2.8945837	1
64-9-36	we make no conjugacy assumptions and support a broad class of models .	with we can use variational inference on any model we write in stan .	1	4	8	-3.776022	3.626297	1
64-9-36	we compare to sampling across hierarchical generalized linear models , nonconjugate matrix factorization , and a mixture model .	the algorithm automatically determines an appropriate variational family and optimizes the variational objective .	0	6	5	4.9379635	-4.4012046	0
64-9-36	the algorithm automatically determines an appropriate variational family and optimizes the variational objective .	we train the mixture model on a quarter million images .	1	5	7	-4.5151215	4.239291	1
64-9-36	with we can use variational inference on any model we write in stan .	the algorithm automatically determines an appropriate variational family and optimizes the variational objective .	0	8	5	2.069093	-2.0194204	0
64-9-36	we train the mixture model on a quarter million images .	we compare to sampling across hierarchical generalized linear models , nonconjugate matrix factorization , and a mixture model .	0	7	6	1.257226	-1.0509043	0
64-9-36	with we can use variational inference on any model we write in stan .	we compare to sampling across hierarchical generalized linear models , nonconjugate matrix factorization , and a mixture model .	0	8	6	-4.1468897	3.8518794	1
64-9-36	with we can use variational inference on any model we write in stan .	we train the mixture model on a quarter million images .	0	8	7	-0.009651765	0.06589944	1
65-6-15	we extend the attention-mechanism with features needed for speech recognition .	recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks including machine translation , handwriting synthesis and image caption generation .	0	1	0	5.29809	-4.649912	0
65-6-15	we show that while an adaptation of the model used for machine translation in reaches a competitive 18.7 % phoneme error rate ( per ) on the timit phoneme recognition task , it can only be applied to utterances which are roughly as long as the ones it was trained on .	recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks including machine translation , handwriting synthesis and image caption generation .	0	2	0	5.630609	-5.0019803	0
65-6-15	recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks including machine translation , handwriting synthesis and image caption generation .	we offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue .	1	0	3	-5.939356	5.0430746	1
65-6-15	recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks including machine translation , handwriting synthesis and image caption generation .	the new method yields a model that is robust to long inputs and achieves 18 % per in single utterances and 20 % in 10-times longer ( repeated ) utterances .	1	0	4	-5.992438	5.225706	1
65-6-15	finally , we propose a change to the attention mechanism that prevents it from concentrating too much on single frames , which further reduces per to 17.6 % level .	recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks including machine translation , handwriting synthesis and image caption generation .	0	5	0	5.5628633	-4.9490366	0
65-6-15	we extend the attention-mechanism with features needed for speech recognition .	we show that while an adaptation of the model used for machine translation in reaches a competitive 18.7 % phoneme error rate ( per ) on the timit phoneme recognition task , it can only be applied to utterances which are roughly as long as the ones it was trained on .	1	1	2	-5.924371	5.204603	1
65-6-15	we offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue .	we extend the attention-mechanism with features needed for speech recognition .	0	3	1	5.2105694	-4.675926	0
65-6-15	the new method yields a model that is robust to long inputs and achieves 18 % per in single utterances and 20 % in 10-times longer ( repeated ) utterances .	we extend the attention-mechanism with features needed for speech recognition .	0	4	1	5.5021143	-4.9942436	0
65-6-15	we extend the attention-mechanism with features needed for speech recognition .	finally , we propose a change to the attention mechanism that prevents it from concentrating too much on single frames , which further reduces per to 17.6 % level .	1	1	5	-5.4312696	4.8962116	1
65-6-15	we show that while an adaptation of the model used for machine translation in reaches a competitive 18.7 % phoneme error rate ( per ) on the timit phoneme recognition task , it can only be applied to utterances which are roughly as long as the ones it was trained on .	we offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue .	1	2	3	2.5937457	-2.3117537	0
65-6-15	the new method yields a model that is robust to long inputs and achieves 18 % per in single utterances and 20 % in 10-times longer ( repeated ) utterances .	we show that while an adaptation of the model used for machine translation in reaches a competitive 18.7 % phoneme error rate ( per ) on the timit phoneme recognition task , it can only be applied to utterances which are roughly as long as the ones it was trained on .	0	4	2	0.21853593	0.09297824	0
65-6-15	finally , we propose a change to the attention mechanism that prevents it from concentrating too much on single frames , which further reduces per to 17.6 % level .	we show that while an adaptation of the model used for machine translation in reaches a competitive 18.7 % phoneme error rate ( per ) on the timit phoneme recognition task , it can only be applied to utterances which are roughly as long as the ones it was trained on .	0	5	2	3.170752	-3.003529	0
65-6-15	the new method yields a model that is robust to long inputs and achieves 18 % per in single utterances and 20 % in 10-times longer ( repeated ) utterances .	we offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue .	0	4	3	4.6607337	-4.2026863	0
65-6-15	finally , we propose a change to the attention mechanism that prevents it from concentrating too much on single frames , which further reduces per to 17.6 % level .	we offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue .	0	5	3	4.3103	-3.8858938	0
65-6-15	the new method yields a model that is robust to long inputs and achieves 18 % per in single utterances and 20 % in 10-times longer ( repeated ) utterances .	finally , we propose a change to the attention mechanism that prevents it from concentrating too much on single frames , which further reduces per to 17.6 % level .	1	4	5	0.91591287	-0.58981407	0
66-6-15	our class of estimators is based on deriving closed-form variants of the vanilla unregularized mle but which are ( a ) well-defined even under high-dimensional settings , and ( b ) available in closed-form .	we propose a class of closed-form estimators for glms under high-dimensional sampling regimes .	0	1	0	5.026722	-4.4290614	0
66-6-15	we propose a class of closed-form estimators for glms under high-dimensional sampling regimes .	we then perform thresholding operations on this mle variant to obtain our class of estimators .	1	0	2	-5.952002	5.189191	1
66-6-15	we propose a class of closed-form estimators for glms under high-dimensional sampling regimes .	we derive a unified statistical analysis of our class of estimators , and show that it enjoys strong statistical guarantees in both parameter error as well as variable selection , that surprisingly match those of the more complex regularized glm mles , even while our closed-form estimators are computationally much simpler .	1	0	3	-5.96537	5.176652	1
66-6-15	we propose a class of closed-form estimators for glms under high-dimensional sampling regimes .	we derive instantiations of our class of closed-form estimators , as well as corollaries of our general theorem , for the special cases of logistic , exponential and poisson regression models .	1	0	4	-5.9810476	5.185094	1
66-6-15	we propose a class of closed-form estimators for glms under high-dimensional sampling regimes .	we corroborate the surprising statistical and computational performance of our class of estimators via extensive simulations .	1	0	5	-5.9981384	5.0696545	1
66-6-15	we then perform thresholding operations on this mle variant to obtain our class of estimators .	our class of estimators is based on deriving closed-form variants of the vanilla unregularized mle but which are ( a ) well-defined even under high-dimensional settings , and ( b ) available in closed-form .	0	2	1	5.3000093	-4.716429	0
66-6-15	[CLS] our class of estimators is based on deriving closed - form variants of the vanilla unregularized mle but which are ( a ) well - defined even under high - dimensional settings, and ( b ) available in closed -	[CLS] we derive a unified statistical analysis of our class of estimators, and show that it enjoys strong statistical guarantees in both parameter error as well as variable selection, that surprisingly match those of the more complex regularized glm mles, even while	1	1	3	-4.940052	4.6125207	1
66-6-15	our class of estimators is based on deriving closed-form variants of the vanilla unregularized mle but which are ( a ) well-defined even under high-dimensional settings , and ( b ) available in closed-form .	we derive instantiations of our class of closed-form estimators , as well as corollaries of our general theorem , for the special cases of logistic , exponential and poisson regression models .	1	1	4	-5.5144415	5.016966	1
66-6-15	our class of estimators is based on deriving closed-form variants of the vanilla unregularized mle but which are ( a ) well-defined even under high-dimensional settings , and ( b ) available in closed-form .	we corroborate the surprising statistical and computational performance of our class of estimators via extensive simulations .	1	1	5	-5.938874	5.1132812	1
66-6-15	we derive a unified statistical analysis of our class of estimators , and show that it enjoys strong statistical guarantees in both parameter error as well as variable selection , that surprisingly match those of the more complex regularized glm mles , even while our closed-form estimators are computationally much simpler .	we then perform thresholding operations on this mle variant to obtain our class of estimators .	0	3	2	3.3980162	-3.1257005	0
66-6-15	we derive instantiations of our class of closed-form estimators , as well as corollaries of our general theorem , for the special cases of logistic , exponential and poisson regression models .	we then perform thresholding operations on this mle variant to obtain our class of estimators .	0	4	2	2.204448	-2.0227823	0
66-6-15	we then perform thresholding operations on this mle variant to obtain our class of estimators .	we corroborate the surprising statistical and computational performance of our class of estimators via extensive simulations .	1	2	5	-4.9426875	4.5478497	1
66-6-15	we derive instantiations of our class of closed-form estimators , as well as corollaries of our general theorem , for the special cases of logistic , exponential and poisson regression models .	[CLS] we derive a unified statistical analysis of our class of estimators, and show that it enjoys strong statistical guarantees in both parameter error as well as variable selection, that surprisingly match those of the more complex regularized glm mles, even while our closed - form estima	0	4	3	-3.3731232	3.217812	1
66-6-15	we derive a unified statistical analysis of our class of estimators , and show that it enjoys strong statistical guarantees in both parameter error as well as variable selection , that surprisingly match those of the more complex regularized glm mles , even while our closed-form estimators are computationally much simpler .	we corroborate the surprising statistical and computational performance of our class of estimators via extensive simulations .	1	3	5	-1.9128194	1.9969469	1
66-6-15	we corroborate the surprising statistical and computational performance of our class of estimators via extensive simulations .	we derive instantiations of our class of closed-form estimators , as well as corollaries of our general theorem , for the special cases of logistic , exponential and poisson regression models .	0	5	4	2.385195	-2.1914105	0
67-6-15	by combining precision and recall into a single score , it avoids disadvantages of simple metrics like the error rate , especially in cases of imbalanced class distributions .	the f-measure is an important and commonly used performance metric for binary prediction tasks .	0	1	0	5.3964663	-4.812722	0
67-6-15	the f-measure is an important and commonly used performance metric for binary prediction tasks .	the problem of optimizing the f-measure , that is , of developing learning algorithms that perform optimally in the sense of this measure , has recently been tackled by several authors .	1	0	2	-5.862334	5.173451	1
67-6-15	in this paper , we study the problem of f-measure maximization in the setting of online learning .	the f-measure is an important and commonly used performance metric for binary prediction tasks .	0	3	0	5.549694	-4.964493	0
67-6-15	the f-measure is an important and commonly used performance metric for binary prediction tasks .	we propose an efficient online algorithm and provide a formal analysis of its convergence properties .	1	0	4	-5.9422684	5.045519	1
67-6-15	the f-measure is an important and commonly used performance metric for binary prediction tasks .	moreover , first experimental results are presented , showing that our method performs well in practice .	1	0	5	-5.997876	5.1277747	1
67-6-15	the problem of optimizing the f-measure , that is , of developing learning algorithms that perform optimally in the sense of this measure , has recently been tackled by several authors .	by combining precision and recall into a single score , it avoids disadvantages of simple metrics like the error rate , especially in cases of imbalanced class distributions .	0	2	1	-5.8370275	5.2726145	1
67-6-15	in this paper , we study the problem of f-measure maximization in the setting of online learning .	by combining precision and recall into a single score , it avoids disadvantages of simple metrics like the error rate , especially in cases of imbalanced class distributions .	0	3	1	-5.626767	5.1371665	1
67-6-15	by combining precision and recall into a single score , it avoids disadvantages of simple metrics like the error rate , especially in cases of imbalanced class distributions .	we propose an efficient online algorithm and provide a formal analysis of its convergence properties .	1	1	4	-4.113515	3.897722	1
67-6-15	moreover , first experimental results are presented , showing that our method performs well in practice .	by combining precision and recall into a single score , it avoids disadvantages of simple metrics like the error rate , especially in cases of imbalanced class distributions .	0	5	1	4.921146	-4.5078487	0
67-6-15	in this paper , we study the problem of f-measure maximization in the setting of online learning .	the problem of optimizing the f-measure , that is , of developing learning algorithms that perform optimally in the sense of this measure , has recently been tackled by several authors .	0	3	2	5.1432915	-4.5519667	0
67-6-15	we propose an efficient online algorithm and provide a formal analysis of its convergence properties .	the problem of optimizing the f-measure , that is , of developing learning algorithms that perform optimally in the sense of this measure , has recently been tackled by several authors .	0	4	2	5.5129194	-4.9668283	0
67-6-15	the problem of optimizing the f-measure , that is , of developing learning algorithms that perform optimally in the sense of this measure , has recently been tackled by several authors .	moreover , first experimental results are presented , showing that our method performs well in practice .	1	2	5	-5.9823904	5.158966	1
67-6-15	in this paper , we study the problem of f-measure maximization in the setting of online learning .	we propose an efficient online algorithm and provide a formal analysis of its convergence properties .	1	3	4	-5.9530087	5.201786	1
67-6-15	moreover , first experimental results are presented , showing that our method performs well in practice .	in this paper , we study the problem of f-measure maximization in the setting of online learning .	0	5	3	5.6684685	-5.0666323	0
67-6-15	moreover , first experimental results are presented , showing that our method performs well in practice .	we propose an efficient online algorithm and provide a formal analysis of its convergence properties .	0	5	4	5.049286	-4.563611	0
68-5-10	we study the problem of online rank elicitation , assuming that rankings of a set of alternatives obey the plackett-luce distribution .	following the setting of the dueling bandits problem , the learner is allowed to query pairwise comparisons between alternatives , i.e. , to sample pairwise marginals of the distribution in an online fashion .	1	0	1	-5.865664	5.198528	1
68-5-10	we study the problem of online rank elicitation , assuming that rankings of a set of alternatives obey the plackett-luce distribution .	using this information , the learner seeks to reliably predict the most probable ranking ( or top-alternative ) .	1	0	2	-5.895262	5.168965	1
68-5-10	our approach is based on constructing a surrogate probability distribution over rankings based on a sorting procedure , for which the pairwise marginals provably coincide with the marginals of the plackettluce distribution .	we study the problem of online rank elicitation , assuming that rankings of a set of alternatives obey the plackett-luce distribution .	0	3	0	5.5248694	-4.9777956	0
68-5-10	we study the problem of online rank elicitation , assuming that rankings of a set of alternatives obey the plackett-luce distribution .	in addition to a formal performance and complexity analysis , we present first experimental studies .	1	0	4	-5.952363	5.159646	1
68-5-10	using this information , the learner seeks to reliably predict the most probable ranking ( or top-alternative ) .	following the setting of the dueling bandits problem , the learner is allowed to query pairwise comparisons between alternatives , i.e. , to sample pairwise marginals of the distribution in an online fashion .	0	2	1	-0.57833034	0.7605145	1
68-5-10	our approach is based on constructing a surrogate probability distribution over rankings based on a sorting procedure , for which the pairwise marginals provably coincide with the marginals of the plackettluce distribution .	following the setting of the dueling bandits problem , the learner is allowed to query pairwise comparisons between alternatives , i.e. , to sample pairwise marginals of the distribution in an online fashion .	0	3	1	-0.13617063	0.37310654	1
68-5-10	in addition to a formal performance and complexity analysis , we present first experimental studies .	following the setting of the dueling bandits problem , the learner is allowed to query pairwise comparisons between alternatives , i.e. , to sample pairwise marginals of the distribution in an online fashion .	0	4	1	5.1131797	-4.530842	0
68-5-10	our approach is based on constructing a surrogate probability distribution over rankings based on a sorting procedure , for which the pairwise marginals provably coincide with the marginals of the plackettluce distribution .	using this information , the learner seeks to reliably predict the most probable ranking ( or top-alternative ) .	0	3	2	1.8567907	-1.8358588	0
68-5-10	using this information , the learner seeks to reliably predict the most probable ranking ( or top-alternative ) .	in addition to a formal performance and complexity analysis , we present first experimental studies .	1	2	4	-5.9959726	5.136883	1
68-5-10	in addition to a formal performance and complexity analysis , we present first experimental studies .	our approach is based on constructing a surrogate probability distribution over rankings based on a sorting procedure , for which the pairwise marginals provably coincide with the marginals of the plackettluce distribution .	0	4	3	4.8857327	-4.2758207	0
69-6-15	we consider the problem of finding m best diverse solutions of energy minimization problems for graphical models .	contrary to the sequential method of batra et al. , which greedily finds one solution after another , we infer all m solutions jointly .	1	0	1	-5.8995266	5.24238	1
69-6-15	it was shown recently that such jointly inferred labelings not only have smaller total energy but also qualitatively outperform the sequentially obtained ones .	we consider the problem of finding m best diverse solutions of energy minimization problems for graphical models .	0	2	0	5.5284023	-4.96834	0
69-6-15	we consider the problem of finding m best diverse solutions of energy minimization problems for graphical models .	the only obstacle for using this new technique is the complexity of the corresponding inference problem , since it is considerably slower algorithm than the method of batra et al .	1	0	3	-5.901911	5.194333	1
69-6-15	in this work we show that the joint inference of m best diverse solutions can be formulated as a submodular energy minimization if the original map-inference problem is submodular , hence fast inference techniques can be used .	we consider the problem of finding m best diverse solutions of energy minimization problems for graphical models .	0	4	0	5.4516053	-4.922164	0
69-6-15	in addition to the theoretical results we provide practical algorithms that outperform the current state-of-the-art and can be used in both submodular and non-submodular case .	we consider the problem of finding m best diverse solutions of energy minimization problems for graphical models .	0	5	0	5.4984975	-4.9760323	0
69-6-15	contrary to the sequential method of batra et al. , which greedily finds one solution after another , we infer all m solutions jointly .	it was shown recently that such jointly inferred labelings not only have smaller total energy but also qualitatively outperform the sequentially obtained ones .	1	1	2	-3.8297112	3.6494339	1
69-6-15	contrary to the sequential method of batra et al. , which greedily finds one solution after another , we infer all m solutions jointly .	the only obstacle for using this new technique is the complexity of the corresponding inference problem , since it is considerably slower algorithm than the method of batra et al .	1	1	3	-5.6124134	4.975814	1
69-6-15	in this work we show that the joint inference of m best diverse solutions can be formulated as a submodular energy minimization if the original map-inference problem is submodular , hence fast inference techniques can be used .	contrary to the sequential method of batra et al. , which greedily finds one solution after another , we infer all m solutions jointly .	0	4	1	-2.127666	2.182601	1
69-6-15	contrary to the sequential method of batra et al. , which greedily finds one solution after another , we infer all m solutions jointly .	in addition to the theoretical results we provide practical algorithms that outperform the current state-of-the-art and can be used in both submodular and non-submodular case .	1	1	5	-5.967103	5.083107	1
69-6-15	the only obstacle for using this new technique is the complexity of the corresponding inference problem , since it is considerably slower algorithm than the method of batra et al .	it was shown recently that such jointly inferred labelings not only have smaller total energy but also qualitatively outperform the sequentially obtained ones .	0	3	2	4.4587517	-3.9947646	0
69-6-15	it was shown recently that such jointly inferred labelings not only have smaller total energy but also qualitatively outperform the sequentially obtained ones .	in this work we show that the joint inference of m best diverse solutions can be formulated as a submodular energy minimization if the original map-inference problem is submodular , hence fast inference techniques can be used .	1	2	4	-2.4960527	2.5091457	1
69-6-15	in addition to the theoretical results we provide practical algorithms that outperform the current state-of-the-art and can be used in both submodular and non-submodular case .	it was shown recently that such jointly inferred labelings not only have smaller total energy but also qualitatively outperform the sequentially obtained ones .	0	5	2	5.323242	-4.755746	0
69-6-15	in this work we show that the joint inference of m best diverse solutions can be formulated as a submodular energy minimization if the original map-inference problem is submodular , hence fast inference techniques can be used .	the only obstacle for using this new technique is the complexity of the corresponding inference problem , since it is considerably slower algorithm than the method of batra et al .	0	4	3	-5.7408695	5.076852	1
69-6-15	in addition to the theoretical results we provide practical algorithms that outperform the current state-of-the-art and can be used in both submodular and non-submodular case .	the only obstacle for using this new technique is the complexity of the corresponding inference problem , since it is considerably slower algorithm than the method of batra et al .	0	5	3	4.1386976	-3.7966475	0
69-6-15	in addition to the theoretical results we provide practical algorithms that outperform the current state-of-the-art and can be used in both submodular and non-submodular case .	in this work we show that the joint inference of m best diverse solutions can be formulated as a submodular energy minimization if the original map-inference problem is submodular , hence fast inference techniques can be used .	0	5	4	5.426759	-4.818449	0
70-4-6	bounded tree-width bayesian networks have recently received a lot of attention as a way to circumvent this complexity issue ; however , while inference on bounded tree-width networks is tractable , the learning problem remains np-hard even for tree-width 2 .	both learning and inference tasks on bayesian networks are np-hard in general .	0	1	0	-0.06353711	0.37312657	1
70-4-6	in this paper , we propose bounded vertex cover number bayesian networks as an alternative to bounded tree-width networks .	both learning and inference tasks on bayesian networks are np-hard in general .	0	2	0	-4.203728	3.861166	1
70-4-6	[CLS] in particular, we show that both inference and learning can be done in polynomial time for any fixed vertex cover number bound k, in contrast to the general and bounded tree - width cases ; on the other hand, we also show that learning problem is w - hard in parameter k. furthermore, we give an alternative way to learn bounded vertex cover number bayesian networks using integer linear programming ( ilp ),	both learning and inference tasks on bayesian networks are np-hard in general .	0	3	0	4.5286355	-4.034184	0
70-4-6	bounded tree-width bayesian networks have recently received a lot of attention as a way to circumvent this complexity issue ; however , while inference on bounded tree-width networks is tractable , the learning problem remains np-hard even for tree-width 2 .	in this paper , we propose bounded vertex cover number bayesian networks as an alternative to bounded tree-width networks .	1	1	2	-5.143035	4.8172045	1
70-4-6	[CLS] in particular, we show that both inference and learning can be done in polynomial time for any fixed vertex cover number bound k, in contrast to the general and bounded tree - width cases ; on the other hand, we also show that learning problem is	[CLS] bounded tree - width bayesian networks have recently received a lot of attention as a way to circumvent this complexity issue ; however, while inference on bounded tree - width networks is tractable, the learning problem remains np - hard even for	0	3	1	3.7495728	-3.3170042	0
70-4-6	[CLS] in particular, we show that both inference and learning can be done in polynomial time for any fixed vertex cover number bound k, in contrast to the general and bounded tree - width cases ; on the other hand, we also show that learning problem is w - hard in parameter k. furthermore, we give an alternative way to learn bounded vertex cover number bayesian networks using integer	in this paper , we propose bounded vertex cover number bayesian networks as an alternative to bounded tree-width networks .	0	3	2	5.331301	-4.634578	0
71-6-15	we define general poisson dag models as models where each node is a poisson random variable with rate parameter depending on the values of the parents in the underlying dag .	in this paper , we address the question of identifiability and learning algorithms for large-scale poisson directed acyclic graphical ( dag ) models .	0	1	0	5.666388	-5.05729	0
71-6-15	first , we prove that poisson dag models are identifiable from observational data , and present a polynomial-time algorithm that learns the poisson dag model under suitable regularity conditions .	in this paper , we address the question of identifiability and learning algorithms for large-scale poisson directed acyclic graphical ( dag ) models .	0	2	0	5.7105904	-5.1077695	0
71-6-15	the main idea behind our algorithm is based on overdispersion , in that variables that are conditionally poisson are overdispersed relative to variables that are marginally poisson .	in this paper , we address the question of identifiability and learning algorithms for large-scale poisson directed acyclic graphical ( dag ) models .	0	3	0	5.644724	-5.13898	0
71-6-15	in this paper , we address the question of identifiability and learning algorithms for large-scale poisson directed acyclic graphical ( dag ) models .	our algorithms exploits overdispersion along with methods for learning sparse poisson undirected graphical models for faster computation .	1	0	4	-5.96158	5.2271824	1
71-6-15	we provide both theoretical guarantees and simulation results for both small and large-scale dags .	in this paper , we address the question of identifiability and learning algorithms for large-scale poisson directed acyclic graphical ( dag ) models .	0	5	0	5.657153	-5.0289774	0
71-6-15	we define general poisson dag models as models where each node is a poisson random variable with rate parameter depending on the values of the parents in the underlying dag .	first , we prove that poisson dag models are identifiable from observational data , and present a polynomial-time algorithm that learns the poisson dag model under suitable regularity conditions .	1	1	2	-5.6722746	5.2229056	1
71-6-15	the main idea behind our algorithm is based on overdispersion , in that variables that are conditionally poisson are overdispersed relative to variables that are marginally poisson .	we define general poisson dag models as models where each node is a poisson random variable with rate parameter depending on the values of the parents in the underlying dag .	0	3	1	2.1388292	-1.7440705	0
71-6-15	our algorithms exploits overdispersion along with methods for learning sparse poisson undirected graphical models for faster computation .	we define general poisson dag models as models where each node is a poisson random variable with rate parameter depending on the values of the parents in the underlying dag .	0	4	1	5.1951513	-4.572621	0
71-6-15	we provide both theoretical guarantees and simulation results for both small and large-scale dags .	we define general poisson dag models as models where each node is a poisson random variable with rate parameter depending on the values of the parents in the underlying dag .	0	5	1	5.1718187	-4.5223436	0
71-6-15	first , we prove that poisson dag models are identifiable from observational data , and present a polynomial-time algorithm that learns the poisson dag model under suitable regularity conditions .	the main idea behind our algorithm is based on overdispersion , in that variables that are conditionally poisson are overdispersed relative to variables that are marginally poisson .	1	2	3	-1.9267755	2.0457993	1
71-6-15	first , we prove that poisson dag models are identifiable from observational data , and present a polynomial-time algorithm that learns the poisson dag model under suitable regularity conditions .	our algorithms exploits overdispersion along with methods for learning sparse poisson undirected graphical models for faster computation .	1	2	4	-5.5048394	5.0277443	1
71-6-15	first , we prove that poisson dag models are identifiable from observational data , and present a polynomial-time algorithm that learns the poisson dag model under suitable regularity conditions .	we provide both theoretical guarantees and simulation results for both small and large-scale dags .	1	2	5	-5.2377257	4.85453	1
71-6-15	the main idea behind our algorithm is based on overdispersion , in that variables that are conditionally poisson are overdispersed relative to variables that are marginally poisson .	our algorithms exploits overdispersion along with methods for learning sparse poisson undirected graphical models for faster computation .	1	3	4	-5.4998913	5.025504	1
71-6-15	the main idea behind our algorithm is based on overdispersion , in that variables that are conditionally poisson are overdispersed relative to variables that are marginally poisson .	we provide both theoretical guarantees and simulation results for both small and large-scale dags .	1	3	5	-5.8206367	5.1876802	1
71-6-15	we provide both theoretical guarantees and simulation results for both small and large-scale dags .	our algorithms exploits overdispersion along with methods for learning sparse poisson undirected graphical models for faster computation .	0	5	4	-0.67124486	0.91104746	1
72-5-10	restricted boltzmann machines are undirected neural networks which have been shown to be effective in many applications , including serving as initializations for training deep multi-layer neural networks .	one of the main reasons for their success is the existence of efficient and practical stochastic algorithms , such as contrastive divergence , for unsupervised training .	1	0	1	-5.8220677	5.145811	1
72-5-10	we propose an alternative deterministic iterative procedure based on an improved mean field method from statistical physics known as the thouless-anderson-palmer approach .	restricted boltzmann machines are undirected neural networks which have been shown to be effective in many applications , including serving as initializations for training deep multi-layer neural networks .	0	2	0	5.5475335	-4.9531927	0
72-5-10	we demonstrate that our algorithm provides performance equal to , and sometimes superior to , persistent contrastive divergence , while also providing a clear and easy to evaluate objective function .	restricted boltzmann machines are undirected neural networks which have been shown to be effective in many applications , including serving as initializations for training deep multi-layer neural networks .	0	3	0	5.6544476	-5.1049385	0
72-5-10	we believe that this strategy can be easily generalized to other models as well as to more accurate higher-order approximations , paving the way for systematic improvements in training boltzmann machines with hidden units .	restricted boltzmann machines are undirected neural networks which have been shown to be effective in many applications , including serving as initializations for training deep multi-layer neural networks .	0	4	0	5.5837345	-4.9910107	0
72-5-10	one of the main reasons for their success is the existence of efficient and practical stochastic algorithms , such as contrastive divergence , for unsupervised training .	we propose an alternative deterministic iterative procedure based on an improved mean field method from statistical physics known as the thouless-anderson-palmer approach .	1	1	2	-5.6343074	5.1204147	1
72-5-10	we demonstrate that our algorithm provides performance equal to , and sometimes superior to , persistent contrastive divergence , while also providing a clear and easy to evaluate objective function .	one of the main reasons for their success is the existence of efficient and practical stochastic algorithms , such as contrastive divergence , for unsupervised training .	0	3	1	5.533933	-4.9079447	0
72-5-10	one of the main reasons for their success is the existence of efficient and practical stochastic algorithms , such as contrastive divergence , for unsupervised training .	we believe that this strategy can be easily generalized to other models as well as to more accurate higher-order approximations , paving the way for systematic improvements in training boltzmann machines with hidden units .	1	1	4	-5.9486284	5.0684633	1
72-5-10	we demonstrate that our algorithm provides performance equal to , and sometimes superior to , persistent contrastive divergence , while also providing a clear and easy to evaluate objective function .	we propose an alternative deterministic iterative procedure based on an improved mean field method from statistical physics known as the thouless-anderson-palmer approach .	0	3	2	5.509531	-4.866873	0
72-5-10	we believe that this strategy can be easily generalized to other models as well as to more accurate higher-order approximations , paving the way for systematic improvements in training boltzmann machines with hidden units .	we propose an alternative deterministic iterative procedure based on an improved mean field method from statistical physics known as the thouless-anderson-palmer approach .	0	4	2	4.9541597	-4.398162	0
72-5-10	we demonstrate that our algorithm provides performance equal to , and sometimes superior to , persistent contrastive divergence , while also providing a clear and easy to evaluate objective function .	we believe that this strategy can be easily generalized to other models as well as to more accurate higher-order approximations , paving the way for systematic improvements in training boltzmann machines with hidden units .	1	3	4	-2.0509965	2.1315575	1
73-3-3	we constructed several largescale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results .	this article offers an empirical exploration on the use of character-level convolutional networks ( convnets ) for text classification .	0	1	0	4.9131813	-4.314227	0
73-3-3	comparisons are offered against traditional models such as bag of words , n-grams and their tfidf variants , and deep learning models such as word-based convnets and recurrent neural networks .	this article offers an empirical exploration on the use of character-level convolutional networks ( convnets ) for text classification .	0	2	0	5.6294003	-5.029022	0
73-3-3	we constructed several largescale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results .	comparisons are offered against traditional models such as bag of words , n-grams and their tfidf variants , and deep learning models such as word-based convnets and recurrent neural networks .	1	1	2	-4.4973154	4.149722	1
74-12-66	a wide spectrum of discriminative methods is increasingly used in diverse applications for classification or regression tasks .	however , many existing discriminative methods assume that the input data is nearly noise-free , which limits their applications to solve real-world problems .	1	0	1	-5.738348	5.2128563	1
74-12-66	particularly for disease diagnosis , the data acquired by the neuroimaging devices are always prone to different sources of noise .	a wide spectrum of discriminative methods is increasingly used in diverse applications for classification or regression tasks .	0	2	0	5.500944	-4.956849	0
74-12-66	a wide spectrum of discriminative methods is increasingly used in diverse applications for classification or regression tasks .	robust discriminative models are somewhat scarce and only a few attempts have been made to make them robust against noise or outliers .	1	0	3	-5.833956	5.225127	1
74-12-66	these methods focus on detecting either the sample-outliers or feature-noises .	a wide spectrum of discriminative methods is increasingly used in diverse applications for classification or regression tasks .	0	4	0	5.637589	-5.08763	0
74-12-66	moreover , they usually use unsupervised de-noising procedures , or separately de-noise the training and the testing data .	a wide spectrum of discriminative methods is increasingly used in diverse applications for classification or regression tasks .	0	5	0	5.647407	-5.07297	0
74-12-66	all these factors may induce biases in the learning process , and thus limit its performance .	a wide spectrum of discriminative methods is increasingly used in diverse applications for classification or regression tasks .	0	6	0	5.5865517	-5.0172176	0
74-12-66	a wide spectrum of discriminative methods is increasingly used in diverse applications for classification or regression tasks .	in this paper , we propose a classification method based on the least-squares formulation of linear discriminant analysis , which simultaneously detects the sample-outliers and feature-noises .	1	0	7	-5.950778	5.1689816	1
74-12-66	a wide spectrum of discriminative methods is increasingly used in diverse applications for classification or regression tasks .	the proposed method operates under a semi-supervised setting , in which both labeled training and unlabeled testing data are incorporated to form the intrinsic geometry of the sample space .	1	0	8	-5.922745	5.146673	1
74-12-66	therefore , the violating samples or feature values are identified as sample-outliers or feature-noises , respectively .	a wide spectrum of discriminative methods is increasingly used in diverse applications for classification or regression tasks .	0	9	0	5.704652	-5.1426516	0
74-12-66	a wide spectrum of discriminative methods is increasingly used in diverse applications for classification or regression tasks .	we test our algorithm on one synthetic and two brain neurodegenerative databases ( particularly for parkinson 's disease and alzheimer 's disease ) .	1	0	10	-5.953951	5.1765094	1
74-12-66	the results demonstrate that our method outperforms all baseline and state-of-the-art methods , in terms of both accuracy and the area under the roc curve .	a wide spectrum of discriminative methods is increasingly used in diverse applications for classification or regression tasks .	0	11	0	5.6148634	-5.0722075	0
74-12-66	particularly for disease diagnosis , the data acquired by the neuroimaging devices are always prone to different sources of noise .	however , many existing discriminative methods assume that the input data is nearly noise-free , which limits their applications to solve real-world problems .	0	2	1	-3.5560534	3.4093285	1
74-12-66	however , many existing discriminative methods assume that the input data is nearly noise-free , which limits their applications to solve real-world problems .	robust discriminative models are somewhat scarce and only a few attempts have been made to make them robust against noise or outliers .	1	1	3	-1.1736146	1.4729578	1
74-12-66	these methods focus on detecting either the sample-outliers or feature-noises .	however , many existing discriminative methods assume that the input data is nearly noise-free , which limits their applications to solve real-world problems .	0	4	1	3.7270637	-3.3084235	0
74-12-66	moreover , they usually use unsupervised de-noising procedures , or separately de-noise the training and the testing data .	however , many existing discriminative methods assume that the input data is nearly noise-free , which limits their applications to solve real-world problems .	0	5	1	4.8638806	-4.2311683	0
74-12-66	all these factors may induce biases in the learning process , and thus limit its performance .	however , many existing discriminative methods assume that the input data is nearly noise-free , which limits their applications to solve real-world problems .	0	6	1	4.031378	-3.6604319	0
74-12-66	in this paper , we propose a classification method based on the least-squares formulation of linear discriminant analysis , which simultaneously detects the sample-outliers and feature-noises .	however , many existing discriminative methods assume that the input data is nearly noise-free , which limits their applications to solve real-world problems .	0	7	1	5.509856	-4.82205	0
74-12-66	the proposed method operates under a semi-supervised setting , in which both labeled training and unlabeled testing data are incorporated to form the intrinsic geometry of the sample space .	however , many existing discriminative methods assume that the input data is nearly noise-free , which limits their applications to solve real-world problems .	0	8	1	5.674764	-5.02622	0
74-12-66	therefore , the violating samples or feature values are identified as sample-outliers or feature-noises , respectively .	however , many existing discriminative methods assume that the input data is nearly noise-free , which limits their applications to solve real-world problems .	0	9	1	5.248369	-4.644127	0
74-12-66	however , many existing discriminative methods assume that the input data is nearly noise-free , which limits their applications to solve real-world problems .	we test our algorithm on one synthetic and two brain neurodegenerative databases ( particularly for parkinson 's disease and alzheimer 's disease ) .	1	1	10	-5.9323764	5.094412	1
74-12-66	however , many existing discriminative methods assume that the input data is nearly noise-free , which limits their applications to solve real-world problems .	the results demonstrate that our method outperforms all baseline and state-of-the-art methods , in terms of both accuracy and the area under the roc curve .	1	1	11	-5.953838	5.1015935	1
74-12-66	robust discriminative models are somewhat scarce and only a few attempts have been made to make them robust against noise or outliers .	particularly for disease diagnosis , the data acquired by the neuroimaging devices are always prone to different sources of noise .	0	3	2	3.2658906	-3.0346785	0
74-12-66	particularly for disease diagnosis , the data acquired by the neuroimaging devices are always prone to different sources of noise .	these methods focus on detecting either the sample-outliers or feature-noises .	1	2	4	-2.4776504	2.531255	1
74-12-66	moreover , they usually use unsupervised de-noising procedures , or separately de-noise the training and the testing data .	particularly for disease diagnosis , the data acquired by the neuroimaging devices are always prone to different sources of noise .	0	5	2	5.0794497	-4.399176	0
74-12-66	all these factors may induce biases in the learning process , and thus limit its performance .	particularly for disease diagnosis , the data acquired by the neuroimaging devices are always prone to different sources of noise .	0	6	2	3.943678	-3.6120472	0
74-12-66	particularly for disease diagnosis , the data acquired by the neuroimaging devices are always prone to different sources of noise .	in this paper , we propose a classification method based on the least-squares formulation of linear discriminant analysis , which simultaneously detects the sample-outliers and feature-noises .	1	2	7	-5.97494	5.230789	1
74-12-66	the proposed method operates under a semi-supervised setting , in which both labeled training and unlabeled testing data are incorporated to form the intrinsic geometry of the sample space .	particularly for disease diagnosis , the data acquired by the neuroimaging devices are always prone to different sources of noise .	0	8	2	5.3138757	-4.67502	0
74-12-66	therefore , the violating samples or feature values are identified as sample-outliers or feature-noises , respectively .	particularly for disease diagnosis , the data acquired by the neuroimaging devices are always prone to different sources of noise .	0	9	2	5.1015396	-4.534298	0
74-12-66	particularly for disease diagnosis , the data acquired by the neuroimaging devices are always prone to different sources of noise .	we test our algorithm on one synthetic and two brain neurodegenerative databases ( particularly for parkinson 's disease and alzheimer 's disease ) .	1	2	10	-6.0015507	5.1779003	1
74-12-66	the results demonstrate that our method outperforms all baseline and state-of-the-art methods , in terms of both accuracy and the area under the roc curve .	particularly for disease diagnosis , the data acquired by the neuroimaging devices are always prone to different sources of noise .	0	11	2	5.51059	-4.8670397	0
74-12-66	these methods focus on detecting either the sample-outliers or feature-noises .	robust discriminative models are somewhat scarce and only a few attempts have been made to make them robust against noise or outliers .	0	4	3	2.647163	-2.2675934	0
74-12-66	robust discriminative models are somewhat scarce and only a few attempts have been made to make them robust against noise or outliers .	moreover , they usually use unsupervised de-noising procedures , or separately de-noise the training and the testing data .	1	3	5	-4.362605	4.1971436	1
74-12-66	robust discriminative models are somewhat scarce and only a few attempts have been made to make them robust against noise or outliers .	all these factors may induce biases in the learning process , and thus limit its performance .	1	3	6	-2.3980324	2.5306942	1
74-12-66	robust discriminative models are somewhat scarce and only a few attempts have been made to make them robust against noise or outliers .	in this paper , we propose a classification method based on the least-squares formulation of linear discriminant analysis , which simultaneously detects the sample-outliers and feature-noises .	1	3	7	-5.902782	5.231409	1
74-12-66	the proposed method operates under a semi-supervised setting , in which both labeled training and unlabeled testing data are incorporated to form the intrinsic geometry of the sample space .	robust discriminative models are somewhat scarce and only a few attempts have been made to make them robust against noise or outliers .	0	8	3	5.457471	-4.855638	0
74-12-66	robust discriminative models are somewhat scarce and only a few attempts have been made to make them robust against noise or outliers .	therefore , the violating samples or feature values are identified as sample-outliers or feature-noises , respectively .	1	3	9	-5.361805	5.02426	1
74-12-66	robust discriminative models are somewhat scarce and only a few attempts have been made to make them robust against noise or outliers .	we test our algorithm on one synthetic and two brain neurodegenerative databases ( particularly for parkinson 's disease and alzheimer 's disease ) .	1	3	10	-5.940505	5.1823344	1
74-12-66	the results demonstrate that our method outperforms all baseline and state-of-the-art methods , in terms of both accuracy and the area under the roc curve .	robust discriminative models are somewhat scarce and only a few attempts have been made to make them robust against noise or outliers .	0	11	3	5.5599484	-4.970169	0
74-12-66	these methods focus on detecting either the sample-outliers or feature-noises .	moreover , they usually use unsupervised de-noising procedures , or separately de-noise the training and the testing data .	1	4	5	-4.579382	4.303556	1
74-12-66	these methods focus on detecting either the sample-outliers or feature-noises .	all these factors may induce biases in the learning process , and thus limit its performance .	1	4	6	-3.6534505	3.5543728	1
74-12-66	these methods focus on detecting either the sample-outliers or feature-noises .	in this paper , we propose a classification method based on the least-squares formulation of linear discriminant analysis , which simultaneously detects the sample-outliers and feature-noises .	1	4	7	-5.461465	5.0662503	1
74-12-66	the proposed method operates under a semi-supervised setting , in which both labeled training and unlabeled testing data are incorporated to form the intrinsic geometry of the sample space .	these methods focus on detecting either the sample-outliers or feature-noises .	0	8	4	5.243705	-4.672413	0
74-12-66	these methods focus on detecting either the sample-outliers or feature-noises .	therefore , the violating samples or feature values are identified as sample-outliers or feature-noises , respectively .	1	4	9	-5.322198	4.952157	1
74-12-66	these methods focus on detecting either the sample-outliers or feature-noises .	we test our algorithm on one synthetic and two brain neurodegenerative databases ( particularly for parkinson 's disease and alzheimer 's disease ) .	1	4	10	-6.013648	5.1459665	1
74-12-66	the results demonstrate that our method outperforms all baseline and state-of-the-art methods , in terms of both accuracy and the area under the roc curve .	these methods focus on detecting either the sample-outliers or feature-noises .	0	11	4	5.3839793	-4.786598	0
74-12-66	moreover , they usually use unsupervised de-noising procedures , or separately de-noise the training and the testing data .	all these factors may induce biases in the learning process , and thus limit its performance .	1	5	6	3.2204037	-2.9021654	0
74-12-66	moreover , they usually use unsupervised de-noising procedures , or separately de-noise the training and the testing data .	in this paper , we propose a classification method based on the least-squares formulation of linear discriminant analysis , which simultaneously detects the sample-outliers and feature-noises .	1	5	7	-5.750739	5.23331	1
74-12-66	the proposed method operates under a semi-supervised setting , in which both labeled training and unlabeled testing data are incorporated to form the intrinsic geometry of the sample space .	moreover , they usually use unsupervised de-noising procedures , or separately de-noise the training and the testing data .	0	8	5	5.3972263	-4.8204174	0
74-12-66	therefore , the violating samples or feature values are identified as sample-outliers or feature-noises , respectively .	moreover , they usually use unsupervised de-noising procedures , or separately de-noise the training and the testing data .	0	9	5	3.199953	-2.941661	0
74-12-66	moreover , they usually use unsupervised de-noising procedures , or separately de-noise the training and the testing data .	we test our algorithm on one synthetic and two brain neurodegenerative databases ( particularly for parkinson 's disease and alzheimer 's disease ) .	1	5	10	-5.972537	5.154731	1
74-12-66	the results demonstrate that our method outperforms all baseline and state-of-the-art methods , in terms of both accuracy and the area under the roc curve .	moreover , they usually use unsupervised de-noising procedures , or separately de-noise the training and the testing data .	0	11	5	5.4610076	-4.908968	0
74-12-66	all these factors may induce biases in the learning process , and thus limit its performance .	in this paper , we propose a classification method based on the least-squares formulation of linear discriminant analysis , which simultaneously detects the sample-outliers and feature-noises .	1	6	7	-5.739191	5.23596	1
74-12-66	all these factors may induce biases in the learning process , and thus limit its performance .	the proposed method operates under a semi-supervised setting , in which both labeled training and unlabeled testing data are incorporated to form the intrinsic geometry of the sample space .	1	6	8	-5.768815	5.076807	1
74-12-66	therefore , the violating samples or feature values are identified as sample-outliers or feature-noises , respectively .	all these factors may induce biases in the learning process , and thus limit its performance .	0	9	6	4.379366	-4.0034466	0
74-12-66	all these factors may induce biases in the learning process , and thus limit its performance .	we test our algorithm on one synthetic and two brain neurodegenerative databases ( particularly for parkinson 's disease and alzheimer 's disease ) .	1	6	10	-5.9488163	5.0786667	1
74-12-66	all these factors may induce biases in the learning process , and thus limit its performance .	the results demonstrate that our method outperforms all baseline and state-of-the-art methods , in terms of both accuracy and the area under the roc curve .	1	6	11	-5.9274845	5.0279613	1
74-12-66	the proposed method operates under a semi-supervised setting , in which both labeled training and unlabeled testing data are incorporated to form the intrinsic geometry of the sample space .	in this paper , we propose a classification method based on the least-squares formulation of linear discriminant analysis , which simultaneously detects the sample-outliers and feature-noises .	0	8	7	5.550454	-4.8849435	0
74-12-66	therefore , the violating samples or feature values are identified as sample-outliers or feature-noises , respectively .	in this paper , we propose a classification method based on the least-squares formulation of linear discriminant analysis , which simultaneously detects the sample-outliers and feature-noises .	0	9	7	-1.6319282	1.7707871	1
74-12-66	in this paper , we propose a classification method based on the least-squares formulation of linear discriminant analysis , which simultaneously detects the sample-outliers and feature-noises .	we test our algorithm on one synthetic and two brain neurodegenerative databases ( particularly for parkinson 's disease and alzheimer 's disease ) .	1	7	10	-5.9648194	5.207868	1
74-12-66	in this paper , we propose a classification method based on the least-squares formulation of linear discriminant analysis , which simultaneously detects the sample-outliers and feature-noises .	the results demonstrate that our method outperforms all baseline and state-of-the-art methods , in terms of both accuracy and the area under the roc curve .	1	7	11	-5.994246	5.1311464	1
74-12-66	the proposed method operates under a semi-supervised setting , in which both labeled training and unlabeled testing data are incorporated to form the intrinsic geometry of the sample space .	therefore , the violating samples or feature values are identified as sample-outliers or feature-noises , respectively .	1	8	9	-1.741189	1.8802102	1
74-12-66	we test our algorithm on one synthetic and two brain neurodegenerative databases ( particularly for parkinson 's disease and alzheimer 's disease ) .	the proposed method operates under a semi-supervised setting , in which both labeled training and unlabeled testing data are incorporated to form the intrinsic geometry of the sample space .	0	10	8	5.1402173	-4.5641456	0
74-12-66	the results demonstrate that our method outperforms all baseline and state-of-the-art methods , in terms of both accuracy and the area under the roc curve .	the proposed method operates under a semi-supervised setting , in which both labeled training and unlabeled testing data are incorporated to form the intrinsic geometry of the sample space .	0	11	8	5.3154135	-4.7237062	0
74-12-66	we test our algorithm on one synthetic and two brain neurodegenerative databases ( particularly for parkinson 's disease and alzheimer 's disease ) .	therefore , the violating samples or feature values are identified as sample-outliers or feature-noises , respectively .	0	10	9	4.7915916	-4.3872294	0
74-12-66	therefore , the violating samples or feature values are identified as sample-outliers or feature-noises , respectively .	the results demonstrate that our method outperforms all baseline and state-of-the-art methods , in terms of both accuracy and the area under the roc curve .	1	9	11	-5.964938	5.1130753	1
74-12-66	we test our algorithm on one synthetic and two brain neurodegenerative databases ( particularly for parkinson 's disease and alzheimer 's disease ) .	the results demonstrate that our method outperforms all baseline and state-of-the-art methods , in terms of both accuracy and the area under the roc curve .	1	10	11	-4.989945	4.575298	1
75-6-15	we study the problem of black-box optimization of a function f of any dimension , given function evaluations perturbed by noise .	the function is assumed to be locally smooth around one of its global optima , but this smoothness is unknown .	1	0	1	-5.7987704	5.1724415	1
75-6-15	we study the problem of black-box optimization of a function f of any dimension , given function evaluations perturbed by noise .	our contribution is an adaptive optimization algorithm , poo or parallel optimistic optimization , that is able to deal with this setting .	1	0	2	-5.800536	5.143076	1
75-6-15	we study the problem of black-box optimization of a function f of any dimension , given function evaluations perturbed by noise .	poo performs almost as well as the best known algorithms requiring the knowledge of the smoothness .	1	0	3	-5.857124	5.184613	1
75-6-15	we study the problem of black-box optimization of a function f of any dimension , given function evaluations perturbed by noise .	furthermore , poo works for a larger class of functions than what was previously considered , especially for functions that are difficult to optimize , in a very precise sense .	1	0	4	-5.875077	5.2147617	1
75-6-15	we provide a finite-time analysis of poo'sperformance , which shows that its error after n evaluations is at most a factor of ln n away from the error of the best known optimization algorithms using the knowledge of the smoothness .	we study the problem of black-box optimization of a function f of any dimension , given function evaluations perturbed by noise .	0	5	0	5.6125603	-4.981658	0
75-6-15	the function is assumed to be locally smooth around one of its global optima , but this smoothness is unknown .	our contribution is an adaptive optimization algorithm , poo or parallel optimistic optimization , that is able to deal with this setting .	1	1	2	-5.726185	5.1106195	1
75-6-15	the function is assumed to be locally smooth around one of its global optima , but this smoothness is unknown .	poo performs almost as well as the best known algorithms requiring the knowledge of the smoothness .	1	1	3	-6.005551	5.0867863	1
75-6-15	furthermore , poo works for a larger class of functions than what was previously considered , especially for functions that are difficult to optimize , in a very precise sense .	the function is assumed to be locally smooth around one of its global optima , but this smoothness is unknown .	0	4	1	4.417433	-3.9285755	0
75-6-15	we provide a finite-time analysis of poo'sperformance , which shows that its error after n evaluations is at most a factor of ln n away from the error of the best known optimization algorithms using the knowledge of the smoothness .	the function is assumed to be locally smooth around one of its global optima , but this smoothness is unknown .	0	5	1	4.9775867	-4.4358606	0
75-6-15	our contribution is an adaptive optimization algorithm , poo or parallel optimistic optimization , that is able to deal with this setting .	poo performs almost as well as the best known algorithms requiring the knowledge of the smoothness .	1	2	3	-5.7672215	4.8469324	1
75-6-15	our contribution is an adaptive optimization algorithm , poo or parallel optimistic optimization , that is able to deal with this setting .	furthermore , poo works for a larger class of functions than what was previously considered , especially for functions that are difficult to optimize , in a very precise sense .	1	2	4	-5.60001	4.993782	1
75-6-15	our contribution is an adaptive optimization algorithm , poo or parallel optimistic optimization , that is able to deal with this setting .	we provide a finite-time analysis of poo'sperformance , which shows that its error after n evaluations is at most a factor of ln n away from the error of the best known optimization algorithms using the knowledge of the smoothness .	1	2	5	-4.4007998	4.088869	1
75-6-15	poo performs almost as well as the best known algorithms requiring the knowledge of the smoothness .	furthermore , poo works for a larger class of functions than what was previously considered , especially for functions that are difficult to optimize , in a very precise sense .	1	3	4	1.8587832	-1.7078936	0
75-6-15	we provide a finite-time analysis of poo'sperformance , which shows that its error after n evaluations is at most a factor of ln n away from the error of the best known optimization algorithms using the knowledge of the smoothness .	poo performs almost as well as the best known algorithms requiring the knowledge of the smoothness .	0	5	3	-5.4172897	4.8843694	1
75-6-15	furthermore , poo works for a larger class of functions than what was previously considered , especially for functions that are difficult to optimize , in a very precise sense .	we provide a finite-time analysis of poo'sperformance , which shows that its error after n evaluations is at most a factor of ln n away from the error of the best known optimization algorithms using the knowledge of the smoothness .	1	4	5	2.8074303	-2.6278265	0
76-7-21	however , most developments rely on the knowledge of the model parameters , or at least on the number of communities .	the stochastic block model ( sbm ) has recently gathered significant attention due to new threshold phenomena .	0	1	0	5.396091	-4.7688723	0
76-7-21	this paper introduces efficient algorithms that do not require such knowledge and yet achieve the optimal information-theoretic tradeoffs identified in abbe-sandon focs15 .	the stochastic block model ( sbm ) has recently gathered significant attention due to new threshold phenomena .	0	2	0	5.6176786	-4.9256554	0
76-7-21	in the constant degree regime , an algorithm is developed that requires only a lower-bound on the relative sizes of the communities and achieves the optimal accuracy scaling for large degrees .	the stochastic block model ( sbm ) has recently gathered significant attention due to new threshold phenomena .	0	3	0	5.6069803	-5.0076895	0
76-7-21	this lower-bound requirement is removed for the regime of arbitrarily slowly diverging degrees , and the model parameters are learned efficiently .	the stochastic block model ( sbm ) has recently gathered significant attention due to new threshold phenomena .	0	4	0	5.6375346	-5.03201	0
76-7-21	for the logarithmic degree regime , this is further enhanced into a fully agnostic algorithm that achieves the ch-limit for exact recovery in quasilinear time .	the stochastic block model ( sbm ) has recently gathered significant attention due to new threshold phenomena .	0	5	0	5.5988426	-4.956306	0
76-7-21	these provide the first algorithms affording efficiency , universality and information-theoretic optimality for strong and weak consistency in the sbm .	the stochastic block model ( sbm ) has recently gathered significant attention due to new threshold phenomena .	0	6	0	5.632632	-5.0201783	0
76-7-21	however , most developments rely on the knowledge of the model parameters , or at least on the number of communities .	this paper introduces efficient algorithms that do not require such knowledge and yet achieve the optimal information-theoretic tradeoffs identified in abbe-sandon focs15 .	1	1	2	-6.00117	5.163151	1
76-7-21	in the constant degree regime , an algorithm is developed that requires only a lower-bound on the relative sizes of the communities and achieves the optimal accuracy scaling for large degrees .	however , most developments rely on the knowledge of the model parameters , or at least on the number of communities .	0	3	1	5.593769	-4.9630804	0
76-7-21	however , most developments rely on the knowledge of the model parameters , or at least on the number of communities .	this lower-bound requirement is removed for the regime of arbitrarily slowly diverging degrees , and the model parameters are learned efficiently .	1	1	4	-5.838895	5.1796417	1
76-7-21	however , most developments rely on the knowledge of the model parameters , or at least on the number of communities .	for the logarithmic degree regime , this is further enhanced into a fully agnostic algorithm that achieves the ch-limit for exact recovery in quasilinear time .	1	1	5	-5.954742	5.14954	1
76-7-21	however , most developments rely on the knowledge of the model parameters , or at least on the number of communities .	these provide the first algorithms affording efficiency , universality and information-theoretic optimality for strong and weak consistency in the sbm .	1	1	6	-5.9606285	5.101793	1
76-7-21	in the constant degree regime , an algorithm is developed that requires only a lower-bound on the relative sizes of the communities and achieves the optimal accuracy scaling for large degrees .	this paper introduces efficient algorithms that do not require such knowledge and yet achieve the optimal information-theoretic tradeoffs identified in abbe-sandon focs15 .	0	3	2	4.7389345	-4.2851877	0
76-7-21	this lower-bound requirement is removed for the regime of arbitrarily slowly diverging degrees , and the model parameters are learned efficiently .	this paper introduces efficient algorithms that do not require such knowledge and yet achieve the optimal information-theoretic tradeoffs identified in abbe-sandon focs15 .	0	4	2	2.6763844	-2.507878	0
76-7-21	for the logarithmic degree regime , this is further enhanced into a fully agnostic algorithm that achieves the ch-limit for exact recovery in quasilinear time .	this paper introduces efficient algorithms that do not require such knowledge and yet achieve the optimal information-theoretic tradeoffs identified in abbe-sandon focs15 .	0	5	2	4.4097176	-3.9895527	0
76-7-21	this paper introduces efficient algorithms that do not require such knowledge and yet achieve the optimal information-theoretic tradeoffs identified in abbe-sandon focs15 .	these provide the first algorithms affording efficiency , universality and information-theoretic optimality for strong and weak consistency in the sbm .	1	2	6	-5.390772	5.014967	1
76-7-21	in the constant degree regime , an algorithm is developed that requires only a lower-bound on the relative sizes of the communities and achieves the optimal accuracy scaling for large degrees .	this lower-bound requirement is removed for the regime of arbitrarily slowly diverging degrees , and the model parameters are learned efficiently .	1	3	4	2.481425	-2.356721	0
76-7-21	in the constant degree regime , an algorithm is developed that requires only a lower-bound on the relative sizes of the communities and achieves the optimal accuracy scaling for large degrees .	for the logarithmic degree regime , this is further enhanced into a fully agnostic algorithm that achieves the ch-limit for exact recovery in quasilinear time .	1	3	5	-2.5806427	2.613047	1
76-7-21	in the constant degree regime , an algorithm is developed that requires only a lower-bound on the relative sizes of the communities and achieves the optimal accuracy scaling for large degrees .	these provide the first algorithms affording efficiency , universality and information-theoretic optimality for strong and weak consistency in the sbm .	1	3	6	3.1391	-2.9559226	0
76-7-21	for the logarithmic degree regime , this is further enhanced into a fully agnostic algorithm that achieves the ch-limit for exact recovery in quasilinear time .	this lower-bound requirement is removed for the regime of arbitrarily slowly diverging degrees , and the model parameters are learned efficiently .	0	5	4	3.586363	-3.3917663	0
76-7-21	these provide the first algorithms affording efficiency , universality and information-theoretic optimality for strong and weak consistency in the sbm .	this lower-bound requirement is removed for the regime of arbitrarily slowly diverging degrees , and the model parameters are learned efficiently .	0	6	4	2.6594527	-2.5376983	0
76-7-21	these provide the first algorithms affording efficiency , universality and information-theoretic optimality for strong and weak consistency in the sbm .	for the logarithmic degree regime , this is further enhanced into a fully agnostic algorithm that achieves the ch-limit for exact recovery in quasilinear time .	0	6	5	-0.35863942	0.54656184	1
77-11-55	a new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes ( local workers ) , is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server ( master ) .	we study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints .	0	1	0	5.4679384	-4.933698	0
77-11-55	we study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints .	the algorithm enables the local workers to perform more exploration , i.e .	1	0	2	-5.929366	5.1577153	1
77-11-55	we study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints .	the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master .	1	0	3	-5.8583326	5.1920843	1
77-11-55	we study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints .	we empirically demonstrate that in the deep learning setting , due to the existence of many local optima , allowing more exploration can lead to the improved performance .	1	0	4	-5.922504	5.15832	1
77-11-55	we study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints .	we propose synchronous and asynchronous variants of the new algorithm .	1	0	5	-5.9369955	5.1588817	1
77-11-55	we provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method admm .	we study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints .	0	6	0	5.561848	-4.9893284	0
77-11-55	we study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints .	we show that the stability of easgd is guaranteed when a simple stability condition is satisfied , which is not the case for admm .	1	0	7	-5.8923945	5.1816425	1
77-11-55	we study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints .	we additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings .	1	0	8	-5.90584	5.1566706	1
77-11-55	we study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints .	asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the cifar and imagenet datasets .	1	0	9	-5.9181113	5.165203	1
77-11-55	experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to downpour and other common baseline approaches and furthermore is very communication efficient .	we study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints .	0	10	0	5.5850067	-5.0211134	0
77-11-55	the algorithm enables the local workers to perform more exploration , i.e .	a new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes ( local workers ) , is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server ( master ) .	0	2	1	5.6078215	-5.0111933	0
77-11-55	the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master .	a new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes ( local workers ) , is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server ( master ) .	0	3	1	5.654421	-5.063266	0
77-11-55	we empirically demonstrate that in the deep learning setting , due to the existence of many local optima , allowing more exploration can lead to the improved performance .	a new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes ( local workers ) , is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server ( master ) .	0	4	1	5.0648847	-4.437078	0
77-11-55	a new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes ( local workers ) , is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server ( master ) .	we propose synchronous and asynchronous variants of the new algorithm .	1	1	5	-5.886593	5.223381	1
77-11-55	a new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes ( local workers ) , is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server ( master ) .	we provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method admm .	1	1	6	-5.8692875	5.225986	1
77-11-55	we show that the stability of easgd is guaranteed when a simple stability condition is satisfied , which is not the case for admm .	a new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes ( local workers ) , is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server ( master ) .	0	7	1	3.6398642	-3.317128	0
77-11-55	a new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes ( local workers ) , is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server ( master ) .	we additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings .	1	1	8	-5.959189	5.198477	1
77-11-55	a new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes ( local workers ) , is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server ( master ) .	asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the cifar and imagenet datasets .	1	1	9	-5.9172163	5.2577214	1
77-11-55	experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to downpour and other common baseline approaches and furthermore is very communication efficient .	a new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes ( local workers ) , is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server ( master ) .	0	10	1	5.4778867	-4.854394	0
77-11-55	the algorithm enables the local workers to perform more exploration , i.e .	the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master .	1	2	3	2.0531178	-1.905062	0
77-11-55	we empirically demonstrate that in the deep learning setting , due to the existence of many local optima , allowing more exploration can lead to the improved performance .	the algorithm enables the local workers to perform more exploration , i.e .	0	4	2	4.5725164	-4.130021	0
77-11-55	we propose synchronous and asynchronous variants of the new algorithm .	the algorithm enables the local workers to perform more exploration , i.e .	0	5	2	-0.8047389	0.921813	1
77-11-55	the algorithm enables the local workers to perform more exploration , i.e .	we provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method admm .	1	2	6	-4.5807896	4.3026066	1
77-11-55	we show that the stability of easgd is guaranteed when a simple stability condition is satisfied , which is not the case for admm .	the algorithm enables the local workers to perform more exploration , i.e .	0	7	2	-2.704799	2.7708545	1
77-11-55	the algorithm enables the local workers to perform more exploration , i.e .	we additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings .	1	2	8	-4.3329444	4.0846977	1
77-11-55	asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the cifar and imagenet datasets .	the algorithm enables the local workers to perform more exploration , i.e .	0	9	2	3.9572759	-3.692556	0
77-11-55	experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to downpour and other common baseline approaches and furthermore is very communication efficient .	the algorithm enables the local workers to perform more exploration , i.e .	0	10	2	4.40088	-4.058042	0
77-11-55	we empirically demonstrate that in the deep learning setting , due to the existence of many local optima , allowing more exploration can lead to the improved performance .	the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master .	0	4	3	4.539192	-4.1485786	0
77-11-55	we propose synchronous and asynchronous variants of the new algorithm .	the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master .	0	5	3	1.2035073	-1.0704318	0
77-11-55	the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master .	we provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method admm .	1	3	6	-5.464113	4.953746	1
77-11-55	the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master .	we show that the stability of easgd is guaranteed when a simple stability condition is satisfied , which is not the case for admm .	1	3	7	1.7922829	-1.5892336	0
77-11-55	the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master .	we additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings .	1	3	8	-5.61024	4.9973125	1
77-11-55	asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the cifar and imagenet datasets .	the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master .	0	9	3	4.908019	-4.4109745	0
77-11-55	the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master .	experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to downpour and other common baseline approaches and furthermore is very communication efficient .	1	3	10	-5.7017365	5.107744	1
77-11-55	we propose synchronous and asynchronous variants of the new algorithm .	we empirically demonstrate that in the deep learning setting , due to the existence of many local optima , allowing more exploration can lead to the improved performance .	0	5	4	-5.568825	4.9669905	1
77-11-55	we empirically demonstrate that in the deep learning setting , due to the existence of many local optima , allowing more exploration can lead to the improved performance .	we provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method admm .	1	4	6	2.3068283	-2.0966585	0
77-11-55	we empirically demonstrate that in the deep learning setting , due to the existence of many local optima , allowing more exploration can lead to the improved performance .	we show that the stability of easgd is guaranteed when a simple stability condition is satisfied , which is not the case for admm .	1	4	7	4.5110426	-4.127967	0
77-11-55	we empirically demonstrate that in the deep learning setting , due to the existence of many local optima , allowing more exploration can lead to the improved performance .	we additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings .	1	4	8	-2.0520275	2.2053456	1
77-11-55	asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the cifar and imagenet datasets .	we empirically demonstrate that in the deep learning setting , due to the existence of many local optima , allowing more exploration can lead to the improved performance .	0	9	4	0.18135315	0.18793112	1
77-11-55	experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to downpour and other common baseline approaches and furthermore is very communication efficient .	we empirically demonstrate that in the deep learning setting , due to the existence of many local optima , allowing more exploration can lead to the improved performance .	0	10	4	2.755603	-2.3889375	0
77-11-55	we provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method admm .	we propose synchronous and asynchronous variants of the new algorithm .	0	6	5	5.3892174	-4.718773	0
77-11-55	we propose synchronous and asynchronous variants of the new algorithm .	we show that the stability of easgd is guaranteed when a simple stability condition is satisfied , which is not the case for admm .	1	5	7	1.99578	-1.8658977	0
77-11-55	we additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings .	we propose synchronous and asynchronous variants of the new algorithm .	0	8	5	4.8289375	-4.276128	0
77-11-55	we propose synchronous and asynchronous variants of the new algorithm .	asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the cifar and imagenet datasets .	1	5	9	-5.902728	5.081809	1
77-11-55	experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to downpour and other common baseline approaches and furthermore is very communication efficient .	we propose synchronous and asynchronous variants of the new algorithm .	0	10	5	4.9895964	-4.520178	0
77-11-55	we show that the stability of easgd is guaranteed when a simple stability condition is satisfied , which is not the case for admm .	we provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method admm .	0	7	6	-4.688603	4.3761263	1
77-11-55	we additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings .	we provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method admm .	0	8	6	2.1321342	-2.0029356	0
77-11-55	asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the cifar and imagenet datasets .	we provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method admm .	0	9	6	-2.09241	2.1660714	1
77-11-55	we provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method admm .	experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to downpour and other common baseline approaches and furthermore is very communication efficient .	1	6	10	-4.3412666	4.046547	1
77-11-55	we show that the stability of easgd is guaranteed when a simple stability condition is satisfied , which is not the case for admm .	we additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings .	1	7	8	-5.88801	5.107602	1
77-11-55	asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the cifar and imagenet datasets .	we show that the stability of easgd is guaranteed when a simple stability condition is satisfied , which is not the case for admm .	0	9	7	5.0896473	-4.534061	0
77-11-55	we show that the stability of easgd is guaranteed when a simple stability condition is satisfied , which is not the case for admm .	experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to downpour and other common baseline approaches and furthermore is very communication efficient .	1	7	10	-5.742917	5.130629	1
77-11-55	asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the cifar and imagenet datasets .	we additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings .	0	9	8	2.3603952	-2.1332273	0
77-11-55	we additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings .	experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to downpour and other common baseline approaches and furthermore is very communication efficient .	1	8	10	-3.0737462	2.9087744	1
77-11-55	experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to downpour and other common baseline approaches and furthermore is very communication efficient .	asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the cifar and imagenet datasets .	0	10	9	1.8419225	-1.6228628	0
78-5-10	a k-submodular function is a generalization of a submodular function , where the input consists of k disjoint subsets , instead of a single subset , of the domain .	many machine learning problems , including influence maximization with k kinds of topics and sensor placement with k kinds of sensors , can be naturally modeled as the problem of maximizing monotone k-submodular functions .	1	0	1	1.1638718	-0.6893329	0
78-5-10	in this paper , we give constant-factor approximation algorithms for maximizing monotone ksubmodular functions subject to several size constraints .	a k-submodular function is a generalization of a submodular function , where the input consists of k disjoint subsets , instead of a single subset , of the domain .	0	2	0	5.488116	-4.8371477	0
78-5-10	the running time of our algorithms are almost linear in the domain size .	a k-submodular function is a generalization of a submodular function , where the input consists of k disjoint subsets , instead of a single subset , of the domain .	0	3	0	5.601349	-4.9757385	0
78-5-10	we experimentally demonstrate that our algorithms outperform baseline algorithms in terms of the solution quality .	a k-submodular function is a generalization of a submodular function , where the input consists of k disjoint subsets , instead of a single subset , of the domain .	0	4	0	5.696747	-5.111394	0
78-5-10	many machine learning problems , including influence maximization with k kinds of topics and sensor placement with k kinds of sensors , can be naturally modeled as the problem of maximizing monotone k-submodular functions .	in this paper , we give constant-factor approximation algorithms for maximizing monotone ksubmodular functions subject to several size constraints .	1	1	2	-5.886818	5.163289	1
78-5-10	the running time of our algorithms are almost linear in the domain size .	many machine learning problems , including influence maximization with k kinds of topics and sensor placement with k kinds of sensors , can be naturally modeled as the problem of maximizing monotone k-submodular functions .	0	3	1	5.6427727	-5.0783772	0
78-5-10	we experimentally demonstrate that our algorithms outperform baseline algorithms in terms of the solution quality .	many machine learning problems , including influence maximization with k kinds of topics and sensor placement with k kinds of sensors , can be naturally modeled as the problem of maximizing monotone k-submodular functions .	0	4	1	5.7089024	-5.152112	0
78-5-10	in this paper , we give constant-factor approximation algorithms for maximizing monotone ksubmodular functions subject to several size constraints .	the running time of our algorithms are almost linear in the domain size .	1	2	3	-5.9584413	5.193399	1
78-5-10	we experimentally demonstrate that our algorithms outperform baseline algorithms in terms of the solution quality .	in this paper , we give constant-factor approximation algorithms for maximizing monotone ksubmodular functions subject to several size constraints .	0	4	2	5.5690517	-4.9564495	0
78-5-10	we experimentally demonstrate that our algorithms outperform baseline algorithms in terms of the solution quality .	the running time of our algorithms are almost linear in the domain size .	0	4	3	3.1937065	-3.049317	0
79-5-10	this work addresses active learning with labels obtained from strong and weak labelers , where in addition to the standard active learning setting , we have an extra weak labeler which may occasionally provide incorrect labels .	[CLS] an active learner is given a hypothesis class, a large set of unlabeled examples and the ability to interactively query labels to an oracle of a subset of these examples ; the goal of the learner is to learn a hypothesis in the class that fits the data well by making	0	1	0	-0.46571952	0.6659229	1
79-5-10	[CLS] an active learner is given a hypothesis class, a large set of unlabeled examples and the ability to interactively query labels to an oracle of a subset of these examples ; the goal of the learner is to learn a hypothesis in the class that	an example is learning to classify medical images where either expensive labels may be obtained from a physician ( oracle or strong labeler ) , or cheaper but occasionally incorrect labels may be obtained from a medical resident ( weak labeler ) .	1	0	2	4.5290394	-3.9873881	0
79-5-10	[CLS] an active learner is given a hypothesis class, a large set of unlabeled examples and the ability to interactively query labels to an oracle of a subset of these examples ; the goal of the learner is to learn a hypothesis in the class that fits the data well by making as	our goal is to learn a classifier with low error on data labeled by the oracle , while using the weak labeler to reduce the number of label queries made to this labeler .	1	0	3	-5.6194296	5.116192	1
79-5-10	[CLS] an active learner is given a hypothesis class, a large set of unlabeled examples and the ability to interactively query labels to an oracle of a subset of these examples ; the goal of the learner is to learn a hypothesis in the class that fits the data well by making as few	we provide an active learning algorithm for this setting , establish its statistical consistency , and analyze its label complexity to characterize when it can provide label savings over using the strong labeler alone .	1	0	4	-5.798666	5.154663	1
79-5-10	this work addresses active learning with labels obtained from strong and weak labelers , where in addition to the standard active learning setting , we have an extra weak labeler which may occasionally provide incorrect labels .	an example is learning to classify medical images where either expensive labels may be obtained from a physician ( oracle or strong labeler ) , or cheaper but occasionally incorrect labels may be obtained from a medical resident ( weak labeler ) .	1	1	2	4.7145314	-4.179494	0
79-5-10	our goal is to learn a classifier with low error on data labeled by the oracle , while using the weak labeler to reduce the number of label queries made to this labeler .	this work addresses active learning with labels obtained from strong and weak labelers , where in addition to the standard active learning setting , we have an extra weak labeler which may occasionally provide incorrect labels .	0	3	1	2.5164442	-2.2998068	0
79-5-10	this work addresses active learning with labels obtained from strong and weak labelers , where in addition to the standard active learning setting , we have an extra weak labeler which may occasionally provide incorrect labels .	we provide an active learning algorithm for this setting , establish its statistical consistency , and analyze its label complexity to characterize when it can provide label savings over using the strong labeler alone .	1	1	4	-5.7546487	5.1502743	1
79-5-10	an example is learning to classify medical images where either expensive labels may be obtained from a physician ( oracle or strong labeler ) , or cheaper but occasionally incorrect labels may be obtained from a medical resident ( weak labeler ) .	our goal is to learn a classifier with low error on data labeled by the oracle , while using the weak labeler to reduce the number of label queries made to this labeler .	1	2	3	-5.8678484	5.251179	1
79-5-10	we provide an active learning algorithm for this setting , establish its statistical consistency , and analyze its label complexity to characterize when it can provide label savings over using the strong labeler alone .	an example is learning to classify medical images where either expensive labels may be obtained from a physician ( oracle or strong labeler ) , or cheaper but occasionally incorrect labels may be obtained from a medical resident ( weak labeler ) .	0	4	2	5.3936787	-4.7313614	0
79-5-10	our goal is to learn a classifier with low error on data labeled by the oracle , while using the weak labeler to reduce the number of label queries made to this labeler .	we provide an active learning algorithm for this setting , establish its statistical consistency , and analyze its label complexity to characterize when it can provide label savings over using the strong labeler alone .	1	3	4	-5.3875813	4.8619947	1
80-6-15	since its performance is sensitive to the choice of label order , the key issue is how to determine the optimal label order for cc .	to capture the interdependencies between labels in multi-label classification problems , classifier chain ( cc ) tries to take the multiple labels of each instance into account under a deterministic high-order markov chain model .	0	1	0	5.622845	-5.019163	0
80-6-15	to capture the interdependencies between labels in multi-label classification problems , classifier chain ( cc ) tries to take the multiple labels of each instance into account under a deterministic high-order markov chain model .	in this work , we first generalize the cc model over a random label order .	1	0	2	-5.9606514	5.1542883	1
80-6-15	then , we present a theoretical analysis of the generalization error for the proposed generalized model .	to capture the interdependencies between labels in multi-label classification problems , classifier chain ( cc ) tries to take the multiple labels of each instance into account under a deterministic high-order markov chain model .	0	3	0	5.7362	-5.113098	0
80-6-15	based on our results , we propose a dynamic programming based classifier chain ( cc-dp ) algorithm to search the globally optimal label order for cc and a greedy classifier chain ( cc-greedy ) algorithm to find a locally optimal cc .	to capture the interdependencies between labels in multi-label classification problems , classifier chain ( cc ) tries to take the multiple labels of each instance into account under a deterministic high-order markov chain model .	0	4	0	5.6682186	-5.0623093	0
80-6-15	[CLS] comprehensive experiments on a number of real - world multi - label data sets from various domains demonstrate that our proposed cc - dp algorithm outperforms state - of - the - art approaches and the ccgreedy algorithm achieves comparable prediction performance with cc	to capture the interdependencies between labels in multi-label classification problems , classifier chain ( cc ) tries to take the multiple labels of each instance into account under a deterministic high-order markov chain model .	0	5	0	5.701234	-5.1012626	0
80-6-15	since its performance is sensitive to the choice of label order , the key issue is how to determine the optimal label order for cc .	in this work , we first generalize the cc model over a random label order .	1	1	2	-4.3551717	4.176507	1
80-6-15	then , we present a theoretical analysis of the generalization error for the proposed generalized model .	since its performance is sensitive to the choice of label order , the key issue is how to determine the optimal label order for cc .	0	3	1	5.1046133	-4.4586134	0
80-6-15	since its performance is sensitive to the choice of label order , the key issue is how to determine the optimal label order for cc .	based on our results , we propose a dynamic programming based classifier chain ( cc-dp ) algorithm to search the globally optimal label order for cc and a greedy classifier chain ( cc-greedy ) algorithm to find a locally optimal cc .	1	1	4	-5.9347425	5.0413	1
80-6-15	comprehensive experiments on a number of real-world multi-label data sets from various domains demonstrate that our proposed cc-dp algorithm outperforms state-of-the-art approaches and the ccgreedy algorithm achieves comparable prediction performance with cc-dp .	since its performance is sensitive to the choice of label order , the key issue is how to determine the optimal label order for cc .	0	5	1	5.553782	-4.9077196	0
80-6-15	in this work , we first generalize the cc model over a random label order .	then , we present a theoretical analysis of the generalization error for the proposed generalized model .	1	2	3	-5.9841557	5.19938	1
80-6-15	based on our results , we propose a dynamic programming based classifier chain ( cc-dp ) algorithm to search the globally optimal label order for cc and a greedy classifier chain ( cc-greedy ) algorithm to find a locally optimal cc .	in this work , we first generalize the cc model over a random label order .	0	4	2	5.4963737	-4.847947	0
80-6-15	in this work , we first generalize the cc model over a random label order .	comprehensive experiments on a number of real-world multi-label data sets from various domains demonstrate that our proposed cc-dp algorithm outperforms state-of-the-art approaches and the ccgreedy algorithm achieves comparable prediction performance with cc-dp .	1	2	5	-5.9832163	5.1614637	1
80-6-15	based on our results , we propose a dynamic programming based classifier chain ( cc-dp ) algorithm to search the globally optimal label order for cc and a greedy classifier chain ( cc-greedy ) algorithm to find a locally optimal cc .	then , we present a theoretical analysis of the generalization error for the proposed generalized model .	0	4	3	2.3824766	-2.163711	0
80-6-15	comprehensive experiments on a number of real-world multi-label data sets from various domains demonstrate that our proposed cc-dp algorithm outperforms state-of-the-art approaches and the ccgreedy algorithm achieves comparable prediction performance with cc-dp .	then , we present a theoretical analysis of the generalization error for the proposed generalized model .	0	5	3	5.138382	-4.5197945	0
80-6-15	[CLS] based on our results, we propose a dynamic programming based classifier chain ( cc - dp ) algorithm to search the globally optimal label order for cc and a greedy classifier chain ( cc - greedy ) algorithm to find a locally optimal cc.	[CLS] comprehensive experiments on a number of real - world multi - label data sets from various domains demonstrate that our proposed cc - dp algorithm outperforms state - of - the - art approaches and the ccgreedy algorithm achieves comparable prediction performance with	1	4	5	-5.808904	5.0340395	1
81-12-66	more specifically , for a data matrix x rpxn and an underlying model w , the response vector is generated as y = x t w + b where b rn is the corruption vector supported over at most c * n coordinates .	we study the problem of robust least squares regression ( rlsr ) where several response variables can be adversarially corrupted .	0	1	0	5.493421	-4.9013815	0
81-12-66	we study the problem of robust least squares regression ( rlsr ) where several response variables can be adversarially corrupted .	existing exact recovery results for rlsr focus solely on l1 penalty based convex formulations and impose relatively strict model assumptions such as requiring the corruptions b to be selected independently of x .	1	0	2	-5.775183	5.161134	1
81-12-66	in this work , we study a simple hard-thresholding algorithm called t orrent which , under mild conditions on x , can recover w exactly even if b corrupts the response variables in an adversarial manner , i.e .	we study the problem of robust least squares regression ( rlsr ) where several response variables can be adversarially corrupted .	0	3	0	5.5695896	-4.9828963	0
81-12-66	we study the problem of robust least squares regression ( rlsr ) where several response variables can be adversarially corrupted .	both the support and entries of b are selected adversarially after observing x and w .	1	0	4	-5.8281264	5.161519	1
81-12-66	our results hold under deterministic assumptions which are satisfied if x is sampled from any sub-gaussian distribution .	we study the problem of robust least squares regression ( rlsr ) where several response variables can be adversarially corrupted .	0	5	0	5.4805884	-4.938463	0
81-12-66	finally unlike existing results that apply only to a fixed w , generated independently of x , our results are universal and hold for any w rp .	we study the problem of robust least squares regression ( rlsr ) where several response variables can be adversarially corrupted .	0	6	0	5.4684415	-4.9059343	0
81-12-66	we study the problem of robust least squares regression ( rlsr ) where several response variables can be adversarially corrupted .	next , we propose gradient descent-based extensions of t orrent that can scale efficiently to large scale problems , such as high dimensional sparse recovery .	1	0	7	-5.880047	5.2089777	1
81-12-66	we study the problem of robust least squares regression ( rlsr ) where several response variables can be adversarially corrupted .	and prove similar recovery guarantees for these extensions .	1	0	8	-5.9204397	5.205075	1
81-12-66	empirically we find t or rent , and more so its extensions , offering significantly faster recovery than the state-of-the-art l1 solvers .	we study the problem of robust least squares regression ( rlsr ) where several response variables can be adversarially corrupted .	0	9	0	5.6075406	-5.001987	0
81-12-66	we study the problem of robust least squares regression ( rlsr ) where several response variables can be adversarially corrupted .	for instance , even on moderate-sized datasets ( with p = 50k ) with around 40 % corrupted responses , a variant of our proposed method called t orrent-hyb is more than 20x faster than the best l1 solver .	1	0	10	-5.872591	5.1829653	1
81-12-66	[CLS] ` ` if among these errors are some which appear too large to be admissible, then those equations which produced these errors will be rejected, as coming from too faulty experiments, and the unknowns will be determined by means of the other equations, which will then give much smaller errors.'' a. m. legendre, on the method of least	we study the problem of robust least squares regression ( rlsr ) where several response variables can be adversarially corrupted .	0	11	0	5.410635	-4.856454	0
81-12-66	more specifically , for a data matrix x rpxn and an underlying model w , the response vector is generated as y = x t w + b where b rn is the corruption vector supported over at most c * n coordinates .	existing exact recovery results for rlsr focus solely on l1 penalty based convex formulations and impose relatively strict model assumptions such as requiring the corruptions b to be selected independently of x .	1	1	2	-0.093250796	0.44840035	1
81-12-66	in this work , we study a simple hard-thresholding algorithm called t orrent which , under mild conditions on x , can recover w exactly even if b corrupts the response variables in an adversarial manner , i.e .	more specifically , for a data matrix x rpxn and an underlying model w , the response vector is generated as y = x t w + b where b rn is the corruption vector supported over at most c * n coordinates .	0	3	1	1.5006938	-1.2570149	0
81-12-66	both the support and entries of b are selected adversarially after observing x and w .	more specifically , for a data matrix x rpxn and an underlying model w , the response vector is generated as y = x t w + b where b rn is the corruption vector supported over at most c * n coordinates .	0	4	1	4.0757217	-3.734322	0
81-12-66	our results hold under deterministic assumptions which are satisfied if x is sampled from any sub-gaussian distribution .	more specifically , for a data matrix x rpxn and an underlying model w , the response vector is generated as y = x t w + b where b rn is the corruption vector supported over at most c * n coordinates .	0	5	1	4.723736	-4.231776	0
81-12-66	more specifically , for a data matrix x rpxn and an underlying model w , the response vector is generated as y = x t w + b where b rn is the corruption vector supported over at most c * n coordinates .	finally unlike existing results that apply only to a fixed w , generated independently of x , our results are universal and hold for any w rp .	1	1	6	-5.9721303	5.1582813	1
81-12-66	next , we propose gradient descent-based extensions of t orrent that can scale efficiently to large scale problems , such as high dimensional sparse recovery .	more specifically , for a data matrix x rpxn and an underlying model w , the response vector is generated as y = x t w + b where b rn is the corruption vector supported over at most c * n coordinates .	0	7	1	4.9681396	-4.4229994	0
81-12-66	more specifically , for a data matrix x rpxn and an underlying model w , the response vector is generated as y = x t w + b where b rn is the corruption vector supported over at most c * n coordinates .	and prove similar recovery guarantees for these extensions .	1	1	8	-5.027448	4.6334305	1
81-12-66	empirically we find t or rent , and more so its extensions , offering significantly faster recovery than the state-of-the-art l1 solvers .	more specifically , for a data matrix x rpxn and an underlying model w , the response vector is generated as y = x t w + b where b rn is the corruption vector supported over at most c * n coordinates .	0	9	1	5.359716	-4.783953	0
81-12-66	more specifically , for a data matrix x rpxn and an underlying model w , the response vector is generated as y = x t w + b where b rn is the corruption vector supported over at most c * n coordinates .	[CLS] for instance, even on moderate - sized datasets ( with p = 50k ) with around 40 % corrupted responses, a variant of our proposed method called t orrent - hyb is more than 20x faster than the best l1	1	1	10	-5.893831	5.00064	1
81-12-66	more specifically , for a data matrix x rpxn and an underlying model w , the response vector is generated as y = x t w + b where b rn is the corruption vector supported over at most c * n coordinates .	[CLS] ` ` if among these errors are some which appear too large to be admissible, then those equations which produced these errors will be rejected, as coming from too faulty experiments, and the unknowns will be determined by means of the other equations, which	1	1	11	-0.6068336	0.8808859	1
81-12-66	in this work , we study a simple hard-thresholding algorithm called t orrent which , under mild conditions on x , can recover w exactly even if b corrupts the response variables in an adversarial manner , i.e .	existing exact recovery results for rlsr focus solely on l1 penalty based convex formulations and impose relatively strict model assumptions such as requiring the corruptions b to be selected independently of x .	0	3	2	2.9020567	-2.7657542	0
81-12-66	existing exact recovery results for rlsr focus solely on l1 penalty based convex formulations and impose relatively strict model assumptions such as requiring the corruptions b to be selected independently of x .	both the support and entries of b are selected adversarially after observing x and w .	1	2	4	-5.1462874	4.8014336	1
81-12-66	our results hold under deterministic assumptions which are satisfied if x is sampled from any sub-gaussian distribution .	existing exact recovery results for rlsr focus solely on l1 penalty based convex formulations and impose relatively strict model assumptions such as requiring the corruptions b to be selected independently of x .	0	5	2	5.4058514	-4.8006597	0
81-12-66	existing exact recovery results for rlsr focus solely on l1 penalty based convex formulations and impose relatively strict model assumptions such as requiring the corruptions b to be selected independently of x .	finally unlike existing results that apply only to a fixed w , generated independently of x , our results are universal and hold for any w rp .	1	2	6	-5.995636	5.096337	1
81-12-66	next , we propose gradient descent-based extensions of t orrent that can scale efficiently to large scale problems , such as high dimensional sparse recovery .	existing exact recovery results for rlsr focus solely on l1 penalty based convex formulations and impose relatively strict model assumptions such as requiring the corruptions b to be selected independently of x .	0	7	2	5.3502073	-4.714452	0
81-12-66	existing exact recovery results for rlsr focus solely on l1 penalty based convex formulations and impose relatively strict model assumptions such as requiring the corruptions b to be selected independently of x .	and prove similar recovery guarantees for these extensions .	1	2	8	-5.807214	5.134771	1
81-12-66	existing exact recovery results for rlsr focus solely on l1 penalty based convex formulations and impose relatively strict model assumptions such as requiring the corruptions b to be selected independently of x .	empirically we find t or rent , and more so its extensions , offering significantly faster recovery than the state-of-the-art l1 solvers .	1	2	9	-5.948146	5.0155883	1
81-12-66	for instance , even on moderate-sized datasets ( with p = 50k ) with around 40 % corrupted responses , a variant of our proposed method called t orrent-hyb is more than 20x faster than the best l1 solver .	existing exact recovery results for rlsr focus solely on l1 penalty based convex formulations and impose relatively strict model assumptions such as requiring the corruptions b to be selected independently of x .	0	10	2	5.419841	-4.730532	0
81-12-66	existing exact recovery results for rlsr focus solely on l1 penalty based convex formulations and impose relatively strict model assumptions such as requiring the corruptions b to be selected independently of x .	[CLS] ` ` if among these errors are some which appear too large to be admissible, then those equations which produced these errors will be rejected, as coming from too faulty experiments, and the unknowns will be determined by means of the other equations, which will then give much smaller errors.''	1	2	11	-4.37549	4.142447	1
81-12-66	both the support and entries of b are selected adversarially after observing x and w .	in this work , we study a simple hard-thresholding algorithm called t orrent which , under mild conditions on x , can recover w exactly even if b corrupts the response variables in an adversarial manner , i.e .	0	4	3	2.8794088	-2.7057939	0
81-12-66	our results hold under deterministic assumptions which are satisfied if x is sampled from any sub-gaussian distribution .	in this work , we study a simple hard-thresholding algorithm called t orrent which , under mild conditions on x , can recover w exactly even if b corrupts the response variables in an adversarial manner , i.e .	0	5	3	5.365282	-4.7695713	0
81-12-66	in this work , we study a simple hard-thresholding algorithm called t orrent which , under mild conditions on x , can recover w exactly even if b corrupts the response variables in an adversarial manner , i.e .	finally unlike existing results that apply only to a fixed w , generated independently of x , our results are universal and hold for any w rp .	1	3	6	-5.8366423	5.2307224	1
81-12-66	in this work , we study a simple hard-thresholding algorithm called t orrent which , under mild conditions on x , can recover w exactly even if b corrupts the response variables in an adversarial manner , i.e .	next , we propose gradient descent-based extensions of t orrent that can scale efficiently to large scale problems , such as high dimensional sparse recovery .	1	3	7	-6.0053997	5.1782227	1
81-12-66	and prove similar recovery guarantees for these extensions .	in this work , we study a simple hard-thresholding algorithm called t orrent which , under mild conditions on x , can recover w exactly even if b corrupts the response variables in an adversarial manner , i.e .	0	8	3	4.1735067	-3.7429438	0
81-12-66	empirically we find t or rent , and more so its extensions , offering significantly faster recovery than the state-of-the-art l1 solvers .	in this work , we study a simple hard-thresholding algorithm called t orrent which , under mild conditions on x , can recover w exactly even if b corrupts the response variables in an adversarial manner , i.e .	0	9	3	5.48493	-4.8062553	0
81-12-66	[CLS] in this work, we study a simple hard - thresholding algorithm called t orrent which, under mild conditions on x, can recover w exactly even if b corrupts the response variables in an adversarial manner, i. e.	[CLS] for instance, even on moderate - sized datasets ( with p = 50k ) with around 40 % corrupted responses, a variant of our proposed method called t orrent - hyb is more than 20x faster than the best l	1	3	10	-5.9856105	5.185093	1
81-12-66	[CLS] in this work, we study a simple hard - thresholding algorithm called t orrent which, under mild conditions on x, can recover w exactly even if b corrupts the response variables in an adversarial manner, i. e.	[CLS] ` ` if among these errors are some which appear too large to be admissible, then those equations which produced these errors will be rejected, as coming from too faulty experiments, and the unknowns will be determined by means of the other equations,	1	3	11	-1.6181493	1.6654147	1
81-12-66	our results hold under deterministic assumptions which are satisfied if x is sampled from any sub-gaussian distribution .	both the support and entries of b are selected adversarially after observing x and w .	0	5	4	4.4428763	-4.078637	0
81-12-66	finally unlike existing results that apply only to a fixed w , generated independently of x , our results are universal and hold for any w rp .	both the support and entries of b are selected adversarially after observing x and w .	0	6	4	4.5344124	-4.1388526	0
81-12-66	both the support and entries of b are selected adversarially after observing x and w .	next , we propose gradient descent-based extensions of t orrent that can scale efficiently to large scale problems , such as high dimensional sparse recovery .	1	4	7	-5.261237	4.8520136	1
81-12-66	and prove similar recovery guarantees for these extensions .	both the support and entries of b are selected adversarially after observing x and w .	0	8	4	3.8979948	-3.636881	0
81-12-66	both the support and entries of b are selected adversarially after observing x and w .	empirically we find t or rent , and more so its extensions , offering significantly faster recovery than the state-of-the-art l1 solvers .	1	4	9	-5.901253	5.1622696	1
81-12-66	for instance , even on moderate-sized datasets ( with p = 50k ) with around 40 % corrupted responses , a variant of our proposed method called t orrent-hyb is more than 20x faster than the best l1 solver .	both the support and entries of b are selected adversarially after observing x and w .	0	10	4	5.0371714	-4.542108	0
81-12-66	`` if among these errors are some which appear too large to be admissible , then those equations which produced these errors will be rejected , as coming from too faulty experiments , and the unknowns will be determined by means of the other equations , which will then give much smaller errors . '' a. m. legendre , on the method of least squares .	both the support and entries of b are selected adversarially after observing x and w .	0	11	4	-0.2233811	0.41142422	1
81-12-66	finally unlike existing results that apply only to a fixed w , generated independently of x , our results are universal and hold for any w rp .	our results hold under deterministic assumptions which are satisfied if x is sampled from any sub-gaussian distribution .	0	6	5	4.0871105	-3.759428	0
81-12-66	our results hold under deterministic assumptions which are satisfied if x is sampled from any sub-gaussian distribution .	next , we propose gradient descent-based extensions of t orrent that can scale efficiently to large scale problems , such as high dimensional sparse recovery .	1	5	7	-3.4248102	3.4711776	1
81-12-66	and prove similar recovery guarantees for these extensions .	our results hold under deterministic assumptions which are satisfied if x is sampled from any sub-gaussian distribution .	0	8	5	0.43578216	-0.17146629	0
81-12-66	empirically we find t or rent , and more so its extensions , offering significantly faster recovery than the state-of-the-art l1 solvers .	our results hold under deterministic assumptions which are satisfied if x is sampled from any sub-gaussian distribution .	0	9	5	4.4096084	-4.0468974	0
81-12-66	our results hold under deterministic assumptions which are satisfied if x is sampled from any sub-gaussian distribution .	for instance , even on moderate-sized datasets ( with p = 50k ) with around 40 % corrupted responses , a variant of our proposed method called t orrent-hyb is more than 20x faster than the best l1 solver .	1	5	10	-5.8116293	5.1173754	1
81-12-66	`` if among these errors are some which appear too large to be admissible , then those equations which produced these errors will be rejected , as coming from too faulty experiments , and the unknowns will be determined by means of the other equations , which will then give much smaller errors . '' a. m. legendre , on the method of least squares .	our results hold under deterministic assumptions which are satisfied if x is sampled from any sub-gaussian distribution .	0	11	5	-3.7299433	3.5667233	1
81-12-66	next , we propose gradient descent-based extensions of t orrent that can scale efficiently to large scale problems , such as high dimensional sparse recovery .	finally unlike existing results that apply only to a fixed w , generated independently of x , our results are universal and hold for any w rp .	0	7	6	-3.6944985	3.442368	1
81-12-66	and prove similar recovery guarantees for these extensions .	finally unlike existing results that apply only to a fixed w , generated independently of x , our results are universal and hold for any w rp .	0	8	6	-4.52211	4.176073	1
81-12-66	finally unlike existing results that apply only to a fixed w , generated independently of x , our results are universal and hold for any w rp .	empirically we find t or rent , and more so its extensions , offering significantly faster recovery than the state-of-the-art l1 solvers .	1	6	9	-2.2803488	2.367061	1
81-12-66	finally unlike existing results that apply only to a fixed w , generated independently of x , our results are universal and hold for any w rp .	for instance , even on moderate-sized datasets ( with p = 50k ) with around 40 % corrupted responses , a variant of our proposed method called t orrent-hyb is more than 20x faster than the best l1 solver .	1	6	10	-3.3768687	3.233488	1
81-12-66	[CLS] ` ` if among these errors are some which appear too large to be admissible, then those equations which produced these errors will be rejected, as coming from too faulty experiments, and the unknowns will be determined by means of the other equations, which will then give much smaller errors.'' a. m. legendre, on the	finally unlike existing results that apply only to a fixed w , generated independently of x , our results are universal and hold for any w rp .	0	11	6	-5.883153	5.172591	1
81-12-66	next , we propose gradient descent-based extensions of t orrent that can scale efficiently to large scale problems , such as high dimensional sparse recovery .	and prove similar recovery guarantees for these extensions .	1	7	8	-1.2344182	1.4108057	1
81-12-66	next , we propose gradient descent-based extensions of t orrent that can scale efficiently to large scale problems , such as high dimensional sparse recovery .	empirically we find t or rent , and more so its extensions , offering significantly faster recovery than the state-of-the-art l1 solvers .	1	7	9	-4.6645894	4.2875314	1
81-12-66	for instance , even on moderate-sized datasets ( with p = 50k ) with around 40 % corrupted responses , a variant of our proposed method called t orrent-hyb is more than 20x faster than the best l1 solver .	next , we propose gradient descent-based extensions of t orrent that can scale efficiently to large scale problems , such as high dimensional sparse recovery .	0	10	7	2.7024367	-2.5271792	0
81-12-66	next , we propose gradient descent-based extensions of t orrent that can scale efficiently to large scale problems , such as high dimensional sparse recovery .	[CLS] ` ` if among these errors are some which appear too large to be admissible, then those equations which produced these errors will be rejected, as coming from too faulty experiments, and the unknowns will be determined by means of the other equations, which will then give much smaller errors.'' a. m. legendre, on the	1	7	11	4.1640368	-3.8330388	0
81-12-66	and prove similar recovery guarantees for these extensions .	empirically we find t or rent , and more so its extensions , offering significantly faster recovery than the state-of-the-art l1 solvers .	1	8	9	-4.866527	4.490932	1
81-12-66	and prove similar recovery guarantees for these extensions .	for instance , even on moderate-sized datasets ( with p = 50k ) with around 40 % corrupted responses , a variant of our proposed method called t orrent-hyb is more than 20x faster than the best l1 solver .	1	8	10	-5.3795905	4.7964926	1
81-12-66	`` if among these errors are some which appear too large to be admissible , then those equations which produced these errors will be rejected , as coming from too faulty experiments , and the unknowns will be determined by means of the other equations , which will then give much smaller errors . '' a. m. legendre , on the method of least squares .	and prove similar recovery guarantees for these extensions .	0	11	8	-3.4183087	3.3753824	1
81-12-66	for instance , even on moderate-sized datasets ( with p = 50k ) with around 40 % corrupted responses , a variant of our proposed method called t orrent-hyb is more than 20x faster than the best l1 solver .	empirically we find t or rent , and more so its extensions , offering significantly faster recovery than the state-of-the-art l1 solvers .	0	10	9	0.9079195	-0.53474087	0
81-12-66	empirically we find t or rent , and more so its extensions , offering significantly faster recovery than the state-of-the-art l1 solvers .	[CLS] ` ` if among these errors are some which appear too large to be admissible, then those equations which produced these errors will be rejected, as coming from too faulty experiments, and the unknowns will be determined by means of the other equations, which will then give much smaller errors.'' a. m. legendre	1	9	11	4.8338747	-4.4099483	0
81-12-66	[CLS] for instance, even on moderate - sized datasets ( with p = 50k ) with around 40 % corrupted responses, a variant of our proposed method called t orrent - hyb is more than 20x faster than the best l	[CLS] ` ` if among these errors are some which appear too large to be admissible, then those equations which produced these errors will be rejected, as coming from too faulty experiments, and the unknowns will be determined by means of the other equations,	1	10	11	4.881683	-4.2934628	0
82-9-36	embedding based approaches attempt to make training and prediction tractable by assuming that the training label matrix is low-rank and reducing the effective number of labels by projecting the high dimensional label vectors onto a low dimensional linear subspace .	the objective in extreme multi-label learning is to train a classifier that can automatically tag a novel data point with the most relevant subset of labels from an extremely large label set .	0	1	0	5.0781116	-4.454449	0
82-9-36	still , leading embedding approaches have been unable to deliver high prediction accuracies , or scale to large problems as the low rank assumption is violated in most real world applications .	the objective in extreme multi-label learning is to train a classifier that can automatically tag a novel data point with the most relevant subset of labels from an extremely large label set .	0	2	0	5.3759737	-4.8416038	0
82-9-36	the objective in extreme multi-label learning is to train a classifier that can automatically tag a novel data point with the most relevant subset of labels from an extremely large label set .	in this paper we develop the sleec classifier to address both limitations .	1	0	3	-5.9549046	5.2147856	1
82-9-36	the objective in extreme multi-label learning is to train a classifier that can automatically tag a novel data point with the most relevant subset of labels from an extremely large label set .	the main technical contribution in sleec is a formulation for learning a small ensemble of local distance preserving embeddings which can accurately predict infrequently occurring ( tail ) labels .	1	0	4	-5.958749	5.1949415	1
82-9-36	this allows sleec to break free of the traditional low-rank assumption and boost classification accuracy by learning embeddings which preserve pairwise distances between only the nearest label vectors .	the objective in extreme multi-label learning is to train a classifier that can automatically tag a novel data point with the most relevant subset of labels from an extremely large label set .	0	5	0	5.5819073	-5.022052	0
82-9-36	we conducted extensive experiments on several real-world , as well as benchmark data sets and compared our method against state-of-the-art methods for extreme multi-label classification .	the objective in extreme multi-label learning is to train a classifier that can automatically tag a novel data point with the most relevant subset of labels from an extremely large label set .	0	6	0	5.5500727	-4.9242125	0
82-9-36	experiments reveal that sleec can make significantly more accurate predictions then the state-of-the-art methods including both embedding-based ( by as much as 35 % ) as well as tree-based ( by as much as 6 % ) methods .	the objective in extreme multi-label learning is to train a classifier that can automatically tag a novel data point with the most relevant subset of labels from an extremely large label set .	0	7	0	5.5746737	-4.993853	0
82-9-36	the objective in extreme multi-label learning is to train a classifier that can automatically tag a novel data point with the most relevant subset of labels from an extremely large label set .	sleec can also scale efficiently to data sets with a million labels which are beyond the pale of leading embedding methods .	1	0	8	-5.983637	5.189458	1
82-9-36	still , leading embedding approaches have been unable to deliver high prediction accuracies , or scale to large problems as the low rank assumption is violated in most real world applications .	embedding based approaches attempt to make training and prediction tractable by assuming that the training label matrix is low-rank and reducing the effective number of labels by projecting the high dimensional label vectors onto a low dimensional linear subspace .	0	2	1	5.1561117	-4.550719	0
82-9-36	embedding based approaches attempt to make training and prediction tractable by assuming that the training label matrix is low-rank and reducing the effective number of labels by projecting the high dimensional label vectors onto a low dimensional linear subspace .	in this paper we develop the sleec classifier to address both limitations .	1	1	3	-5.759983	5.1578727	1
82-9-36	the main technical contribution in sleec is a formulation for learning a small ensemble of local distance preserving embeddings which can accurately predict infrequently occurring ( tail ) labels .	embedding based approaches attempt to make training and prediction tractable by assuming that the training label matrix is low-rank and reducing the effective number of labels by projecting the high dimensional label vectors onto a low dimensional linear subspace .	0	4	1	4.8572135	-4.2192287	0
82-9-36	this allows sleec to break free of the traditional low-rank assumption and boost classification accuracy by learning embeddings which preserve pairwise distances between only the nearest label vectors .	embedding based approaches attempt to make training and prediction tractable by assuming that the training label matrix is low-rank and reducing the effective number of labels by projecting the high dimensional label vectors onto a low dimensional linear subspace .	0	5	1	5.333637	-4.6905494	0
82-9-36	we conducted extensive experiments on several real-world , as well as benchmark data sets and compared our method against state-of-the-art methods for extreme multi-label classification .	embedding based approaches attempt to make training and prediction tractable by assuming that the training label matrix is low-rank and reducing the effective number of labels by projecting the high dimensional label vectors onto a low dimensional linear subspace .	0	6	1	5.6201715	-4.9810257	0
82-9-36	embedding based approaches attempt to make training and prediction tractable by assuming that the training label matrix is low-rank and reducing the effective number of labels by projecting the high dimensional label vectors onto a low dimensional linear subspace .	[CLS] experiments reveal that sleec can make significantly more accurate predictions then the state - of - the - art methods including both embedding - based ( by as much as 35 % ) as well as tree - based ( by as much as 6 % )	1	1	7	-5.9520607	5.0248895	1
82-9-36	embedding based approaches attempt to make training and prediction tractable by assuming that the training label matrix is low-rank and reducing the effective number of labels by projecting the high dimensional label vectors onto a low dimensional linear subspace .	sleec can also scale efficiently to data sets with a million labels which are beyond the pale of leading embedding methods .	1	1	8	-5.9866242	5.1247525	1
82-9-36	still , leading embedding approaches have been unable to deliver high prediction accuracies , or scale to large problems as the low rank assumption is violated in most real world applications .	in this paper we develop the sleec classifier to address both limitations .	1	2	3	-5.8653984	5.240154	1
82-9-36	still , leading embedding approaches have been unable to deliver high prediction accuracies , or scale to large problems as the low rank assumption is violated in most real world applications .	the main technical contribution in sleec is a formulation for learning a small ensemble of local distance preserving embeddings which can accurately predict infrequently occurring ( tail ) labels .	1	2	4	-5.8846416	5.219405	1
82-9-36	this allows sleec to break free of the traditional low-rank assumption and boost classification accuracy by learning embeddings which preserve pairwise distances between only the nearest label vectors .	still , leading embedding approaches have been unable to deliver high prediction accuracies , or scale to large problems as the low rank assumption is violated in most real world applications .	0	5	2	4.3811364	-3.958963	0
82-9-36	still , leading embedding approaches have been unable to deliver high prediction accuracies , or scale to large problems as the low rank assumption is violated in most real world applications .	we conducted extensive experiments on several real-world , as well as benchmark data sets and compared our method against state-of-the-art methods for extreme multi-label classification .	1	2	6	-5.9499407	5.1715493	1
82-9-36	still , leading embedding approaches have been unable to deliver high prediction accuracies , or scale to large problems as the low rank assumption is violated in most real world applications .	experiments reveal that sleec can make significantly more accurate predictions then the state-of-the-art methods including both embedding-based ( by as much as 35 % ) as well as tree-based ( by as much as 6 % ) methods .	1	2	7	-5.958892	5.197882	1
82-9-36	sleec can also scale efficiently to data sets with a million labels which are beyond the pale of leading embedding methods .	still , leading embedding approaches have been unable to deliver high prediction accuracies , or scale to large problems as the low rank assumption is violated in most real world applications .	0	8	2	5.5654726	-4.919513	0
82-9-36	in this paper we develop the sleec classifier to address both limitations .	the main technical contribution in sleec is a formulation for learning a small ensemble of local distance preserving embeddings which can accurately predict infrequently occurring ( tail ) labels .	1	3	4	-5.707003	5.019678	1
82-9-36	in this paper we develop the sleec classifier to address both limitations .	this allows sleec to break free of the traditional low-rank assumption and boost classification accuracy by learning embeddings which preserve pairwise distances between only the nearest label vectors .	1	3	5	-4.229046	3.995583	1
82-9-36	we conducted extensive experiments on several real-world , as well as benchmark data sets and compared our method against state-of-the-art methods for extreme multi-label classification .	in this paper we develop the sleec classifier to address both limitations .	0	6	3	5.660111	-5.038084	0
82-9-36	experiments reveal that sleec can make significantly more accurate predictions then the state-of-the-art methods including both embedding-based ( by as much as 35 % ) as well as tree-based ( by as much as 6 % ) methods .	in this paper we develop the sleec classifier to address both limitations .	0	7	3	5.6393967	-5.0379615	0
82-9-36	in this paper we develop the sleec classifier to address both limitations .	sleec can also scale efficiently to data sets with a million labels which are beyond the pale of leading embedding methods .	1	3	8	-5.786558	4.9965134	1
82-9-36	this allows sleec to break free of the traditional low-rank assumption and boost classification accuracy by learning embeddings which preserve pairwise distances between only the nearest label vectors .	the main technical contribution in sleec is a formulation for learning a small ensemble of local distance preserving embeddings which can accurately predict infrequently occurring ( tail ) labels .	0	5	4	-4.3441896	4.024507	1
82-9-36	the main technical contribution in sleec is a formulation for learning a small ensemble of local distance preserving embeddings which can accurately predict infrequently occurring ( tail ) labels .	we conducted extensive experiments on several real-world , as well as benchmark data sets and compared our method against state-of-the-art methods for extreme multi-label classification .	1	4	6	-5.864251	5.194012	1
82-9-36	experiments reveal that sleec can make significantly more accurate predictions then the state-of-the-art methods including both embedding-based ( by as much as 35 % ) as well as tree-based ( by as much as 6 % ) methods .	the main technical contribution in sleec is a formulation for learning a small ensemble of local distance preserving embeddings which can accurately predict infrequently occurring ( tail ) labels .	0	7	4	4.9485807	-4.421633	0
82-9-36	sleec can also scale efficiently to data sets with a million labels which are beyond the pale of leading embedding methods .	the main technical contribution in sleec is a formulation for learning a small ensemble of local distance preserving embeddings which can accurately predict infrequently occurring ( tail ) labels .	0	8	4	4.262847	-3.8913796	0
82-9-36	we conducted extensive experiments on several real-world , as well as benchmark data sets and compared our method against state-of-the-art methods for extreme multi-label classification .	this allows sleec to break free of the traditional low-rank assumption and boost classification accuracy by learning embeddings which preserve pairwise distances between only the nearest label vectors .	0	6	5	5.259097	-4.700712	0
82-9-36	this allows sleec to break free of the traditional low-rank assumption and boost classification accuracy by learning embeddings which preserve pairwise distances between only the nearest label vectors .	experiments reveal that sleec can make significantly more accurate predictions then the state-of-the-art methods including both embedding-based ( by as much as 35 % ) as well as tree-based ( by as much as 6 % ) methods .	1	5	7	-5.945638	5.1725473	1
82-9-36	sleec can also scale efficiently to data sets with a million labels which are beyond the pale of leading embedding methods .	this allows sleec to break free of the traditional low-rank assumption and boost classification accuracy by learning embeddings which preserve pairwise distances between only the nearest label vectors .	0	8	5	5.1462965	-4.57407	0
82-9-36	experiments reveal that sleec can make significantly more accurate predictions then the state-of-the-art methods including both embedding-based ( by as much as 35 % ) as well as tree-based ( by as much as 6 % ) methods .	we conducted extensive experiments on several real-world , as well as benchmark data sets and compared our method against state-of-the-art methods for extreme multi-label classification .	0	7	6	4.190998	-3.81747	0
82-9-36	we conducted extensive experiments on several real-world , as well as benchmark data sets and compared our method against state-of-the-art methods for extreme multi-label classification .	sleec can also scale efficiently to data sets with a million labels which are beyond the pale of leading embedding methods .	1	6	8	-3.8548715	3.5867186	1
82-9-36	sleec can also scale efficiently to data sets with a million labels which are beyond the pale of leading embedding methods .	experiments reveal that sleec can make significantly more accurate predictions then the state-of-the-art methods including both embedding-based ( by as much as 35 % ) as well as tree-based ( by as much as 6 % ) methods .	0	8	7	2.0260966	-1.7702558	0
83-9-36	this paper is concerned with finding a solution x to a quadratic system of equations yi = | ai , x |2 , i = 1 , .	.	1	0	1	-2.8794117	2.7442224	1
83-9-36	this paper is concerned with finding a solution x to a quadratic system of equations yi = | ai , x |2 , i = 1 , .	.	1	0	2	-2.8794117	2.7442224	1
83-9-36	, m. we demonstrate that it is possible to solve unstructured random quadratic systems in n variables exactly from o ( n ) equations in linear time , that is , in time proportional to reading the data { ai } and { yi } .	this paper is concerned with finding a solution x to a quadratic system of equations yi = | ai , x |2 , i = 1 , .	0	3	0	5.6196847	-5.072483	0
83-9-36	this is accomplished by a novel procedure , which starting from an initial guess given by a spectral initialization procedure , attempts to minimize a nonconvex objective .	this paper is concerned with finding a solution x to a quadratic system of equations yi = | ai , x |2 , i = 1 , .	0	4	0	5.2655125	-4.684551	0
83-9-36	this paper is concerned with finding a solution x to a quadratic system of equations yi = | ai , x |2 , i = 1 , .	the proposed algorithm distinguishes from prior approaches by regularizing the initialization and descent procedures in an adaptive fashion , which discard terms bearing too much influence on the initial estimate or search directions .	1	0	5	-6.0269346	5.1789074	1
83-9-36	these careful selection rules -- which effectively serve as a variance reduction scheme -- provide a tighter initial guess , more robust descent directions , and thus enhanced practical performance .	this paper is concerned with finding a solution x to a quadratic system of equations yi = | ai , x |2 , i = 1 , .	0	6	0	5.5897107	-5.0024343	0
83-9-36	this paper is concerned with finding a solution x to a quadratic system of equations yi = | ai , x |2 , i = 1 , .	further , this procedure also achieves a nearoptimal statistical accuracy in the presence of noise .	1	0	7	-6.018595	5.174121	1
83-9-36	this paper is concerned with finding a solution x to a quadratic system of equations yi = | ai , x |2 , i = 1 , .	empirically , we demonstrate that the computational cost of our algorithm is about four times that of solving a least-squares problem of the same size .	1	0	8	-6.033017	5.174279	1
83-9-36	.	.	0	2	1	-0.43447864	0.5102596	1
83-9-36	.	, m. we demonstrate that it is possible to solve unstructured random quadratic systems in n variables exactly from o ( n ) equations in linear time , that is , in time proportional to reading the data { ai } and { yi } .	1	1	3	-1.4016855	1.5189384	1
83-9-36	this is accomplished by a novel procedure , which starting from an initial guess given by a spectral initialization procedure , attempts to minimize a nonconvex objective .	.	0	4	1	1.3101931	-1.1706684	0
83-9-36	.	the proposed algorithm distinguishes from prior approaches by regularizing the initialization and descent procedures in an adaptive fashion , which discard terms bearing too much influence on the initial estimate or search directions .	1	1	5	-0.72438097	0.8185488	1
83-9-36	these careful selection rules -- which effectively serve as a variance reduction scheme -- provide a tighter initial guess , more robust descent directions , and thus enhanced practical performance .	.	0	6	1	1.5084134	-1.3893433	0
83-9-36	.	further , this procedure also achieves a nearoptimal statistical accuracy in the presence of noise .	1	1	7	-2.224357	2.3354387	1
83-9-36	empirically , we demonstrate that the computational cost of our algorithm is about four times that of solving a least-squares problem of the same size .	.	0	8	1	1.5156543	-1.4426517	0
83-9-36	.	, m. we demonstrate that it is possible to solve unstructured random quadratic systems in n variables exactly from o ( n ) equations in linear time , that is , in time proportional to reading the data { ai } and { yi } .	1	2	3	-1.4016855	1.5189384	1
83-9-36	this is accomplished by a novel procedure , which starting from an initial guess given by a spectral initialization procedure , attempts to minimize a nonconvex objective .	.	0	4	2	1.3101931	-1.1706684	0
83-9-36	.	the proposed algorithm distinguishes from prior approaches by regularizing the initialization and descent procedures in an adaptive fashion , which discard terms bearing too much influence on the initial estimate or search directions .	1	2	5	-0.72438097	0.8185488	1
83-9-36	.	these careful selection rules -- which effectively serve as a variance reduction scheme -- provide a tighter initial guess , more robust descent directions , and thus enhanced practical performance .	1	2	6	-1.5301342	1.6509635	1
83-9-36	.	further , this procedure also achieves a nearoptimal statistical accuracy in the presence of noise .	1	2	7	-2.224357	2.3354387	1
83-9-36	empirically , we demonstrate that the computational cost of our algorithm is about four times that of solving a least-squares problem of the same size .	.	0	8	2	1.5156543	-1.4426517	0
83-9-36	, m. we demonstrate that it is possible to solve unstructured random quadratic systems in n variables exactly from o ( n ) equations in linear time , that is , in time proportional to reading the data { ai } and { yi } .	this is accomplished by a novel procedure , which starting from an initial guess given by a spectral initialization procedure , attempts to minimize a nonconvex objective .	1	3	4	1.1626064	-0.8725623	0
83-9-36	, m. we demonstrate that it is possible to solve unstructured random quadratic systems in n variables exactly from o ( n ) equations in linear time , that is , in time proportional to reading the data { ai } and { yi } .	the proposed algorithm distinguishes from prior approaches by regularizing the initialization and descent procedures in an adaptive fashion , which discard terms bearing too much influence on the initial estimate or search directions .	1	3	5	-2.4657621	2.5374544	1
83-9-36	these careful selection rules -- which effectively serve as a variance reduction scheme -- provide a tighter initial guess , more robust descent directions , and thus enhanced practical performance .	, m. we demonstrate that it is possible to solve unstructured random quadratic systems in n variables exactly from o ( n ) equations in linear time , that is , in time proportional to reading the data { ai } and { yi } .	0	6	3	3.8889904	-3.5546408	0
83-9-36	, m. we demonstrate that it is possible to solve unstructured random quadratic systems in n variables exactly from o ( n ) equations in linear time , that is , in time proportional to reading the data { ai } and { yi } .	further , this procedure also achieves a nearoptimal statistical accuracy in the presence of noise .	1	3	7	-5.9918184	5.203628	1
83-9-36	empirically , we demonstrate that the computational cost of our algorithm is about four times that of solving a least-squares problem of the same size .	, m. we demonstrate that it is possible to solve unstructured random quadratic systems in n variables exactly from o ( n ) equations in linear time , that is , in time proportional to reading the data { ai } and { yi } .	0	8	3	4.924198	-4.3899317	0
83-9-36	this is accomplished by a novel procedure , which starting from an initial guess given by a spectral initialization procedure , attempts to minimize a nonconvex objective .	the proposed algorithm distinguishes from prior approaches by regularizing the initialization and descent procedures in an adaptive fashion , which discard terms bearing too much influence on the initial estimate or search directions .	1	4	5	-5.789034	5.111574	1
83-9-36	this is accomplished by a novel procedure , which starting from an initial guess given by a spectral initialization procedure , attempts to minimize a nonconvex objective .	these careful selection rules -- which effectively serve as a variance reduction scheme -- provide a tighter initial guess , more robust descent directions , and thus enhanced practical performance .	1	4	6	-5.8328342	5.17057	1
83-9-36	further , this procedure also achieves a nearoptimal statistical accuracy in the presence of noise .	this is accomplished by a novel procedure , which starting from an initial guess given by a spectral initialization procedure , attempts to minimize a nonconvex objective .	0	7	4	5.4687233	-4.866343	0
83-9-36	empirically , we demonstrate that the computational cost of our algorithm is about four times that of solving a least-squares problem of the same size .	this is accomplished by a novel procedure , which starting from an initial guess given by a spectral initialization procedure , attempts to minimize a nonconvex objective .	0	8	4	5.2403	-4.5996957	0
83-9-36	the proposed algorithm distinguishes from prior approaches by regularizing the initialization and descent procedures in an adaptive fashion , which discard terms bearing too much influence on the initial estimate or search directions .	these careful selection rules -- which effectively serve as a variance reduction scheme -- provide a tighter initial guess , more robust descent directions , and thus enhanced practical performance .	1	5	6	-4.8558474	4.5217824	1
83-9-36	the proposed algorithm distinguishes from prior approaches by regularizing the initialization and descent procedures in an adaptive fashion , which discard terms bearing too much influence on the initial estimate or search directions .	further , this procedure also achieves a nearoptimal statistical accuracy in the presence of noise .	1	5	7	-5.9674735	5.229689	1
83-9-36	the proposed algorithm distinguishes from prior approaches by regularizing the initialization and descent procedures in an adaptive fashion , which discard terms bearing too much influence on the initial estimate or search directions .	empirically , we demonstrate that the computational cost of our algorithm is about four times that of solving a least-squares problem of the same size .	1	5	8	-5.793723	5.190931	1
83-9-36	these careful selection rules -- which effectively serve as a variance reduction scheme -- provide a tighter initial guess , more robust descent directions , and thus enhanced practical performance .	further , this procedure also achieves a nearoptimal statistical accuracy in the presence of noise .	1	6	7	-5.693853	5.077195	1
83-9-36	empirically , we demonstrate that the computational cost of our algorithm is about four times that of solving a least-squares problem of the same size .	these careful selection rules -- which effectively serve as a variance reduction scheme -- provide a tighter initial guess , more robust descent directions , and thus enhanced practical performance .	0	8	6	3.596806	-3.3787024	0
83-9-36	further , this procedure also achieves a nearoptimal statistical accuracy in the presence of noise .	empirically , we demonstrate that the computational cost of our algorithm is about four times that of solving a least-squares problem of the same size .	1	7	8	2.9472702	-2.8110588	0
84-8-28	the challenge of personalized medicine is to develop tools that can accurately predict the trajectory of an individual 's disease , which can in turn enable clinicians to optimize treatments .	for many complex diseases , there is a wide variety of ways in which an individual can manifest the disease .	0	1	0	3.8379855	-3.1303332	0
84-8-28	we represent an individual 's disease trajectory as a continuous-valued continuous-time function describing the severity of the disease over time .	for many complex diseases , there is a wide variety of ways in which an individual can manifest the disease .	0	2	0	5.6446247	-5.0859647	0
84-8-28	we propose a hierarchical latent variable model that individualizes predictions of disease trajectories .	for many complex diseases , there is a wide variety of ways in which an individual can manifest the disease .	0	3	0	5.704589	-5.0547066	0
84-8-28	for many complex diseases , there is a wide variety of ways in which an individual can manifest the disease .	this model shares statistical strength across observations at different resolutions-the population , subpopulation and the individual level .	1	0	4	-5.958413	5.204004	1
84-8-28	for many complex diseases , there is a wide variety of ways in which an individual can manifest the disease .	we describe an algorithm for learning population and subpopulation parameters offline , and an online procedure for dynamically learning individual-specific parameters .	1	0	5	-5.941678	5.203314	1
84-8-28	for many complex diseases , there is a wide variety of ways in which an individual can manifest the disease .	finally , we validate our model on the task of predicting the course of interstitial lung disease , a leading cause of death among patients with the autoimmune disease scleroderma .	1	0	6	-5.9082212	5.1065383	1
84-8-28	we compare our approach against state-of-the-art and demonstrate significant improvements in predictive accuracy .	for many complex diseases , there is a wide variety of ways in which an individual can manifest the disease .	0	7	0	5.6149426	-5.033186	0
84-8-28	the challenge of personalized medicine is to develop tools that can accurately predict the trajectory of an individual 's disease , which can in turn enable clinicians to optimize treatments .	we represent an individual 's disease trajectory as a continuous-valued continuous-time function describing the severity of the disease over time .	1	1	2	-5.5815125	5.144994	1
84-8-28	the challenge of personalized medicine is to develop tools that can accurately predict the trajectory of an individual 's disease , which can in turn enable clinicians to optimize treatments .	we propose a hierarchical latent variable model that individualizes predictions of disease trajectories .	1	1	3	-5.90657	5.18089	1
84-8-28	this model shares statistical strength across observations at different resolutions-the population , subpopulation and the individual level .	the challenge of personalized medicine is to develop tools that can accurately predict the trajectory of an individual 's disease , which can in turn enable clinicians to optimize treatments .	0	4	1	5.651383	-5.073801	0
84-8-28	the challenge of personalized medicine is to develop tools that can accurately predict the trajectory of an individual 's disease , which can in turn enable clinicians to optimize treatments .	we describe an algorithm for learning population and subpopulation parameters offline , and an online procedure for dynamically learning individual-specific parameters .	1	1	5	-5.9285135	5.215256	1
84-8-28	finally , we validate our model on the task of predicting the course of interstitial lung disease , a leading cause of death among patients with the autoimmune disease scleroderma .	the challenge of personalized medicine is to develop tools that can accurately predict the trajectory of an individual 's disease , which can in turn enable clinicians to optimize treatments .	0	6	1	5.6046853	-5.045695	0
84-8-28	we compare our approach against state-of-the-art and demonstrate significant improvements in predictive accuracy .	the challenge of personalized medicine is to develop tools that can accurately predict the trajectory of an individual 's disease , which can in turn enable clinicians to optimize treatments .	0	7	1	5.6058564	-5.0649476	0
84-8-28	we propose a hierarchical latent variable model that individualizes predictions of disease trajectories .	we represent an individual 's disease trajectory as a continuous-valued continuous-time function describing the severity of the disease over time .	0	3	2	-0.62543917	0.8907651	1
84-8-28	this model shares statistical strength across observations at different resolutions-the population , subpopulation and the individual level .	we represent an individual 's disease trajectory as a continuous-valued continuous-time function describing the severity of the disease over time .	0	4	2	4.9797125	-4.401035	0
84-8-28	we represent an individual 's disease trajectory as a continuous-valued continuous-time function describing the severity of the disease over time .	we describe an algorithm for learning population and subpopulation parameters offline , and an online procedure for dynamically learning individual-specific parameters .	1	2	5	-0.6833405	0.9852395	1
84-8-28	we represent an individual 's disease trajectory as a continuous-valued continuous-time function describing the severity of the disease over time .	finally , we validate our model on the task of predicting the course of interstitial lung disease , a leading cause of death among patients with the autoimmune disease scleroderma .	1	2	6	-5.8328905	4.910553	1
84-8-28	we represent an individual 's disease trajectory as a continuous-valued continuous-time function describing the severity of the disease over time .	we compare our approach against state-of-the-art and demonstrate significant improvements in predictive accuracy .	1	2	7	-6.0107284	5.127331	1
84-8-28	this model shares statistical strength across observations at different resolutions-the population , subpopulation and the individual level .	we propose a hierarchical latent variable model that individualizes predictions of disease trajectories .	0	4	3	5.095781	-4.495569	0
84-8-28	we propose a hierarchical latent variable model that individualizes predictions of disease trajectories .	we describe an algorithm for learning population and subpopulation parameters offline , and an online procedure for dynamically learning individual-specific parameters .	1	3	5	-4.754338	4.4252677	1
84-8-28	we propose a hierarchical latent variable model that individualizes predictions of disease trajectories .	finally , we validate our model on the task of predicting the course of interstitial lung disease , a leading cause of death among patients with the autoimmune disease scleroderma .	1	3	6	-5.9665737	5.107744	1
84-8-28	we compare our approach against state-of-the-art and demonstrate significant improvements in predictive accuracy .	we propose a hierarchical latent variable model that individualizes predictions of disease trajectories .	0	7	3	5.503962	-4.8275847	0
84-8-28	this model shares statistical strength across observations at different resolutions-the population , subpopulation and the individual level .	we describe an algorithm for learning population and subpopulation parameters offline , and an online procedure for dynamically learning individual-specific parameters .	1	4	5	-3.7337086	3.603297	1
84-8-28	finally , we validate our model on the task of predicting the course of interstitial lung disease , a leading cause of death among patients with the autoimmune disease scleroderma .	this model shares statistical strength across observations at different resolutions-the population , subpopulation and the individual level .	0	6	4	5.200443	-4.6325884	0
84-8-28	we compare our approach against state-of-the-art and demonstrate significant improvements in predictive accuracy .	this model shares statistical strength across observations at different resolutions-the population , subpopulation and the individual level .	0	7	4	5.3379416	-4.752723	0
84-8-28	we describe an algorithm for learning population and subpopulation parameters offline , and an online procedure for dynamically learning individual-specific parameters .	finally , we validate our model on the task of predicting the course of interstitial lung disease , a leading cause of death among patients with the autoimmune disease scleroderma .	1	5	6	-5.9280596	5.143937	1
84-8-28	we compare our approach against state-of-the-art and demonstrate significant improvements in predictive accuracy .	we describe an algorithm for learning population and subpopulation parameters offline , and an online procedure for dynamically learning individual-specific parameters .	0	7	5	5.35381	-4.7226796	0
84-8-28	finally , we validate our model on the task of predicting the course of interstitial lung disease , a leading cause of death among patients with the autoimmune disease scleroderma .	we compare our approach against state-of-the-art and demonstrate significant improvements in predictive accuracy .	1	6	7	1.3077538	-1.0818932	0
85-5-10	this paper considers the subspace clustering problem where the data contains irrelevant or corrupted features .	we propose a method termed `` robust dantzig selector '' which can successfully identify the clustering structure even with the presence of irrelevant features .	1	0	1	-5.919482	5.202893	1
85-5-10	the idea is simple yet powerful : we replace the inner product by its robust counterpart , which is insensitive to the irrelevant features given an upper bound of the number of irrelevant features .	this paper considers the subspace clustering problem where the data contains irrelevant or corrupted features .	0	2	0	5.5277166	-4.943203	0
85-5-10	this paper considers the subspace clustering problem where the data contains irrelevant or corrupted features .	we establish theoretical guarantees for the algorithm to identify the correct subspace , and demonstrate the effectiveness of the algorithm via numerical simulations .	1	0	3	-5.9183807	5.2012672	1
85-5-10	this paper considers the subspace clustering problem where the data contains irrelevant or corrupted features .	to the best of our knowledge , this is the first method developed to tackle subspace clustering with irrelevant features .	1	0	4	-5.9798703	5.1568313	1
85-5-10	the idea is simple yet powerful : we replace the inner product by its robust counterpart , which is insensitive to the irrelevant features given an upper bound of the number of irrelevant features .	we propose a method termed `` robust dantzig selector '' which can successfully identify the clustering structure even with the presence of irrelevant features .	0	2	1	1.4131354	-1.1158898	0
85-5-10	we propose a method termed `` robust dantzig selector '' which can successfully identify the clustering structure even with the presence of irrelevant features .	we establish theoretical guarantees for the algorithm to identify the correct subspace , and demonstrate the effectiveness of the algorithm via numerical simulations .	1	1	3	-5.8676653	5.205644	1
85-5-10	to the best of our knowledge , this is the first method developed to tackle subspace clustering with irrelevant features .	we propose a method termed `` robust dantzig selector '' which can successfully identify the clustering structure even with the presence of irrelevant features .	0	4	1	3.928222	-3.546083	0
85-5-10	we establish theoretical guarantees for the algorithm to identify the correct subspace , and demonstrate the effectiveness of the algorithm via numerical simulations .	the idea is simple yet powerful : we replace the inner product by its robust counterpart , which is insensitive to the irrelevant features given an upper bound of the number of irrelevant features .	0	3	2	5.2328324	-4.611332	0
85-5-10	to the best of our knowledge , this is the first method developed to tackle subspace clustering with irrelevant features .	the idea is simple yet powerful : we replace the inner product by its robust counterpart , which is insensitive to the irrelevant features given an upper bound of the number of irrelevant features .	0	4	2	4.665102	-4.219264	0
85-5-10	to the best of our knowledge , this is the first method developed to tackle subspace clustering with irrelevant features .	we establish theoretical guarantees for the algorithm to identify the correct subspace , and demonstrate the effectiveness of the algorithm via numerical simulations .	0	4	3	-3.753116	3.6012475	1
86-8-28	such components can be computed one by one , repeatedly solving the single-component problem and deflating the input data matrix , but this greedy procedure is suboptimal .	we consider the following multi-component sparse pca problem : given a set of data points , we seek to extract a small number of sparse components with disjoint supports that jointly capture the maximum possible variance .	0	1	0	5.634633	-5.0412397	0
86-8-28	we consider the following multi-component sparse pca problem : given a set of data points , we seek to extract a small number of sparse components with disjoint supports that jointly capture the maximum possible variance .	we present a novel algorithm for sparse pca that jointly optimizes multiple disjoint components .	1	0	2	-5.749382	5.237367	1
86-8-28	we consider the following multi-component sparse pca problem : given a set of data points , we seek to extract a small number of sparse components with disjoint supports that jointly capture the maximum possible variance .	the extracted features capture variance that lies within a multiplicative factor arbitrarily close to 1 from the optimal .	1	0	3	-5.824322	5.1816845	1
86-8-28	we consider the following multi-component sparse pca problem : given a set of data points , we seek to extract a small number of sparse components with disjoint supports that jointly capture the maximum possible variance .	our algorithm is combinatorial and computes the desired components by solving multiple instances of the bipartite maximum weight matching problem .	1	0	4	-5.880992	5.1992455	1
86-8-28	we consider the following multi-component sparse pca problem : given a set of data points , we seek to extract a small number of sparse components with disjoint supports that jointly capture the maximum possible variance .	its complexity grows as a low order polynomial in the ambient dimension of the input data , but exponentially in its rank .	1	0	5	-5.7129264	5.1512456	1
86-8-28	we consider the following multi-component sparse pca problem : given a set of data points , we seek to extract a small number of sparse components with disjoint supports that jointly capture the maximum possible variance .	however , it can be effectively applied on a low-dimensional sketch of the input data .	1	0	6	-6.0223436	5.173375	1
86-8-28	we consider the following multi-component sparse pca problem : given a set of data points , we seek to extract a small number of sparse components with disjoint supports that jointly capture the maximum possible variance .	we evaluate our algorithm on real datasets and empirically demonstrate that in many cases it outperforms existing , deflation-based approaches .	1	0	7	-5.9754963	5.1729774	1
86-8-28	such components can be computed one by one , repeatedly solving the single-component problem and deflating the input data matrix , but this greedy procedure is suboptimal .	we present a novel algorithm for sparse pca that jointly optimizes multiple disjoint components .	1	1	2	5.2810335	-4.717903	0
86-8-28	such components can be computed one by one , repeatedly solving the single-component problem and deflating the input data matrix , but this greedy procedure is suboptimal .	the extracted features capture variance that lies within a multiplicative factor arbitrarily close to 1 from the optimal .	1	1	3	-4.0157833	3.8463717	1
86-8-28	our algorithm is combinatorial and computes the desired components by solving multiple instances of the bipartite maximum weight matching problem .	such components can be computed one by one , repeatedly solving the single-component problem and deflating the input data matrix , but this greedy procedure is suboptimal .	0	4	1	-3.013382	2.945814	1
86-8-28	such components can be computed one by one , repeatedly solving the single-component problem and deflating the input data matrix , but this greedy procedure is suboptimal .	its complexity grows as a low order polynomial in the ambient dimension of the input data , but exponentially in its rank .	1	1	5	-0.7564552	0.89170575	1
86-8-28	such components can be computed one by one , repeatedly solving the single-component problem and deflating the input data matrix , but this greedy procedure is suboptimal .	however , it can be effectively applied on a low-dimensional sketch of the input data .	1	1	6	-4.8576	4.5045033	1
86-8-28	such components can be computed one by one , repeatedly solving the single-component problem and deflating the input data matrix , but this greedy procedure is suboptimal .	we evaluate our algorithm on real datasets and empirically demonstrate that in many cases it outperforms existing , deflation-based approaches .	1	1	7	-5.9504805	5.18592	1
86-8-28	we present a novel algorithm for sparse pca that jointly optimizes multiple disjoint components .	the extracted features capture variance that lies within a multiplicative factor arbitrarily close to 1 from the optimal .	1	2	3	-5.9075866	5.1722574	1
86-8-28	our algorithm is combinatorial and computes the desired components by solving multiple instances of the bipartite maximum weight matching problem .	we present a novel algorithm for sparse pca that jointly optimizes multiple disjoint components .	0	4	2	5.636591	-4.9902	0
86-8-28	its complexity grows as a low order polynomial in the ambient dimension of the input data , but exponentially in its rank .	we present a novel algorithm for sparse pca that jointly optimizes multiple disjoint components .	0	5	2	4.626189	-4.1597514	0
86-8-28	however , it can be effectively applied on a low-dimensional sketch of the input data .	we present a novel algorithm for sparse pca that jointly optimizes multiple disjoint components .	0	6	2	3.9256337	-3.5969963	0
86-8-28	we present a novel algorithm for sparse pca that jointly optimizes multiple disjoint components .	we evaluate our algorithm on real datasets and empirically demonstrate that in many cases it outperforms existing , deflation-based approaches .	1	2	7	-5.9634004	5.211132	1
86-8-28	the extracted features capture variance that lies within a multiplicative factor arbitrarily close to 1 from the optimal .	our algorithm is combinatorial and computes the desired components by solving multiple instances of the bipartite maximum weight matching problem .	1	3	4	3.7729092	-3.466773	0
86-8-28	the extracted features capture variance that lies within a multiplicative factor arbitrarily close to 1 from the optimal .	its complexity grows as a low order polynomial in the ambient dimension of the input data , but exponentially in its rank .	1	3	5	4.3167543	-3.9850247	0
86-8-28	however , it can be effectively applied on a low-dimensional sketch of the input data .	the extracted features capture variance that lies within a multiplicative factor arbitrarily close to 1 from the optimal .	0	6	3	2.452344	-2.321165	0
86-8-28	we evaluate our algorithm on real datasets and empirically demonstrate that in many cases it outperforms existing , deflation-based approaches .	the extracted features capture variance that lies within a multiplicative factor arbitrarily close to 1 from the optimal .	0	7	3	4.887635	-4.4196925	0
86-8-28	its complexity grows as a low order polynomial in the ambient dimension of the input data , but exponentially in its rank .	our algorithm is combinatorial and computes the desired components by solving multiple instances of the bipartite maximum weight matching problem .	0	5	4	1.3134462	-1.183759	0
86-8-28	our algorithm is combinatorial and computes the desired components by solving multiple instances of the bipartite maximum weight matching problem .	however , it can be effectively applied on a low-dimensional sketch of the input data .	1	4	6	-4.668173	4.306198	1
86-8-28	our algorithm is combinatorial and computes the desired components by solving multiple instances of the bipartite maximum weight matching problem .	we evaluate our algorithm on real datasets and empirically demonstrate that in many cases it outperforms existing , deflation-based approaches .	1	4	7	-5.826484	5.192012	1
86-8-28	its complexity grows as a low order polynomial in the ambient dimension of the input data , but exponentially in its rank .	however , it can be effectively applied on a low-dimensional sketch of the input data .	1	5	6	-5.227881	4.735143	1
86-8-28	its complexity grows as a low order polynomial in the ambient dimension of the input data , but exponentially in its rank .	we evaluate our algorithm on real datasets and empirically demonstrate that in many cases it outperforms existing , deflation-based approaches .	1	5	7	-5.977016	5.156059	1
86-8-28	we evaluate our algorithm on real datasets and empirically demonstrate that in many cases it outperforms existing , deflation-based approaches .	however , it can be effectively applied on a low-dimensional sketch of the input data .	0	7	6	4.2070446	-3.8811326	0
87-9-36	one approach to improving the running time of kernel-based methods is to build a small sketch of the kernel matrix and use it in lieu of the full matrix in the machine learning task of interest .	here , we describe a version of this approach that comes with running time guarantees as well as improved guarantees on its statistical performance .	1	0	1	-5.9616375	5.2281966	1
87-9-36	one approach to improving the running time of kernel-based methods is to build a small sketch of the kernel matrix and use it in lieu of the full matrix in the machine learning task of interest .	by extending the notion of statistical leverage scores to the setting of kernel ridge regression , we are able to identify a sampling distribution that reduces the size of the sketch ( i.e. , the required number of columns to be sampled ) to the effective dimensionality of the problem .	1	0	2	-5.977854	5.2234464	1
87-9-36	one approach to improving the running time of kernel-based methods is to build a small sketch of the kernel matrix and use it in lieu of the full matrix in the machine learning task of interest .	this latter quantity is often much smaller than previous bounds that depend on the maximal degrees of freedom .	1	0	3	-5.6461997	5.1801643	1
87-9-36	we give an empirical evidence supporting this fact .	one approach to improving the running time of kernel-based methods is to build a small sketch of the kernel matrix and use it in lieu of the full matrix in the machine learning task of interest .	0	4	0	5.504853	-4.8454027	0
87-9-36	one approach to improving the running time of kernel-based methods is to build a small sketch of the kernel matrix and use it in lieu of the full matrix in the machine learning task of interest .	our second contribution is to present a fast algorithm to quickly compute coarse approximations to these scores in time linear in the number of samples .	1	0	5	-5.970904	5.216716	1
87-9-36	one approach to improving the running time of kernel-based methods is to build a small sketch of the kernel matrix and use it in lieu of the full matrix in the machine learning task of interest .	more precisely , the running time of the algorithm is o ( np2 ) with p only depending on the trace of the kernel matrix and the regularization parameter .	1	0	6	-5.7698336	5.253544	1
87-9-36	this is obtained via a variant of squared length sampling that we adapt to the kernel setting .	one approach to improving the running time of kernel-based methods is to build a small sketch of the kernel matrix and use it in lieu of the full matrix in the machine learning task of interest .	0	7	0	5.449627	-4.8416224	0
87-9-36	lastly , we discuss how this new notion of the leverage of a data point captures a fine notion of the difficulty of the learning problem .	one approach to improving the running time of kernel-based methods is to build a small sketch of the kernel matrix and use it in lieu of the full matrix in the machine learning task of interest .	0	8	0	5.5563946	-4.9223256	0
87-9-36	here , we describe a version of this approach that comes with running time guarantees as well as improved guarantees on its statistical performance .	by extending the notion of statistical leverage scores to the setting of kernel ridge regression , we are able to identify a sampling distribution that reduces the size of the sketch ( i.e. , the required number of columns to be sampled ) to the effective dimensionality of the problem .	1	1	2	-5.393527	4.905385	1
87-9-36	this latter quantity is often much smaller than previous bounds that depend on the maximal degrees of freedom .	here , we describe a version of this approach that comes with running time guarantees as well as improved guarantees on its statistical performance .	0	3	1	-3.0821977	3.0860968	1
87-9-36	here , we describe a version of this approach that comes with running time guarantees as well as improved guarantees on its statistical performance .	we give an empirical evidence supporting this fact .	1	1	4	-5.776079	4.872269	1
87-9-36	here , we describe a version of this approach that comes with running time guarantees as well as improved guarantees on its statistical performance .	our second contribution is to present a fast algorithm to quickly compute coarse approximations to these scores in time linear in the number of samples .	1	1	5	-5.878747	5.1238394	1
87-9-36	more precisely , the running time of the algorithm is o ( np2 ) with p only depending on the trace of the kernel matrix and the regularization parameter .	here , we describe a version of this approach that comes with running time guarantees as well as improved guarantees on its statistical performance .	0	6	1	3.2829647	-3.0527608	0
87-9-36	this is obtained via a variant of squared length sampling that we adapt to the kernel setting .	here , we describe a version of this approach that comes with running time guarantees as well as improved guarantees on its statistical performance .	0	7	1	4.8325806	-4.3549104	0
87-9-36	here , we describe a version of this approach that comes with running time guarantees as well as improved guarantees on its statistical performance .	lastly , we discuss how this new notion of the leverage of a data point captures a fine notion of the difficulty of the learning problem .	1	1	8	-5.9068804	5.1673717	1
87-9-36	this latter quantity is often much smaller than previous bounds that depend on the maximal degrees of freedom .	by extending the notion of statistical leverage scores to the setting of kernel ridge regression , we are able to identify a sampling distribution that reduces the size of the sketch ( i.e. , the required number of columns to be sampled ) to the effective dimensionality of the problem .	0	3	2	-3.2228737	3.247468	1
87-9-36	we give an empirical evidence supporting this fact .	by extending the notion of statistical leverage scores to the setting of kernel ridge regression , we are able to identify a sampling distribution that reduces the size of the sketch ( i.e. , the required number of columns to be sampled ) to the effective dimensionality of the problem .	0	4	2	4.3689823	-3.975289	0
87-9-36	our second contribution is to present a fast algorithm to quickly compute coarse approximations to these scores in time linear in the number of samples .	by extending the notion of statistical leverage scores to the setting of kernel ridge regression , we are able to identify a sampling distribution that reduces the size of the sketch ( i.e. , the required number of columns to be sampled ) to the effective dimensionality of the problem .	0	5	2	3.9452407	-3.5581386	0
87-9-36	by extending the notion of statistical leverage scores to the setting of kernel ridge regression , we are able to identify a sampling distribution that reduces the size of the sketch ( i.e. , the required number of columns to be sampled ) to the effective dimensionality of the problem .	more precisely , the running time of the algorithm is o ( np2 ) with p only depending on the trace of the kernel matrix and the regularization parameter .	1	2	6	-4.9685316	4.563947	1
87-9-36	by extending the notion of statistical leverage scores to the setting of kernel ridge regression , we are able to identify a sampling distribution that reduces the size of the sketch ( i.e. , the required number of columns to be sampled ) to the effective dimensionality of the problem .	this is obtained via a variant of squared length sampling that we adapt to the kernel setting .	1	2	7	-5.3825803	4.906145	1
87-9-36	by extending the notion of statistical leverage scores to the setting of kernel ridge regression , we are able to identify a sampling distribution that reduces the size of the sketch ( i.e. , the required number of columns to be sampled ) to the effective dimensionality of the problem .	lastly , we discuss how this new notion of the leverage of a data point captures a fine notion of the difficulty of the learning problem .	1	2	8	-5.7950726	4.999874	1
87-9-36	we give an empirical evidence supporting this fact .	this latter quantity is often much smaller than previous bounds that depend on the maximal degrees of freedom .	0	4	3	4.7582483	-4.3267703	0
87-9-36	this latter quantity is often much smaller than previous bounds that depend on the maximal degrees of freedom .	our second contribution is to present a fast algorithm to quickly compute coarse approximations to these scores in time linear in the number of samples .	1	3	5	-5.4659367	4.946513	1
87-9-36	this latter quantity is often much smaller than previous bounds that depend on the maximal degrees of freedom .	more precisely , the running time of the algorithm is o ( np2 ) with p only depending on the trace of the kernel matrix and the regularization parameter .	1	3	6	-5.588085	5.07645	1
87-9-36	this latter quantity is often much smaller than previous bounds that depend on the maximal degrees of freedom .	this is obtained via a variant of squared length sampling that we adapt to the kernel setting .	1	3	7	-4.980914	4.6569715	1
87-9-36	this latter quantity is often much smaller than previous bounds that depend on the maximal degrees of freedom .	lastly , we discuss how this new notion of the leverage of a data point captures a fine notion of the difficulty of the learning problem .	1	3	8	-5.858431	4.9588118	1
87-9-36	we give an empirical evidence supporting this fact .	our second contribution is to present a fast algorithm to quickly compute coarse approximations to these scores in time linear in the number of samples .	1	4	5	3.308729	-3.1139746	0
87-9-36	more precisely , the running time of the algorithm is o ( np2 ) with p only depending on the trace of the kernel matrix and the regularization parameter .	we give an empirical evidence supporting this fact .	0	6	4	-4.0950146	3.8748088	1
87-9-36	we give an empirical evidence supporting this fact .	this is obtained via a variant of squared length sampling that we adapt to the kernel setting .	1	4	7	2.899425	-2.737643	0
87-9-36	we give an empirical evidence supporting this fact .	lastly , we discuss how this new notion of the leverage of a data point captures a fine notion of the difficulty of the learning problem .	1	4	8	0.9309297	-0.81332016	0
87-9-36	our second contribution is to present a fast algorithm to quickly compute coarse approximations to these scores in time linear in the number of samples .	more precisely , the running time of the algorithm is o ( np2 ) with p only depending on the trace of the kernel matrix and the regularization parameter .	1	5	6	-3.5576704	3.360752	1
87-9-36	this is obtained via a variant of squared length sampling that we adapt to the kernel setting .	our second contribution is to present a fast algorithm to quickly compute coarse approximations to these scores in time linear in the number of samples .	0	7	5	-2.2461388	2.326205	1
87-9-36	lastly , we discuss how this new notion of the leverage of a data point captures a fine notion of the difficulty of the learning problem .	our second contribution is to present a fast algorithm to quickly compute coarse approximations to these scores in time linear in the number of samples .	0	8	5	3.6640263	-3.4001582	0
87-9-36	this is obtained via a variant of squared length sampling that we adapt to the kernel setting .	more precisely , the running time of the algorithm is o ( np2 ) with p only depending on the trace of the kernel matrix and the regularization parameter .	0	7	6	-2.9558754	2.850779	1
87-9-36	more precisely , the running time of the algorithm is o ( np2 ) with p only depending on the trace of the kernel matrix and the regularization parameter .	lastly , we discuss how this new notion of the leverage of a data point captures a fine notion of the difficulty of the learning problem .	1	6	8	-5.261154	4.793289	1
87-9-36	lastly , we discuss how this new notion of the leverage of a data point captures a fine notion of the difficulty of the learning problem .	this is obtained via a variant of squared length sampling that we adapt to the kernel setting .	0	8	7	3.481275	-3.2665946	0
88-5-10	the framework of online learning with memory naturally captures learning problems with temporal effects , and was previously studied for the experts setting .	in this work we extend the notion of learning with memory to the general online convex optimization ( oco ) framework , and present two algorithms that attain low regret .	1	0	1	2.242595	-2.033872	0
88-5-10	the first algorithm applies to lipschitz continuous loss functions , obtaining optimal regret bounds for both convex and strongly convex losses .	the framework of online learning with memory naturally captures learning problems with temporal effects , and was previously studied for the experts setting .	0	2	0	4.831272	-4.2908964	0
88-5-10	the second algorithm attains the optimal regret bounds and applies more broadly to convex losses without requiring lipschitz continuity , yet is more complicated to implement .	the framework of online learning with memory naturally captures learning problems with temporal effects , and was previously studied for the experts setting .	0	3	0	5.040982	-4.500318	0
88-5-10	we complement the theoretical results with two applications : statistical arbitrage in finance , and multi-step ahead prediction in statistics .	the framework of online learning with memory naturally captures learning problems with temporal effects , and was previously studied for the experts setting .	0	4	0	5.195862	-4.572897	0
88-5-10	the first algorithm applies to lipschitz continuous loss functions , obtaining optimal regret bounds for both convex and strongly convex losses .	in this work we extend the notion of learning with memory to the general online convex optimization ( oco ) framework , and present two algorithms that attain low regret .	0	2	1	5.460707	-4.858364	0
88-5-10	the second algorithm attains the optimal regret bounds and applies more broadly to convex losses without requiring lipschitz continuity , yet is more complicated to implement .	in this work we extend the notion of learning with memory to the general online convex optimization ( oco ) framework , and present two algorithms that attain low regret .	0	3	1	5.4771266	-4.8656445	0
88-5-10	we complement the theoretical results with two applications : statistical arbitrage in finance , and multi-step ahead prediction in statistics .	in this work we extend the notion of learning with memory to the general online convex optimization ( oco ) framework , and present two algorithms that attain low regret .	0	4	1	5.4996085	-4.839177	0
88-5-10	the first algorithm applies to lipschitz continuous loss functions , obtaining optimal regret bounds for both convex and strongly convex losses .	the second algorithm attains the optimal regret bounds and applies more broadly to convex losses without requiring lipschitz continuity , yet is more complicated to implement .	1	2	3	-5.6702247	5.172833	1
88-5-10	we complement the theoretical results with two applications : statistical arbitrage in finance , and multi-step ahead prediction in statistics .	the first algorithm applies to lipschitz continuous loss functions , obtaining optimal regret bounds for both convex and strongly convex losses .	0	4	2	2.8931587	-2.7320352	0
88-5-10	we complement the theoretical results with two applications : statistical arbitrage in finance , and multi-step ahead prediction in statistics .	the second algorithm attains the optimal regret bounds and applies more broadly to convex losses without requiring lipschitz continuity , yet is more complicated to implement .	0	4	3	1.1694577	-0.8795108	0
89-8-28	subunit models provide a powerful yet parsimonious description of neural responses to complex stimuli .	they are defined by a cascade of two linear-nonlinear ( ln ) stages , with the first stage defined by a linear convolution with one or more filters and common point nonlinearity , and the second by pooling weights and an output nonlinearity .	1	0	1	-5.811125	5.2172976	1
89-8-28	recent interest in such models has surged due to their biological plausibility and accuracy for characterizing early sensory responses .	subunit models provide a powerful yet parsimonious description of neural responses to complex stimuli .	0	2	0	3.8201206	-3.2667394	0
89-8-28	subunit models provide a powerful yet parsimonious description of neural responses to complex stimuli .	however , fitting poses a difficult computational challenge due to the expense of evaluating the log-likelihood and the ubiquity of local optima .	1	0	3	-5.918974	5.219528	1
89-8-28	here we address this problem by providing a theoretical connection between spike-triggered covariance analysis and nonlinear subunit models .	subunit models provide a powerful yet parsimonious description of neural responses to complex stimuli .	0	4	0	3.9502654	-3.2880998	0
89-8-28	subunit models provide a powerful yet parsimonious description of neural responses to complex stimuli .	specifically , we show that a `` convolutional '' decomposition of a spike-triggered average ( sta ) and covariance ( stc ) matrix provides an asymptotically efficient estimator for class of quadratic subunit models .	1	0	5	-5.8562827	5.2743425	1
89-8-28	we establish theoretical conditions for identifiability of the subunit and pooling weights , and show that our estimator performs well even in cases of model mismatch .	subunit models provide a powerful yet parsimonious description of neural responses to complex stimuli .	0	6	0	5.584244	-4.9552603	0
89-8-28	finally , we analyze neural data from macaque primary visual cortex and show that our moment-based estimator outperforms a highly regularized generalized quadratic model ( gqm ) , and achieves nearly the same prediction performance as the full maximum-likelihood estimator , yet at substantially lower cost .	subunit models provide a powerful yet parsimonious description of neural responses to complex stimuli .	0	7	0	5.532913	-4.932481	0
89-8-28	recent interest in such models has surged due to their biological plausibility and accuracy for characterizing early sensory responses .	they are defined by a cascade of two linear-nonlinear ( ln ) stages , with the first stage defined by a linear convolution with one or more filters and common point nonlinearity , and the second by pooling weights and an output nonlinearity .	0	2	1	-5.7022285	5.1817575	1
89-8-28	however , fitting poses a difficult computational challenge due to the expense of evaluating the log-likelihood and the ubiquity of local optima .	they are defined by a cascade of two linear-nonlinear ( ln ) stages , with the first stage defined by a linear convolution with one or more filters and common point nonlinearity , and the second by pooling weights and an output nonlinearity .	0	3	1	-5.281953	4.9769993	1
89-8-28	here we address this problem by providing a theoretical connection between spike-triggered covariance analysis and nonlinear subunit models .	they are defined by a cascade of two linear-nonlinear ( ln ) stages , with the first stage defined by a linear convolution with one or more filters and common point nonlinearity , and the second by pooling weights and an output nonlinearity .	0	4	1	-5.5556564	5.126514	1
89-8-28	[CLS] specifically, we show that a ` ` convolutional'' decomposition of a spike - triggered average ( sta ) and covariance ( stc ) matrix provides an asymptotically efficient estimator for class of	[CLS] they are defined by a cascade of two linear - nonlinear ( ln ) stages, with the first stage defined by a linear convolution with one or more filters and common point nonlinearity, and the second by pooling weights and an	0	5	1	-5.1147556	4.679268	1
89-8-28	we establish theoretical conditions for identifiability of the subunit and pooling weights , and show that our estimator performs well even in cases of model mismatch .	they are defined by a cascade of two linear-nonlinear ( ln ) stages , with the first stage defined by a linear convolution with one or more filters and common point nonlinearity , and the second by pooling weights and an output nonlinearity .	0	6	1	5.228777	-4.724411	0
89-8-28	[CLS] they are defined by a cascade of two linear - nonlinear ( ln ) stages, with the first stage defined by a linear convolution with one or more filters and common point nonlinearity, and the second by pooling weights and an	[CLS] finally, we analyze neural data from macaque primary visual cortex and show that our moment - based estimator outperforms a highly regularized generalized quadratic model ( gqm ), and achieves nearly the same prediction performance as	1	1	7	-4.55711	4.0979767	1
89-8-28	however , fitting poses a difficult computational challenge due to the expense of evaluating the log-likelihood and the ubiquity of local optima .	recent interest in such models has surged due to their biological plausibility and accuracy for characterizing early sensory responses .	0	3	2	5.2387123	-4.628027	0
89-8-28	here we address this problem by providing a theoretical connection between spike-triggered covariance analysis and nonlinear subunit models .	recent interest in such models has surged due to their biological plausibility and accuracy for characterizing early sensory responses .	0	4	2	5.410385	-4.7463403	0
89-8-28	specifically , we show that a `` convolutional '' decomposition of a spike-triggered average ( sta ) and covariance ( stc ) matrix provides an asymptotically efficient estimator for class of quadratic subunit models .	recent interest in such models has surged due to their biological plausibility and accuracy for characterizing early sensory responses .	0	5	2	4.973491	-4.4368534	0
89-8-28	we establish theoretical conditions for identifiability of the subunit and pooling weights , and show that our estimator performs well even in cases of model mismatch .	recent interest in such models has surged due to their biological plausibility and accuracy for characterizing early sensory responses .	0	6	2	5.6817827	-5.0417156	0
89-8-28	finally , we analyze neural data from macaque primary visual cortex and show that our moment-based estimator outperforms a highly regularized generalized quadratic model ( gqm ) , and achieves nearly the same prediction performance as the full maximum-likelihood estimator , yet at substantially lower cost .	recent interest in such models has surged due to their biological plausibility and accuracy for characterizing early sensory responses .	0	7	2	5.6136265	-4.989185	0
89-8-28	here we address this problem by providing a theoretical connection between spike-triggered covariance analysis and nonlinear subunit models .	however , fitting poses a difficult computational challenge due to the expense of evaluating the log-likelihood and the ubiquity of local optima .	0	4	3	4.351336	-3.9751196	0
89-8-28	specifically , we show that a `` convolutional '' decomposition of a spike-triggered average ( sta ) and covariance ( stc ) matrix provides an asymptotically efficient estimator for class of quadratic subunit models .	however , fitting poses a difficult computational challenge due to the expense of evaluating the log-likelihood and the ubiquity of local optima .	0	5	3	-3.4520378	3.4115167	1
89-8-28	we establish theoretical conditions for identifiability of the subunit and pooling weights , and show that our estimator performs well even in cases of model mismatch .	however , fitting poses a difficult computational challenge due to the expense of evaluating the log-likelihood and the ubiquity of local optima .	0	6	3	5.2244577	-4.634045	0
89-8-28	finally , we analyze neural data from macaque primary visual cortex and show that our moment-based estimator outperforms a highly regularized generalized quadratic model ( gqm ) , and achieves nearly the same prediction performance as the full maximum-likelihood estimator , yet at substantially lower cost .	however , fitting poses a difficult computational challenge due to the expense of evaluating the log-likelihood and the ubiquity of local optima .	0	7	3	5.4665613	-4.892126	0
89-8-28	specifically , we show that a `` convolutional '' decomposition of a spike-triggered average ( sta ) and covariance ( stc ) matrix provides an asymptotically efficient estimator for class of quadratic subunit models .	here we address this problem by providing a theoretical connection between spike-triggered covariance analysis and nonlinear subunit models .	0	5	4	-2.5649793	2.6571445	1
89-8-28	we establish theoretical conditions for identifiability of the subunit and pooling weights , and show that our estimator performs well even in cases of model mismatch .	here we address this problem by providing a theoretical connection between spike-triggered covariance analysis and nonlinear subunit models .	0	6	4	5.7396994	-5.1038666	0
89-8-28	finally , we analyze neural data from macaque primary visual cortex and show that our moment-based estimator outperforms a highly regularized generalized quadratic model ( gqm ) , and achieves nearly the same prediction performance as the full maximum-likelihood estimator , yet at substantially lower cost .	here we address this problem by providing a theoretical connection between spike-triggered covariance analysis and nonlinear subunit models .	0	7	4	5.7261233	-5.097953	0
89-8-28	we establish theoretical conditions for identifiability of the subunit and pooling weights , and show that our estimator performs well even in cases of model mismatch .	specifically , we show that a `` convolutional '' decomposition of a spike-triggered average ( sta ) and covariance ( stc ) matrix provides an asymptotically efficient estimator for class of quadratic subunit models .	0	6	5	5.5067835	-4.8747478	0
89-8-28	[CLS] finally, we analyze neural data from macaque primary visual cortex and show that our moment - based estimator outperforms a highly regularized generalized quadratic model ( gqm ), and achieves nearly the same prediction performance as	[CLS] specifically, we show that a ` ` convolutional'' decomposition of a spike - triggered average ( sta ) and covariance ( stc ) matrix provides an asymptotically efficient estimator for class of	0	7	5	5.3411365	-4.656925	0
89-8-28	we establish theoretical conditions for identifiability of the subunit and pooling weights , and show that our estimator performs well even in cases of model mismatch .	[CLS] finally, we analyze neural data from macaque primary visual cortex and show that our moment - based estimator outperforms a highly regularized generalized quadratic model ( gqm ), and achieves nearly the same prediction performance as the full maximum - likelihood estimator, yet at substantially	1	6	7	-3.9180844	3.57755	1
90-5-10	very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective .	the goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time .	0	1	0	2.752232	-2.6327543	0
90-5-10	in this paper , we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences .	the goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time .	0	2	0	2.022879	-1.8409506	0
90-5-10	[CLS] by extending the fully connected lstm ( fc - lstm ) to have convolutional structures in both the input - to - state and state - to - state transitions, we propose the convolutional lstm ( convlstm ) and use it to build an end - to - end trainable model for	the goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time .	0	3	0	5.259086	-4.744855	0
90-5-10	experiments show that our convlstm network captures spatiotemporal correlations better and consistently outperforms fc-lstm and the state-of-theart operational rover algorithm for precipitation nowcasting .	the goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time .	0	4	0	5.546028	-4.9846983	0
90-5-10	very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective .	in this paper , we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences .	1	1	2	-5.7375293	5.227738	1
90-5-10	[CLS] by extending the fully connected lstm ( fc - lstm ) to have convolutional structures in both the input - to - state and state - to - state transitions, we propose the convolutional lstm ( convlstm ) and use it to build an end - to - end trainable model for the precipitation nowcasting problem	very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective .	0	3	1	5.6152186	-4.9932623	0
90-5-10	very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective .	experiments show that our convlstm network captures spatiotemporal correlations better and consistently outperforms fc-lstm and the state-of-theart operational rover algorithm for precipitation nowcasting .	1	1	4	-5.9478693	5.1898074	1
90-5-10	in this paper , we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences .	[CLS] by extending the fully connected lstm ( fc - lstm ) to have convolutional structures in both the input - to - state and state - to - state transitions, we propose the convolutional lstm ( convlstm ) and use	1	2	3	-5.917057	5.190048	1
90-5-10	in this paper , we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences .	experiments show that our convlstm network captures spatiotemporal correlations better and consistently outperforms fc-lstm and the state-of-theart operational rover algorithm for precipitation nowcasting .	1	2	4	-5.959708	5.0711985	1
90-5-10	experiments show that our convlstm network captures spatiotemporal correlations better and consistently outperforms fc-lstm and the state-of-theart operational rover algorithm for precipitation nowcasting .	[CLS] by extending the fully connected lstm ( fc - lstm ) to have convolutional structures in both the input - to - state and state - to - state transitions, we propose the convolutional lstm ( con	0	4	3	5.5584793	-4.9476013	0
91-7-21	screening rules leverage the known sparsity of the solution by ignoring some variables in the optimization , hence speeding up solvers .	high dimensional regression benefits from sparsity promoting regularizations .	0	1	0	5.6480875	-5.08356	0
91-7-21	high dimensional regression benefits from sparsity promoting regularizations .	when the procedure is proven not to discard features wrongly the rules are said to be safe .	1	0	2	-5.9692636	5.172263	1
91-7-21	in this paper we derive new safe rules for generalized linear models regularized with 1 and 1 { 2 norms .	high dimensional regression benefits from sparsity promoting regularizations .	0	3	0	5.188594	-4.5788784	0
91-7-21	the rules are based on duality gap computations and spherical safe regions whose diameters converge to zero .	high dimensional regression benefits from sparsity promoting regularizations .	0	4	0	5.5876727	-5.020033	0
91-7-21	high dimensional regression benefits from sparsity promoting regularizations .	this allows to discard safely more variables , in particular for low regularization parameters .	1	0	5	-5.9701047	5.073584	1
91-7-21	the gap safe rule can cope with any iterative solver and we illustrate its performance on coordinate descent for multi-task lasso , binary and multinomial logistic regression , demonstrating significant speed ups on all tested datasets with respect to previous safe rules .	high dimensional regression benefits from sparsity promoting regularizations .	0	6	0	5.585845	-4.9738064	0
91-7-21	screening rules leverage the known sparsity of the solution by ignoring some variables in the optimization , hence speeding up solvers .	when the procedure is proven not to discard features wrongly the rules are said to be safe .	1	1	2	-4.8581996	4.501054	1
91-7-21	screening rules leverage the known sparsity of the solution by ignoring some variables in the optimization , hence speeding up solvers .	in this paper we derive new safe rules for generalized linear models regularized with 1 and 1 { 2 norms .	1	1	3	-1.82017	2.0015383	1
91-7-21	screening rules leverage the known sparsity of the solution by ignoring some variables in the optimization , hence speeding up solvers .	the rules are based on duality gap computations and spherical safe regions whose diameters converge to zero .	1	1	4	-3.8926437	3.7839103	1
91-7-21	screening rules leverage the known sparsity of the solution by ignoring some variables in the optimization , hence speeding up solvers .	this allows to discard safely more variables , in particular for low regularization parameters .	1	1	5	-5.7715697	5.1305513	1
91-7-21	screening rules leverage the known sparsity of the solution by ignoring some variables in the optimization , hence speeding up solvers .	the gap safe rule can cope with any iterative solver and we illustrate its performance on coordinate descent for multi-task lasso , binary and multinomial logistic regression , demonstrating significant speed ups on all tested datasets with respect to previous safe rules .	1	1	6	-5.9716783	5.1313505	1
91-7-21	in this paper we derive new safe rules for generalized linear models regularized with 1 and 1 { 2 norms .	when the procedure is proven not to discard features wrongly the rules are said to be safe .	0	3	2	-5.215899	4.7596703	1
91-7-21	the rules are based on duality gap computations and spherical safe regions whose diameters converge to zero .	when the procedure is proven not to discard features wrongly the rules are said to be safe .	0	4	2	-3.6370723	3.4417055	1
91-7-21	this allows to discard safely more variables , in particular for low regularization parameters .	when the procedure is proven not to discard features wrongly the rules are said to be safe .	0	5	2	1.1801587	-0.9574559	0
91-7-21	the gap safe rule can cope with any iterative solver and we illustrate its performance on coordinate descent for multi-task lasso , binary and multinomial logistic regression , demonstrating significant speed ups on all tested datasets with respect to previous safe rules .	when the procedure is proven not to discard features wrongly the rules are said to be safe .	0	6	2	4.9676423	-4.479506	0
91-7-21	the rules are based on duality gap computations and spherical safe regions whose diameters converge to zero .	in this paper we derive new safe rules for generalized linear models regularized with 1 and 1 { 2 norms .	0	4	3	4.5101647	-4.1124454	0
91-7-21	in this paper we derive new safe rules for generalized linear models regularized with 1 and 1 { 2 norms .	this allows to discard safely more variables , in particular for low regularization parameters .	1	3	5	-5.5401697	4.9702253	1
91-7-21	the gap safe rule can cope with any iterative solver and we illustrate its performance on coordinate descent for multi-task lasso , binary and multinomial logistic regression , demonstrating significant speed ups on all tested datasets with respect to previous safe rules .	in this paper we derive new safe rules for generalized linear models regularized with 1 and 1 { 2 norms .	0	6	3	5.500003	-4.836864	0
91-7-21	the rules are based on duality gap computations and spherical safe regions whose diameters converge to zero .	this allows to discard safely more variables , in particular for low regularization parameters .	1	4	5	-4.526273	4.223179	1
91-7-21	the gap safe rule can cope with any iterative solver and we illustrate its performance on coordinate descent for multi-task lasso , binary and multinomial logistic regression , demonstrating significant speed ups on all tested datasets with respect to previous safe rules .	the rules are based on duality gap computations and spherical safe regions whose diameters converge to zero .	0	6	4	5.3986063	-4.741293	0
91-7-21	the gap safe rule can cope with any iterative solver and we illustrate its performance on coordinate descent for multi-task lasso , binary and multinomial logistic regression , demonstrating significant speed ups on all tested datasets with respect to previous safe rules .	this allows to discard safely more variables , in particular for low regularization parameters .	0	6	5	4.3659363	-3.9874794	0
92-4-6	the proposed estimator is derived from minimization of homogeneous divergence and can be constructed without calculation of the normalization constant , which is frequently infeasible for models in the discrete space .	in this paper , we propose a novel parameter estimator for probabilistic models on discrete space .	0	1	0	5.5356545	-4.9102798	0
92-4-6	we investigate statistical properties of the proposed estimator such as consistency and asymptotic normality , and reveal a relationship with the information geometry .	in this paper , we propose a novel parameter estimator for probabilistic models on discrete space .	0	2	0	5.6230564	-4.9988136	0
92-4-6	in this paper , we propose a novel parameter estimator for probabilistic models on discrete space .	some experiments show that the proposed estimator attains comparable performance to the maximum likelihood estimator with drastically lower computational cost .	1	0	3	-5.9860334	5.181415	1
92-4-6	we investigate statistical properties of the proposed estimator such as consistency and asymptotic normality , and reveal a relationship with the information geometry .	the proposed estimator is derived from minimization of homogeneous divergence and can be constructed without calculation of the normalization constant , which is frequently infeasible for models in the discrete space .	0	2	1	2.9407043	-2.7565382	0
92-4-6	the proposed estimator is derived from minimization of homogeneous divergence and can be constructed without calculation of the normalization constant , which is frequently infeasible for models in the discrete space .	some experiments show that the proposed estimator attains comparable performance to the maximum likelihood estimator with drastically lower computational cost .	1	1	3	-5.525839	4.990565	1
92-4-6	we investigate statistical properties of the proposed estimator such as consistency and asymptotic normality , and reveal a relationship with the information geometry .	some experiments show that the proposed estimator attains comparable performance to the maximum likelihood estimator with drastically lower computational cost .	1	2	3	-5.6446714	5.030365	1
93-5-10	we propose an exploratory approach to statistical model criticism using maximum mean discrepancy ( mmd ) two sample tests .	typical approaches to model criticism require a practitioner to select a statistic by which to measure discrepancies between data and a statistical model .	1	0	1	5.56141	-4.8829346	0
93-5-10	we propose an exploratory approach to statistical model criticism using maximum mean discrepancy ( mmd ) two sample tests .	mmd two sample tests are instead constructed as an analytic maximisation over a large space of possible statistics and therefore automatically select the statistic which most shows any discrepancy .	1	0	2	-5.817131	5.2100935	1
93-5-10	we demonstrate on synthetic data that the selected statistic , called the witness function , can be used to identify where a statistical model most misrepresents the data it was trained on .	we propose an exploratory approach to statistical model criticism using maximum mean discrepancy ( mmd ) two sample tests .	0	3	0	5.1446667	-4.465863	0
93-5-10	we then apply the procedure to real data where the models being assessed are restricted boltzmann machines , deep belief networks and gaussian process regression and demonstrate the ways in which these models fail to capture the properties of the data they are trained on .	we propose an exploratory approach to statistical model criticism using maximum mean discrepancy ( mmd ) two sample tests .	0	4	0	5.5589604	-4.9305696	0
93-5-10	typical approaches to model criticism require a practitioner to select a statistic by which to measure discrepancies between data and a statistical model .	mmd two sample tests are instead constructed as an analytic maximisation over a large space of possible statistics and therefore automatically select the statistic which most shows any discrepancy .	1	1	2	-5.8832684	5.1880608	1
93-5-10	we demonstrate on synthetic data that the selected statistic , called the witness function , can be used to identify where a statistical model most misrepresents the data it was trained on .	typical approaches to model criticism require a practitioner to select a statistic by which to measure discrepancies between data and a statistical model .	0	3	1	5.7609925	-5.1584835	0
93-5-10	we then apply the procedure to real data where the models being assessed are restricted boltzmann machines , deep belief networks and gaussian process regression and demonstrate the ways in which these models fail to capture the properties of the data they are trained on .	typical approaches to model criticism require a practitioner to select a statistic by which to measure discrepancies between data and a statistical model .	0	4	1	5.7406855	-5.1589565	0
93-5-10	we demonstrate on synthetic data that the selected statistic , called the witness function , can be used to identify where a statistical model most misrepresents the data it was trained on .	mmd two sample tests are instead constructed as an analytic maximisation over a large space of possible statistics and therefore automatically select the statistic which most shows any discrepancy .	0	3	2	-2.17992	2.2796946	1
93-5-10	mmd two sample tests are instead constructed as an analytic maximisation over a large space of possible statistics and therefore automatically select the statistic which most shows any discrepancy .	we then apply the procedure to real data where the models being assessed are restricted boltzmann machines , deep belief networks and gaussian process regression and demonstrate the ways in which these models fail to capture the properties of the data they are trained on .	1	2	4	-3.56004	3.3655524	1
93-5-10	we then apply the procedure to real data where the models being assessed are restricted boltzmann machines , deep belief networks and gaussian process regression and demonstrate the ways in which these models fail to capture the properties of the data they are trained on .	we demonstrate on synthetic data that the selected statistic , called the witness function , can be used to identify where a statistical model most misrepresents the data it was trained on .	0	4	3	3.9093778	-3.5778346	0
94-6-15	precision-recall analysis abounds in applications of binary classification where true negatives do not add value and hence should not affect assessment of the classifier 's performance .	perhaps inspired by the many advantages of receiver operating characteristic ( roc ) curves and the area under such curves for accuracybased performance assessment , many researchers have taken to report precisionrecall ( pr ) curves and associated areas as performance metric .	1	0	1	-4.939314	4.495709	1
94-6-15	we demonstrate in this paper that this practice is fraught with difficulties , mainly because of incoherent scale assumptions - e.g. , the area under a pr curve takes the arithmetic mean of precision values whereas the f score applies the harmonic mean .	precision-recall analysis abounds in applications of binary classification where true negatives do not add value and hence should not affect assessment of the classifier 's performance .	0	2	0	5.5628114	-5.0276756	0
94-6-15	precision-recall analysis abounds in applications of binary classification where true negatives do not add value and hence should not affect assessment of the classifier 's performance .	we show how to fix this by plotting pr curves in a different coordinate system , and demonstrate that the new precision-recall-gain curves inherit all key advantages of roc curves .	1	0	3	-5.940588	5.204961	1
94-6-15	[CLS] in particular, the area under precision - recall - gain curves conveys an expected f1 score on a harmonic scale, and the convex hull of a precisionrecall - gain curve allows us to calibrate the classifier's scores so as to determine, for each operating point on the convex hull, the	precision-recall analysis abounds in applications of binary classification where true negatives do not add value and hence should not affect assessment of the classifier 's performance .	0	4	0	5.5912194	-4.983369	0
94-6-15	precision-recall analysis abounds in applications of binary classification where true negatives do not add value and hence should not affect assessment of the classifier 's performance .	we demonstrate experimentally that the area under traditional pr curves can easily favour models with lower expected f1 score than others , and so the use of precision-recall-gain curves will result in better model selection .	1	0	5	-5.9640565	5.227294	1
94-6-15	[CLS] perhaps inspired by the many advantages of receiver operating characteristic ( roc ) curves and the area under such curves for accuracybased performance assessment, many researchers have taken to report precisionrecall ( pr ) curves and associated areas as performance metric. [SEP]	[CLS] we demonstrate in this paper that this practice is fraught with difficulties, mainly because of incoherent scale assumptions - e. g., the area under a pr curve takes the arithmetic mean of precision values whereas the f score applies the harmonic	1	1	2	-5.9996986	5.1458035	1
94-6-15	we show how to fix this by plotting pr curves in a different coordinate system , and demonstrate that the new precision-recall-gain curves inherit all key advantages of roc curves .	perhaps inspired by the many advantages of receiver operating characteristic ( roc ) curves and the area under such curves for accuracybased performance assessment , many researchers have taken to report precisionrecall ( pr ) curves and associated areas as performance metric .	0	3	1	5.6586657	-5.0304413	0
94-6-15	[CLS] in particular, the area under precision - recall - gain curves conveys an expected f1 score on a harmonic scale, and the convex hull of a precisionrecall - gain curve allows us to calibrate the classifier's scores so	[CLS] perhaps inspired by the many advantages of receiver operating characteristic ( roc ) curves and the area under such curves for accuracybased performance assessment, many researchers have taken to report precisionrecall ( pr ) curves and associated areas as performance metric. [SEP]	0	4	1	4.5331907	-3.987544	0
94-6-15	we demonstrate experimentally that the area under traditional pr curves can easily favour models with lower expected f1 score than others , and so the use of precision-recall-gain curves will result in better model selection .	perhaps inspired by the many advantages of receiver operating characteristic ( roc ) curves and the area under such curves for accuracybased performance assessment , many researchers have taken to report precisionrecall ( pr ) curves and associated areas as performance metric .	0	5	1	5.6577034	-5.0610914	0
94-6-15	we demonstrate in this paper that this practice is fraught with difficulties , mainly because of incoherent scale assumptions - e.g. , the area under a pr curve takes the arithmetic mean of precision values whereas the f score applies the harmonic mean .	we show how to fix this by plotting pr curves in a different coordinate system , and demonstrate that the new precision-recall-gain curves inherit all key advantages of roc curves .	1	2	3	-5.993884	5.131975	1
94-6-15	[CLS] we demonstrate in this paper that this practice is fraught with difficulties, mainly because of incoherent scale assumptions - e. g., the area under a pr curve takes the arithmetic mean of precision values whereas the f score applies the harmonic	[CLS] in particular, the area under precision - recall - gain curves conveys an expected f1 score on a harmonic scale, and the convex hull of a precisionrecall - gain curve allows us to calibrate the classifier's scores so	1	2	4	-4.219197	3.9858131	1
94-6-15	we demonstrate in this paper that this practice is fraught with difficulties , mainly because of incoherent scale assumptions - e.g. , the area under a pr curve takes the arithmetic mean of precision values whereas the f score applies the harmonic mean .	we demonstrate experimentally that the area under traditional pr curves can easily favour models with lower expected f1 score than others , and so the use of precision-recall-gain curves will result in better model selection .	1	2	5	-6.002903	5.1051702	1
94-6-15	we show how to fix this by plotting pr curves in a different coordinate system , and demonstrate that the new precision-recall-gain curves inherit all key advantages of roc curves .	[CLS] in particular, the area under precision - recall - gain curves conveys an expected f1 score on a harmonic scale, and the convex hull of a precisionrecall - gain curve allows us to calibrate the classifier's scores so as to determine, for each operating point on the convex hull,	1	3	4	3.8988652	-3.577762	0
94-6-15	we demonstrate experimentally that the area under traditional pr curves can easily favour models with lower expected f1 score than others , and so the use of precision-recall-gain curves will result in better model selection .	we show how to fix this by plotting pr curves in a different coordinate system , and demonstrate that the new precision-recall-gain curves inherit all key advantages of roc curves .	0	5	3	4.534897	-4.131046	0
94-6-15	we demonstrate experimentally that the area under traditional pr curves can easily favour models with lower expected f1 score than others , and so the use of precision-recall-gain curves will result in better model selection .	[CLS] in particular, the area under precision - recall - gain curves conveys an expected f1 score on a harmonic scale, and the convex hull of a precisionrecall - gain curve allows us to calibrate the classifier's scores so as to determine, for each operating	0	5	4	4.8140182	-4.3050094	0
95-7-21	we are motivated by real scenarios in machine learning that can not be captured by ( traditional ) submodular set functions .	we consider a generalization of the submodular cover problem based on the concept of diminishing return property on the integer lattice .	0	1	0	-3.662373	3.567118	1
95-7-21	we show that the generalized submodular cover problem can be applied to various problems and devise a bicriteria approximation algorithm .	we consider a generalization of the submodular cover problem based on the concept of diminishing return property on the integer lattice .	0	2	0	5.4200864	-4.786866	0
95-7-21	our algorithm is guaranteed to output a log-factor approximate solution that satisfies the constraints with the desired accuracy .	we consider a generalization of the submodular cover problem based on the concept of diminishing return property on the integer lattice .	0	3	0	5.109388	-4.5456285	0
95-7-21	we consider a generalization of the submodular cover problem based on the concept of diminishing return property on the integer lattice .	the running time of our algorithm is roughly o ( n log ( nr ) log r ) , where n is the size of the ground set and r is the maximum value of a coordinate .	1	0	4	-5.908337	5.2365913	1
95-7-21	the dependency on r is exponentially better than the naive reduction algorithms .	we consider a generalization of the submodular cover problem based on the concept of diminishing return property on the integer lattice .	0	5	0	5.42022	-4.8271136	0
95-7-21	several experiments on real and artificial datasets demonstrate that the solution quality of our algorithm is comparable to naive algorithms , while the running time is several orders of magnitude faster .	we consider a generalization of the submodular cover problem based on the concept of diminishing return property on the integer lattice .	0	6	0	5.5998755	-4.929045	0
95-7-21	we are motivated by real scenarios in machine learning that can not be captured by ( traditional ) submodular set functions .	we show that the generalized submodular cover problem can be applied to various problems and devise a bicriteria approximation algorithm .	1	1	2	-5.967455	5.0871797	1
95-7-21	we are motivated by real scenarios in machine learning that can not be captured by ( traditional ) submodular set functions .	our algorithm is guaranteed to output a log-factor approximate solution that satisfies the constraints with the desired accuracy .	1	1	3	-5.6465244	5.16858	1
95-7-21	the running time of our algorithm is roughly o ( n log ( nr ) log r ) , where n is the size of the ground set and r is the maximum value of a coordinate .	we are motivated by real scenarios in machine learning that can not be captured by ( traditional ) submodular set functions .	0	4	1	5.067575	-4.5820627	0
95-7-21	we are motivated by real scenarios in machine learning that can not be captured by ( traditional ) submodular set functions .	the dependency on r is exponentially better than the naive reduction algorithms .	1	1	5	-5.9033775	5.1937385	1
95-7-21	several experiments on real and artificial datasets demonstrate that the solution quality of our algorithm is comparable to naive algorithms , while the running time is several orders of magnitude faster .	we are motivated by real scenarios in machine learning that can not be captured by ( traditional ) submodular set functions .	0	6	1	5.5809107	-4.977302	0
95-7-21	our algorithm is guaranteed to output a log-factor approximate solution that satisfies the constraints with the desired accuracy .	we show that the generalized submodular cover problem can be applied to various problems and devise a bicriteria approximation algorithm .	0	3	2	-3.9867995	3.8546822	1
95-7-21	we show that the generalized submodular cover problem can be applied to various problems and devise a bicriteria approximation algorithm .	the running time of our algorithm is roughly o ( n log ( nr ) log r ) , where n is the size of the ground set and r is the maximum value of a coordinate .	1	2	4	-2.1486394	2.1601586	1
95-7-21	we show that the generalized submodular cover problem can be applied to various problems and devise a bicriteria approximation algorithm .	the dependency on r is exponentially better than the naive reduction algorithms .	1	2	5	2.6815891	-2.5839906	0
95-7-21	several experiments on real and artificial datasets demonstrate that the solution quality of our algorithm is comparable to naive algorithms , while the running time is several orders of magnitude faster .	we show that the generalized submodular cover problem can be applied to various problems and devise a bicriteria approximation algorithm .	0	6	2	4.934947	-4.4364977	0
95-7-21	the running time of our algorithm is roughly o ( n log ( nr ) log r ) , where n is the size of the ground set and r is the maximum value of a coordinate .	our algorithm is guaranteed to output a log-factor approximate solution that satisfies the constraints with the desired accuracy .	0	4	3	3.9246237	-3.6114826	0
95-7-21	our algorithm is guaranteed to output a log-factor approximate solution that satisfies the constraints with the desired accuracy .	the dependency on r is exponentially better than the naive reduction algorithms .	1	3	5	-4.3808618	4.0476284	1
95-7-21	our algorithm is guaranteed to output a log-factor approximate solution that satisfies the constraints with the desired accuracy .	several experiments on real and artificial datasets demonstrate that the solution quality of our algorithm is comparable to naive algorithms , while the running time is several orders of magnitude faster .	1	3	6	-5.9419675	5.2121143	1
95-7-21	the dependency on r is exponentially better than the naive reduction algorithms .	the running time of our algorithm is roughly o ( n log ( nr ) log r ) , where n is the size of the ground set and r is the maximum value of a coordinate .	0	5	4	1.4053986	-1.3066604	0
95-7-21	several experiments on real and artificial datasets demonstrate that the solution quality of our algorithm is comparable to naive algorithms , while the running time is several orders of magnitude faster .	the running time of our algorithm is roughly o ( n log ( nr ) log r ) , where n is the size of the ground set and r is the maximum value of a coordinate .	0	6	4	4.984791	-4.464487	0
95-7-21	several experiments on real and artificial datasets demonstrate that the solution quality of our algorithm is comparable to naive algorithms , while the running time is several orders of magnitude faster .	the dependency on r is exponentially better than the naive reduction algorithms .	0	6	5	3.867454	-3.6426394	0
96-9-36	bidirectional recurrent neural networks ( rnn ) are trained to predict both in the positive and negative time directions simultaneously .	they have not been used commonly in unsupervised tasks , because a probabilistic interpretation of the model has been difficult .	1	0	1	-4.630162	4.272727	1
96-9-36	bidirectional recurrent neural networks ( rnn ) are trained to predict both in the positive and negative time directions simultaneously .	recently , two different frameworks , gsn and nade , provide a connection between reconstruction and probabilistic modeling , which makes the interpretation possible .	1	0	2	-2.1339166	2.3113961	1
96-9-36	bidirectional recurrent neural networks ( rnn ) are trained to predict both in the positive and negative time directions simultaneously .	as far as we know , neither gsn or nade have been studied in the context of time series before .	1	0	3	-4.961686	4.494961	1
96-9-36	bidirectional recurrent neural networks ( rnn ) are trained to predict both in the positive and negative time directions simultaneously .	as an example of an unsupervised task , we study the problem of filling in gaps in high-dimensional time series with complex dynamics .	1	0	4	5.2202973	-4.723748	0
96-9-36	bidirectional recurrent neural networks ( rnn ) are trained to predict both in the positive and negative time directions simultaneously .	although unidirectional rnns have recently been trained successfully to model such time series , inference in the negative time direction is non-trivial .	1	0	5	-5.805601	5.1864715	1
96-9-36	bidirectional recurrent neural networks ( rnn ) are trained to predict both in the positive and negative time directions simultaneously .	we propose two probabilistic interpretations of bidirectional rnns that can be used to reconstruct missing gaps efficiently .	1	0	6	-5.987081	5.118161	1
96-9-36	bidirectional recurrent neural networks ( rnn ) are trained to predict both in the positive and negative time directions simultaneously .	our experiments on text data show that both proposed methods are much more accurate than unidirectional reconstructions , although a bit less accurate than a computationally complex bidirectional bayesian inference on the unidirectional rnn .	1	0	7	-5.975706	5.1074934	1
96-9-36	we also provide results on music data for which the bayesian inference is computationally infeasible , demonstrating the scalability of the proposed methods .	bidirectional recurrent neural networks ( rnn ) are trained to predict both in the positive and negative time directions simultaneously .	0	8	0	5.405277	-4.754919	0
96-9-36	they have not been used commonly in unsupervised tasks , because a probabilistic interpretation of the model has been difficult .	recently , two different frameworks , gsn and nade , provide a connection between reconstruction and probabilistic modeling , which makes the interpretation possible .	1	1	2	-2.0501137	2.172068	1
96-9-36	as far as we know , neither gsn or nade have been studied in the context of time series before .	they have not been used commonly in unsupervised tasks , because a probabilistic interpretation of the model has been difficult .	0	3	1	0.12723361	0.11802448	0
96-9-36	as an example of an unsupervised task , we study the problem of filling in gaps in high-dimensional time series with complex dynamics .	they have not been used commonly in unsupervised tasks , because a probabilistic interpretation of the model has been difficult .	0	4	1	-2.9527745	3.0360448	1
96-9-36	although unidirectional rnns have recently been trained successfully to model such time series , inference in the negative time direction is non-trivial .	they have not been used commonly in unsupervised tasks , because a probabilistic interpretation of the model has been difficult .	0	5	1	-3.902948	3.6988084	1
96-9-36	they have not been used commonly in unsupervised tasks , because a probabilistic interpretation of the model has been difficult .	we propose two probabilistic interpretations of bidirectional rnns that can be used to reconstruct missing gaps efficiently .	1	1	6	-5.5326343	5.0731583	1
96-9-36	they have not been used commonly in unsupervised tasks , because a probabilistic interpretation of the model has been difficult .	our experiments on text data show that both proposed methods are much more accurate than unidirectional reconstructions , although a bit less accurate than a computationally complex bidirectional bayesian inference on the unidirectional rnn .	1	1	7	-6.0352716	5.17694	1
96-9-36	they have not been used commonly in unsupervised tasks , because a probabilistic interpretation of the model has been difficult .	we also provide results on music data for which the bayesian inference is computationally infeasible , demonstrating the scalability of the proposed methods .	1	1	8	-5.962887	5.1203103	1
96-9-36	as far as we know , neither gsn or nade have been studied in the context of time series before .	recently , two different frameworks , gsn and nade , provide a connection between reconstruction and probabilistic modeling , which makes the interpretation possible .	0	3	2	4.346717	-3.8515358	0
96-9-36	as an example of an unsupervised task , we study the problem of filling in gaps in high-dimensional time series with complex dynamics .	recently , two different frameworks , gsn and nade , provide a connection between reconstruction and probabilistic modeling , which makes the interpretation possible .	0	4	2	-5.3753676	4.9356346	1
96-9-36	although unidirectional rnns have recently been trained successfully to model such time series , inference in the negative time direction is non-trivial .	recently , two different frameworks , gsn and nade , provide a connection between reconstruction and probabilistic modeling , which makes the interpretation possible .	0	5	2	-2.308371	2.474001	1
96-9-36	we propose two probabilistic interpretations of bidirectional rnns that can be used to reconstruct missing gaps efficiently .	recently , two different frameworks , gsn and nade , provide a connection between reconstruction and probabilistic modeling , which makes the interpretation possible .	0	6	2	2.9481943	-2.632632	0
96-9-36	our experiments on text data show that both proposed methods are much more accurate than unidirectional reconstructions , although a bit less accurate than a computationally complex bidirectional bayesian inference on the unidirectional rnn .	recently , two different frameworks , gsn and nade , provide a connection between reconstruction and probabilistic modeling , which makes the interpretation possible .	0	7	2	5.5863237	-4.930854	0
96-9-36	we also provide results on music data for which the bayesian inference is computationally infeasible , demonstrating the scalability of the proposed methods .	recently , two different frameworks , gsn and nade , provide a connection between reconstruction and probabilistic modeling , which makes the interpretation possible .	0	8	2	5.5008974	-4.884473	0
96-9-36	as an example of an unsupervised task , we study the problem of filling in gaps in high-dimensional time series with complex dynamics .	as far as we know , neither gsn or nade have been studied in the context of time series before .	0	4	3	-5.785198	5.1023035	1
96-9-36	as far as we know , neither gsn or nade have been studied in the context of time series before .	although unidirectional rnns have recently been trained successfully to model such time series , inference in the negative time direction is non-trivial .	1	3	5	2.8592696	-2.3811178	0
96-9-36	we propose two probabilistic interpretations of bidirectional rnns that can be used to reconstruct missing gaps efficiently .	as far as we know , neither gsn or nade have been studied in the context of time series before .	0	6	3	3.3481827	-3.099121	0
96-9-36	our experiments on text data show that both proposed methods are much more accurate than unidirectional reconstructions , although a bit less accurate than a computationally complex bidirectional bayesian inference on the unidirectional rnn .	as far as we know , neither gsn or nade have been studied in the context of time series before .	0	7	3	5.1422553	-4.5657268	0
96-9-36	we also provide results on music data for which the bayesian inference is computationally infeasible , demonstrating the scalability of the proposed methods .	as far as we know , neither gsn or nade have been studied in the context of time series before .	0	8	3	5.4174714	-4.795504	0
96-9-36	although unidirectional rnns have recently been trained successfully to model such time series , inference in the negative time direction is non-trivial .	as an example of an unsupervised task , we study the problem of filling in gaps in high-dimensional time series with complex dynamics .	0	5	4	3.7213726	-3.3942041	0
96-9-36	as an example of an unsupervised task , we study the problem of filling in gaps in high-dimensional time series with complex dynamics .	we propose two probabilistic interpretations of bidirectional rnns that can be used to reconstruct missing gaps efficiently .	1	4	6	-5.902595	5.22025	1
96-9-36	our experiments on text data show that both proposed methods are much more accurate than unidirectional reconstructions , although a bit less accurate than a computationally complex bidirectional bayesian inference on the unidirectional rnn .	as an example of an unsupervised task , we study the problem of filling in gaps in high-dimensional time series with complex dynamics .	0	7	4	5.56902	-5.0125313	0
96-9-36	as an example of an unsupervised task , we study the problem of filling in gaps in high-dimensional time series with complex dynamics .	we also provide results on music data for which the bayesian inference is computationally infeasible , demonstrating the scalability of the proposed methods .	1	4	8	-5.9667783	5.1453466	1
96-9-36	although unidirectional rnns have recently been trained successfully to model such time series , inference in the negative time direction is non-trivial .	we propose two probabilistic interpretations of bidirectional rnns that can be used to reconstruct missing gaps efficiently .	1	5	6	-5.8900576	5.2513075	1
96-9-36	although unidirectional rnns have recently been trained successfully to model such time series , inference in the negative time direction is non-trivial .	our experiments on text data show that both proposed methods are much more accurate than unidirectional reconstructions , although a bit less accurate than a computationally complex bidirectional bayesian inference on the unidirectional rnn .	1	5	7	-6.008068	5.183111	1
96-9-36	we also provide results on music data for which the bayesian inference is computationally infeasible , demonstrating the scalability of the proposed methods .	although unidirectional rnns have recently been trained successfully to model such time series , inference in the negative time direction is non-trivial .	0	8	5	5.5278134	-4.916215	0
96-9-36	our experiments on text data show that both proposed methods are much more accurate than unidirectional reconstructions , although a bit less accurate than a computationally complex bidirectional bayesian inference on the unidirectional rnn .	we propose two probabilistic interpretations of bidirectional rnns that can be used to reconstruct missing gaps efficiently .	0	7	6	5.401999	-4.826479	0
96-9-36	we propose two probabilistic interpretations of bidirectional rnns that can be used to reconstruct missing gaps efficiently .	we also provide results on music data for which the bayesian inference is computationally infeasible , demonstrating the scalability of the proposed methods .	1	6	8	-5.967616	5.12649	1
96-9-36	we also provide results on music data for which the bayesian inference is computationally infeasible , demonstrating the scalability of the proposed methods .	our experiments on text data show that both proposed methods are much more accurate than unidirectional reconstructions , although a bit less accurate than a computationally complex bidirectional bayesian inference on the unidirectional rnn .	0	8	7	4.2440743	-3.8163805	0
97-7-21	we study the problem of minimizing the average of a large number of smooth convex functions penalized with a strongly convex regularizer .	we propose and analyze a novel primal-dual method ( quartz ) which at every iteration samples and updates a random subset of the dual variables , chosen according to an arbitrary distribution .	1	0	1	-5.8768535	5.1830764	1
97-7-21	in contrast to typical analysis , we directly bound the decrease of the primal-dual error ( in expectation ) , without the need to first analyze the dual error .	we study the problem of minimizing the average of a large number of smooth convex functions penalized with a strongly convex regularizer .	0	2	0	5.5699916	-4.970005	0
97-7-21	depending on the choice of the sampling , we obtain efficient serial and mini-batch variants of the method .	we study the problem of minimizing the average of a large number of smooth convex functions penalized with a strongly convex regularizer .	0	3	0	5.6294594	-5.04634	0
97-7-21	we study the problem of minimizing the average of a large number of smooth convex functions penalized with a strongly convex regularizer .	in the serial case , our bounds match the best known bounds for sdca ( both with uniform and importance sampling ) .	1	0	4	-5.930067	5.1822824	1
97-7-21	with standard mini-batching , our bounds predict initial data-independent speedup as well as additional data-driven speedup which depends on spectral and sparsity properties of the data .	we study the problem of minimizing the average of a large number of smooth convex functions penalized with a strongly convex regularizer .	0	5	0	5.6260257	-5.0060616	0
97-7-21	keywords : empirical risk minimization , dual coordinate ascent , arbitrary sampling , data-driven speedup .	we study the problem of minimizing the average of a large number of smooth convex functions penalized with a strongly convex regularizer .	0	6	0	5.563844	-5.0214105	0
97-7-21	we propose and analyze a novel primal-dual method ( quartz ) which at every iteration samples and updates a random subset of the dual variables , chosen according to an arbitrary distribution .	in contrast to typical analysis , we directly bound the decrease of the primal-dual error ( in expectation ) , without the need to first analyze the dual error .	1	1	2	-5.6376324	5.074977	1
97-7-21	depending on the choice of the sampling , we obtain efficient serial and mini-batch variants of the method .	we propose and analyze a novel primal-dual method ( quartz ) which at every iteration samples and updates a random subset of the dual variables , chosen according to an arbitrary distribution .	0	3	1	5.5522394	-4.86427	0
97-7-21	in the serial case , our bounds match the best known bounds for sdca ( both with uniform and importance sampling ) .	we propose and analyze a novel primal-dual method ( quartz ) which at every iteration samples and updates a random subset of the dual variables , chosen according to an arbitrary distribution .	0	4	1	5.3458357	-4.71054	0
97-7-21	with standard mini-batching , our bounds predict initial data-independent speedup as well as additional data-driven speedup which depends on spectral and sparsity properties of the data .	we propose and analyze a novel primal-dual method ( quartz ) which at every iteration samples and updates a random subset of the dual variables , chosen according to an arbitrary distribution .	0	5	1	5.02855	-4.4062266	0
97-7-21	keywords : empirical risk minimization , dual coordinate ascent , arbitrary sampling , data-driven speedup .	we propose and analyze a novel primal-dual method ( quartz ) which at every iteration samples and updates a random subset of the dual variables , chosen according to an arbitrary distribution .	0	6	1	5.4202747	-4.8836956	0
97-7-21	in contrast to typical analysis , we directly bound the decrease of the primal-dual error ( in expectation ) , without the need to first analyze the dual error .	depending on the choice of the sampling , we obtain efficient serial and mini-batch variants of the method .	1	2	3	-5.5201163	4.971409	1
97-7-21	in the serial case , our bounds match the best known bounds for sdca ( both with uniform and importance sampling ) .	in contrast to typical analysis , we directly bound the decrease of the primal-dual error ( in expectation ) , without the need to first analyze the dual error .	0	4	2	4.5048766	-4.10436	0
97-7-21	with standard mini-batching , our bounds predict initial data-independent speedup as well as additional data-driven speedup which depends on spectral and sparsity properties of the data .	in contrast to typical analysis , we directly bound the decrease of the primal-dual error ( in expectation ) , without the need to first analyze the dual error .	0	5	2	4.0052795	-3.6276023	0
97-7-21	keywords : empirical risk minimization , dual coordinate ascent , arbitrary sampling , data-driven speedup .	in contrast to typical analysis , we directly bound the decrease of the primal-dual error ( in expectation ) , without the need to first analyze the dual error .	0	6	2	5.20893	-4.722734	0
97-7-21	depending on the choice of the sampling , we obtain efficient serial and mini-batch variants of the method .	in the serial case , our bounds match the best known bounds for sdca ( both with uniform and importance sampling ) .	1	3	4	-2.5158658	2.5194468	1
97-7-21	with standard mini-batching , our bounds predict initial data-independent speedup as well as additional data-driven speedup which depends on spectral and sparsity properties of the data .	depending on the choice of the sampling , we obtain efficient serial and mini-batch variants of the method .	0	5	3	-4.8604145	4.545045	1
97-7-21	depending on the choice of the sampling , we obtain efficient serial and mini-batch variants of the method .	keywords : empirical risk minimization , dual coordinate ascent , arbitrary sampling , data-driven speedup .	1	3	6	-5.4758687	4.889936	1
97-7-21	in the serial case , our bounds match the best known bounds for sdca ( both with uniform and importance sampling ) .	with standard mini-batching , our bounds predict initial data-independent speedup as well as additional data-driven speedup which depends on spectral and sparsity properties of the data .	1	4	5	3.5410166	-3.2845712	0
97-7-21	keywords : empirical risk minimization , dual coordinate ascent , arbitrary sampling , data-driven speedup .	in the serial case , our bounds match the best known bounds for sdca ( both with uniform and importance sampling ) .	0	6	4	3.8439827	-3.63337	0
97-7-21	keywords : empirical risk minimization , dual coordinate ascent , arbitrary sampling , data-driven speedup .	with standard mini-batching , our bounds predict initial data-independent speedup as well as additional data-driven speedup which depends on spectral and sparsity properties of the data .	0	6	5	5.0368676	-4.5656734	0
98-8-28	inference is typically intractable in high-treewidth undirected graphical models , making maximum likelihood learning a challenge .	one way to overcome this is to restrict parameters to a tractable set , most typically the set of tree-structured parameters .	1	0	1	-5.5284786	5.0782213	1
98-8-28	inference is typically intractable in high-treewidth undirected graphical models , making maximum likelihood learning a challenge .	this paper explores an alternative notion of a tractable set , namely a set of `` fast-mixing parameters '' where markov chain monte carlo ( mcmc ) inference can be guaranteed to quickly converge to the stationary distribution .	1	0	2	-5.210766	4.8780127	1
98-8-28	while it is common in practice to approximate the likelihood gradient using samples obtained from mcmc , such procedures lack theoretical guarantees .	inference is typically intractable in high-treewidth undirected graphical models , making maximum likelihood learning a challenge .	0	3	0	4.8741612	-4.2022095	0
98-8-28	inference is typically intractable in high-treewidth undirected graphical models , making maximum likelihood learning a challenge .	this paper proves that for any exponential family with bounded sufficient statistics , ( not just graphical models ) when parameters are constrained to a fast-mixing set , gradient descent with gradients approximated by sampling will approximate the maximum likelihood solution inside the set with high-probability .	1	0	4	-5.707801	5.177436	1
98-8-28	when unregularized , to find a solution -accurate in log-likelihood requires a total amount of effort cubic in 1/ , disregarding logarithmic factors .	inference is typically intractable in high-treewidth undirected graphical models , making maximum likelihood learning a challenge .	0	5	0	5.588036	-4.7902126	0
98-8-28	inference is typically intractable in high-treewidth undirected graphical models , making maximum likelihood learning a challenge .	when ridge-regularized , strong convexity allows a solution -accurate in parameter distance with effort quadratic in 1/ .	1	0	6	-5.8714237	5.218321	1
98-8-28	both of these provide of a fully-polynomial time randomized approximation scheme .	inference is typically intractable in high-treewidth undirected graphical models , making maximum likelihood learning a challenge .	0	7	0	5.589484	-4.9406166	0
98-8-28	this paper explores an alternative notion of a tractable set , namely a set of `` fast-mixing parameters '' where markov chain monte carlo ( mcmc ) inference can be guaranteed to quickly converge to the stationary distribution .	one way to overcome this is to restrict parameters to a tractable set , most typically the set of tree-structured parameters .	0	2	1	3.571984	-3.2822824	0
98-8-28	one way to overcome this is to restrict parameters to a tractable set , most typically the set of tree-structured parameters .	while it is common in practice to approximate the likelihood gradient using samples obtained from mcmc , such procedures lack theoretical guarantees .	1	1	3	4.3434963	-3.8943827	0
98-8-28	this paper proves that for any exponential family with bounded sufficient statistics , ( not just graphical models ) when parameters are constrained to a fast-mixing set , gradient descent with gradients approximated by sampling will approximate the maximum likelihood solution inside the set with high-probability .	one way to overcome this is to restrict parameters to a tractable set , most typically the set of tree-structured parameters .	0	4	1	4.0748425	-3.7501402	0
98-8-28	when unregularized , to find a solution -accurate in log-likelihood requires a total amount of effort cubic in 1/ , disregarding logarithmic factors .	one way to overcome this is to restrict parameters to a tractable set , most typically the set of tree-structured parameters .	0	5	1	3.4265609	-3.1445131	0
98-8-28	one way to overcome this is to restrict parameters to a tractable set , most typically the set of tree-structured parameters .	when ridge-regularized , strong convexity allows a solution -accurate in parameter distance with effort quadratic in 1/ .	1	1	6	-5.4916124	5.0115247	1
98-8-28	one way to overcome this is to restrict parameters to a tractable set , most typically the set of tree-structured parameters .	both of these provide of a fully-polynomial time randomized approximation scheme .	1	1	7	-6.004141	5.1395535	1
98-8-28	this paper explores an alternative notion of a tractable set , namely a set of `` fast-mixing parameters '' where markov chain monte carlo ( mcmc ) inference can be guaranteed to quickly converge to the stationary distribution .	while it is common in practice to approximate the likelihood gradient using samples obtained from mcmc , such procedures lack theoretical guarantees .	1	2	3	-2.8955166	2.93917	1
98-8-28	this paper explores an alternative notion of a tractable set , namely a set of `` fast-mixing parameters '' where markov chain monte carlo ( mcmc ) inference can be guaranteed to quickly converge to the stationary distribution .	[CLS] this paper proves that for any exponential family with bounded sufficient statistics, ( not just graphical models ) when parameters are constrained to a fast - mixing set, gradient descent with gradients approximated by sampling will approximate the maximum likelihood solution inside the set with high -	1	2	4	-5.5431333	5.0299144	1
98-8-28	this paper explores an alternative notion of a tractable set , namely a set of `` fast-mixing parameters '' where markov chain monte carlo ( mcmc ) inference can be guaranteed to quickly converge to the stationary distribution .	when unregularized , to find a solution -accurate in log-likelihood requires a total amount of effort cubic in 1/ , disregarding logarithmic factors .	1	2	5	-4.7094994	4.311102	1
98-8-28	when ridge-regularized , strong convexity allows a solution -accurate in parameter distance with effort quadratic in 1/ .	this paper explores an alternative notion of a tractable set , namely a set of `` fast-mixing parameters '' where markov chain monte carlo ( mcmc ) inference can be guaranteed to quickly converge to the stationary distribution .	0	6	2	4.809779	-4.3575954	0
98-8-28	both of these provide of a fully-polynomial time randomized approximation scheme .	this paper explores an alternative notion of a tractable set , namely a set of `` fast-mixing parameters '' where markov chain monte carlo ( mcmc ) inference can be guaranteed to quickly converge to the stationary distribution .	0	7	2	5.52081	-4.8639317	0
98-8-28	while it is common in practice to approximate the likelihood gradient using samples obtained from mcmc , such procedures lack theoretical guarantees .	this paper proves that for any exponential family with bounded sufficient statistics , ( not just graphical models ) when parameters are constrained to a fast-mixing set , gradient descent with gradients approximated by sampling will approximate the maximum likelihood solution inside the set with high-probability .	1	3	4	-4.3656216	4.0485907	1
98-8-28	when unregularized , to find a solution -accurate in log-likelihood requires a total amount of effort cubic in 1/ , disregarding logarithmic factors .	while it is common in practice to approximate the likelihood gradient using samples obtained from mcmc , such procedures lack theoretical guarantees .	0	5	3	5.1036315	-4.441244	0
98-8-28	when ridge-regularized , strong convexity allows a solution -accurate in parameter distance with effort quadratic in 1/ .	while it is common in practice to approximate the likelihood gradient using samples obtained from mcmc , such procedures lack theoretical guarantees .	0	6	3	4.4752464	-4.0033092	0
98-8-28	both of these provide of a fully-polynomial time randomized approximation scheme .	while it is common in practice to approximate the likelihood gradient using samples obtained from mcmc , such procedures lack theoretical guarantees .	0	7	3	5.568803	-4.922285	0
98-8-28	when unregularized , to find a solution -accurate in log-likelihood requires a total amount of effort cubic in 1/ , disregarding logarithmic factors .	this paper proves that for any exponential family with bounded sufficient statistics , ( not just graphical models ) when parameters are constrained to a fast-mixing set , gradient descent with gradients approximated by sampling will approximate the maximum likelihood solution inside the set with high-probability .	0	5	4	4.457346	-3.986002	0
98-8-28	this paper proves that for any exponential family with bounded sufficient statistics , ( not just graphical models ) when parameters are constrained to a fast-mixing set , gradient descent with gradients approximated by sampling will approximate the maximum likelihood solution inside the set with high-probability .	when ridge-regularized , strong convexity allows a solution -accurate in parameter distance with effort quadratic in 1/ .	1	4	6	-4.0664444	3.7981453	1
98-8-28	this paper proves that for any exponential family with bounded sufficient statistics , ( not just graphical models ) when parameters are constrained to a fast-mixing set , gradient descent with gradients approximated by sampling will approximate the maximum likelihood solution inside the set with high-probability .	both of these provide of a fully-polynomial time randomized approximation scheme .	1	4	7	-5.9688077	5.1882	1
98-8-28	when ridge-regularized , strong convexity allows a solution -accurate in parameter distance with effort quadratic in 1/ .	when unregularized , to find a solution -accurate in log-likelihood requires a total amount of effort cubic in 1/ , disregarding logarithmic factors .	0	6	5	-3.7423952	3.494151	1
98-8-28	both of these provide of a fully-polynomial time randomized approximation scheme .	when unregularized , to find a solution -accurate in log-likelihood requires a total amount of effort cubic in 1/ , disregarding logarithmic factors .	0	7	5	2.6486177	-2.5330963	0
98-8-28	both of these provide of a fully-polynomial time randomized approximation scheme .	when ridge-regularized , strong convexity allows a solution -accurate in parameter distance with effort quadratic in 1/ .	0	7	6	2.6530378	-2.5490808	0
99-5-10	the performance of an mdrnn is improved by further increasing its depth , and the difficulty of learning the deeper network is overcome by using hessian-free ( hf ) optimization .	multidimensional recurrent neural networks ( mdrnns ) have shown a remarkable performance in the area of speech and handwriting recognition .	0	1	0	5.6396403	-5.0284014	0
99-5-10	given that connectionist temporal classification ( ctc ) is utilized as an objective of learning an mdrnn for sequence labeling , the non-convexity of ctc poses a problem when applying hf to the network .	multidimensional recurrent neural networks ( mdrnns ) have shown a remarkable performance in the area of speech and handwriting recognition .	0	2	0	5.7493963	-5.0356092	0
99-5-10	as a solution , a convex approximation of ctc is formulated and its relationship with the em algorithm and the fisher information matrix is discussed .	multidimensional recurrent neural networks ( mdrnns ) have shown a remarkable performance in the area of speech and handwriting recognition .	0	3	0	5.660343	-5.088977	0
99-5-10	an mdrnn up to a depth of 15 layers is successfully trained using hf , resulting in an improved performance for sequence labeling .	multidimensional recurrent neural networks ( mdrnns ) have shown a remarkable performance in the area of speech and handwriting recognition .	0	4	0	5.654315	-5.0498705	0
99-5-10	the performance of an mdrnn is improved by further increasing its depth , and the difficulty of learning the deeper network is overcome by using hessian-free ( hf ) optimization .	given that connectionist temporal classification ( ctc ) is utilized as an objective of learning an mdrnn for sequence labeling , the non-convexity of ctc poses a problem when applying hf to the network .	1	1	2	5.2401686	-4.7028875	0
99-5-10	the performance of an mdrnn is improved by further increasing its depth , and the difficulty of learning the deeper network is overcome by using hessian-free ( hf ) optimization .	as a solution , a convex approximation of ctc is formulated and its relationship with the em algorithm and the fisher information matrix is discussed .	1	1	3	1.777548	-1.5101438	0
99-5-10	the performance of an mdrnn is improved by further increasing its depth , and the difficulty of learning the deeper network is overcome by using hessian-free ( hf ) optimization .	an mdrnn up to a depth of 15 layers is successfully trained using hf , resulting in an improved performance for sequence labeling .	1	1	4	-5.7347155	5.0755463	1
99-5-10	as a solution , a convex approximation of ctc is formulated and its relationship with the em algorithm and the fisher information matrix is discussed .	given that connectionist temporal classification ( ctc ) is utilized as an objective of learning an mdrnn for sequence labeling , the non-convexity of ctc poses a problem when applying hf to the network .	0	3	2	5.423291	-4.888766	0
99-5-10	an mdrnn up to a depth of 15 layers is successfully trained using hf , resulting in an improved performance for sequence labeling .	given that connectionist temporal classification ( ctc ) is utilized as an objective of learning an mdrnn for sequence labeling , the non-convexity of ctc poses a problem when applying hf to the network .	0	4	2	5.508187	-4.905761	0
99-5-10	as a solution , a convex approximation of ctc is formulated and its relationship with the em algorithm and the fisher information matrix is discussed .	an mdrnn up to a depth of 15 layers is successfully trained using hf , resulting in an improved performance for sequence labeling .	1	3	4	-5.189335	4.765107	1
100-3-3	this paper studies theoretically and empirically a method of turning machinelearning algorithms into probabilistic predictors that automatically enjoys a property of validity ( perfect calibration ) and is computationally efficient .	the price to pay for perfect calibration is that these probabilistic predictors produce imprecise ( in practice , almost precise for large data sets ) probabilities .	1	0	1	-5.938886	5.1855454	1
100-3-3	this paper studies theoretically and empirically a method of turning machinelearning algorithms into probabilistic predictors that automatically enjoys a property of validity ( perfect calibration ) and is computationally efficient .	when these imprecise probabilities are merged into precise probabilities , the resulting predictors , while losing the theoretical property of perfect calibration , are consistently more accurate than the existing methods in empirical studies .	1	0	2	-5.938652	5.1620216	1
100-3-3	when these imprecise probabilities are merged into precise probabilities , the resulting predictors , while losing the theoretical property of perfect calibration , are consistently more accurate than the existing methods in empirical studies .	the price to pay for perfect calibration is that these probabilistic predictors produce imprecise ( in practice , almost precise for large data sets ) probabilities .	0	2	1	5.0331297	-4.562922	0
101-7-21	promising results have been obtained in a number of tasks including super-resolution , inpainting , deconvolution , filtering , etc .	deep learning has recently been introduced to the field of low-level computer vision and image processing .	0	1	0	5.6499987	-5.161209	0
101-7-21	however , previously adopted neural network approaches such as convolutional neural networks and sparse auto-encoders are inherently with translation invariant operators .	deep learning has recently been introduced to the field of low-level computer vision and image processing .	0	2	0	5.6435695	-5.0534725	0
101-7-21	deep learning has recently been introduced to the field of low-level computer vision and image processing .	we found this property prevents the deep learning approaches from outperforming the state-of-the-art if the task itself requires translation variant interpolation ( tvi ) .	1	0	3	-5.9149914	5.1736917	1
101-7-21	deep learning has recently been introduced to the field of low-level computer vision and image processing .	in this paper , we draw on shepard interpolation and design shepard convolutional neural networks ( shcnn ) which efficiently realizes endto-end trainable tvi operators in the network .	1	0	4	-5.8857203	5.2297287	1
101-7-21	we show that by adding only a few feature maps in the new shepard layers , the network is able to achieve stronger results than a much deeper architecture .	deep learning has recently been introduced to the field of low-level computer vision and image processing .	0	5	0	5.63173	-5.0659533	0
101-7-21	superior performance on both image inpainting and super-resolution is obtained where our system outperforms previous ones while keeping the running time competitive .	deep learning has recently been introduced to the field of low-level computer vision and image processing .	0	6	0	5.5151014	-5.0325885	0
101-7-21	promising results have been obtained in a number of tasks including super-resolution , inpainting , deconvolution , filtering , etc .	however , previously adopted neural network approaches such as convolutional neural networks and sparse auto-encoders are inherently with translation invariant operators .	1	1	2	2.6378276	-2.4030685	0
101-7-21	promising results have been obtained in a number of tasks including super-resolution , inpainting , deconvolution , filtering , etc .	we found this property prevents the deep learning approaches from outperforming the state-of-the-art if the task itself requires translation variant interpolation ( tvi ) .	1	1	3	-5.573272	5.0521283	1
101-7-21	in this paper , we draw on shepard interpolation and design shepard convolutional neural networks ( shcnn ) which efficiently realizes endto-end trainable tvi operators in the network .	promising results have been obtained in a number of tasks including super-resolution , inpainting , deconvolution , filtering , etc .	0	4	1	-0.62011313	0.833432	1
101-7-21	we show that by adding only a few feature maps in the new shepard layers , the network is able to achieve stronger results than a much deeper architecture .	promising results have been obtained in a number of tasks including super-resolution , inpainting , deconvolution , filtering , etc .	0	5	1	2.9243321	-2.865272	0
101-7-21	promising results have been obtained in a number of tasks including super-resolution , inpainting , deconvolution , filtering , etc .	superior performance on both image inpainting and super-resolution is obtained where our system outperforms previous ones while keeping the running time competitive .	1	1	6	-5.649483	5.1590075	1
101-7-21	we found this property prevents the deep learning approaches from outperforming the state-of-the-art if the task itself requires translation variant interpolation ( tvi ) .	however , previously adopted neural network approaches such as convolutional neural networks and sparse auto-encoders are inherently with translation invariant operators .	0	3	2	5.377431	-4.7371902	0
101-7-21	in this paper , we draw on shepard interpolation and design shepard convolutional neural networks ( shcnn ) which efficiently realizes endto-end trainable tvi operators in the network .	however , previously adopted neural network approaches such as convolutional neural networks and sparse auto-encoders are inherently with translation invariant operators .	0	4	2	5.310183	-4.64001	0
101-7-21	we show that by adding only a few feature maps in the new shepard layers , the network is able to achieve stronger results than a much deeper architecture .	however , previously adopted neural network approaches such as convolutional neural networks and sparse auto-encoders are inherently with translation invariant operators .	0	5	2	5.4666767	-4.825867	0
101-7-21	superior performance on both image inpainting and super-resolution is obtained where our system outperforms previous ones while keeping the running time competitive .	however , previously adopted neural network approaches such as convolutional neural networks and sparse auto-encoders are inherently with translation invariant operators .	0	6	2	5.4846497	-4.874651	0
101-7-21	we found this property prevents the deep learning approaches from outperforming the state-of-the-art if the task itself requires translation variant interpolation ( tvi ) .	in this paper , we draw on shepard interpolation and design shepard convolutional neural networks ( shcnn ) which efficiently realizes endto-end trainable tvi operators in the network .	1	3	4	4.1736116	-3.7788675	0
101-7-21	we show that by adding only a few feature maps in the new shepard layers , the network is able to achieve stronger results than a much deeper architecture .	we found this property prevents the deep learning approaches from outperforming the state-of-the-art if the task itself requires translation variant interpolation ( tvi ) .	0	5	3	0.5384159	-0.30396324	0
101-7-21	superior performance on both image inpainting and super-resolution is obtained where our system outperforms previous ones while keeping the running time competitive .	we found this property prevents the deep learning approaches from outperforming the state-of-the-art if the task itself requires translation variant interpolation ( tvi ) .	0	6	3	4.330353	-3.9087057	0
101-7-21	we show that by adding only a few feature maps in the new shepard layers , the network is able to achieve stronger results than a much deeper architecture .	in this paper , we draw on shepard interpolation and design shepard convolutional neural networks ( shcnn ) which efficiently realizes endto-end trainable tvi operators in the network .	0	5	4	5.549741	-4.893735	0
101-7-21	in this paper , we draw on shepard interpolation and design shepard convolutional neural networks ( shcnn ) which efficiently realizes endto-end trainable tvi operators in the network .	superior performance on both image inpainting and super-resolution is obtained where our system outperforms previous ones while keeping the running time competitive .	1	4	6	-5.959535	5.218278	1
101-7-21	we show that by adding only a few feature maps in the new shepard layers , the network is able to achieve stronger results than a much deeper architecture .	superior performance on both image inpainting and super-resolution is obtained where our system outperforms previous ones while keeping the running time competitive .	1	5	6	-4.316333	3.9629285	1
102-6-15	specifically , we advance riemannian manifold optimization ( on the manifold of positive definite matrices ) as a potential replacement for expectation maximization ( em ) , which has been the de facto standard for decades .	we take a new look at parameter estimation for gaussian mixture model ( gmms ) .	0	1	0	5.318701	-4.7328587	0
102-6-15	we take a new look at parameter estimation for gaussian mixture model ( gmms ) .	an out-of-the-box invocation of riemannian optimization , however , fails spectacularly : it obtains the same solution as em , but vastly slower .	1	0	2	-5.8368154	5.1490374	1
102-6-15	we take a new look at parameter estimation for gaussian mixture model ( gmms ) .	building on intuition from geometric convexity , we propose a simple reformulation that has remarkable consequences : it makes riemannian optimization not only match em ( a nontrivial result on its own , given the poor record nonlinear programming has had against em ) , but also outperforms it in many settings .	1	0	3	-5.9312253	5.210313	1
102-6-15	to bring our ideas to fruition , we develop a welltuned riemannian lbfgs method that proves superior to known competing methods ( e.g. , riemannian conjugate gradient ) .	we take a new look at parameter estimation for gaussian mixture model ( gmms ) .	0	4	0	5.574132	-5.0015574	0
102-6-15	we hope that our results encourage a wider consideration of manifold optimization in machine learning and statistics .	we take a new look at parameter estimation for gaussian mixture model ( gmms ) .	0	5	0	5.487282	-4.9068074	0
102-6-15	specifically , we advance riemannian manifold optimization ( on the manifold of positive definite matrices ) as a potential replacement for expectation maximization ( em ) , which has been the de facto standard for decades .	an out-of-the-box invocation of riemannian optimization , however , fails spectacularly : it obtains the same solution as em , but vastly slower .	1	1	2	-4.9160314	4.6102486	1
102-6-15	specifically , we advance riemannian manifold optimization ( on the manifold of positive definite matrices ) as a potential replacement for expectation maximization ( em ) , which has been the de facto standard for decades .	[CLS] building on intuition from geometric convexity, we propose a simple reformulation that has remarkable consequences : it makes riemannian optimization not only match em ( a nontrivial result on its own, given the poor record nonlinear programming has had against em ), but also outperforms it	1	1	3	-5.929782	5.191903	1
102-6-15	specifically , we advance riemannian manifold optimization ( on the manifold of positive definite matrices ) as a potential replacement for expectation maximization ( em ) , which has been the de facto standard for decades .	to bring our ideas to fruition , we develop a welltuned riemannian lbfgs method that proves superior to known competing methods ( e.g. , riemannian conjugate gradient ) .	1	1	4	-5.8635006	5.1362877	1
102-6-15	we hope that our results encourage a wider consideration of manifold optimization in machine learning and statistics .	specifically , we advance riemannian manifold optimization ( on the manifold of positive definite matrices ) as a potential replacement for expectation maximization ( em ) , which has been the de facto standard for decades .	0	5	1	5.2604437	-4.693152	0
102-6-15	an out-of-the-box invocation of riemannian optimization , however , fails spectacularly : it obtains the same solution as em , but vastly slower .	building on intuition from geometric convexity , we propose a simple reformulation that has remarkable consequences : it makes riemannian optimization not only match em ( a nontrivial result on its own , given the poor record nonlinear programming has had against em ) , but also outperforms it in many settings .	1	2	3	-5.7732015	5.1773653	1
102-6-15	an out-of-the-box invocation of riemannian optimization , however , fails spectacularly : it obtains the same solution as em , but vastly slower .	to bring our ideas to fruition , we develop a welltuned riemannian lbfgs method that proves superior to known competing methods ( e.g. , riemannian conjugate gradient ) .	1	2	4	-5.889636	5.1045284	1
102-6-15	we hope that our results encourage a wider consideration of manifold optimization in machine learning and statistics .	an out-of-the-box invocation of riemannian optimization , however , fails spectacularly : it obtains the same solution as em , but vastly slower .	0	5	2	5.2434354	-4.712184	0
102-6-15	[CLS] building on intuition from geometric convexity, we propose a simple reformulation that has remarkable consequences : it makes riemannian optimization not only match em ( a nontrivial result on its own, given the poor record nonlinear programming has had against em ), but also out	to bring our ideas to fruition , we develop a welltuned riemannian lbfgs method that proves superior to known competing methods ( e.g. , riemannian conjugate gradient ) .	1	3	4	-4.8562765	4.4260592	1
102-6-15	we hope that our results encourage a wider consideration of manifold optimization in machine learning and statistics .	building on intuition from geometric convexity , we propose a simple reformulation that has remarkable consequences : it makes riemannian optimization not only match em ( a nontrivial result on its own , given the poor record nonlinear programming has had against em ) , but also outperforms it in many settings .	0	5	3	4.8594584	-4.3868227	0
102-6-15	we hope that our results encourage a wider consideration of manifold optimization in machine learning and statistics .	to bring our ideas to fruition , we develop a welltuned riemannian lbfgs method that proves superior to known competing methods ( e.g. , riemannian conjugate gradient ) .	0	5	4	4.1219683	-3.7976382	0
103-4-6	unlike the previous approaches that rely on word embeddings , our method learns embeddings of small text regions from unlabeled data for integration into a supervised cnn .	this paper presents a new semi-supervised framework with convolutional neural networks ( cnns ) for text categorization .	0	1	0	5.7368574	-5.0654316	0
103-4-6	the proposed scheme for embedding learning is based on the idea of two-view semi-supervised learning , which is intended to be useful for the task of interest even though the training is done on unlabeled data .	this paper presents a new semi-supervised framework with convolutional neural networks ( cnns ) for text categorization .	0	2	0	5.254675	-4.6845593	0
103-4-6	our models achieve better results than previous approaches on sentiment classification and topic classification tasks .	this paper presents a new semi-supervised framework with convolutional neural networks ( cnns ) for text categorization .	0	3	0	5.648182	-5.0372396	0
103-4-6	the proposed scheme for embedding learning is based on the idea of two-view semi-supervised learning , which is intended to be useful for the task of interest even though the training is done on unlabeled data .	unlike the previous approaches that rely on word embeddings , our method learns embeddings of small text regions from unlabeled data for integration into a supervised cnn .	0	2	1	-0.0850774	0.44701052	1
103-4-6	our models achieve better results than previous approaches on sentiment classification and topic classification tasks .	unlike the previous approaches that rely on word embeddings , our method learns embeddings of small text regions from unlabeled data for integration into a supervised cnn .	0	3	1	4.8073244	-4.413966	0
103-4-6	the proposed scheme for embedding learning is based on the idea of two-view semi-supervised learning , which is intended to be useful for the task of interest even though the training is done on unlabeled data .	our models achieve better results than previous approaches on sentiment classification and topic classification tasks .	1	2	3	-5.772176	5.0988183	1
104-3-3	we introduce a new parallel shared-memory recursive best-first and/or search algorithm , called sprbfaoo , that explores the search space in a best-first manner while operating with restricted memory .	the paper presents and evaluates the power of parallel search for exact map inference in graphical models .	0	1	0	-3.9045625	3.6579223	1
104-3-3	our experiments show that sprbfaoo is often superior to the current state-of-the-art sequential and/or search approaches , leading to considerable speed-ups ( up to 7-fold with 12 threads ) , especially on hard problem instances .	the paper presents and evaluates the power of parallel search for exact map inference in graphical models .	0	2	0	5.4179506	-4.905479	0
104-3-3	our experiments show that sprbfaoo is often superior to the current state-of-the-art sequential and/or search approaches , leading to considerable speed-ups ( up to 7-fold with 12 threads ) , especially on hard problem instances .	we introduce a new parallel shared-memory recursive best-first and/or search algorithm , called sprbfaoo , that explores the search space in a best-first manner while operating with restricted memory .	0	2	1	5.617135	-4.9974284	0
105-10-45	it requires the use of both local discriminative features and global context information .	scene labeling is a challenging computer vision task .	0	1	0	5.5615973	-5.0249076	0
105-10-45	scene labeling is a challenging computer vision task .	we adopt a deep recurrent convolutional neural network ( rcnn ) for this task , which is originally proposed for object recognition .	1	0	2	-5.818084	5.1184225	1
105-10-45	different from traditional convolutional neural networks ( cnn ) , this model has intra-layer recurrent connections in the convolutional layers .	scene labeling is a challenging computer vision task .	0	3	0	5.603769	-5.04424	0
105-10-45	scene labeling is a challenging computer vision task .	therefore each convolutional layer becomes a two-dimensional recurrent neural network .	1	0	4	-5.8762236	5.181692	1
105-10-45	scene labeling is a challenging computer vision task .	the units receive constant feed-forward inputs from the previous layer and recurrent inputs from their neighborhoods .	1	0	5	-5.8156223	5.149114	1
105-10-45	while recurrent iterations proceed , the region of context captured by each unit expands .	scene labeling is a challenging computer vision task .	0	6	0	5.633717	-5.1144943	0
105-10-45	in this way , feature extraction and context modulation are seamlessly integrated , which is different from typical methods that entail separate modules for the two steps .	scene labeling is a challenging computer vision task .	0	7	0	5.622825	-5.0490685	0
105-10-45	scene labeling is a challenging computer vision task .	to further utilize the context , a multi-scale rcnn is proposed .	1	0	8	-5.8421926	5.104216	1
105-10-45	over two benchmark datasets , standford background and sift flow , the model outperforms many state-of-the-art models in accuracy and efficiency .	scene labeling is a challenging computer vision task .	0	9	0	5.5893254	-5.0410357	0
105-10-45	it requires the use of both local discriminative features and global context information .	we adopt a deep recurrent convolutional neural network ( rcnn ) for this task , which is originally proposed for object recognition .	1	1	2	-1.0134546	1.1790118	1
105-10-45	different from traditional convolutional neural networks ( cnn ) , this model has intra-layer recurrent connections in the convolutional layers .	it requires the use of both local discriminative features and global context information .	0	3	1	-2.4897788	2.5997152	1
105-10-45	it requires the use of both local discriminative features and global context information .	therefore each convolutional layer becomes a two-dimensional recurrent neural network .	1	1	4	-4.6414137	4.3759484	1
105-10-45	the units receive constant feed-forward inputs from the previous layer and recurrent inputs from their neighborhoods .	it requires the use of both local discriminative features and global context information .	0	5	1	-0.5105591	0.76288486	1
105-10-45	while recurrent iterations proceed , the region of context captured by each unit expands .	it requires the use of both local discriminative features and global context information .	0	6	1	2.2422552	-2.172532	0
105-10-45	in this way , feature extraction and context modulation are seamlessly integrated , which is different from typical methods that entail separate modules for the two steps .	it requires the use of both local discriminative features and global context information .	0	7	1	4.9217896	-4.4392943	0
105-10-45	it requires the use of both local discriminative features and global context information .	to further utilize the context , a multi-scale rcnn is proposed .	1	1	8	-5.9785223	5.146901	1
105-10-45	it requires the use of both local discriminative features and global context information .	over two benchmark datasets , standford background and sift flow , the model outperforms many state-of-the-art models in accuracy and efficiency .	1	1	9	-5.93121	5.246296	1
105-10-45	we adopt a deep recurrent convolutional neural network ( rcnn ) for this task , which is originally proposed for object recognition .	different from traditional convolutional neural networks ( cnn ) , this model has intra-layer recurrent connections in the convolutional layers .	1	2	3	-2.6206183	2.7060404	1
105-10-45	we adopt a deep recurrent convolutional neural network ( rcnn ) for this task , which is originally proposed for object recognition .	therefore each convolutional layer becomes a two-dimensional recurrent neural network .	1	2	4	-4.4447336	4.196882	1
105-10-45	we adopt a deep recurrent convolutional neural network ( rcnn ) for this task , which is originally proposed for object recognition .	the units receive constant feed-forward inputs from the previous layer and recurrent inputs from their neighborhoods .	1	2	5	-3.9728885	3.7557082	1
105-10-45	while recurrent iterations proceed , the region of context captured by each unit expands .	we adopt a deep recurrent convolutional neural network ( rcnn ) for this task , which is originally proposed for object recognition .	0	6	2	4.0358467	-3.6799564	0
105-10-45	in this way , feature extraction and context modulation are seamlessly integrated , which is different from typical methods that entail separate modules for the two steps .	we adopt a deep recurrent convolutional neural network ( rcnn ) for this task , which is originally proposed for object recognition .	0	7	2	4.7338796	-4.221074	0
105-10-45	to further utilize the context , a multi-scale rcnn is proposed .	we adopt a deep recurrent convolutional neural network ( rcnn ) for this task , which is originally proposed for object recognition .	0	8	2	5.556929	-4.9107103	0
105-10-45	over two benchmark datasets , standford background and sift flow , the model outperforms many state-of-the-art models in accuracy and efficiency .	we adopt a deep recurrent convolutional neural network ( rcnn ) for this task , which is originally proposed for object recognition .	0	9	2	5.386951	-4.760879	0
105-10-45	therefore each convolutional layer becomes a two-dimensional recurrent neural network .	different from traditional convolutional neural networks ( cnn ) , this model has intra-layer recurrent connections in the convolutional layers .	0	4	3	4.496558	-4.0269966	0
105-10-45	the units receive constant feed-forward inputs from the previous layer and recurrent inputs from their neighborhoods .	different from traditional convolutional neural networks ( cnn ) , this model has intra-layer recurrent connections in the convolutional layers .	0	5	3	4.3486137	-3.8682997	0
105-10-45	different from traditional convolutional neural networks ( cnn ) , this model has intra-layer recurrent connections in the convolutional layers .	while recurrent iterations proceed , the region of context captured by each unit expands .	1	3	6	-5.1193514	4.7026434	1
105-10-45	different from traditional convolutional neural networks ( cnn ) , this model has intra-layer recurrent connections in the convolutional layers .	in this way , feature extraction and context modulation are seamlessly integrated , which is different from typical methods that entail separate modules for the two steps .	1	3	7	-5.4067135	4.961164	1
105-10-45	different from traditional convolutional neural networks ( cnn ) , this model has intra-layer recurrent connections in the convolutional layers .	to further utilize the context , a multi-scale rcnn is proposed .	1	3	8	-5.7076154	5.0773726	1
105-10-45	different from traditional convolutional neural networks ( cnn ) , this model has intra-layer recurrent connections in the convolutional layers .	over two benchmark datasets , standford background and sift flow , the model outperforms many state-of-the-art models in accuracy and efficiency .	1	3	9	-5.958353	5.239338	1
105-10-45	the units receive constant feed-forward inputs from the previous layer and recurrent inputs from their neighborhoods .	therefore each convolutional layer becomes a two-dimensional recurrent neural network .	0	5	4	0.5031106	-0.17372543	0
105-10-45	while recurrent iterations proceed , the region of context captured by each unit expands .	therefore each convolutional layer becomes a two-dimensional recurrent neural network .	0	6	4	-0.41427982	0.7560961	1
105-10-45	therefore each convolutional layer becomes a two-dimensional recurrent neural network .	in this way , feature extraction and context modulation are seamlessly integrated , which is different from typical methods that entail separate modules for the two steps .	1	4	7	-5.131952	4.7751074	1
105-10-45	therefore each convolutional layer becomes a two-dimensional recurrent neural network .	to further utilize the context , a multi-scale rcnn is proposed .	1	4	8	-4.62056	4.362934	1
105-10-45	over two benchmark datasets , standford background and sift flow , the model outperforms many state-of-the-art models in accuracy and efficiency .	therefore each convolutional layer becomes a two-dimensional recurrent neural network .	0	9	4	5.177243	-4.6665587	0
105-10-45	the units receive constant feed-forward inputs from the previous layer and recurrent inputs from their neighborhoods .	while recurrent iterations proceed , the region of context captured by each unit expands .	1	5	6	-4.6827173	4.3443737	1
105-10-45	the units receive constant feed-forward inputs from the previous layer and recurrent inputs from their neighborhoods .	in this way , feature extraction and context modulation are seamlessly integrated , which is different from typical methods that entail separate modules for the two steps .	1	5	7	-5.169688	4.792253	1
105-10-45	to further utilize the context , a multi-scale rcnn is proposed .	the units receive constant feed-forward inputs from the previous layer and recurrent inputs from their neighborhoods .	0	8	5	2.7852948	-2.5695415	0
105-10-45	the units receive constant feed-forward inputs from the previous layer and recurrent inputs from their neighborhoods .	over two benchmark datasets , standford background and sift flow , the model outperforms many state-of-the-art models in accuracy and efficiency .	1	5	9	-5.937771	5.184862	1
105-10-45	while recurrent iterations proceed , the region of context captured by each unit expands .	in this way , feature extraction and context modulation are seamlessly integrated , which is different from typical methods that entail separate modules for the two steps .	1	6	7	-5.3924146	4.930452	1
105-10-45	while recurrent iterations proceed , the region of context captured by each unit expands .	to further utilize the context , a multi-scale rcnn is proposed .	1	6	8	-5.5473695	5.03771	1
105-10-45	while recurrent iterations proceed , the region of context captured by each unit expands .	over two benchmark datasets , standford background and sift flow , the model outperforms many state-of-the-art models in accuracy and efficiency .	1	6	9	-6.0155334	5.1286764	1
105-10-45	to further utilize the context , a multi-scale rcnn is proposed .	in this way , feature extraction and context modulation are seamlessly integrated , which is different from typical methods that entail separate modules for the two steps .	0	8	7	1.1965293	-0.9596116	0
105-10-45	in this way , feature extraction and context modulation are seamlessly integrated , which is different from typical methods that entail separate modules for the two steps .	over two benchmark datasets , standford background and sift flow , the model outperforms many state-of-the-art models in accuracy and efficiency .	1	7	9	-5.6452117	5.0946198	1
105-10-45	over two benchmark datasets , standford background and sift flow , the model outperforms many state-of-the-art models in accuracy and efficiency .	to further utilize the context , a multi-scale rcnn is proposed .	0	9	8	4.0918097	-3.6942277	0
106-7-21	recently , there has been growing interest in systematic search-based and importance sampling-based lifted inference algorithms for statistical relational models ( srms ) .	these lifted algorithms achieve significant complexity reductions over their propositional counterparts by using lifting rules that leverage symmetries in the relational representation .	1	0	1	-5.9149256	5.174287	1
106-7-21	recently , there has been growing interest in systematic search-based and importance sampling-based lifted inference algorithms for statistical relational models ( srms ) .	one drawback of these algorithms is that they use an inference-blind representation of the search space , which makes it difficult to efficiently pre-compute tight upper bounds on the exact cost of inference without running the algorithm to completion .	1	0	2	-5.815838	5.186899	1
106-7-21	in this paper , we present a principled approach to address this problem .	recently , there has been growing interest in systematic search-based and importance sampling-based lifted inference algorithms for statistical relational models ( srms ) .	0	3	0	5.6689625	-5.1150584	0
106-7-21	recently , there has been growing interest in systematic search-based and importance sampling-based lifted inference algorithms for statistical relational models ( srms ) .	we introduce a lifted analogue of the propositional and/or search space framework , which we call a lifted and/or schematic .	1	0	4	-5.973873	5.183037	1
106-7-21	recently , there has been growing interest in systematic search-based and importance sampling-based lifted inference algorithms for statistical relational models ( srms ) .	given a schematic-based representation of an srm , we show how to efficiently compute a tight upper bound on the time and space cost of exact inference from a current assignment and the remaining schematic .	1	0	5	-5.8205805	5.165336	1
106-7-21	we show how our bounding method can be used within a lifted importance sampling algorithm , in order to perform effective rao-blackwellisation , and demonstrate experimentally that the rao-blackwellised version of the algorithm yields more accurate estimates on several real-world datasets .	recently , there has been growing interest in systematic search-based and importance sampling-based lifted inference algorithms for statistical relational models ( srms ) .	0	6	0	5.681063	-5.0422935	0
106-7-21	these lifted algorithms achieve significant complexity reductions over their propositional counterparts by using lifting rules that leverage symmetries in the relational representation .	one drawback of these algorithms is that they use an inference-blind representation of the search space , which makes it difficult to efficiently pre-compute tight upper bounds on the exact cost of inference without running the algorithm to completion .	1	1	2	-0.5021857	0.63329715	1
106-7-21	in this paper , we present a principled approach to address this problem .	these lifted algorithms achieve significant complexity reductions over their propositional counterparts by using lifting rules that leverage symmetries in the relational representation .	0	3	1	1.3283646	-1.10996	0
106-7-21	these lifted algorithms achieve significant complexity reductions over their propositional counterparts by using lifting rules that leverage symmetries in the relational representation .	we introduce a lifted analogue of the propositional and/or search space framework , which we call a lifted and/or schematic .	1	1	4	5.024212	-4.5300035	0
106-7-21	given a schematic-based representation of an srm , we show how to efficiently compute a tight upper bound on the time and space cost of exact inference from a current assignment and the remaining schematic .	these lifted algorithms achieve significant complexity reductions over their propositional counterparts by using lifting rules that leverage symmetries in the relational representation .	0	5	1	-2.5973673	2.6711583	1
106-7-21	we show how our bounding method can be used within a lifted importance sampling algorithm , in order to perform effective rao-blackwellisation , and demonstrate experimentally that the rao-blackwellised version of the algorithm yields more accurate estimates on several real-world datasets .	these lifted algorithms achieve significant complexity reductions over their propositional counterparts by using lifting rules that leverage symmetries in the relational representation .	0	6	1	5.20757	-4.6210127	0
106-7-21	one drawback of these algorithms is that they use an inference-blind representation of the search space , which makes it difficult to efficiently pre-compute tight upper bounds on the exact cost of inference without running the algorithm to completion .	in this paper , we present a principled approach to address this problem .	1	2	3	-5.8129973	5.218821	1
106-7-21	we introduce a lifted analogue of the propositional and/or search space framework , which we call a lifted and/or schematic .	one drawback of these algorithms is that they use an inference-blind representation of the search space , which makes it difficult to efficiently pre-compute tight upper bounds on the exact cost of inference without running the algorithm to completion .	0	4	2	3.6397257	-3.3767052	0
106-7-21	given a schematic-based representation of an srm , we show how to efficiently compute a tight upper bound on the time and space cost of exact inference from a current assignment and the remaining schematic .	one drawback of these algorithms is that they use an inference-blind representation of the search space , which makes it difficult to efficiently pre-compute tight upper bounds on the exact cost of inference without running the algorithm to completion .	0	5	2	4.4448795	-4.0782933	0
106-7-21	[CLS] we show how our bounding method can be used within a lifted importance sampling algorithm, in order to perform effective rao - blackwellisation, and demonstrate experimentally that the rao - blackwellised version of the algorithm yields more accurate estimates on several real - world dataset	one drawback of these algorithms is that they use an inference-blind representation of the search space , which makes it difficult to efficiently pre-compute tight upper bounds on the exact cost of inference without running the algorithm to completion .	0	6	2	5.589749	-4.940458	0
106-7-21	in this paper , we present a principled approach to address this problem .	we introduce a lifted analogue of the propositional and/or search space framework , which we call a lifted and/or schematic .	1	3	4	-4.9395294	4.6281257	1
106-7-21	in this paper , we present a principled approach to address this problem .	given a schematic-based representation of an srm , we show how to efficiently compute a tight upper bound on the time and space cost of exact inference from a current assignment and the remaining schematic .	1	3	5	-5.1870537	4.815118	1
106-7-21	we show how our bounding method can be used within a lifted importance sampling algorithm , in order to perform effective rao-blackwellisation , and demonstrate experimentally that the rao-blackwellised version of the algorithm yields more accurate estimates on several real-world datasets .	in this paper , we present a principled approach to address this problem .	0	6	3	5.732711	-5.0847826	0
106-7-21	given a schematic-based representation of an srm , we show how to efficiently compute a tight upper bound on the time and space cost of exact inference from a current assignment and the remaining schematic .	we introduce a lifted analogue of the propositional and/or search space framework , which we call a lifted and/or schematic .	0	5	4	4.330623	-3.8711782	0
106-7-21	we introduce a lifted analogue of the propositional and/or search space framework , which we call a lifted and/or schematic .	we show how our bounding method can be used within a lifted importance sampling algorithm , in order to perform effective rao-blackwellisation , and demonstrate experimentally that the rao-blackwellised version of the algorithm yields more accurate estimates on several real-world datasets .	1	4	6	-5.9841948	5.149265	1
106-7-21	we show how our bounding method can be used within a lifted importance sampling algorithm , in order to perform effective rao-blackwellisation , and demonstrate experimentally that the rao-blackwellised version of the algorithm yields more accurate estimates on several real-world datasets .	given a schematic-based representation of an srm , we show how to efficiently compute a tight upper bound on the time and space cost of exact inference from a current assignment and the remaining schematic .	0	6	5	5.6109467	-4.9752545	0
107-5-10	on target densities where classical hmc is not an option due to intractable gradients , kmc adaptively learns the target 's gradient structure by fitting an exponential family model in a reproducing kernel hilbert space .	we propose kernel hamiltonian monte carlo ( kmc ) , a gradient-free adaptive mcmc algorithm based on hamiltonian monte carlo ( hmc ) .	0	1	0	5.6639	-5.0270743	0
107-5-10	computational costs are reduced by two novel efficient approximations to this gradient .	we propose kernel hamiltonian monte carlo ( kmc ) , a gradient-free adaptive mcmc algorithm based on hamiltonian monte carlo ( hmc ) .	0	2	0	5.353756	-4.6817136	0
107-5-10	while being asymptotically exact , kmc mimics hmc in terms of sampling efficiency , and offers substantial mixing improvements over state-of-the-art gradient free samplers .	we propose kernel hamiltonian monte carlo ( kmc ) , a gradient-free adaptive mcmc algorithm based on hamiltonian monte carlo ( hmc ) .	0	3	0	5.6404686	-4.998486	0
107-5-10	we support our claims with experimental studies on both toy and real-world applications , including approximate bayesian computation and exact-approximate mcmc .	we propose kernel hamiltonian monte carlo ( kmc ) , a gradient-free adaptive mcmc algorithm based on hamiltonian monte carlo ( hmc ) .	0	4	0	5.377	-4.7125053	0
107-5-10	on target densities where classical hmc is not an option due to intractable gradients , kmc adaptively learns the target 's gradient structure by fitting an exponential family model in a reproducing kernel hilbert space .	computational costs are reduced by two novel efficient approximations to this gradient .	1	1	2	-5.7166934	5.120949	1
107-5-10	while being asymptotically exact , kmc mimics hmc in terms of sampling efficiency , and offers substantial mixing improvements over state-of-the-art gradient free samplers .	on target densities where classical hmc is not an option due to intractable gradients , kmc adaptively learns the target 's gradient structure by fitting an exponential family model in a reproducing kernel hilbert space .	0	3	1	4.4568844	-4.0552235	0
107-5-10	on target densities where classical hmc is not an option due to intractable gradients , kmc adaptively learns the target 's gradient structure by fitting an exponential family model in a reproducing kernel hilbert space .	we support our claims with experimental studies on both toy and real-world applications , including approximate bayesian computation and exact-approximate mcmc .	1	1	4	-5.9374084	5.007766	1
107-5-10	while being asymptotically exact , kmc mimics hmc in terms of sampling efficiency , and offers substantial mixing improvements over state-of-the-art gradient free samplers .	computational costs are reduced by two novel efficient approximations to this gradient .	0	3	2	2.089345	-1.9585881	0
107-5-10	computational costs are reduced by two novel efficient approximations to this gradient .	we support our claims with experimental studies on both toy and real-world applications , including approximate bayesian computation and exact-approximate mcmc .	1	2	4	-5.833248	5.110042	1
107-5-10	we support our claims with experimental studies on both toy and real-world applications , including approximate bayesian computation and exact-approximate mcmc .	while being asymptotically exact , kmc mimics hmc in terms of sampling efficiency , and offers substantial mixing improvements over state-of-the-art gradient free samplers .	0	4	3	3.3747993	-3.193469	0
108-7-21	in each time step the learner chooses an allocation of several resource types between a number of tasks .	we study an idealised sequential resource allocation problem .	0	1	0	5.5068746	-4.9963617	0
108-7-21	assigning more resources to a task increases the probability that it is completed .	we study an idealised sequential resource allocation problem .	0	2	0	5.411695	-4.914007	0
108-7-21	the problem is challenging because the alignment of the tasks to the resource types is unknown and the feedback is noisy .	we study an idealised sequential resource allocation problem .	0	3	0	5.539683	-4.990692	0
108-7-21	our main contribution is the new setting and an algorithm with nearly-optimal regret analysis .	we study an idealised sequential resource allocation problem .	0	4	0	5.492302	-4.9667406	0
108-7-21	we study an idealised sequential resource allocation problem .	along the way we draw connections to the problem of minimising regret for stochastic linear bandits with heteroscedastic noise .	1	0	5	-5.946296	5.160095	1
108-7-21	we study an idealised sequential resource allocation problem .	we also present some new results for stochastic linear bandits on the hypercube that significantly improve on existing work , especially in the sparse case .	1	0	6	-5.9543285	5.1538897	1
108-7-21	in each time step the learner chooses an allocation of several resource types between a number of tasks .	assigning more resources to a task increases the probability that it is completed .	1	1	2	-2.9719791	2.9232283	1
108-7-21	in each time step the learner chooses an allocation of several resource types between a number of tasks .	the problem is challenging because the alignment of the tasks to the resource types is unknown and the feedback is noisy .	1	1	3	-5.353856	4.841962	1
108-7-21	our main contribution is the new setting and an algorithm with nearly-optimal regret analysis .	in each time step the learner chooses an allocation of several resource types between a number of tasks .	0	4	1	5.103765	-4.597585	0
108-7-21	along the way we draw connections to the problem of minimising regret for stochastic linear bandits with heteroscedastic noise .	in each time step the learner chooses an allocation of several resource types between a number of tasks .	0	5	1	5.2999344	-4.704463	0
108-7-21	in each time step the learner chooses an allocation of several resource types between a number of tasks .	we also present some new results for stochastic linear bandits on the hypercube that significantly improve on existing work , especially in the sparse case .	1	1	6	-6.031162	5.1350975	1
108-7-21	the problem is challenging because the alignment of the tasks to the resource types is unknown and the feedback is noisy .	assigning more resources to a task increases the probability that it is completed .	0	3	2	3.2435374	-3.0583968	0
108-7-21	assigning more resources to a task increases the probability that it is completed .	our main contribution is the new setting and an algorithm with nearly-optimal regret analysis .	1	2	4	-5.978922	5.120306	1
108-7-21	assigning more resources to a task increases the probability that it is completed .	along the way we draw connections to the problem of minimising regret for stochastic linear bandits with heteroscedastic noise .	1	2	5	-5.953569	5.1078815	1
108-7-21	assigning more resources to a task increases the probability that it is completed .	we also present some new results for stochastic linear bandits on the hypercube that significantly improve on existing work , especially in the sparse case .	1	2	6	-5.9993153	5.130756	1
108-7-21	our main contribution is the new setting and an algorithm with nearly-optimal regret analysis .	the problem is challenging because the alignment of the tasks to the resource types is unknown and the feedback is noisy .	0	4	3	4.768091	-4.308136	0
108-7-21	along the way we draw connections to the problem of minimising regret for stochastic linear bandits with heteroscedastic noise .	the problem is challenging because the alignment of the tasks to the resource types is unknown and the feedback is noisy .	0	5	3	4.58469	-4.135111	0
108-7-21	the problem is challenging because the alignment of the tasks to the resource types is unknown and the feedback is noisy .	we also present some new results for stochastic linear bandits on the hypercube that significantly improve on existing work , especially in the sparse case .	1	3	6	-6.00395	5.172759	1
108-7-21	along the way we draw connections to the problem of minimising regret for stochastic linear bandits with heteroscedastic noise .	our main contribution is the new setting and an algorithm with nearly-optimal regret analysis .	0	5	4	1.5071688	-1.4002323	0
108-7-21	our main contribution is the new setting and an algorithm with nearly-optimal regret analysis .	we also present some new results for stochastic linear bandits on the hypercube that significantly improve on existing work , especially in the sparse case .	1	4	6	-4.587167	4.1860886	1
108-7-21	along the way we draw connections to the problem of minimising regret for stochastic linear bandits with heteroscedastic noise .	we also present some new results for stochastic linear bandits on the hypercube that significantly improve on existing work , especially in the sparse case .	1	5	6	-4.898838	4.4757185	1
109-3-3	we apply our techniques to both a visual learning domain and a language learning problem , showing that our algorithm can learn many visual concepts from only a few examples and that it can recover some english inflectional morphology .	we introduce an unsupervised learning algorithm that combines probabilistic modeling with solver-based techniques for program synthesis .	0	1	0	5.6166515	-4.961768	0
109-3-3	taken together , these results give both a new approach to unsupervised learning of symbolic compositional structures , and a technique for applying program synthesis tools to noisy data .	we introduce an unsupervised learning algorithm that combines probabilistic modeling with solver-based techniques for program synthesis .	0	2	0	5.541937	-4.873954	0
109-3-3	taken together , these results give both a new approach to unsupervised learning of symbolic compositional structures , and a technique for applying program synthesis tools to noisy data .	we apply our techniques to both a visual learning domain and a language learning problem , showing that our algorithm can learn many visual concepts from only a few examples and that it can recover some english inflectional morphology .	0	2	1	2.5748367	-2.3817616	0
110-13-78	to predict sensory inputs or control motor trajectories , the brain must constantly learn temporal dynamics based on error feedback .	however , it remains unclear how such supervised learning is implemented in biological neural networks .	1	0	1	-5.7229056	5.1400194	1
110-13-78	to predict sensory inputs or control motor trajectories , the brain must constantly learn temporal dynamics based on error feedback .	learning in recurrent spiking networks is notoriously difficult because local changes in connectivity may have an unpredictable effect on the global dynamics .	1	0	2	-5.864431	5.1334333	1
110-13-78	to predict sensory inputs or control motor trajectories , the brain must constantly learn temporal dynamics based on error feedback .	the most commonly used learning rules , such as temporal back-propagation , are not local and thus not biologically plausible .	1	0	3	-5.726279	5.1035275	1
110-13-78	to predict sensory inputs or control motor trajectories , the brain must constantly learn temporal dynamics based on error feedback .	furthermore , reproducing the poisson-like statistics of neural responses requires the use of networks with balanced excitation and inhibition .	1	0	4	-5.798062	5.203826	1
110-13-78	such balance is easily destroyed during learning .	to predict sensory inputs or control motor trajectories , the brain must constantly learn temporal dynamics based on error feedback .	0	5	0	5.712984	-5.139927	0
110-13-78	using a top-down approach , we show how networks of integrate-and-fire neurons can learn arbitrary linear dynamical systems by feeding back their error as a feed-forward input .	to predict sensory inputs or control motor trajectories , the brain must constantly learn temporal dynamics based on error feedback .	0	6	0	5.7456264	-5.1506987	0
110-13-78	the network uses two types of recurrent connections : fast and slow .	to predict sensory inputs or control motor trajectories , the brain must constantly learn temporal dynamics based on error feedback .	0	7	0	5.6832714	-5.1168213	0
110-13-78	to predict sensory inputs or control motor trajectories , the brain must constantly learn temporal dynamics based on error feedback .	the fast connections learn to balance excitation and inhibition using a voltage-based plasticity rule .	1	0	8	-5.8858294	5.1651964	1
110-13-78	the slow connections are trained to minimize the error feedback using a current-based hebbian learning rule .	to predict sensory inputs or control motor trajectories , the brain must constantly learn temporal dynamics based on error feedback .	0	9	0	5.625279	-5.0609646	0
110-13-78	to predict sensory inputs or control motor trajectories , the brain must constantly learn temporal dynamics based on error feedback .	importantly , the balance maintained by fast connections is crucial to ensure that global error signals are available locally in each neuron , in turn resulting in a local learning rule for the slow connections .	1	0	10	-5.808256	5.1929674	1
110-13-78	to predict sensory inputs or control motor trajectories , the brain must constantly learn temporal dynamics based on error feedback .	this demonstrates that spiking networks can learn complex dynamics using purely local learning rules , using e/i balance as the key rather than an additional constraint .	1	0	11	-5.9197626	5.129718	1
110-13-78	the resulting network implements a given function within the predictive coding scheme , with minimal dimensions and activity .	to predict sensory inputs or control motor trajectories , the brain must constantly learn temporal dynamics based on error feedback .	0	12	0	5.7238765	-5.159629	0
110-13-78	learning in recurrent spiking networks is notoriously difficult because local changes in connectivity may have an unpredictable effect on the global dynamics .	however , it remains unclear how such supervised learning is implemented in biological neural networks .	0	2	1	1.8912678	-1.4915788	0
110-13-78	however , it remains unclear how such supervised learning is implemented in biological neural networks .	the most commonly used learning rules , such as temporal back-propagation , are not local and thus not biologically plausible .	1	1	3	-3.4373686	3.4461455	1
110-13-78	furthermore , reproducing the poisson-like statistics of neural responses requires the use of networks with balanced excitation and inhibition .	however , it remains unclear how such supervised learning is implemented in biological neural networks .	0	4	1	4.7312517	-4.1467257	0
110-13-78	however , it remains unclear how such supervised learning is implemented in biological neural networks .	such balance is easily destroyed during learning .	1	1	5	-5.2379723	4.841973	1
110-13-78	using a top-down approach , we show how networks of integrate-and-fire neurons can learn arbitrary linear dynamical systems by feeding back their error as a feed-forward input .	however , it remains unclear how such supervised learning is implemented in biological neural networks .	0	6	1	5.601752	-4.9844666	0
110-13-78	however , it remains unclear how such supervised learning is implemented in biological neural networks .	the network uses two types of recurrent connections : fast and slow .	1	1	7	-5.938182	5.2128983	1
110-13-78	however , it remains unclear how such supervised learning is implemented in biological neural networks .	the fast connections learn to balance excitation and inhibition using a voltage-based plasticity rule .	1	1	8	-5.810262	5.172231	1
110-13-78	the slow connections are trained to minimize the error feedback using a current-based hebbian learning rule .	however , it remains unclear how such supervised learning is implemented in biological neural networks .	0	9	1	5.477831	-4.872916	0
110-13-78	however , it remains unclear how such supervised learning is implemented in biological neural networks .	importantly , the balance maintained by fast connections is crucial to ensure that global error signals are available locally in each neuron , in turn resulting in a local learning rule for the slow connections .	1	1	10	-5.723484	5.20815	1
110-13-78	however , it remains unclear how such supervised learning is implemented in biological neural networks .	this demonstrates that spiking networks can learn complex dynamics using purely local learning rules , using e/i balance as the key rather than an additional constraint .	1	1	11	-5.998921	5.075141	1
110-13-78	the resulting network implements a given function within the predictive coding scheme , with minimal dimensions and activity .	however , it remains unclear how such supervised learning is implemented in biological neural networks .	0	12	1	5.679839	-5.088853	0
110-13-78	the most commonly used learning rules , such as temporal back-propagation , are not local and thus not biologically plausible .	learning in recurrent spiking networks is notoriously difficult because local changes in connectivity may have an unpredictable effect on the global dynamics .	0	3	2	2.8965273	-2.6540444	0
110-13-78	furthermore , reproducing the poisson-like statistics of neural responses requires the use of networks with balanced excitation and inhibition .	learning in recurrent spiking networks is notoriously difficult because local changes in connectivity may have an unpredictable effect on the global dynamics .	0	4	2	-2.758762	2.7391906	1
110-13-78	learning in recurrent spiking networks is notoriously difficult because local changes in connectivity may have an unpredictable effect on the global dynamics .	such balance is easily destroyed during learning .	1	2	5	-5.7688346	5.216904	1
110-13-78	using a top-down approach , we show how networks of integrate-and-fire neurons can learn arbitrary linear dynamical systems by feeding back their error as a feed-forward input .	learning in recurrent spiking networks is notoriously difficult because local changes in connectivity may have an unpredictable effect on the global dynamics .	0	6	2	3.8851857	-3.5741916	0
110-13-78	the network uses two types of recurrent connections : fast and slow .	learning in recurrent spiking networks is notoriously difficult because local changes in connectivity may have an unpredictable effect on the global dynamics .	0	7	2	3.1662772	-2.999179	0
110-13-78	learning in recurrent spiking networks is notoriously difficult because local changes in connectivity may have an unpredictable effect on the global dynamics .	the fast connections learn to balance excitation and inhibition using a voltage-based plasticity rule .	1	2	8	-5.7190037	5.161617	1
110-13-78	learning in recurrent spiking networks is notoriously difficult because local changes in connectivity may have an unpredictable effect on the global dynamics .	the slow connections are trained to minimize the error feedback using a current-based hebbian learning rule .	1	2	9	-5.9404597	5.1916556	1
110-13-78	importantly , the balance maintained by fast connections is crucial to ensure that global error signals are available locally in each neuron , in turn resulting in a local learning rule for the slow connections .	learning in recurrent spiking networks is notoriously difficult because local changes in connectivity may have an unpredictable effect on the global dynamics .	0	10	2	4.9867144	-4.3658714	0
110-13-78	this demonstrates that spiking networks can learn complex dynamics using purely local learning rules , using e/i balance as the key rather than an additional constraint .	learning in recurrent spiking networks is notoriously difficult because local changes in connectivity may have an unpredictable effect on the global dynamics .	0	11	2	5.2006993	-4.576153	0
110-13-78	learning in recurrent spiking networks is notoriously difficult because local changes in connectivity may have an unpredictable effect on the global dynamics .	the resulting network implements a given function within the predictive coding scheme , with minimal dimensions and activity .	1	2	12	-5.533681	5.100011	1
110-13-78	the most commonly used learning rules , such as temporal back-propagation , are not local and thus not biologically plausible .	furthermore , reproducing the poisson-like statistics of neural responses requires the use of networks with balanced excitation and inhibition .	1	3	4	-0.24354696	0.4985674	1
110-13-78	the most commonly used learning rules , such as temporal back-propagation , are not local and thus not biologically plausible .	such balance is easily destroyed during learning .	1	3	5	-5.2524786	4.859085	1
110-13-78	the most commonly used learning rules , such as temporal back-propagation , are not local and thus not biologically plausible .	using a top-down approach , we show how networks of integrate-and-fire neurons can learn arbitrary linear dynamical systems by feeding back their error as a feed-forward input .	1	3	6	-5.80728	5.0945244	1
110-13-78	the most commonly used learning rules , such as temporal back-propagation , are not local and thus not biologically plausible .	the network uses two types of recurrent connections : fast and slow .	1	3	7	-5.727924	5.1469455	1
110-13-78	the fast connections learn to balance excitation and inhibition using a voltage-based plasticity rule .	the most commonly used learning rules , such as temporal back-propagation , are not local and thus not biologically plausible .	0	8	3	4.8462853	-4.3302865	0
110-13-78	the slow connections are trained to minimize the error feedback using a current-based hebbian learning rule .	the most commonly used learning rules , such as temporal back-propagation , are not local and thus not biologically plausible .	0	9	3	5.4113045	-4.805727	0
110-13-78	importantly , the balance maintained by fast connections is crucial to ensure that global error signals are available locally in each neuron , in turn resulting in a local learning rule for the slow connections .	the most commonly used learning rules , such as temporal back-propagation , are not local and thus not biologically plausible .	0	10	3	5.104872	-4.4961133	0
110-13-78	this demonstrates that spiking networks can learn complex dynamics using purely local learning rules , using e/i balance as the key rather than an additional constraint .	the most commonly used learning rules , such as temporal back-propagation , are not local and thus not biologically plausible .	0	11	3	5.1877775	-4.582201	0
110-13-78	the most commonly used learning rules , such as temporal back-propagation , are not local and thus not biologically plausible .	the resulting network implements a given function within the predictive coding scheme , with minimal dimensions and activity .	1	3	12	-5.7786355	5.1934986	1
110-13-78	such balance is easily destroyed during learning .	furthermore , reproducing the poisson-like statistics of neural responses requires the use of networks with balanced excitation and inhibition .	0	5	4	4.0573025	-3.7085397	0
110-13-78	furthermore , reproducing the poisson-like statistics of neural responses requires the use of networks with balanced excitation and inhibition .	using a top-down approach , we show how networks of integrate-and-fire neurons can learn arbitrary linear dynamical systems by feeding back their error as a feed-forward input .	1	4	6	-5.0184345	4.7333927	1
110-13-78	furthermore , reproducing the poisson-like statistics of neural responses requires the use of networks with balanced excitation and inhibition .	the network uses two types of recurrent connections : fast and slow .	1	4	7	-5.938135	5.2313366	1
110-13-78	furthermore , reproducing the poisson-like statistics of neural responses requires the use of networks with balanced excitation and inhibition .	the fast connections learn to balance excitation and inhibition using a voltage-based plasticity rule .	1	4	8	-5.7205896	5.188913	1
110-13-78	the slow connections are trained to minimize the error feedback using a current-based hebbian learning rule .	furthermore , reproducing the poisson-like statistics of neural responses requires the use of networks with balanced excitation and inhibition .	0	9	4	5.2506657	-4.6480703	0
110-13-78	furthermore , reproducing the poisson-like statistics of neural responses requires the use of networks with balanced excitation and inhibition .	importantly , the balance maintained by fast connections is crucial to ensure that global error signals are available locally in each neuron , in turn resulting in a local learning rule for the slow connections .	1	4	10	-5.711641	5.2234836	1
110-13-78	furthermore , reproducing the poisson-like statistics of neural responses requires the use of networks with balanced excitation and inhibition .	this demonstrates that spiking networks can learn complex dynamics using purely local learning rules , using e/i balance as the key rather than an additional constraint .	1	4	11	-5.9731617	5.0900383	1
110-13-78	furthermore , reproducing the poisson-like statistics of neural responses requires the use of networks with balanced excitation and inhibition .	the resulting network implements a given function within the predictive coding scheme , with minimal dimensions and activity .	1	4	12	-5.9506006	5.199722	1
110-13-78	such balance is easily destroyed during learning .	using a top-down approach , we show how networks of integrate-and-fire neurons can learn arbitrary linear dynamical systems by feeding back their error as a feed-forward input .	1	5	6	2.795548	-2.5553706	0
110-13-78	the network uses two types of recurrent connections : fast and slow .	such balance is easily destroyed during learning .	0	7	5	-1.3947995	1.599252	1
110-13-78	the fast connections learn to balance excitation and inhibition using a voltage-based plasticity rule .	such balance is easily destroyed during learning .	0	8	5	-3.8507419	3.6833878	1
110-13-78	the slow connections are trained to minimize the error feedback using a current-based hebbian learning rule .	such balance is easily destroyed during learning .	0	9	5	2.9586186	-2.76463	0
110-13-78	importantly , the balance maintained by fast connections is crucial to ensure that global error signals are available locally in each neuron , in turn resulting in a local learning rule for the slow connections .	such balance is easily destroyed during learning .	0	10	5	-2.299636	2.415564	1
110-13-78	such balance is easily destroyed during learning .	this demonstrates that spiking networks can learn complex dynamics using purely local learning rules , using e/i balance as the key rather than an additional constraint .	1	5	11	-1.9684895	2.0803642	1
110-13-78	the resulting network implements a given function within the predictive coding scheme , with minimal dimensions and activity .	such balance is easily destroyed during learning .	0	12	5	1.530652	-1.4655149	0
110-13-78	the network uses two types of recurrent connections : fast and slow .	using a top-down approach , we show how networks of integrate-and-fire neurons can learn arbitrary linear dynamical systems by feeding back their error as a feed-forward input .	0	7	6	4.0199738	-3.5784998	0
110-13-78	using a top-down approach , we show how networks of integrate-and-fire neurons can learn arbitrary linear dynamical systems by feeding back their error as a feed-forward input .	the fast connections learn to balance excitation and inhibition using a voltage-based plasticity rule .	1	6	8	-4.467903	4.196398	1
110-13-78	the slow connections are trained to minimize the error feedback using a current-based hebbian learning rule .	using a top-down approach , we show how networks of integrate-and-fire neurons can learn arbitrary linear dynamical systems by feeding back their error as a feed-forward input .	0	9	6	4.5440235	-4.0309353	0
110-13-78	importantly , the balance maintained by fast connections is crucial to ensure that global error signals are available locally in each neuron , in turn resulting in a local learning rule for the slow connections .	using a top-down approach , we show how networks of integrate-and-fire neurons can learn arbitrary linear dynamical systems by feeding back their error as a feed-forward input .	0	10	6	-2.9408371	2.9740603	1
110-13-78	using a top-down approach , we show how networks of integrate-and-fire neurons can learn arbitrary linear dynamical systems by feeding back their error as a feed-forward input .	this demonstrates that spiking networks can learn complex dynamics using purely local learning rules , using e/i balance as the key rather than an additional constraint .	1	6	11	-5.152774	4.709213	1
110-13-78	the resulting network implements a given function within the predictive coding scheme , with minimal dimensions and activity .	using a top-down approach , we show how networks of integrate-and-fire neurons can learn arbitrary linear dynamical systems by feeding back their error as a feed-forward input .	0	12	6	4.835581	-4.3170805	0
110-13-78	the network uses two types of recurrent connections : fast and slow .	the fast connections learn to balance excitation and inhibition using a voltage-based plasticity rule .	1	7	8	-5.9729867	5.070428	1
110-13-78	the slow connections are trained to minimize the error feedback using a current-based hebbian learning rule .	the network uses two types of recurrent connections : fast and slow .	0	9	7	5.2664247	-4.67225	0
110-13-78	the network uses two types of recurrent connections : fast and slow .	importantly , the balance maintained by fast connections is crucial to ensure that global error signals are available locally in each neuron , in turn resulting in a local learning rule for the slow connections .	1	7	10	-5.5805287	5.0418515	1
110-13-78	the network uses two types of recurrent connections : fast and slow .	this demonstrates that spiking networks can learn complex dynamics using purely local learning rules , using e/i balance as the key rather than an additional constraint .	1	7	11	-4.3583775	4.099045	1
110-13-78	the resulting network implements a given function within the predictive coding scheme , with minimal dimensions and activity .	the network uses two types of recurrent connections : fast and slow .	0	12	7	2.6384006	-2.4357233	0
110-13-78	the fast connections learn to balance excitation and inhibition using a voltage-based plasticity rule .	the slow connections are trained to minimize the error feedback using a current-based hebbian learning rule .	1	8	9	-0.9883254	1.2405413	1
110-13-78	importantly , the balance maintained by fast connections is crucial to ensure that global error signals are available locally in each neuron , in turn resulting in a local learning rule for the slow connections .	the fast connections learn to balance excitation and inhibition using a voltage-based plasticity rule .	0	10	8	1.430728	-1.1213473	0
110-13-78	the fast connections learn to balance excitation and inhibition using a voltage-based plasticity rule .	this demonstrates that spiking networks can learn complex dynamics using purely local learning rules , using e/i balance as the key rather than an additional constraint .	1	8	11	-2.683009	2.7411141	1
110-13-78	the fast connections learn to balance excitation and inhibition using a voltage-based plasticity rule .	the resulting network implements a given function within the predictive coding scheme , with minimal dimensions and activity .	1	8	12	1.5851243	-1.2759547	0
110-13-78	importantly , the balance maintained by fast connections is crucial to ensure that global error signals are available locally in each neuron , in turn resulting in a local learning rule for the slow connections .	the slow connections are trained to minimize the error feedback using a current-based hebbian learning rule .	0	10	9	-2.7149053	2.7705524	1
110-13-78	the slow connections are trained to minimize the error feedback using a current-based hebbian learning rule .	this demonstrates that spiking networks can learn complex dynamics using purely local learning rules , using e/i balance as the key rather than an additional constraint .	1	9	11	0.023769114	0.3353466	1
110-13-78	the resulting network implements a given function within the predictive coding scheme , with minimal dimensions and activity .	the slow connections are trained to minimize the error feedback using a current-based hebbian learning rule .	0	12	9	-3.3592513	3.2824898	1
110-13-78	this demonstrates that spiking networks can learn complex dynamics using purely local learning rules , using e/i balance as the key rather than an additional constraint .	importantly , the balance maintained by fast connections is crucial to ensure that global error signals are available locally in each neuron , in turn resulting in a local learning rule for the slow connections .	0	11	10	2.4807024	-2.278972	0
110-13-78	the resulting network implements a given function within the predictive coding scheme , with minimal dimensions and activity .	importantly , the balance maintained by fast connections is crucial to ensure that global error signals are available locally in each neuron , in turn resulting in a local learning rule for the slow connections .	0	12	10	3.5906892	-3.2746391	0
110-13-78	this demonstrates that spiking networks can learn complex dynamics using purely local learning rules , using e/i balance as the key rather than an additional constraint .	the resulting network implements a given function within the predictive coding scheme , with minimal dimensions and activity .	1	11	12	3.6916308	-3.4526544	0
111-10-45	tensor candecomp/parafac ( cp ) decomposition has wide applications in statistical learning of latent variable models and in data mining .	in this paper , we propose fast and randomized tensor cp decomposition algorithms based on sketching .	1	0	1	-5.8974023	5.244919	1
111-10-45	tensor candecomp/parafac ( cp ) decomposition has wide applications in statistical learning of latent variable models and in data mining .	we build on the idea of count sketches , but introduce many novel ideas which are unique to tensors .	1	0	2	-5.903761	5.290798	1
111-10-45	tensor candecomp/parafac ( cp ) decomposition has wide applications in statistical learning of latent variable models and in data mining .	we develop novel methods for randomized computation of tensor contractions via ffts , without explicitly forming the tensors .	1	0	3	-5.854102	5.317915	1
111-10-45	such tensor contractions are encountered in decomposition methods such as tensor power iterations and alternating least squares .	tensor candecomp/parafac ( cp ) decomposition has wide applications in statistical learning of latent variable models and in data mining .	0	4	0	5.191017	-4.4760313	0
111-10-45	we also design novel colliding hashes for symmetric tensors to further save time in computing the sketches .	tensor candecomp/parafac ( cp ) decomposition has wide applications in statistical learning of latent variable models and in data mining .	0	5	0	5.474176	-4.848366	0
111-10-45	we then combine these sketching ideas with existing whitening and tensor power iterative techniques to obtain the fastest algorithm on both sparse and dense tensors .	tensor candecomp/parafac ( cp ) decomposition has wide applications in statistical learning of latent variable models and in data mining .	0	6	0	5.545788	-4.960785	0
111-10-45	the quality of approximation under our method does not depend on properties such as sparsity , uniformity of elements , etc .	tensor candecomp/parafac ( cp ) decomposition has wide applications in statistical learning of latent variable models and in data mining .	0	7	0	5.4453697	-4.826418	0
111-10-45	tensor candecomp/parafac ( cp ) decomposition has wide applications in statistical learning of latent variable models and in data mining .	we apply the method for topic modeling and obtain competitive results .	1	0	8	-5.989854	5.242771	1
111-10-45	keywords : tensor cp decomposition , count sketch , randomized methods , spectral methods , topic modeling	tensor candecomp/parafac ( cp ) decomposition has wide applications in statistical learning of latent variable models and in data mining .	0	9	0	5.486021	-4.84406	0
111-10-45	in this paper , we propose fast and randomized tensor cp decomposition algorithms based on sketching .	we build on the idea of count sketches , but introduce many novel ideas which are unique to tensors .	1	1	2	-5.668753	5.1111364	1
111-10-45	in this paper , we propose fast and randomized tensor cp decomposition algorithms based on sketching .	we develop novel methods for randomized computation of tensor contractions via ffts , without explicitly forming the tensors .	1	1	3	-3.4532156	3.310421	1
111-10-45	in this paper , we propose fast and randomized tensor cp decomposition algorithms based on sketching .	such tensor contractions are encountered in decomposition methods such as tensor power iterations and alternating least squares .	1	1	4	2.3306303	-2.139667	0
111-10-45	in this paper , we propose fast and randomized tensor cp decomposition algorithms based on sketching .	we also design novel colliding hashes for symmetric tensors to further save time in computing the sketches .	1	1	5	-5.997037	5.1464148	1
111-10-45	in this paper , we propose fast and randomized tensor cp decomposition algorithms based on sketching .	we then combine these sketching ideas with existing whitening and tensor power iterative techniques to obtain the fastest algorithm on both sparse and dense tensors .	1	1	6	-5.967224	5.189357	1
111-10-45	in this paper , we propose fast and randomized tensor cp decomposition algorithms based on sketching .	the quality of approximation under our method does not depend on properties such as sparsity , uniformity of elements , etc .	1	1	7	-5.709812	5.128562	1
111-10-45	we apply the method for topic modeling and obtain competitive results .	in this paper , we propose fast and randomized tensor cp decomposition algorithms based on sketching .	0	8	1	5.540409	-4.9052	0
111-10-45	keywords : tensor cp decomposition , count sketch , randomized methods , spectral methods , topic modeling	in this paper , we propose fast and randomized tensor cp decomposition algorithms based on sketching .	0	9	1	5.380685	-4.831578	0
111-10-45	we build on the idea of count sketches , but introduce many novel ideas which are unique to tensors .	we develop novel methods for randomized computation of tensor contractions via ffts , without explicitly forming the tensors .	1	2	3	-2.0577252	2.2651536	1
111-10-45	such tensor contractions are encountered in decomposition methods such as tensor power iterations and alternating least squares .	we build on the idea of count sketches , but introduce many novel ideas which are unique to tensors .	0	4	2	-3.1650634	3.1182346	1
111-10-45	we build on the idea of count sketches , but introduce many novel ideas which are unique to tensors .	we also design novel colliding hashes for symmetric tensors to further save time in computing the sketches .	1	2	5	-5.873504	5.0897884	1
111-10-45	we then combine these sketching ideas with existing whitening and tensor power iterative techniques to obtain the fastest algorithm on both sparse and dense tensors .	we build on the idea of count sketches , but introduce many novel ideas which are unique to tensors .	0	6	2	5.216326	-4.5843077	0
111-10-45	we build on the idea of count sketches , but introduce many novel ideas which are unique to tensors .	the quality of approximation under our method does not depend on properties such as sparsity , uniformity of elements , etc .	1	2	7	-4.573495	4.220597	1
111-10-45	we apply the method for topic modeling and obtain competitive results .	we build on the idea of count sketches , but introduce many novel ideas which are unique to tensors .	0	8	2	5.192178	-4.705044	0
111-10-45	keywords : tensor cp decomposition , count sketch , randomized methods , spectral methods , topic modeling	we build on the idea of count sketches , but introduce many novel ideas which are unique to tensors .	0	9	2	5.2607927	-4.7999344	0
111-10-45	we develop novel methods for randomized computation of tensor contractions via ffts , without explicitly forming the tensors .	such tensor contractions are encountered in decomposition methods such as tensor power iterations and alternating least squares .	1	3	4	3.0741067	-2.8303127	0
111-10-45	we also design novel colliding hashes for symmetric tensors to further save time in computing the sketches .	we develop novel methods for randomized computation of tensor contractions via ffts , without explicitly forming the tensors .	0	5	3	5.1806498	-4.541594	0
111-10-45	we then combine these sketching ideas with existing whitening and tensor power iterative techniques to obtain the fastest algorithm on both sparse and dense tensors .	we develop novel methods for randomized computation of tensor contractions via ffts , without explicitly forming the tensors .	0	6	3	5.092716	-4.432042	0
111-10-45	the quality of approximation under our method does not depend on properties such as sparsity , uniformity of elements , etc .	we develop novel methods for randomized computation of tensor contractions via ffts , without explicitly forming the tensors .	0	7	3	5.325329	-4.684707	0
111-10-45	we develop novel methods for randomized computation of tensor contractions via ffts , without explicitly forming the tensors .	we apply the method for topic modeling and obtain competitive results .	1	3	8	-6.012043	5.0869026	1
111-10-45	we develop novel methods for randomized computation of tensor contractions via ffts , without explicitly forming the tensors .	keywords : tensor cp decomposition , count sketch , randomized methods , spectral methods , topic modeling	1	3	9	-5.960477	5.0597563	1
111-10-45	we also design novel colliding hashes for symmetric tensors to further save time in computing the sketches .	such tensor contractions are encountered in decomposition methods such as tensor power iterations and alternating least squares .	0	5	4	4.302497	-3.927847	0
111-10-45	we then combine these sketching ideas with existing whitening and tensor power iterative techniques to obtain the fastest algorithm on both sparse and dense tensors .	such tensor contractions are encountered in decomposition methods such as tensor power iterations and alternating least squares .	0	6	4	4.7513537	-4.2570496	0
111-10-45	such tensor contractions are encountered in decomposition methods such as tensor power iterations and alternating least squares .	the quality of approximation under our method does not depend on properties such as sparsity , uniformity of elements , etc .	1	4	7	-5.7901344	5.10181	1
111-10-45	we apply the method for topic modeling and obtain competitive results .	such tensor contractions are encountered in decomposition methods such as tensor power iterations and alternating least squares .	0	8	4	5.3323846	-4.766579	0
111-10-45	such tensor contractions are encountered in decomposition methods such as tensor power iterations and alternating least squares .	keywords : tensor cp decomposition , count sketch , randomized methods , spectral methods , topic modeling	1	4	9	-5.896426	5.003929	1
111-10-45	we then combine these sketching ideas with existing whitening and tensor power iterative techniques to obtain the fastest algorithm on both sparse and dense tensors .	we also design novel colliding hashes for symmetric tensors to further save time in computing the sketches .	0	6	5	-1.5537639	1.7699438	1
111-10-45	the quality of approximation under our method does not depend on properties such as sparsity , uniformity of elements , etc .	we also design novel colliding hashes for symmetric tensors to further save time in computing the sketches .	0	7	5	-2.040808	2.203811	1
111-10-45	we also design novel colliding hashes for symmetric tensors to further save time in computing the sketches .	we apply the method for topic modeling and obtain competitive results .	1	5	8	-4.3967853	4.044884	1
111-10-45	we also design novel colliding hashes for symmetric tensors to further save time in computing the sketches .	keywords : tensor cp decomposition , count sketch , randomized methods , spectral methods , topic modeling	1	5	9	-5.3732696	4.7668943	1
111-10-45	the quality of approximation under our method does not depend on properties such as sparsity , uniformity of elements , etc .	we then combine these sketching ideas with existing whitening and tensor power iterative techniques to obtain the fastest algorithm on both sparse and dense tensors .	0	7	6	-0.8960881	1.143468	1
111-10-45	we then combine these sketching ideas with existing whitening and tensor power iterative techniques to obtain the fastest algorithm on both sparse and dense tensors .	we apply the method for topic modeling and obtain competitive results .	1	6	8	-4.948553	4.504391	1
111-10-45	we then combine these sketching ideas with existing whitening and tensor power iterative techniques to obtain the fastest algorithm on both sparse and dense tensors .	keywords : tensor cp decomposition , count sketch , randomized methods , spectral methods , topic modeling	1	6	9	-5.661154	4.9117823	1
111-10-45	the quality of approximation under our method does not depend on properties such as sparsity , uniformity of elements , etc .	we apply the method for topic modeling and obtain competitive results .	1	7	8	-4.9488688	4.5405283	1
111-10-45	the quality of approximation under our method does not depend on properties such as sparsity , uniformity of elements , etc .	keywords : tensor cp decomposition , count sketch , randomized methods , spectral methods , topic modeling	1	7	9	-5.622818	5.0127754	1
111-10-45	we apply the method for topic modeling and obtain competitive results .	keywords : tensor cp decomposition , count sketch , randomized methods , spectral methods , topic modeling	1	8	9	-4.41269	4.026892	1
112-6-15	it is originally motivated by 3d motion segmentation in computer vision , but has recently been generically applied to a wide range of statistical machine learning problems , which often involves sensitive datasets about human subjects .	subspace clustering is an unsupervised learning problem that aims at grouping data points into multiple `` clusters '' so that data points in a single cluster lie approximately on a low-dimensional linear subspace .	0	1	0	5.138976	-4.5747247	0
112-6-15	this raises a dire concern for data privacy .	subspace clustering is an unsupervised learning problem that aims at grouping data points into multiple `` clusters '' so that data points in a single cluster lie approximately on a low-dimensional linear subspace .	0	2	0	5.40476	-4.847127	0
112-6-15	in this work , we build on the framework of differential privacy and present two provably private subspace clustering algorithms .	subspace clustering is an unsupervised learning problem that aims at grouping data points into multiple `` clusters '' so that data points in a single cluster lie approximately on a low-dimensional linear subspace .	0	3	0	5.464371	-4.8697767	0
112-6-15	subspace clustering is an unsupervised learning problem that aims at grouping data points into multiple `` clusters '' so that data points in a single cluster lie approximately on a low-dimensional linear subspace .	we demonstrate via both theory and experiments that one of the presented methods enjoys formal privacy and utility guarantees ; the other one asymptotically preserves differential privacy while having good performance in practice .	1	0	4	-5.9636984	5.1479483	1
112-6-15	along the course of the proof , we also obtain two new provable guarantees for the agnostic subspace clustering and the graph connectivity problem which might be of independent interests .	subspace clustering is an unsupervised learning problem that aims at grouping data points into multiple `` clusters '' so that data points in a single cluster lie approximately on a low-dimensional linear subspace .	0	5	0	5.6218534	-5.024888	0
112-6-15	it is originally motivated by 3d motion segmentation in computer vision , but has recently been generically applied to a wide range of statistical machine learning problems , which often involves sensitive datasets about human subjects .	this raises a dire concern for data privacy .	1	1	2	-1.7437732	1.8873402	1
112-6-15	it is originally motivated by 3d motion segmentation in computer vision , but has recently been generically applied to a wide range of statistical machine learning problems , which often involves sensitive datasets about human subjects .	in this work , we build on the framework of differential privacy and present two provably private subspace clustering algorithms .	1	1	3	-5.5901537	5.132152	1
112-6-15	it is originally motivated by 3d motion segmentation in computer vision , but has recently been generically applied to a wide range of statistical machine learning problems , which often involves sensitive datasets about human subjects .	we demonstrate via both theory and experiments that one of the presented methods enjoys formal privacy and utility guarantees ; the other one asymptotically preserves differential privacy while having good performance in practice .	1	1	4	-5.97808	5.256134	1
112-6-15	along the course of the proof , we also obtain two new provable guarantees for the agnostic subspace clustering and the graph connectivity problem which might be of independent interests .	it is originally motivated by 3d motion segmentation in computer vision , but has recently been generically applied to a wide range of statistical machine learning problems , which often involves sensitive datasets about human subjects .	0	5	1	5.304453	-4.7315955	0
112-6-15	in this work , we build on the framework of differential privacy and present two provably private subspace clustering algorithms .	this raises a dire concern for data privacy .	0	3	2	5.2244353	-4.672693	0
112-6-15	we demonstrate via both theory and experiments that one of the presented methods enjoys formal privacy and utility guarantees ; the other one asymptotically preserves differential privacy while having good performance in practice .	this raises a dire concern for data privacy .	0	4	2	5.5336065	-4.934982	0
112-6-15	along the course of the proof , we also obtain two new provable guarantees for the agnostic subspace clustering and the graph connectivity problem which might be of independent interests .	this raises a dire concern for data privacy .	0	5	2	5.2224574	-4.7258177	0
112-6-15	we demonstrate via both theory and experiments that one of the presented methods enjoys formal privacy and utility guarantees ; the other one asymptotically preserves differential privacy while having good performance in practice .	in this work , we build on the framework of differential privacy and present two provably private subspace clustering algorithms .	0	4	3	5.518339	-4.8931518	0
112-6-15	in this work , we build on the framework of differential privacy and present two provably private subspace clustering algorithms .	along the course of the proof , we also obtain two new provable guarantees for the agnostic subspace clustering and the graph connectivity problem which might be of independent interests .	1	3	5	-5.9992704	5.093998	1
112-6-15	we demonstrate via both theory and experiments that one of the presented methods enjoys formal privacy and utility guarantees ; the other one asymptotically preserves differential privacy while having good performance in practice .	along the course of the proof , we also obtain two new provable guarantees for the agnostic subspace clustering and the graph connectivity problem which might be of independent interests .	1	4	5	-1.8950207	2.0096622	1
113-7-21	modern prediction problems arising in multilabel learning and learning to rank pose unique challenges to the classical theory of supervised learning .	these problems have large prediction and label spaces of a combinatorial nature and involve sophisticated loss functions .	1	0	1	-5.8760676	5.2327547	1
113-7-21	we offer a general framework to derive mistake driven online algorithms and associated loss bounds .	modern prediction problems arising in multilabel learning and learning to rank pose unique challenges to the classical theory of supervised learning .	0	2	0	5.672498	-5.0524626	0
113-7-21	the key ingredients in our framework are a general loss function , a general vector space representation of predictions , and a notion of margin with respect to a general norm .	modern prediction problems arising in multilabel learning and learning to rank pose unique challenges to the classical theory of supervised learning .	0	3	0	5.662225	-5.0410805	0
113-7-21	our general algorithm , predtron , yields the perceptron algorithm and its variants when instantiated on classic problems such as binary classification , multiclass classification , ordinal regression , and multilabel classification .	modern prediction problems arising in multilabel learning and learning to rank pose unique challenges to the classical theory of supervised learning .	0	4	0	5.6532564	-5.0832815	0
113-7-21	modern prediction problems arising in multilabel learning and learning to rank pose unique challenges to the classical theory of supervised learning .	for multilabel ranking and subset ranking , we derive novel algorithms , notions of margins , and loss bounds .	1	0	5	-5.954195	5.1317782	1
113-7-21	a simulation study confirms the behavior predicted by our bounds and demonstrates the flexibility of the design choices in our framework .	modern prediction problems arising in multilabel learning and learning to rank pose unique challenges to the classical theory of supervised learning .	0	6	0	5.6398983	-5.0157385	0
113-7-21	we offer a general framework to derive mistake driven online algorithms and associated loss bounds .	these problems have large prediction and label spaces of a combinatorial nature and involve sophisticated loss functions .	0	2	1	4.437874	-3.9126282	0
113-7-21	the key ingredients in our framework are a general loss function , a general vector space representation of predictions , and a notion of margin with respect to a general norm .	these problems have large prediction and label spaces of a combinatorial nature and involve sophisticated loss functions .	0	3	1	5.2501316	-4.625664	0
113-7-21	these problems have large prediction and label spaces of a combinatorial nature and involve sophisticated loss functions .	our general algorithm , predtron , yields the perceptron algorithm and its variants when instantiated on classic problems such as binary classification , multiclass classification , ordinal regression , and multilabel classification .	1	1	4	-4.930237	4.6368303	1
113-7-21	these problems have large prediction and label spaces of a combinatorial nature and involve sophisticated loss functions .	for multilabel ranking and subset ranking , we derive novel algorithms , notions of margins , and loss bounds .	1	1	5	-5.9081	5.110121	1
113-7-21	these problems have large prediction and label spaces of a combinatorial nature and involve sophisticated loss functions .	a simulation study confirms the behavior predicted by our bounds and demonstrates the flexibility of the design choices in our framework .	1	1	6	-5.967414	5.0678906	1
113-7-21	we offer a general framework to derive mistake driven online algorithms and associated loss bounds .	the key ingredients in our framework are a general loss function , a general vector space representation of predictions , and a notion of margin with respect to a general norm .	1	2	3	-5.882655	5.237811	1
113-7-21	we offer a general framework to derive mistake driven online algorithms and associated loss bounds .	our general algorithm , predtron , yields the perceptron algorithm and its variants when instantiated on classic problems such as binary classification , multiclass classification , ordinal regression , and multilabel classification .	1	2	4	-5.715783	5.2001305	1
113-7-21	for multilabel ranking and subset ranking , we derive novel algorithms , notions of margins , and loss bounds .	we offer a general framework to derive mistake driven online algorithms and associated loss bounds .	0	5	2	4.547375	-4.0298557	0
113-7-21	a simulation study confirms the behavior predicted by our bounds and demonstrates the flexibility of the design choices in our framework .	we offer a general framework to derive mistake driven online algorithms and associated loss bounds .	0	6	2	5.549976	-4.8615627	0
113-7-21	the key ingredients in our framework are a general loss function , a general vector space representation of predictions , and a notion of margin with respect to a general norm .	our general algorithm , predtron , yields the perceptron algorithm and its variants when instantiated on classic problems such as binary classification , multiclass classification , ordinal regression , and multilabel classification .	1	3	4	-0.011038661	0.33346897	1
113-7-21	for multilabel ranking and subset ranking , we derive novel algorithms , notions of margins , and loss bounds .	the key ingredients in our framework are a general loss function , a general vector space representation of predictions , and a notion of margin with respect to a general norm .	0	5	3	-0.5681279	0.78375614	1
113-7-21	the key ingredients in our framework are a general loss function , a general vector space representation of predictions , and a notion of margin with respect to a general norm .	a simulation study confirms the behavior predicted by our bounds and demonstrates the flexibility of the design choices in our framework .	1	3	6	-5.7002974	5.0369196	1
113-7-21	our general algorithm , predtron , yields the perceptron algorithm and its variants when instantiated on classic problems such as binary classification , multiclass classification , ordinal regression , and multilabel classification .	for multilabel ranking and subset ranking , we derive novel algorithms , notions of margins , and loss bounds .	1	4	5	-2.3318434	2.405734	1
113-7-21	a simulation study confirms the behavior predicted by our bounds and demonstrates the flexibility of the design choices in our framework .	our general algorithm , predtron , yields the perceptron algorithm and its variants when instantiated on classic problems such as binary classification , multiclass classification , ordinal regression , and multilabel classification .	0	6	4	5.1249905	-4.545847	0
113-7-21	for multilabel ranking and subset ranking , we derive novel algorithms , notions of margins , and loss bounds .	a simulation study confirms the behavior predicted by our bounds and demonstrates the flexibility of the design choices in our framework .	1	5	6	-5.973362	5.123883	1
114-3-3	we show how it can be computed exactly by semidefinite programming , and how to approximate it using svm computations .	we introduce a unifying generalization of the lovasz theta function , and the associated geometric embedding , for graphs with weights on both nodes and edges .	0	1	0	5.3418884	-4.7183285	0
114-3-3	we introduce a unifying generalization of the lovasz theta function , and the associated geometric embedding , for graphs with weights on both nodes and edges .	we show how the theta function can be interpreted as a measure of diversity in graphs and use this idea , and the graph embedding in algorithms for max-cut , correlation clustering and document summarization , all of which are well represented as problems on weighted graphs .	1	0	2	-5.829493	5.2037554	1
114-3-3	we show how the theta function can be interpreted as a measure of diversity in graphs and use this idea , and the graph embedding in algorithms for max-cut , correlation clustering and document summarization , all of which are well represented as problems on weighted graphs .	we show how it can be computed exactly by semidefinite programming , and how to approximate it using svm computations .	0	2	1	-3.5322268	3.378996	1
115-6-15	in many learning problems , ranging from clustering to ranking through metric learning , empirical estimates of the risk functional consist of an average over tuples ( e.g. , pairs or triplets ) of observations , rather than over individual observations .	in this paper , we focus on how to best implement a stochastic approximation approach to solve such risk minimization problems .	1	0	1	-5.9616966	5.242271	1
115-6-15	in many learning problems , ranging from clustering to ranking through metric learning , empirical estimates of the risk functional consist of an average over tuples ( e.g. , pairs or triplets ) of observations , rather than over individual observations .	we argue that in the largescale setting , gradient estimates should be obtained by sampling tuples of data points with replacement ( incomplete u -statistics ) instead of sampling data points without replacement ( complete u -statistics based on subsamples ) .	1	0	2	-6.0092936	5.225811	1
115-6-15	in many learning problems , ranging from clustering to ranking through metric learning , empirical estimates of the risk functional consist of an average over tuples ( e.g. , pairs or triplets ) of observations , rather than over individual observations .	we develop a theoretical framework accounting for the substantial impact of this strategy on the generalization ability of the prediction model returned by the stochastic gradient descent ( sgd ) algorithm .	1	0	3	-6.014685	5.2148385	1
115-6-15	it reveals that the method we promote achieves a much better trade-off between statistical accuracy and computational cost .	in many learning problems , ranging from clustering to ranking through metric learning , empirical estimates of the risk functional consist of an average over tuples ( e.g. , pairs or triplets ) of observations , rather than over individual observations .	0	4	0	5.611662	-5.0652027	0
115-6-15	in many learning problems , ranging from clustering to ranking through metric learning , empirical estimates of the risk functional consist of an average over tuples ( e.g. , pairs or triplets ) of observations , rather than over individual observations .	beyond the rate bound analysis , experiments on auc maximization and metric learning provide strong empirical evidence of the superiority of the proposed approach .	1	0	5	-5.9174533	5.171529	1
115-6-15	in this paper , we focus on how to best implement a stochastic approximation approach to solve such risk minimization problems .	we argue that in the largescale setting , gradient estimates should be obtained by sampling tuples of data points with replacement ( incomplete u -statistics ) instead of sampling data points without replacement ( complete u -statistics based on subsamples ) .	1	1	2	-5.076522	4.7367687	1
115-6-15	we develop a theoretical framework accounting for the substantial impact of this strategy on the generalization ability of the prediction model returned by the stochastic gradient descent ( sgd ) algorithm .	in this paper , we focus on how to best implement a stochastic approximation approach to solve such risk minimization problems .	0	3	1	5.31973	-4.6743174	0
115-6-15	it reveals that the method we promote achieves a much better trade-off between statistical accuracy and computational cost .	in this paper , we focus on how to best implement a stochastic approximation approach to solve such risk minimization problems .	0	4	1	5.5579305	-4.966396	0
115-6-15	beyond the rate bound analysis , experiments on auc maximization and metric learning provide strong empirical evidence of the superiority of the proposed approach .	in this paper , we focus on how to best implement a stochastic approximation approach to solve such risk minimization problems .	0	5	1	5.6187506	-4.9619703	0
115-6-15	we argue that in the largescale setting , gradient estimates should be obtained by sampling tuples of data points with replacement ( incomplete u -statistics ) instead of sampling data points without replacement ( complete u -statistics based on subsamples ) .	we develop a theoretical framework accounting for the substantial impact of this strategy on the generalization ability of the prediction model returned by the stochastic gradient descent ( sgd ) algorithm .	1	2	3	-4.3923655	4.2272744	1
115-6-15	it reveals that the method we promote achieves a much better trade-off between statistical accuracy and computational cost .	we argue that in the largescale setting , gradient estimates should be obtained by sampling tuples of data points with replacement ( incomplete u -statistics ) instead of sampling data points without replacement ( complete u -statistics based on subsamples ) .	0	4	2	5.289103	-4.7029095	0
115-6-15	beyond the rate bound analysis , experiments on auc maximization and metric learning provide strong empirical evidence of the superiority of the proposed approach .	we argue that in the largescale setting , gradient estimates should be obtained by sampling tuples of data points with replacement ( incomplete u -statistics ) instead of sampling data points without replacement ( complete u -statistics based on subsamples ) .	0	5	2	5.209512	-4.5914435	0
115-6-15	it reveals that the method we promote achieves a much better trade-off between statistical accuracy and computational cost .	we develop a theoretical framework accounting for the substantial impact of this strategy on the generalization ability of the prediction model returned by the stochastic gradient descent ( sgd ) algorithm .	0	4	3	5.409326	-4.7563915	0
115-6-15	beyond the rate bound analysis , experiments on auc maximization and metric learning provide strong empirical evidence of the superiority of the proposed approach .	we develop a theoretical framework accounting for the substantial impact of this strategy on the generalization ability of the prediction model returned by the stochastic gradient descent ( sgd ) algorithm .	0	5	3	5.3117075	-4.603951	0
115-6-15	it reveals that the method we promote achieves a much better trade-off between statistical accuracy and computational cost .	beyond the rate bound analysis , experiments on auc maximization and metric learning provide strong empirical evidence of the superiority of the proposed approach .	1	4	5	2.48169	-2.3578324	0
116-5-10	we study the topic under two standard settings -- multi-armed bandits and hidden bipartite graphs -- which differ in the nature of the input distributions .	this paper discusses how to efficiently choose from n unknown distributions the k ones whose means are the greatest by a certain metric , up to a small relative error .	0	1	0	5.03941	-4.413877	0
116-5-10	this paper discusses how to efficiently choose from n unknown distributions the k ones whose means are the greatest by a certain metric , up to a small relative error .	in the former setting , each distribution can be sampled ( in the i.i.d .	1	0	2	-5.4173584	5.0027237	1
116-5-10	manner ) an arbitrary number of times , whereas in the latter , each distribution is defined on a population of a finite size m ( and hence , is fully revealed after m samples ) .	this paper discusses how to efficiently choose from n unknown distributions the k ones whose means are the greatest by a certain metric , up to a small relative error .	0	3	0	4.8874817	-4.3518763	0
116-5-10	for both settings , we prove lower bounds on the total number of samples needed , and propose optimal algorithms whose sample complexities match those lower bounds .	this paper discusses how to efficiently choose from n unknown distributions the k ones whose means are the greatest by a certain metric , up to a small relative error .	0	4	0	5.6285095	-5.013574	0
116-5-10	we study the topic under two standard settings -- multi-armed bandits and hidden bipartite graphs -- which differ in the nature of the input distributions .	in the former setting , each distribution can be sampled ( in the i.i.d .	1	1	2	-3.3528674	3.2963204	1
116-5-10	manner ) an arbitrary number of times , whereas in the latter , each distribution is defined on a population of a finite size m ( and hence , is fully revealed after m samples ) .	we study the topic under two standard settings -- multi-armed bandits and hidden bipartite graphs -- which differ in the nature of the input distributions .	0	3	1	3.2960424	-3.058462	0
116-5-10	for both settings , we prove lower bounds on the total number of samples needed , and propose optimal algorithms whose sample complexities match those lower bounds .	we study the topic under two standard settings -- multi-armed bandits and hidden bipartite graphs -- which differ in the nature of the input distributions .	0	4	1	5.479643	-4.8865776	0
116-5-10	manner ) an arbitrary number of times , whereas in the latter , each distribution is defined on a population of a finite size m ( and hence , is fully revealed after m samples ) .	in the former setting , each distribution can be sampled ( in the i.i.d .	0	3	2	3.312683	-2.9678566	0
116-5-10	for both settings , we prove lower bounds on the total number of samples needed , and propose optimal algorithms whose sample complexities match those lower bounds .	in the former setting , each distribution can be sampled ( in the i.i.d .	0	4	2	4.881555	-4.306235	0
116-5-10	for both settings , we prove lower bounds on the total number of samples needed , and propose optimal algorithms whose sample complexities match those lower bounds .	manner ) an arbitrary number of times , whereas in the latter , each distribution is defined on a population of a finite size m ( and hence , is fully revealed after m samples ) .	0	4	3	4.92109	-4.4410324	0
117-6-15	in simple perceptual decisions the brain has to identify a stimulus based on noisy sensory samples from the stimulus .	basic statistical considerations state that the reliability of the stimulus information , i.e. , the amount of noise in the samples , should be taken into account when the decision is made .	1	0	1	-5.7490277	5.144652	1
117-6-15	in simple perceptual decisions the brain has to identify a stimulus based on noisy sensory samples from the stimulus .	however , for perceptual decision making experiments it has been questioned whether the brain indeed uses the reliability for making decisions when confronted with unpredictable changes in stimulus reliability .	1	0	2	-5.7082624	5.124369	1
117-6-15	in simple perceptual decisions the brain has to identify a stimulus based on noisy sensory samples from the stimulus .	we here show that even the basic drift diffusion model , which has frequently been used to explain experimental findings in perceptual decision making , implicitly relies on estimates of stimulus reliability .	1	0	3	-5.942725	5.0802746	1
117-6-15	we then show that only those variants of the drift diffusion model which allow stimulusspecific reliabilities are consistent with neurophysiological findings .	in simple perceptual decisions the brain has to identify a stimulus based on noisy sensory samples from the stimulus .	0	4	0	5.696643	-5.0747337	0
117-6-15	our analysis suggests that the brain estimates the reliability of the stimulus on a short time scale of at most a few hundred milliseconds .	in simple perceptual decisions the brain has to identify a stimulus based on noisy sensory samples from the stimulus .	0	5	0	5.70923	-5.1285615	0
117-6-15	basic statistical considerations state that the reliability of the stimulus information , i.e. , the amount of noise in the samples , should be taken into account when the decision is made .	however , for perceptual decision making experiments it has been questioned whether the brain indeed uses the reliability for making decisions when confronted with unpredictable changes in stimulus reliability .	1	1	2	3.4801044	-3.2625048	0
117-6-15	basic statistical considerations state that the reliability of the stimulus information , i.e. , the amount of noise in the samples , should be taken into account when the decision is made .	we here show that even the basic drift diffusion model , which has frequently been used to explain experimental findings in perceptual decision making , implicitly relies on estimates of stimulus reliability .	1	1	3	-5.9576883	5.000273	1
117-6-15	we then show that only those variants of the drift diffusion model which allow stimulusspecific reliabilities are consistent with neurophysiological findings .	basic statistical considerations state that the reliability of the stimulus information , i.e. , the amount of noise in the samples , should be taken into account when the decision is made .	0	4	1	5.5222445	-4.896777	0
117-6-15	basic statistical considerations state that the reliability of the stimulus information , i.e. , the amount of noise in the samples , should be taken into account when the decision is made .	our analysis suggests that the brain estimates the reliability of the stimulus on a short time scale of at most a few hundred milliseconds .	1	1	5	-5.9354	4.9750767	1
117-6-15	however , for perceptual decision making experiments it has been questioned whether the brain indeed uses the reliability for making decisions when confronted with unpredictable changes in stimulus reliability .	we here show that even the basic drift diffusion model , which has frequently been used to explain experimental findings in perceptual decision making , implicitly relies on estimates of stimulus reliability .	1	2	3	-5.978026	5.1783037	1
117-6-15	we then show that only those variants of the drift diffusion model which allow stimulusspecific reliabilities are consistent with neurophysiological findings .	however , for perceptual decision making experiments it has been questioned whether the brain indeed uses the reliability for making decisions when confronted with unpredictable changes in stimulus reliability .	0	4	2	5.572306	-4.981383	0
117-6-15	our analysis suggests that the brain estimates the reliability of the stimulus on a short time scale of at most a few hundred milliseconds .	however , for perceptual decision making experiments it has been questioned whether the brain indeed uses the reliability for making decisions when confronted with unpredictable changes in stimulus reliability .	0	5	2	5.5357876	-4.99598	0
117-6-15	we then show that only those variants of the drift diffusion model which allow stimulusspecific reliabilities are consistent with neurophysiological findings .	we here show that even the basic drift diffusion model , which has frequently been used to explain experimental findings in perceptual decision making , implicitly relies on estimates of stimulus reliability .	0	4	3	5.046523	-4.4923162	0
117-6-15	our analysis suggests that the brain estimates the reliability of the stimulus on a short time scale of at most a few hundred milliseconds .	we here show that even the basic drift diffusion model , which has frequently been used to explain experimental findings in perceptual decision making , implicitly relies on estimates of stimulus reliability .	0	5	3	5.469208	-4.8492227	0
117-6-15	we then show that only those variants of the drift diffusion model which allow stimulusspecific reliabilities are consistent with neurophysiological findings .	our analysis suggests that the brain estimates the reliability of the stimulus on a short time scale of at most a few hundred milliseconds .	1	4	5	0.9526824	-0.621887	0
118-6-15	we consider the problem of binary classification when the covariates conditioned on the each of the response values follow multivariate gaussian distributions .	we focus on the setting where the covariance matrices for the two conditional distributions are the same .	1	0	1	-5.817354	5.192858	1
118-6-15	the corresponding generative model classifier , derived via the bayes rule , also called linear discriminant analysis , has been shown to behave poorly in high-dimensional settings .	we consider the problem of binary classification when the covariates conditioned on the each of the response values follow multivariate gaussian distributions .	0	2	0	5.5917044	-5.074255	0
118-6-15	we present a novel analysis of the classification error of any linear discriminant approach given conditional gaussian models .	we consider the problem of binary classification when the covariates conditioned on the each of the response values follow multivariate gaussian distributions .	0	3	0	5.690021	-5.0687075	0
118-6-15	we consider the problem of binary classification when the covariates conditioned on the each of the response values follow multivariate gaussian distributions .	this allows us to compare the generative model classifier , other recently proposed discriminative approaches that directly learn the discriminant function , and then finally logistic regression which is another classical discriminative model classifier .	1	0	4	-5.8558464	5.1358404	1
118-6-15	we consider the problem of binary classification when the covariates conditioned on the each of the response values follow multivariate gaussian distributions .	[CLS] as we show, under a natural sparsity assumption, and letting s denote the sparsity of the bayes classifier, p the number of covariates, and n the number of samples, the simple ( 1 - regularized ) logistic regression classifier achieves p the fast misclassification error rates of	1	0	5	-5.8366194	5.1638355	1
118-6-15	we focus on the setting where the covariance matrices for the two conditional distributions are the same .	the corresponding generative model classifier , derived via the bayes rule , also called linear discriminant analysis , has been shown to behave poorly in high-dimensional settings .	1	1	2	-3.2820683	3.2600467	1
118-6-15	we present a novel analysis of the classification error of any linear discriminant approach given conditional gaussian models .	we focus on the setting where the covariance matrices for the two conditional distributions are the same .	0	3	1	-2.2061055	2.305296	1
118-6-15	we focus on the setting where the covariance matrices for the two conditional distributions are the same .	this allows us to compare the generative model classifier , other recently proposed discriminative approaches that directly learn the discriminant function , and then finally logistic regression which is another classical discriminative model classifier .	1	1	4	-6.0029435	5.157	1
118-6-15	[CLS] as we show, under a natural sparsity assumption, and letting s denote the sparsity of the bayes classifier, p the number of covariates, and n the number of samples, the simple ( 1 - regularized ) logistic regression classifier achieves p the fast misclassification error rates of o s log, which is much better than the	we focus on the setting where the covariance matrices for the two conditional distributions are the same .	0	5	1	5.3488474	-4.741579	0
118-6-15	we present a novel analysis of the classification error of any linear discriminant approach given conditional gaussian models .	the corresponding generative model classifier , derived via the bayes rule , also called linear discriminant analysis , has been shown to behave poorly in high-dimensional settings .	0	3	2	4.4017467	-3.9163136	0
118-6-15	this allows us to compare the generative model classifier , other recently proposed discriminative approaches that directly learn the discriminant function , and then finally logistic regression which is another classical discriminative model classifier .	the corresponding generative model classifier , derived via the bayes rule , also called linear discriminant analysis , has been shown to behave poorly in high-dimensional settings .	0	4	2	4.9441595	-4.3179574	0
118-6-15	the corresponding generative model classifier , derived via the bayes rule , also called linear discriminant analysis , has been shown to behave poorly in high-dimensional settings .	[CLS] as we show, under a natural sparsity assumption, and letting s denote the sparsity of the bayes classifier, p the number of covariates, and n the number of samples, the simple ( 1 - regularized ) logistic regression classifier achieves p the fast	1	2	5	-5.914992	5.1831474	1
118-6-15	this allows us to compare the generative model classifier , other recently proposed discriminative approaches that directly learn the discriminant function , and then finally logistic regression which is another classical discriminative model classifier .	we present a novel analysis of the classification error of any linear discriminant approach given conditional gaussian models .	0	4	3	4.97081	-4.364064	0
118-6-15	we present a novel analysis of the classification error of any linear discriminant approach given conditional gaussian models .	[CLS] as we show, under a natural sparsity assumption, and letting s denote the sparsity of the bayes classifier, p the number of covariates, and n the number of samples, the simple ( 1 - regularized ) logistic regression classifier achieves p the fast misclassification error rates of o s log, which is	1	3	5	-5.814593	5.180926	1
118-6-15	[CLS] as we show, under a natural sparsity assumption, and letting s denote the sparsity of the bayes classifier, p the number of covariates, and n the number of samples, the simple ( 1 - regular	[CLS] this allows us to compare the generative model classifier, other recently proposed discriminative approaches that directly learn the discriminant function, and then finally logistic regression which is another classical discriminative model classifier. [SEP]	0	5	4	-5.6181583	4.9520845	1
119-8-28	due to the increasing size of data , practitioners interested in clustering have turned to distributed computation methods .	clustering large data is a fundamental problem with a vast number of applications .	0	1	0	5.581688	-4.973878	0
119-8-28	in this work , we consider the widely used kcenter clustering problem and its variant used to handle noisy data , k-center with outliers .	clustering large data is a fundamental problem with a vast number of applications .	0	2	0	5.6914773	-5.0953345	0
119-8-28	clustering large data is a fundamental problem with a vast number of applications .	in the noise-free setting we demonstrate how a previously-proposed distributed method is actually an o ( 1 ) -approximation algorithm , which accurately explains its strong empirical performance .	1	0	3	-5.8599825	5.1358194	1
119-8-28	additionally , in the noisy setting , we develop a novel distributed algorithm that is also an o ( 1 ) -approximation .	clustering large data is a fundamental problem with a vast number of applications .	0	4	0	5.6528025	-5.04057	0
119-8-28	clustering large data is a fundamental problem with a vast number of applications .	these algorithms are highly parallel and lend themselves to virtually any distributed computing framework .	1	0	5	-5.894574	5.1486583	1
119-8-28	we compare each empirically against the best known sequential clustering methods and show that both distributed algorithms are consistently close to their sequential versions .	clustering large data is a fundamental problem with a vast number of applications .	0	6	0	5.6860085	-5.098659	0
119-8-28	the algorithms are all one can hope for in distributed settings : they are fast , memory efficient and they match their sequential counterparts .	clustering large data is a fundamental problem with a vast number of applications .	0	7	0	5.6876307	-5.074073	0
119-8-28	due to the increasing size of data , practitioners interested in clustering have turned to distributed computation methods .	in this work , we consider the widely used kcenter clustering problem and its variant used to handle noisy data , k-center with outliers .	1	1	2	-5.9523835	5.222405	1
119-8-28	in the noise-free setting we demonstrate how a previously-proposed distributed method is actually an o ( 1 ) -approximation algorithm , which accurately explains its strong empirical performance .	due to the increasing size of data , practitioners interested in clustering have turned to distributed computation methods .	0	3	1	5.708579	-5.122948	0
119-8-28	due to the increasing size of data , practitioners interested in clustering have turned to distributed computation methods .	additionally , in the noisy setting , we develop a novel distributed algorithm that is also an o ( 1 ) -approximation .	1	1	4	-5.9402595	5.072192	1
119-8-28	these algorithms are highly parallel and lend themselves to virtually any distributed computing framework .	due to the increasing size of data , practitioners interested in clustering have turned to distributed computation methods .	0	5	1	5.6002407	-5.032357	0
119-8-28	due to the increasing size of data , practitioners interested in clustering have turned to distributed computation methods .	we compare each empirically against the best known sequential clustering methods and show that both distributed algorithms are consistently close to their sequential versions .	1	1	6	-5.9534388	5.1268864	1
119-8-28	the algorithms are all one can hope for in distributed settings : they are fast , memory efficient and they match their sequential counterparts .	due to the increasing size of data , practitioners interested in clustering have turned to distributed computation methods .	0	7	1	5.665871	-5.115178	0
119-8-28	in this work , we consider the widely used kcenter clustering problem and its variant used to handle noisy data , k-center with outliers .	in the noise-free setting we demonstrate how a previously-proposed distributed method is actually an o ( 1 ) -approximation algorithm , which accurately explains its strong empirical performance .	1	2	3	-6.0334616	5.1846	1
119-8-28	additionally , in the noisy setting , we develop a novel distributed algorithm that is also an o ( 1 ) -approximation .	in this work , we consider the widely used kcenter clustering problem and its variant used to handle noisy data , k-center with outliers .	0	4	2	5.523067	-4.8729753	0
119-8-28	these algorithms are highly parallel and lend themselves to virtually any distributed computing framework .	in this work , we consider the widely used kcenter clustering problem and its variant used to handle noisy data , k-center with outliers .	0	5	2	2.2221878	-1.8523356	0
119-8-28	we compare each empirically against the best known sequential clustering methods and show that both distributed algorithms are consistently close to their sequential versions .	in this work , we consider the widely used kcenter clustering problem and its variant used to handle noisy data , k-center with outliers .	0	6	2	5.687127	-5.102866	0
119-8-28	the algorithms are all one can hope for in distributed settings : they are fast , memory efficient and they match their sequential counterparts .	in this work , we consider the widely used kcenter clustering problem and its variant used to handle noisy data , k-center with outliers .	0	7	2	4.0727572	-3.6241012	0
119-8-28	in the noise-free setting we demonstrate how a previously-proposed distributed method is actually an o ( 1 ) -approximation algorithm , which accurately explains its strong empirical performance .	additionally , in the noisy setting , we develop a novel distributed algorithm that is also an o ( 1 ) -approximation .	1	3	4	-0.4657268	0.75742596	1
119-8-28	in the noise-free setting we demonstrate how a previously-proposed distributed method is actually an o ( 1 ) -approximation algorithm , which accurately explains its strong empirical performance .	these algorithms are highly parallel and lend themselves to virtually any distributed computing framework .	1	3	5	3.990898	-3.645188	0
119-8-28	in the noise-free setting we demonstrate how a previously-proposed distributed method is actually an o ( 1 ) -approximation algorithm , which accurately explains its strong empirical performance .	we compare each empirically against the best known sequential clustering methods and show that both distributed algorithms are consistently close to their sequential versions .	1	3	6	-3.8531833	3.639295	1
119-8-28	in the noise-free setting we demonstrate how a previously-proposed distributed method is actually an o ( 1 ) -approximation algorithm , which accurately explains its strong empirical performance .	the algorithms are all one can hope for in distributed settings : they are fast , memory efficient and they match their sequential counterparts .	1	3	7	3.012794	-2.8579001	0
119-8-28	additionally , in the noisy setting , we develop a novel distributed algorithm that is also an o ( 1 ) -approximation .	these algorithms are highly parallel and lend themselves to virtually any distributed computing framework .	1	4	5	3.2408702	-3.0265067	0
119-8-28	additionally , in the noisy setting , we develop a novel distributed algorithm that is also an o ( 1 ) -approximation .	we compare each empirically against the best known sequential clustering methods and show that both distributed algorithms are consistently close to their sequential versions .	1	4	6	-4.0395226	3.746653	1
119-8-28	the algorithms are all one can hope for in distributed settings : they are fast , memory efficient and they match their sequential counterparts .	additionally , in the noisy setting , we develop a novel distributed algorithm that is also an o ( 1 ) -approximation .	0	7	4	-4.2367196	4.0732	1
119-8-28	we compare each empirically against the best known sequential clustering methods and show that both distributed algorithms are consistently close to their sequential versions .	these algorithms are highly parallel and lend themselves to virtually any distributed computing framework .	0	6	5	5.0423665	-4.506758	0
119-8-28	these algorithms are highly parallel and lend themselves to virtually any distributed computing framework .	the algorithms are all one can hope for in distributed settings : they are fast , memory efficient and they match their sequential counterparts .	1	5	7	-2.645013	2.6619413	1
119-8-28	the algorithms are all one can hope for in distributed settings : they are fast , memory efficient and they match their sequential counterparts .	we compare each empirically against the best known sequential clustering methods and show that both distributed algorithms are consistently close to their sequential versions .	0	7	6	-5.6631756	5.132607	1
120-8-28	imagine a random walk that outputs a state only when visiting it for the first time .	the observed output is therefore a repeat-censored version of the underlying walk , and consists of a permutation of the states or a prefix of it .	1	0	1	-5.829868	5.1980405	1
120-8-28	imagine a random walk that outputs a state only when visiting it for the first time .	we call this model initial-visit emitting random walk ( invite ) .	1	0	2	-5.182846	4.787343	1
120-8-28	prior work has shown that the random walks with such a repeat-censoring mechanism explain well human behavior in memory search tasks , which is of great interest in both the study of human cognition and various clinical applications .	imagine a random walk that outputs a state only when visiting it for the first time .	0	3	0	5.1315856	-4.5205903	0
120-8-28	however , parameter estimation in invite is challenging , because naive likelihood computation by marginalizing over infinitely many hidden random walk trajectories is intractable .	imagine a random walk that outputs a state only when visiting it for the first time .	0	4	0	3.9641063	-3.590063	0
120-8-28	imagine a random walk that outputs a state only when visiting it for the first time .	in this paper , we propose the first efficient maximum likelihood estimate ( mle ) for invite by decomposing the censored output into a series of absorbing random walks .	1	0	5	-4.5537343	4.2640986	1
120-8-28	we also prove theoretical properties of the mle including identifiability and consistency .	imagine a random walk that outputs a state only when visiting it for the first time .	0	6	0	5.5397696	-4.8951883	0
120-8-28	we show that invite outperforms several existing methods on real-world human response data from memory search tasks .	imagine a random walk that outputs a state only when visiting it for the first time .	0	7	0	5.567521	-5.00025	0
120-8-28	the observed output is therefore a repeat-censored version of the underlying walk , and consists of a permutation of the states or a prefix of it .	we call this model initial-visit emitting random walk ( invite ) .	1	1	2	1.7891524	-1.6208866	0
120-8-28	prior work has shown that the random walks with such a repeat-censoring mechanism explain well human behavior in memory search tasks , which is of great interest in both the study of human cognition and various clinical applications .	the observed output is therefore a repeat-censored version of the underlying walk , and consists of a permutation of the states or a prefix of it .	0	3	1	0.8361984	-0.67895615	0
120-8-28	the observed output is therefore a repeat-censored version of the underlying walk , and consists of a permutation of the states or a prefix of it .	however , parameter estimation in invite is challenging , because naive likelihood computation by marginalizing over infinitely many hidden random walk trajectories is intractable .	1	1	4	3.7773087	-3.501656	0
120-8-28	the observed output is therefore a repeat-censored version of the underlying walk , and consists of a permutation of the states or a prefix of it .	in this paper , we propose the first efficient maximum likelihood estimate ( mle ) for invite by decomposing the censored output into a series of absorbing random walks .	1	1	5	-0.007281162	0.21361986	1
120-8-28	we also prove theoretical properties of the mle including identifiability and consistency .	the observed output is therefore a repeat-censored version of the underlying walk , and consists of a permutation of the states or a prefix of it .	0	6	1	4.9765463	-4.5214605	0
120-8-28	the observed output is therefore a repeat-censored version of the underlying walk , and consists of a permutation of the states or a prefix of it .	we show that invite outperforms several existing methods on real-world human response data from memory search tasks .	1	1	7	-5.9235806	5.03084	1
120-8-28	we call this model initial-visit emitting random walk ( invite ) .	prior work has shown that the random walks with such a repeat-censoring mechanism explain well human behavior in memory search tasks , which is of great interest in both the study of human cognition and various clinical applications .	1	2	3	1.7117832	-1.6534476	0
120-8-28	we call this model initial-visit emitting random walk ( invite ) .	however , parameter estimation in invite is challenging , because naive likelihood computation by marginalizing over infinitely many hidden random walk trajectories is intractable .	1	2	4	-4.435155	4.106757	1
120-8-28	in this paper , we propose the first efficient maximum likelihood estimate ( mle ) for invite by decomposing the censored output into a series of absorbing random walks .	we call this model initial-visit emitting random walk ( invite ) .	0	5	2	2.0691652	-1.6813965	0
120-8-28	we call this model initial-visit emitting random walk ( invite ) .	we also prove theoretical properties of the mle including identifiability and consistency .	1	2	6	-5.5489073	5.006015	1
120-8-28	we show that invite outperforms several existing methods on real-world human response data from memory search tasks .	we call this model initial-visit emitting random walk ( invite ) .	0	7	2	5.5112643	-4.880915	0
120-8-28	prior work has shown that the random walks with such a repeat-censoring mechanism explain well human behavior in memory search tasks , which is of great interest in both the study of human cognition and various clinical applications .	however , parameter estimation in invite is challenging , because naive likelihood computation by marginalizing over infinitely many hidden random walk trajectories is intractable .	1	3	4	-3.6178966	3.4296985	1
120-8-28	prior work has shown that the random walks with such a repeat-censoring mechanism explain well human behavior in memory search tasks , which is of great interest in both the study of human cognition and various clinical applications .	in this paper , we propose the first efficient maximum likelihood estimate ( mle ) for invite by decomposing the censored output into a series of absorbing random walks .	1	3	5	-5.428108	5.060444	1
120-8-28	we also prove theoretical properties of the mle including identifiability and consistency .	prior work has shown that the random walks with such a repeat-censoring mechanism explain well human behavior in memory search tasks , which is of great interest in both the study of human cognition and various clinical applications .	0	6	3	4.7473974	-4.34916	0
120-8-28	we show that invite outperforms several existing methods on real-world human response data from memory search tasks .	prior work has shown that the random walks with such a repeat-censoring mechanism explain well human behavior in memory search tasks , which is of great interest in both the study of human cognition and various clinical applications .	0	7	3	5.3345366	-4.7917843	0
120-8-28	however , parameter estimation in invite is challenging , because naive likelihood computation by marginalizing over infinitely many hidden random walk trajectories is intractable .	in this paper , we propose the first efficient maximum likelihood estimate ( mle ) for invite by decomposing the censored output into a series of absorbing random walks .	1	4	5	-5.729294	5.2518396	1
120-8-28	however , parameter estimation in invite is challenging , because naive likelihood computation by marginalizing over infinitely many hidden random walk trajectories is intractable .	we also prove theoretical properties of the mle including identifiability and consistency .	1	4	6	-5.983659	5.2062993	1
120-8-28	we show that invite outperforms several existing methods on real-world human response data from memory search tasks .	however , parameter estimation in invite is challenging , because naive likelihood computation by marginalizing over infinitely many hidden random walk trajectories is intractable .	0	7	4	5.409819	-4.8521643	0
120-8-28	we also prove theoretical properties of the mle including identifiability and consistency .	in this paper , we propose the first efficient maximum likelihood estimate ( mle ) for invite by decomposing the censored output into a series of absorbing random walks .	0	6	5	5.675228	-5.0299315	0
120-8-28	we show that invite outperforms several existing methods on real-world human response data from memory search tasks .	in this paper , we propose the first efficient maximum likelihood estimate ( mle ) for invite by decomposing the censored output into a series of absorbing random walks .	0	7	5	5.5146694	-4.924926	0
120-8-28	we also prove theoretical properties of the mle including identifiability and consistency .	we show that invite outperforms several existing methods on real-world human response data from memory search tasks .	1	6	7	-5.198363	4.742634	1
121-6-15	we consider the estimation of sparse graphical models that characterize the dependency structure of high-dimensional tensor-valued data .	to facilitate the estimation of the precision matrix corresponding to each way of the tensor , we assume the data follow a tensor normal distribution whose covariance has a kronecker product structure .	1	0	1	-5.81252	5.1972423	1
121-6-15	we consider the estimation of sparse graphical models that characterize the dependency structure of high-dimensional tensor-valued data .	the penalized maximum likelihood estimation of this model involves minimizing a non-convex objective function .	1	0	2	-5.9053173	5.170206	1
121-6-15	in spite of the non-convexity of this estimation problem , we prove that an alternating minimization algorithm , which iteratively estimates each sparse precision matrix while fixing the others , attains an estimator with the optimal statistical rate of convergence as well as consistent graph recovery .	we consider the estimation of sparse graphical models that characterize the dependency structure of high-dimensional tensor-valued data .	0	3	0	5.6756763	-5.046811	0
121-6-15	we consider the estimation of sparse graphical models that characterize the dependency structure of high-dimensional tensor-valued data .	notably , such an estimator achieves estimation consistency with only one tensor sample , which is unobserved in previous work .	1	0	4	-5.895482	5.187635	1
121-6-15	our theoretical results are backed by thorough numerical studies .	we consider the estimation of sparse graphical models that characterize the dependency structure of high-dimensional tensor-valued data .	0	5	0	5.6247334	-5.0475073	0
121-6-15	the penalized maximum likelihood estimation of this model involves minimizing a non-convex objective function .	to facilitate the estimation of the precision matrix corresponding to each way of the tensor , we assume the data follow a tensor normal distribution whose covariance has a kronecker product structure .	0	2	1	3.1077704	-2.8897936	0
121-6-15	to facilitate the estimation of the precision matrix corresponding to each way of the tensor , we assume the data follow a tensor normal distribution whose covariance has a kronecker product structure .	in spite of the non-convexity of this estimation problem , we prove that an alternating minimization algorithm , which iteratively estimates each sparse precision matrix while fixing the others , attains an estimator with the optimal statistical rate of convergence as well as consistent graph recovery .	1	1	3	-6.0043616	5.140158	1
121-6-15	notably , such an estimator achieves estimation consistency with only one tensor sample , which is unobserved in previous work .	to facilitate the estimation of the precision matrix corresponding to each way of the tensor , we assume the data follow a tensor normal distribution whose covariance has a kronecker product structure .	0	4	1	5.1805296	-4.569231	0
121-6-15	to facilitate the estimation of the precision matrix corresponding to each way of the tensor , we assume the data follow a tensor normal distribution whose covariance has a kronecker product structure .	our theoretical results are backed by thorough numerical studies .	1	1	5	-5.8129454	4.8786726	1
121-6-15	in spite of the non-convexity of this estimation problem , we prove that an alternating minimization algorithm , which iteratively estimates each sparse precision matrix while fixing the others , attains an estimator with the optimal statistical rate of convergence as well as consistent graph recovery .	the penalized maximum likelihood estimation of this model involves minimizing a non-convex objective function .	0	3	2	5.41729	-4.8021717	0
121-6-15	the penalized maximum likelihood estimation of this model involves minimizing a non-convex objective function .	notably , such an estimator achieves estimation consistency with only one tensor sample , which is unobserved in previous work .	1	2	4	-5.7472324	5.109637	1
121-6-15	the penalized maximum likelihood estimation of this model involves minimizing a non-convex objective function .	our theoretical results are backed by thorough numerical studies .	1	2	5	-5.938109	5.0524435	1
121-6-15	in spite of the non-convexity of this estimation problem , we prove that an alternating minimization algorithm , which iteratively estimates each sparse precision matrix while fixing the others , attains an estimator with the optimal statistical rate of convergence as well as consistent graph recovery .	notably , such an estimator achieves estimation consistency with only one tensor sample , which is unobserved in previous work .	1	3	4	-2.569067	2.5920718	1
121-6-15	in spite of the non-convexity of this estimation problem , we prove that an alternating minimization algorithm , which iteratively estimates each sparse precision matrix while fixing the others , attains an estimator with the optimal statistical rate of convergence as well as consistent graph recovery .	our theoretical results are backed by thorough numerical studies .	1	3	5	-5.626619	4.948076	1
121-6-15	notably , such an estimator achieves estimation consistency with only one tensor sample , which is unobserved in previous work .	our theoretical results are backed by thorough numerical studies .	1	4	5	-5.0935435	4.649361	1
122-8-28	previous theoretical work has rigorously characterized label complexity of active learning , but most of this work has focused on the pac or the agnostic pac model .	an active learner is given a class of models , a large set of unlabeled examples , and the ability to interactively query labels of a subset of these examples ; the goal of the learner is to learn a model in the class that fits the data well .	0	1	0	0.20525704	0.030090421	0
122-8-28	in this paper , we shift our attention to a more general setting - maximum likelihood estimation .	an active learner is given a class of models , a large set of unlabeled examples , and the ability to interactively query labels of a subset of these examples ; the goal of the learner is to learn a model in the class that fits the data well .	0	2	0	3.8194342	-3.5338678	0
122-8-28	an active learner is given a class of models , a large set of unlabeled examples , and the ability to interactively query labels of a subset of these examples ; the goal of the learner is to learn a model in the class that fits the data well .	provided certain conditions hold on the model class , we provide a two-stage active learning algorithm for this problem .	1	0	3	-5.9391317	5.213591	1
122-8-28	the conditions we require are fairly general , and cover the widely popular class of generalized linear models , which in turn , include models for binary and multi-class classification , regression , and conditional random fields .	an active learner is given a class of models , a large set of unlabeled examples , and the ability to interactively query labels of a subset of these examples ; the goal of the learner is to learn a model in the class that fits the data well .	0	4	0	5.2209635	-4.6775517	0
122-8-28	we provide an upper bound on the label requirement of our algorithm , and a lower bound that matches it up to lower order terms .	an active learner is given a class of models , a large set of unlabeled examples , and the ability to interactively query labels of a subset of these examples ; the goal of the learner is to learn a model in the class that fits the data well .	0	5	0	5.4160624	-4.796016	0
122-8-28	an active learner is given a class of models , a large set of unlabeled examples , and the ability to interactively query labels of a subset of these examples ; the goal of the learner is to learn a model in the class that fits the data well .	our analysis shows that unlike binary classification in the realizable case , just a single extra round of interaction is sufficient to achieve near-optimal performance in maximum likelihood estimation .	1	0	6	-5.9880323	5.2196875	1
122-8-28	an active learner is given a class of models , a large set of unlabeled examples , and the ability to interactively query labels of a subset of these examples ; the goal of the learner is to learn a model in the class that fits the data well .	on the empirical side , the recent work in and ( on active linear and logistic regression ) shows the promise of this approach .	1	0	7	-5.8877993	4.9461765	1
122-8-28	previous theoretical work has rigorously characterized label complexity of active learning , but most of this work has focused on the pac or the agnostic pac model .	in this paper , we shift our attention to a more general setting - maximum likelihood estimation .	1	1	2	-5.9090815	5.209666	1
122-8-28	previous theoretical work has rigorously characterized label complexity of active learning , but most of this work has focused on the pac or the agnostic pac model .	provided certain conditions hold on the model class , we provide a two-stage active learning algorithm for this problem .	1	1	3	-6.0030775	5.209898	1
122-8-28	previous theoretical work has rigorously characterized label complexity of active learning , but most of this work has focused on the pac or the agnostic pac model .	the conditions we require are fairly general , and cover the widely popular class of generalized linear models , which in turn , include models for binary and multi-class classification , regression , and conditional random fields .	1	1	4	-5.8888693	5.2234087	1
122-8-28	we provide an upper bound on the label requirement of our algorithm , and a lower bound that matches it up to lower order terms .	previous theoretical work has rigorously characterized label complexity of active learning , but most of this work has focused on the pac or the agnostic pac model .	0	5	1	5.602776	-4.9364486	0
122-8-28	our analysis shows that unlike binary classification in the realizable case , just a single extra round of interaction is sufficient to achieve near-optimal performance in maximum likelihood estimation .	previous theoretical work has rigorously characterized label complexity of active learning , but most of this work has focused on the pac or the agnostic pac model .	0	6	1	5.685572	-5.017424	0
122-8-28	previous theoretical work has rigorously characterized label complexity of active learning , but most of this work has focused on the pac or the agnostic pac model .	on the empirical side , the recent work in and ( on active linear and logistic regression ) shows the promise of this approach .	1	1	7	-5.97157	5.179411	1
122-8-28	in this paper , we shift our attention to a more general setting - maximum likelihood estimation .	provided certain conditions hold on the model class , we provide a two-stage active learning algorithm for this problem .	1	2	3	-5.3202925	4.9151697	1
122-8-28	the conditions we require are fairly general , and cover the widely popular class of generalized linear models , which in turn , include models for binary and multi-class classification , regression , and conditional random fields .	in this paper , we shift our attention to a more general setting - maximum likelihood estimation .	0	4	2	2.9514084	-2.6678028	0
122-8-28	in this paper , we shift our attention to a more general setting - maximum likelihood estimation .	we provide an upper bound on the label requirement of our algorithm , and a lower bound that matches it up to lower order terms .	1	2	5	-5.969632	5.0859222	1
122-8-28	our analysis shows that unlike binary classification in the realizable case , just a single extra round of interaction is sufficient to achieve near-optimal performance in maximum likelihood estimation .	in this paper , we shift our attention to a more general setting - maximum likelihood estimation .	0	6	2	5.5849557	-4.952172	0
122-8-28	on the empirical side , the recent work in and ( on active linear and logistic regression ) shows the promise of this approach .	in this paper , we shift our attention to a more general setting - maximum likelihood estimation .	0	7	2	4.1577463	-3.7325401	0
122-8-28	the conditions we require are fairly general , and cover the widely popular class of generalized linear models , which in turn , include models for binary and multi-class classification , regression , and conditional random fields .	provided certain conditions hold on the model class , we provide a two-stage active learning algorithm for this problem .	0	4	3	-0.035366654	0.39967042	1
122-8-28	provided certain conditions hold on the model class , we provide a two-stage active learning algorithm for this problem .	we provide an upper bound on the label requirement of our algorithm , and a lower bound that matches it up to lower order terms .	1	3	5	-5.761585	5.118456	1
122-8-28	provided certain conditions hold on the model class , we provide a two-stage active learning algorithm for this problem .	our analysis shows that unlike binary classification in the realizable case , just a single extra round of interaction is sufficient to achieve near-optimal performance in maximum likelihood estimation .	1	3	6	-5.8084917	5.155526	1
122-8-28	provided certain conditions hold on the model class , we provide a two-stage active learning algorithm for this problem .	on the empirical side , the recent work in and ( on active linear and logistic regression ) shows the promise of this approach .	1	3	7	-5.51555	4.887079	1
122-8-28	we provide an upper bound on the label requirement of our algorithm , and a lower bound that matches it up to lower order terms .	the conditions we require are fairly general , and cover the widely popular class of generalized linear models , which in turn , include models for binary and multi-class classification , regression , and conditional random fields .	0	5	4	3.0014935	-2.8529742	0
122-8-28	the conditions we require are fairly general , and cover the widely popular class of generalized linear models , which in turn , include models for binary and multi-class classification , regression , and conditional random fields .	our analysis shows that unlike binary classification in the realizable case , just a single extra round of interaction is sufficient to achieve near-optimal performance in maximum likelihood estimation .	1	4	6	-5.5870423	5.06427	1
122-8-28	the conditions we require are fairly general , and cover the widely popular class of generalized linear models , which in turn , include models for binary and multi-class classification , regression , and conditional random fields .	on the empirical side , the recent work in and ( on active linear and logistic regression ) shows the promise of this approach .	1	4	7	-5.491683	4.883749	1
122-8-28	our analysis shows that unlike binary classification in the realizable case , just a single extra round of interaction is sufficient to achieve near-optimal performance in maximum likelihood estimation .	we provide an upper bound on the label requirement of our algorithm , and a lower bound that matches it up to lower order terms .	0	6	5	3.0459614	-2.823935	0
122-8-28	on the empirical side , the recent work in and ( on active linear and logistic regression ) shows the promise of this approach .	we provide an upper bound on the label requirement of our algorithm , and a lower bound that matches it up to lower order terms .	0	7	5	1.9617958	-1.8531673	0
122-8-28	on the empirical side , the recent work in and ( on active linear and logistic regression ) shows the promise of this approach .	our analysis shows that unlike binary classification in the realizable case , just a single extra round of interaction is sufficient to achieve near-optimal performance in maximum likelihood estimation .	0	7	6	2.7534697	-2.6165948	0
123-6-15	this is particularly challenging due to the partial observability inherent in projecting a 3d object onto the image space , and the ill-posedness of inferring object shape and pose .	an important problem for both graphics and vision is to synthesize novel views of a 3d object from a single image .	0	1	0	5.622343	-5.0668874	0
123-6-15	an important problem for both graphics and vision is to synthesize novel views of a 3d object from a single image .	however , we can train a neural network to address the problem if we restrict our attention to specific object categories ( in our case faces and chairs ) for which we can gather ample training data .	1	0	2	-5.989027	5.1438036	1
123-6-15	in this paper , we propose a novel recurrent convolutional encoder-decoder network that is trained end-to-end on the task of rendering rotated objects starting from a single image .	an important problem for both graphics and vision is to synthesize novel views of a 3d object from a single image .	0	3	0	5.726231	-5.103009	0
123-6-15	an important problem for both graphics and vision is to synthesize novel views of a 3d object from a single image .	the recurrent structure allows our model to capture long-term dependencies along a sequence of transformations .	1	0	4	-5.9726973	5.1471567	1
123-6-15	an important problem for both graphics and vision is to synthesize novel views of a 3d object from a single image .	we demonstrate the quality of its predictions for human faces on the multi-pie dataset and for a dataset of 3d chair models , and also show its ability to disentangle latent factors of variation ( e.g. , identity and pose ) without using full supervision .	1	0	5	-5.9083705	5.1608787	1
123-6-15	this is particularly challenging due to the partial observability inherent in projecting a 3d object onto the image space , and the ill-posedness of inferring object shape and pose .	however , we can train a neural network to address the problem if we restrict our attention to specific object categories ( in our case faces and chairs ) for which we can gather ample training data .	1	1	2	-5.3002844	4.872837	1
123-6-15	in this paper , we propose a novel recurrent convolutional encoder-decoder network that is trained end-to-end on the task of rendering rotated objects starting from a single image .	this is particularly challenging due to the partial observability inherent in projecting a 3d object onto the image space , and the ill-posedness of inferring object shape and pose .	0	3	1	5.028902	-4.491418	0
123-6-15	this is particularly challenging due to the partial observability inherent in projecting a 3d object onto the image space , and the ill-posedness of inferring object shape and pose .	the recurrent structure allows our model to capture long-term dependencies along a sequence of transformations .	1	1	4	-5.929554	5.214281	1
123-6-15	this is particularly challenging due to the partial observability inherent in projecting a 3d object onto the image space , and the ill-posedness of inferring object shape and pose .	we demonstrate the quality of its predictions for human faces on the multi-pie dataset and for a dataset of 3d chair models , and also show its ability to disentangle latent factors of variation ( e.g. , identity and pose ) without using full supervision .	1	1	5	-5.9951205	5.1564846	1
123-6-15	however , we can train a neural network to address the problem if we restrict our attention to specific object categories ( in our case faces and chairs ) for which we can gather ample training data .	in this paper , we propose a novel recurrent convolutional encoder-decoder network that is trained end-to-end on the task of rendering rotated objects starting from a single image .	1	2	3	-4.4054418	4.1232233	1
123-6-15	the recurrent structure allows our model to capture long-term dependencies along a sequence of transformations .	however , we can train a neural network to address the problem if we restrict our attention to specific object categories ( in our case faces and chairs ) for which we can gather ample training data .	0	4	2	5.1353674	-4.618949	0
123-6-15	however , we can train a neural network to address the problem if we restrict our attention to specific object categories ( in our case faces and chairs ) for which we can gather ample training data .	we demonstrate the quality of its predictions for human faces on the multi-pie dataset and for a dataset of 3d chair models , and also show its ability to disentangle latent factors of variation ( e.g. , identity and pose ) without using full supervision .	1	2	5	-5.9822993	5.1768036	1
123-6-15	the recurrent structure allows our model to capture long-term dependencies along a sequence of transformations .	in this paper , we propose a novel recurrent convolutional encoder-decoder network that is trained end-to-end on the task of rendering rotated objects starting from a single image .	0	4	3	5.5766025	-4.9269342	0
123-6-15	[CLS] we demonstrate the quality of its predictions for human faces on the multi - pie dataset and for a dataset of 3d chair models, and also show its ability to disentangle latent factors of variation ( e. g., identity and pose ) without using full	in this paper , we propose a novel recurrent convolutional encoder-decoder network that is trained end-to-end on the task of rendering rotated objects starting from a single image .	0	5	3	5.5647426	-4.879016	0
123-6-15	the recurrent structure allows our model to capture long-term dependencies along a sequence of transformations .	we demonstrate the quality of its predictions for human faces on the multi-pie dataset and for a dataset of 3d chair models , and also show its ability to disentangle latent factors of variation ( e.g. , identity and pose ) without using full supervision .	1	4	5	-5.1831975	4.7510676	1
124-9-36	an important class of problems involves training deep neural networks with sparse prediction targets of very high dimension d. these occur naturally in e.g .	neural language models or the learning of word-embeddings , often posed as predicting the probability of next words among a vocabulary of size d ( e.g .	1	0	1	2.062436	-1.6257097	0
124-9-36	200 000 ) .	an important class of problems involves training deep neural networks with sparse prediction targets of very high dimension d. these occur naturally in e.g .	0	2	0	5.3284264	-4.7496595	0
124-9-36	an important class of problems involves training deep neural networks with sparse prediction targets of very high dimension d. these occur naturally in e.g .	computing the equally large , but typically non-sparse d-dimensional output vector from a last hidden layer of reasonable dimension d ( e.g .	1	0	3	-5.5401864	5.1316886	1
124-9-36	an important class of problems involves training deep neural networks with sparse prediction targets of very high dimension d. these occur naturally in e.g .	500 ) incurs a prohibitive o ( dd ) computational cost for each example , as does updating the d x d output weight matrix and computing the gradient needed for backpropagation to previous layers .	1	0	4	-5.7332783	5.1761436	1
124-9-36	an important class of problems involves training deep neural networks with sparse prediction targets of very high dimension d. these occur naturally in e.g .	while efficient handling of large sparse network inputs is trivial , the case of large sparse targets is not , and has thus so far been sidestepped with approximate alternatives such as hierarchical softmax or sampling-based approximations during training .	1	0	5	-5.9086757	5.1890287	1
124-9-36	an important class of problems involves training deep neural networks with sparse prediction targets of very high dimension d. these occur naturally in e.g .	[CLS] in this work we develop an original algorithmic approach which, for a family of loss functions that includes squared error and spherical softmax, can compute the exact loss, gradient update for the output weights, and gradient for backpropagation, all in o ( d2 ) per example instead of o ( dd ), remarkably without ever computing the	1	0	6	-5.995549	5.125485	1
124-9-36	an important class of problems involves training deep neural networks with sparse prediction targets of very high dimension d. these occur naturally in e.g .	the proposed algorithm yields d a speedup of 4d , i.e .	1	0	7	-6.0278883	5.1576586	1
124-9-36	an important class of problems involves training deep neural networks with sparse prediction targets of very high dimension d. these occur naturally in e.g .	two orders of magnitude for typical sizes , for that critical part of the computations that often dominates the training time in this kind of network architecture .	1	0	8	-5.4407897	4.985178	1
124-9-36	200 000 ) .	neural language models or the learning of word-embeddings , often posed as predicting the probability of next words among a vocabulary of size d ( e.g .	0	2	1	5.367113	-4.7665796	0
124-9-36	computing the equally large , but typically non-sparse d-dimensional output vector from a last hidden layer of reasonable dimension d ( e.g .	neural language models or the learning of word-embeddings , often posed as predicting the probability of next words among a vocabulary of size d ( e.g .	0	3	1	5.207902	-4.639401	0
124-9-36	500 ) incurs a prohibitive o ( dd ) computational cost for each example , as does updating the d x d output weight matrix and computing the gradient needed for backpropagation to previous layers .	neural language models or the learning of word-embeddings , often posed as predicting the probability of next words among a vocabulary of size d ( e.g .	0	4	1	5.5981817	-5.0224123	0
124-9-36	neural language models or the learning of word-embeddings , often posed as predicting the probability of next words among a vocabulary of size d ( e.g .	while efficient handling of large sparse network inputs is trivial , the case of large sparse targets is not , and has thus so far been sidestepped with approximate alternatives such as hierarchical softmax or sampling-based approximations during training .	1	1	5	-5.9402237	5.1732836	1
124-9-36	neural language models or the learning of word-embeddings , often posed as predicting the probability of next words among a vocabulary of size d ( e.g .	[CLS] in this work we develop an original algorithmic approach which, for a family of loss functions that includes squared error and spherical softmax, can compute the exact loss, gradient update for the output weights, and gradient for backpropagation, all in o ( d2 ) per example instead of o ( dd )	1	1	6	-6.007945	5.111783	1
124-9-36	the proposed algorithm yields d a speedup of 4d , i.e .	neural language models or the learning of word-embeddings , often posed as predicting the probability of next words among a vocabulary of size d ( e.g .	0	7	1	5.4341426	-4.8550153	0
124-9-36	neural language models or the learning of word-embeddings , often posed as predicting the probability of next words among a vocabulary of size d ( e.g .	two orders of magnitude for typical sizes , for that critical part of the computations that often dominates the training time in this kind of network architecture .	1	1	8	-5.483035	4.9736547	1
124-9-36	200 000 ) .	computing the equally large , but typically non-sparse d-dimensional output vector from a last hidden layer of reasonable dimension d ( e.g .	1	2	3	3.6889677	-3.4293513	0
124-9-36	200 000 ) .	500 ) incurs a prohibitive o ( dd ) computational cost for each example , as does updating the d x d output weight matrix and computing the gradient needed for backpropagation to previous layers .	1	2	4	-1.9065361	1.9263225	1
124-9-36	while efficient handling of large sparse network inputs is trivial , the case of large sparse targets is not , and has thus so far been sidestepped with approximate alternatives such as hierarchical softmax or sampling-based approximations during training .	200 000 ) .	0	5	2	-0.35936168	0.5083842	1
124-9-36	in this work we develop an original algorithmic approach which , for a family of loss functions that includes squared error and spherical softmax , can compute the exact loss , gradient update for the output weights , and gradient for backpropagation , all in o ( d2 ) per example instead of o ( dd ) , remarkably without ever computing the d-dimensional output .	200 000 ) .	0	6	2	1.4667119	-1.3199077	0
124-9-36	200 000 ) .	the proposed algorithm yields d a speedup of 4d , i.e .	1	2	7	-4.7938843	4.387183	1
124-9-36	two orders of magnitude for typical sizes , for that critical part of the computations that often dominates the training time in this kind of network architecture .	200 000 ) .	0	8	2	1.4699795	-1.2752619	0
124-9-36	500 ) incurs a prohibitive o ( dd ) computational cost for each example , as does updating the d x d output weight matrix and computing the gradient needed for backpropagation to previous layers .	computing the equally large , but typically non-sparse d-dimensional output vector from a last hidden layer of reasonable dimension d ( e.g .	0	4	3	2.4446983	-2.3246002	0
124-9-36	while efficient handling of large sparse network inputs is trivial , the case of large sparse targets is not , and has thus so far been sidestepped with approximate alternatives such as hierarchical softmax or sampling-based approximations during training .	computing the equally large , but typically non-sparse d-dimensional output vector from a last hidden layer of reasonable dimension d ( e.g .	0	5	3	1.1766751	-1.0549691	0
124-9-36	[CLS] in this work we develop an original algorithmic approach which, for a family of loss functions that includes squared error and spherical softmax, can compute the exact loss, gradient update for the output weights, and gradient for backpropagation, all in o ( d2 ) per example instead of o ( dd ), remarkably without ever computing	computing the equally large , but typically non-sparse d-dimensional output vector from a last hidden layer of reasonable dimension d ( e.g .	0	6	3	4.413278	-3.976001	0
124-9-36	computing the equally large , but typically non-sparse d-dimensional output vector from a last hidden layer of reasonable dimension d ( e.g .	the proposed algorithm yields d a speedup of 4d , i.e .	1	3	7	-5.87079	5.0670547	1
124-9-36	two orders of magnitude for typical sizes , for that critical part of the computations that often dominates the training time in this kind of network architecture .	computing the equally large , but typically non-sparse d-dimensional output vector from a last hidden layer of reasonable dimension d ( e.g .	0	8	3	4.249628	-3.746152	0
124-9-36	while efficient handling of large sparse network inputs is trivial , the case of large sparse targets is not , and has thus so far been sidestepped with approximate alternatives such as hierarchical softmax or sampling-based approximations during training .	500 ) incurs a prohibitive o ( dd ) computational cost for each example , as does updating the d x d output weight matrix and computing the gradient needed for backpropagation to previous layers .	0	5	4	-1.8539723	1.921526	1
124-9-36	[CLS] in this work we develop an original algorithmic approach which, for a family of loss functions that includes squared error and spherical softmax, can compute the exact loss, gradient update for the output weights, and gradient for backpropagation, all in o ( d2 )	500 ) incurs a prohibitive o ( dd ) computational cost for each example , as does updating the d x d output weight matrix and computing the gradient needed for backpropagation to previous layers .	0	6	4	3.5605435	-3.340811	0
124-9-36	the proposed algorithm yields d a speedup of 4d , i.e .	500 ) incurs a prohibitive o ( dd ) computational cost for each example , as does updating the d x d output weight matrix and computing the gradient needed for backpropagation to previous layers .	0	7	4	-0.9235405	0.99762195	1
124-9-36	two orders of magnitude for typical sizes , for that critical part of the computations that often dominates the training time in this kind of network architecture .	500 ) incurs a prohibitive o ( dd ) computational cost for each example , as does updating the d x d output weight matrix and computing the gradient needed for backpropagation to previous layers .	0	8	4	0.5312886	-0.29877955	0
124-9-36	while efficient handling of large sparse network inputs is trivial , the case of large sparse targets is not , and has thus so far been sidestepped with approximate alternatives such as hierarchical softmax or sampling-based approximations during training .	[CLS] in this work we develop an original algorithmic approach which, for a family of loss functions that includes squared error and spherical softmax, can compute the exact loss, gradient update for the output weights, and gradient for backpropagation, all in o	1	5	6	-4.9981894	4.636403	1
124-9-36	while efficient handling of large sparse network inputs is trivial , the case of large sparse targets is not , and has thus so far been sidestepped with approximate alternatives such as hierarchical softmax or sampling-based approximations during training .	the proposed algorithm yields d a speedup of 4d , i.e .	1	5	7	-5.7498035	5.188393	1
124-9-36	two orders of magnitude for typical sizes , for that critical part of the computations that often dominates the training time in this kind of network architecture .	while efficient handling of large sparse network inputs is trivial , the case of large sparse targets is not , and has thus so far been sidestepped with approximate alternatives such as hierarchical softmax or sampling-based approximations during training .	0	8	5	-1.9069498	2.078041	1
124-9-36	in this work we develop an original algorithmic approach which , for a family of loss functions that includes squared error and spherical softmax , can compute the exact loss , gradient update for the output weights , and gradient for backpropagation , all in o ( d2 ) per example instead of o ( dd ) , remarkably without ever computing the d-dimensional output .	the proposed algorithm yields d a speedup of 4d , i.e .	1	6	7	-5.728149	5.1598663	1
124-9-36	two orders of magnitude for typical sizes , for that critical part of the computations that often dominates the training time in this kind of network architecture .	[CLS] in this work we develop an original algorithmic approach which, for a family of loss functions that includes squared error and spherical softmax, can compute the exact loss, gradient update for the output weights, and gradient for backpropagation, all in o ( d2 ) per example instead of o ( dd ), remarkably without ever computing	0	8	6	-4.4601164	4.1207705	1
124-9-36	two orders of magnitude for typical sizes , for that critical part of the computations that often dominates the training time in this kind of network architecture .	the proposed algorithm yields d a speedup of 4d , i.e .	0	8	7	-5.503653	5.025481	1
125-8-28	for the former , deep learning using backpropagation has recently achieved a string of successes across many domains and datasets .	solving real world problems with embedded neural networks requires both training algorithms that achieve high performance and compatible hardware that runs in real time while remaining energy efficient .	0	1	0	5.388276	-4.673492	0
125-8-28	for the latter , neuromorphic chips that run spiking neural networks have recently achieved unprecedented energy efficiency .	solving real world problems with embedded neural networks requires both training algorithms that achieve high performance and compatible hardware that runs in real time while remaining energy efficient .	0	2	0	5.2048197	-4.531484	0
125-8-28	to bring these two advances together , we must first resolve the incompatibility between backpropagation , which uses continuous-output neurons and synaptic weights , and neuromorphic designs , which employ spiking neurons and discrete synapses .	solving real world problems with embedded neural networks requires both training algorithms that achieve high performance and compatible hardware that runs in real time while remaining energy efficient .	0	3	0	5.3336678	-4.665841	0
125-8-28	our approach is to treat spikes and discrete synapses as continuous probabilities , which allows training the network using standard backpropagation .	solving real world problems with embedded neural networks requires both training algorithms that achieve high performance and compatible hardware that runs in real time while remaining energy efficient .	0	4	0	5.2881765	-4.634363	0
125-8-28	solving real world problems with embedded neural networks requires both training algorithms that achieve high performance and compatible hardware that runs in real time while remaining energy efficient .	the trained network naturally maps to neuromorphic hardware by sampling the probabilities to create one or more networks , which are merged using ensemble averaging .	1	0	5	-5.8060026	5.280065	1
125-8-28	solving real world problems with embedded neural networks requires both training algorithms that achieve high performance and compatible hardware that runs in real time while remaining energy efficient .	to demonstrate , we trained a sparsely connected network that runs on the truenorth chip using the mnist dataset .	1	0	6	-5.952226	5.2437196	1
125-8-28	solving real world problems with embedded neural networks requires both training algorithms that achieve high performance and compatible hardware that runs in real time while remaining energy efficient .	with a high performance network ( ensemble of 64 ) , we achieve 99.42 % accuracy at 108 j per image , and with a high efficiency network ( ensemble of 1 ) we achieve 92.7 % accuracy at 0.268 j per image .	1	0	7	-5.9811263	5.2320223	1
125-8-28	for the former , deep learning using backpropagation has recently achieved a string of successes across many domains and datasets .	for the latter , neuromorphic chips that run spiking neural networks have recently achieved unprecedented energy efficiency .	1	1	2	-5.811927	5.2343707	1
125-8-28	to bring these two advances together , we must first resolve the incompatibility between backpropagation , which uses continuous-output neurons and synaptic weights , and neuromorphic designs , which employ spiking neurons and discrete synapses .	for the former , deep learning using backpropagation has recently achieved a string of successes across many domains and datasets .	0	3	1	3.9043357	-3.548594	0
125-8-28	our approach is to treat spikes and discrete synapses as continuous probabilities , which allows training the network using standard backpropagation .	for the former , deep learning using backpropagation has recently achieved a string of successes across many domains and datasets .	0	4	1	4.9453344	-4.479322	0
125-8-28	for the former , deep learning using backpropagation has recently achieved a string of successes across many domains and datasets .	the trained network naturally maps to neuromorphic hardware by sampling the probabilities to create one or more networks , which are merged using ensemble averaging .	1	1	5	-5.610753	5.1856904	1
125-8-28	for the former , deep learning using backpropagation has recently achieved a string of successes across many domains and datasets .	to demonstrate , we trained a sparsely connected network that runs on the truenorth chip using the mnist dataset .	1	1	6	-6.0120087	5.180526	1
125-8-28	for the former , deep learning using backpropagation has recently achieved a string of successes across many domains and datasets .	with a high performance network ( ensemble of 64 ) , we achieve 99.42 % accuracy at 108 j per image , and with a high efficiency network ( ensemble of 1 ) we achieve 92.7 % accuracy at 0.268 j per image .	1	1	7	-5.949855	5.216703	1
125-8-28	to bring these two advances together , we must first resolve the incompatibility between backpropagation , which uses continuous-output neurons and synaptic weights , and neuromorphic designs , which employ spiking neurons and discrete synapses .	for the latter , neuromorphic chips that run spiking neural networks have recently achieved unprecedented energy efficiency .	0	3	2	-4.1775956	3.9982555	1
125-8-28	for the latter , neuromorphic chips that run spiking neural networks have recently achieved unprecedented energy efficiency .	our approach is to treat spikes and discrete synapses as continuous probabilities , which allows training the network using standard backpropagation .	1	2	4	-4.7340612	4.392786	1
125-8-28	for the latter , neuromorphic chips that run spiking neural networks have recently achieved unprecedented energy efficiency .	the trained network naturally maps to neuromorphic hardware by sampling the probabilities to create one or more networks , which are merged using ensemble averaging .	1	2	5	-5.2274933	4.8868523	1
125-8-28	for the latter , neuromorphic chips that run spiking neural networks have recently achieved unprecedented energy efficiency .	to demonstrate , we trained a sparsely connected network that runs on the truenorth chip using the mnist dataset .	1	2	6	-5.8764634	5.178134	1
125-8-28	for the latter , neuromorphic chips that run spiking neural networks have recently achieved unprecedented energy efficiency .	with a high performance network ( ensemble of 64 ) , we achieve 99.42 % accuracy at 108 j per image , and with a high efficiency network ( ensemble of 1 ) we achieve 92.7 % accuracy at 0.268 j per image .	1	2	7	-5.9890594	5.1647105	1
125-8-28	our approach is to treat spikes and discrete synapses as continuous probabilities , which allows training the network using standard backpropagation .	to bring these two advances together , we must first resolve the incompatibility between backpropagation , which uses continuous-output neurons and synaptic weights , and neuromorphic designs , which employ spiking neurons and discrete synapses .	0	4	3	2.5359373	-2.258739	0
125-8-28	to bring these two advances together , we must first resolve the incompatibility between backpropagation , which uses continuous-output neurons and synaptic weights , and neuromorphic designs , which employ spiking neurons and discrete synapses .	the trained network naturally maps to neuromorphic hardware by sampling the probabilities to create one or more networks , which are merged using ensemble averaging .	1	3	5	-5.5256605	5.028128	1
125-8-28	to bring these two advances together , we must first resolve the incompatibility between backpropagation , which uses continuous-output neurons and synaptic weights , and neuromorphic designs , which employ spiking neurons and discrete synapses .	to demonstrate , we trained a sparsely connected network that runs on the truenorth chip using the mnist dataset .	1	3	6	-5.723551	5.1301966	1
125-8-28	[CLS] to bring these two advances together, we must first resolve the incompatibility between backpropagation, which uses continuous - output neurons and synaptic weights, and neuromorphic designs, which employ spiking neurons and discrete syn	[CLS] with a high performance network ( ensemble of 64 ), we achieve 99. 42 % accuracy at 108 j per image, and with a high efficiency network ( ensemble of 1 ) we achieve 92. 7 % accuracy at 0. 268 j per image	1	3	7	-5.900839	5.1844907	1
125-8-28	the trained network naturally maps to neuromorphic hardware by sampling the probabilities to create one or more networks , which are merged using ensemble averaging .	our approach is to treat spikes and discrete synapses as continuous probabilities , which allows training the network using standard backpropagation .	0	5	4	4.8055573	-4.2821965	0
125-8-28	to demonstrate , we trained a sparsely connected network that runs on the truenorth chip using the mnist dataset .	our approach is to treat spikes and discrete synapses as continuous probabilities , which allows training the network using standard backpropagation .	0	6	4	-1.7737983	1.8426323	1
125-8-28	our approach is to treat spikes and discrete synapses as continuous probabilities , which allows training the network using standard backpropagation .	with a high performance network ( ensemble of 64 ) , we achieve 99.42 % accuracy at 108 j per image , and with a high efficiency network ( ensemble of 1 ) we achieve 92.7 % accuracy at 0.268 j per image .	1	4	7	-5.3484917	4.90629	1
125-8-28	to demonstrate , we trained a sparsely connected network that runs on the truenorth chip using the mnist dataset .	the trained network naturally maps to neuromorphic hardware by sampling the probabilities to create one or more networks , which are merged using ensemble averaging .	0	6	5	-4.560057	4.265113	1
125-8-28	with a high performance network ( ensemble of 64 ) , we achieve 99.42 % accuracy at 108 j per image , and with a high efficiency network ( ensemble of 1 ) we achieve 92.7 % accuracy at 0.268 j per image .	the trained network naturally maps to neuromorphic hardware by sampling the probabilities to create one or more networks , which are merged using ensemble averaging .	0	7	5	4.9874306	-4.5690985	0
125-8-28	with a high performance network ( ensemble of 64 ) , we achieve 99.42 % accuracy at 108 j per image , and with a high efficiency network ( ensemble of 1 ) we achieve 92.7 % accuracy at 0.268 j per image .	to demonstrate , we trained a sparsely connected network that runs on the truenorth chip using the mnist dataset .	0	7	6	4.565152	-4.125307	0
126-9-36	neural networks are both computationally intensive and memory intensive , making them difficult to deploy on embedded systems .	also , conventional networks fix the architecture before training starts ; as a result , training can not improve the architecture .	1	0	1	-5.3369827	4.962138	1
126-9-36	to address these limitations , we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections .	neural networks are both computationally intensive and memory intensive , making them difficult to deploy on embedded systems .	0	2	0	5.30281	-4.49401	0
126-9-36	our method prunes redundant connections using a three-step method .	neural networks are both computationally intensive and memory intensive , making them difficult to deploy on embedded systems .	0	3	0	5.1047297	-4.552067	0
126-9-36	first , we train the network to learn which connections are important .	neural networks are both computationally intensive and memory intensive , making them difficult to deploy on embedded systems .	0	4	0	5.4960556	-4.8255625	0
126-9-36	next , we prune the unimportant connections .	neural networks are both computationally intensive and memory intensive , making them difficult to deploy on embedded systems .	0	5	0	5.4409	-4.762875	0
126-9-36	finally , we retrain the network to fine tune the weights of the remaining connections .	neural networks are both computationally intensive and memory intensive , making them difficult to deploy on embedded systems .	0	6	0	5.6457953	-4.9881372	0
126-9-36	on the imagenet dataset , our method reduced the number of parameters of alexnet by a factor of 9x , from 61 million to 6.7 million , without incurring accuracy loss .	neural networks are both computationally intensive and memory intensive , making them difficult to deploy on embedded systems .	0	7	0	5.5084596	-4.8648868	0
126-9-36	similar experiments with vgg-16 found that the total number of parameters can be reduced by 13x , from 138 million to 10.3 million , again with no loss of accuracy .	neural networks are both computationally intensive and memory intensive , making them difficult to deploy on embedded systems .	0	8	0	5.6219044	-4.9413786	0
126-9-36	to address these limitations , we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections .	also , conventional networks fix the architecture before training starts ; as a result , training can not improve the architecture .	0	2	1	5.0534706	-4.4882236	0
126-9-36	our method prunes redundant connections using a three-step method .	also , conventional networks fix the architecture before training starts ; as a result , training can not improve the architecture .	0	3	1	4.941357	-4.4401407	0
126-9-36	also , conventional networks fix the architecture before training starts ; as a result , training can not improve the architecture .	first , we train the network to learn which connections are important .	1	1	4	-3.4971647	3.3862922	1
126-9-36	also , conventional networks fix the architecture before training starts ; as a result , training can not improve the architecture .	next , we prune the unimportant connections .	1	1	5	-5.68416	5.105731	1
126-9-36	also , conventional networks fix the architecture before training starts ; as a result , training can not improve the architecture .	finally , we retrain the network to fine tune the weights of the remaining connections .	1	1	6	-5.975845	5.1553593	1
126-9-36	on the imagenet dataset , our method reduced the number of parameters of alexnet by a factor of 9x , from 61 million to 6.7 million , without incurring accuracy loss .	also , conventional networks fix the architecture before training starts ; as a result , training can not improve the architecture .	0	7	1	5.5518665	-4.9179068	0
126-9-36	similar experiments with vgg-16 found that the total number of parameters can be reduced by 13x , from 138 million to 10.3 million , again with no loss of accuracy .	also , conventional networks fix the architecture before training starts ; as a result , training can not improve the architecture .	0	8	1	5.612548	-4.994664	0
126-9-36	our method prunes redundant connections using a three-step method .	to address these limitations , we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections .	0	3	2	4.2765903	-3.834208	0
126-9-36	first , we train the network to learn which connections are important .	to address these limitations , we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections .	0	4	2	3.457652	-3.0853167	0
126-9-36	to address these limitations , we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections .	next , we prune the unimportant connections .	1	2	5	-4.6980495	4.440066	1
126-9-36	to address these limitations , we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections .	finally , we retrain the network to fine tune the weights of the remaining connections .	1	2	6	-5.9542627	5.1992025	1
126-9-36	to address these limitations , we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections .	on the imagenet dataset , our method reduced the number of parameters of alexnet by a factor of 9x , from 61 million to 6.7 million , without incurring accuracy loss .	1	2	7	-5.8589954	5.2522206	1
126-9-36	to address these limitations , we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections .	similar experiments with vgg-16 found that the total number of parameters can be reduced by 13x , from 138 million to 10.3 million , again with no loss of accuracy .	1	2	8	-5.85643	5.2471027	1
126-9-36	first , we train the network to learn which connections are important .	our method prunes redundant connections using a three-step method .	0	4	3	1.8636792	-1.4672915	0
126-9-36	next , we prune the unimportant connections .	our method prunes redundant connections using a three-step method .	0	5	3	3.5852823	-3.2894425	0
126-9-36	our method prunes redundant connections using a three-step method .	finally , we retrain the network to fine tune the weights of the remaining connections .	1	3	6	-5.9573874	5.102279	1
126-9-36	on the imagenet dataset , our method reduced the number of parameters of alexnet by a factor of 9x , from 61 million to 6.7 million , without incurring accuracy loss .	our method prunes redundant connections using a three-step method .	0	7	3	5.206421	-4.5451603	0
126-9-36	our method prunes redundant connections using a three-step method .	similar experiments with vgg-16 found that the total number of parameters can be reduced by 13x , from 138 million to 10.3 million , again with no loss of accuracy .	1	3	8	-5.7994576	5.193015	1
126-9-36	first , we train the network to learn which connections are important .	next , we prune the unimportant connections .	1	4	5	-5.9182224	5.1896505	1
126-9-36	first , we train the network to learn which connections are important .	finally , we retrain the network to fine tune the weights of the remaining connections .	1	4	6	-5.97325	5.078818	1
126-9-36	on the imagenet dataset , our method reduced the number of parameters of alexnet by a factor of 9x , from 61 million to 6.7 million , without incurring accuracy loss .	first , we train the network to learn which connections are important .	0	7	4	5.017726	-4.4213886	0
126-9-36	first , we train the network to learn which connections are important .	similar experiments with vgg-16 found that the total number of parameters can be reduced by 13x , from 138 million to 10.3 million , again with no loss of accuracy .	1	4	8	-5.8710117	5.2489243	1
126-9-36	finally , we retrain the network to fine tune the weights of the remaining connections .	next , we prune the unimportant connections .	0	6	5	4.4164	-4.0232916	0
126-9-36	on the imagenet dataset , our method reduced the number of parameters of alexnet by a factor of 9x , from 61 million to 6.7 million , without incurring accuracy loss .	next , we prune the unimportant connections .	0	7	5	3.4547424	-3.2200317	0
126-9-36	similar experiments with vgg-16 found that the total number of parameters can be reduced by 13x , from 138 million to 10.3 million , again with no loss of accuracy .	next , we prune the unimportant connections .	0	8	5	4.062614	-3.7819893	0
126-9-36	finally , we retrain the network to fine tune the weights of the remaining connections .	on the imagenet dataset , our method reduced the number of parameters of alexnet by a factor of 9x , from 61 million to 6.7 million , without incurring accuracy loss .	1	6	7	2.636179	-2.4819174	0
126-9-36	similar experiments with vgg-16 found that the total number of parameters can be reduced by 13x , from 138 million to 10.3 million , again with no loss of accuracy .	finally , we retrain the network to fine tune the weights of the remaining connections .	0	8	6	-1.0333923	1.2319002	1
126-9-36	similar experiments with vgg-16 found that the total number of parameters can be reduced by 13x , from 138 million to 10.3 million , again with no loss of accuracy .	on the imagenet dataset , our method reduced the number of parameters of alexnet by a factor of 9x , from 61 million to 6.7 million , without incurring accuracy loss .	0	8	7	3.5586147	-3.3059354	0
127-7-21	kernel methods represent one of the most powerful tools in machine learning to tackle problems expressed in terms of function values and derivatives due to their capability to represent and model complex relations .	while these methods show good versatility , they are computationally intensive and have poor scalability to large data as they require operations on gram matrices .	1	0	1	-5.8058486	5.2133718	1
127-7-21	kernel methods represent one of the most powerful tools in machine learning to tackle problems expressed in terms of function values and derivatives due to their capability to represent and model complex relations .	in order to mitigate this serious computational limitation , recently randomized constructions have been proposed in the literature , which allow the application of fast linear algorithms .	1	0	2	-5.8093348	5.23019	1
127-7-21	random fourier features ( rff ) are among the most popular and widely applied constructions : they provide an easily computable , low-dimensional feature representation for shift-invariant kernels .	kernel methods represent one of the most powerful tools in machine learning to tackle problems expressed in terms of function values and derivatives due to their capability to represent and model complex relations .	0	3	0	2.9574394	-2.366076	0
127-7-21	kernel methods represent one of the most powerful tools in machine learning to tackle problems expressed in terms of function values and derivatives due to their capability to represent and model complex relations .	despite the popularity of rffs , very little is understood theoretically about their approximation quality .	1	0	4	-5.6395802	5.1315646	1
127-7-21	in this paper , we provide a detailed finite-sample theoretical analysis about the approximation quality of rffs by ( i ) establishing optimal ( in terms of the rff dimension , and growing set size ) performance guarantees in uniform norm , and ( ii ) presenting guarantees in lr ( 1 r < ) norms .	kernel methods represent one of the most powerful tools in machine learning to tackle problems expressed in terms of function values and derivatives due to their capability to represent and model complex relations .	0	5	0	5.7031193	-5.067024	0
127-7-21	kernel methods represent one of the most powerful tools in machine learning to tackle problems expressed in terms of function values and derivatives due to their capability to represent and model complex relations .	we also propose an rff approximation to derivatives of a kernel with a theoretical study on its approximation quality .	1	0	6	-5.9057245	5.1796203	1
127-7-21	while these methods show good versatility , they are computationally intensive and have poor scalability to large data as they require operations on gram matrices .	in order to mitigate this serious computational limitation , recently randomized constructions have been proposed in the literature , which allow the application of fast linear algorithms .	1	1	2	-2.0842128	2.2447286	1
127-7-21	random fourier features ( rff ) are among the most popular and widely applied constructions : they provide an easily computable , low-dimensional feature representation for shift-invariant kernels .	while these methods show good versatility , they are computationally intensive and have poor scalability to large data as they require operations on gram matrices .	0	3	1	-5.374464	4.824895	1
127-7-21	despite the popularity of rffs , very little is understood theoretically about their approximation quality .	while these methods show good versatility , they are computationally intensive and have poor scalability to large data as they require operations on gram matrices .	0	4	1	-1.3114439	1.5461669	1
127-7-21	while these methods show good versatility , they are computationally intensive and have poor scalability to large data as they require operations on gram matrices .	in this paper , we provide a detailed finite-sample theoretical analysis about the approximation quality of rffs by ( i ) establishing optimal ( in terms of the rff dimension , and growing set size ) performance guarantees in uniform norm , and ( ii ) presenting guarantees in lr ( 1 r < ) norms .	1	1	5	-5.945269	5.2006893	1
127-7-21	while these methods show good versatility , they are computationally intensive and have poor scalability to large data as they require operations on gram matrices .	we also propose an rff approximation to derivatives of a kernel with a theoretical study on its approximation quality .	1	1	6	-5.970145	5.1550317	1
127-7-21	random fourier features ( rff ) are among the most popular and widely applied constructions : they provide an easily computable , low-dimensional feature representation for shift-invariant kernels .	in order to mitigate this serious computational limitation , recently randomized constructions have been proposed in the literature , which allow the application of fast linear algorithms .	0	3	2	-5.1886086	4.772254	1
127-7-21	in order to mitigate this serious computational limitation , recently randomized constructions have been proposed in the literature , which allow the application of fast linear algorithms .	despite the popularity of rffs , very little is understood theoretically about their approximation quality .	1	2	4	3.666835	-3.2966282	0
127-7-21	in this paper , we provide a detailed finite-sample theoretical analysis about the approximation quality of rffs by ( i ) establishing optimal ( in terms of the rff dimension , and growing set size ) performance guarantees in uniform norm , and ( ii ) presenting guarantees in lr ( 1 r < ) norms .	in order to mitigate this serious computational limitation , recently randomized constructions have been proposed in the literature , which allow the application of fast linear algorithms .	0	5	2	5.5398636	-4.844531	0
127-7-21	in order to mitigate this serious computational limitation , recently randomized constructions have been proposed in the literature , which allow the application of fast linear algorithms .	we also propose an rff approximation to derivatives of a kernel with a theoretical study on its approximation quality .	1	2	6	-6.0023785	5.200495	1
127-7-21	despite the popularity of rffs , very little is understood theoretically about their approximation quality .	random fourier features ( rff ) are among the most popular and widely applied constructions : they provide an easily computable , low-dimensional feature representation for shift-invariant kernels .	0	4	3	5.682025	-5.0544147	0
127-7-21	[CLS] in this paper, we provide a detailed finite - sample theoretical analysis about the approximation quality of rffs by ( i ) establishing optimal ( in terms of the rff dimension, and growing set size ) performance guarantees in uniform norm, and ( ii ) presenting guarantees in lr ( 1 r < )	random fourier features ( rff ) are among the most popular and widely applied constructions : they provide an easily computable , low-dimensional feature representation for shift-invariant kernels .	0	5	3	5.7251577	-5.054973	0
127-7-21	random fourier features ( rff ) are among the most popular and widely applied constructions : they provide an easily computable , low-dimensional feature representation for shift-invariant kernels .	we also propose an rff approximation to derivatives of a kernel with a theoretical study on its approximation quality .	1	3	6	-5.916973	5.0137215	1
127-7-21	despite the popularity of rffs , very little is understood theoretically about their approximation quality .	in this paper , we provide a detailed finite-sample theoretical analysis about the approximation quality of rffs by ( i ) establishing optimal ( in terms of the rff dimension , and growing set size ) performance guarantees in uniform norm , and ( ii ) presenting guarantees in lr ( 1 r < ) norms .	1	4	5	-5.924515	5.165279	1
127-7-21	we also propose an rff approximation to derivatives of a kernel with a theoretical study on its approximation quality .	despite the popularity of rffs , very little is understood theoretically about their approximation quality .	0	6	4	5.451586	-4.8391643	0
127-7-21	we also propose an rff approximation to derivatives of a kernel with a theoretical study on its approximation quality .	in this paper , we provide a detailed finite-sample theoretical analysis about the approximation quality of rffs by ( i ) establishing optimal ( in terms of the rff dimension , and growing set size ) performance guarantees in uniform norm , and ( ii ) presenting guarantees in lr ( 1 r < ) norms .	0	6	5	3.166611	-2.9645357	0
128-6-15	many modern data analysis problems involve inferences from streaming data .	however , streaming data is not easily amenable to the standard probabilistic modeling approaches , which require conditioning on finite data .	1	0	1	-5.843844	5.233333	1
128-6-15	many modern data analysis problems involve inferences from streaming data .	we develop population variational bayes , a new approach for using bayesian modeling to analyze streams of data .	1	0	2	-6.009737	5.169343	1
128-6-15	it approximates a new type of distribution , the population posterior , which combines the notion of a population distribution of the data with bayesian inference in a probabilistic model .	many modern data analysis problems involve inferences from streaming data .	0	3	0	5.6566186	-5.0783157	0
128-6-15	many modern data analysis problems involve inferences from streaming data .	we develop the population posterior for latent dirichlet allocation and dirichlet process mixtures .	1	0	4	-6.000583	5.154498	1
128-6-15	many modern data analysis problems involve inferences from streaming data .	we study our method with several large-scale data sets .	1	0	5	-5.852925	4.9186587	1
128-6-15	we develop population variational bayes , a new approach for using bayesian modeling to analyze streams of data .	however , streaming data is not easily amenable to the standard probabilistic modeling approaches , which require conditioning on finite data .	0	2	1	4.406503	-4.037364	0
128-6-15	it approximates a new type of distribution , the population posterior , which combines the notion of a population distribution of the data with bayesian inference in a probabilistic model .	however , streaming data is not easily amenable to the standard probabilistic modeling approaches , which require conditioning on finite data .	0	3	1	4.8801312	-4.403841	0
128-6-15	however , streaming data is not easily amenable to the standard probabilistic modeling approaches , which require conditioning on finite data .	we develop the population posterior for latent dirichlet allocation and dirichlet process mixtures .	1	1	4	-5.8857965	5.2058926	1
128-6-15	we study our method with several large-scale data sets .	however , streaming data is not easily amenable to the standard probabilistic modeling approaches , which require conditioning on finite data .	0	5	1	5.517496	-4.8358836	0
128-6-15	it approximates a new type of distribution , the population posterior , which combines the notion of a population distribution of the data with bayesian inference in a probabilistic model .	we develop population variational bayes , a new approach for using bayesian modeling to analyze streams of data .	0	3	2	5.167614	-4.5539293	0
128-6-15	we develop population variational bayes , a new approach for using bayesian modeling to analyze streams of data .	we develop the population posterior for latent dirichlet allocation and dirichlet process mixtures .	1	2	4	-5.6659284	5.1644373	1
128-6-15	we develop population variational bayes , a new approach for using bayesian modeling to analyze streams of data .	we study our method with several large-scale data sets .	1	2	5	-5.9808655	5.218386	1
128-6-15	it approximates a new type of distribution , the population posterior , which combines the notion of a population distribution of the data with bayesian inference in a probabilistic model .	we develop the population posterior for latent dirichlet allocation and dirichlet process mixtures .	1	3	4	-4.8749933	4.4836445	1
128-6-15	we study our method with several large-scale data sets .	it approximates a new type of distribution , the population posterior , which combines the notion of a population distribution of the data with bayesian inference in a probabilistic model .	0	5	3	4.6742764	-4.2397585	0
128-6-15	we develop the population posterior for latent dirichlet allocation and dirichlet process mixtures .	we study our method with several large-scale data sets .	1	4	5	-5.8541107	5.0526443	1
129-7-21	current methods , such as bayesian quadrature , demonstrate impressive empirical performance but lack theoretical analysis .	there is renewed interest in formulating integration as a statistical inference problem , motivated by obtaining a full distribution over numerical error that can be propagated through subsequent computation .	0	1	0	4.2494335	-3.6907716	0
129-7-21	an important challenge is therefore to reconcile these probabilistic integrators with rigorous convergence guarantees .	there is renewed interest in formulating integration as a statistical inference problem , motivated by obtaining a full distribution over numerical error that can be propagated through subsequent computation .	0	2	0	5.150018	-4.5220556	0
129-7-21	in this paper , we present the first probabilistic integrator that admits such theoretical treatment , called frank-wolfe bayesian quadrature ( fwbq ) .	there is renewed interest in formulating integration as a statistical inference problem , motivated by obtaining a full distribution over numerical error that can be propagated through subsequent computation .	0	3	0	5.6081743	-4.9837446	0
129-7-21	under fwbq , convergence to the true value of the integral is shown to be up to exponential and posterior contraction rates are proven to be up to super-exponential .	there is renewed interest in formulating integration as a statistical inference problem , motivated by obtaining a full distribution over numerical error that can be propagated through subsequent computation .	0	4	0	5.6119995	-5.0137224	0
129-7-21	in simulations , fwbq is competitive with state-of-the-art methods and out-performs alternatives based on frank-wolfe optimisation .	there is renewed interest in formulating integration as a statistical inference problem , motivated by obtaining a full distribution over numerical error that can be propagated through subsequent computation .	0	5	0	5.583798	-4.962075	0
129-7-21	our approach is applied to successfully quantify numerical error in the solution to a challenging bayesian model choice problem in cellular biology .	there is renewed interest in formulating integration as a statistical inference problem , motivated by obtaining a full distribution over numerical error that can be propagated through subsequent computation .	0	6	0	5.6021166	-4.990306	0
129-7-21	an important challenge is therefore to reconcile these probabilistic integrators with rigorous convergence guarantees .	current methods , such as bayesian quadrature , demonstrate impressive empirical performance but lack theoretical analysis .	0	2	1	3.4832084	-3.1858306	0
129-7-21	current methods , such as bayesian quadrature , demonstrate impressive empirical performance but lack theoretical analysis .	in this paper , we present the first probabilistic integrator that admits such theoretical treatment , called frank-wolfe bayesian quadrature ( fwbq ) .	1	1	3	-5.947696	5.188728	1
129-7-21	current methods , such as bayesian quadrature , demonstrate impressive empirical performance but lack theoretical analysis .	under fwbq , convergence to the true value of the integral is shown to be up to exponential and posterior contraction rates are proven to be up to super-exponential .	1	1	4	-5.9552	5.235101	1
129-7-21	current methods , such as bayesian quadrature , demonstrate impressive empirical performance but lack theoretical analysis .	in simulations , fwbq is competitive with state-of-the-art methods and out-performs alternatives based on frank-wolfe optimisation .	1	1	5	-5.982435	5.216911	1
129-7-21	our approach is applied to successfully quantify numerical error in the solution to a challenging bayesian model choice problem in cellular biology .	current methods , such as bayesian quadrature , demonstrate impressive empirical performance but lack theoretical analysis .	0	6	1	5.4720297	-4.8371634	0
129-7-21	an important challenge is therefore to reconcile these probabilistic integrators with rigorous convergence guarantees .	in this paper , we present the first probabilistic integrator that admits such theoretical treatment , called frank-wolfe bayesian quadrature ( fwbq ) .	1	2	3	-5.563957	5.165682	1
129-7-21	under fwbq , convergence to the true value of the integral is shown to be up to exponential and posterior contraction rates are proven to be up to super-exponential .	an important challenge is therefore to reconcile these probabilistic integrators with rigorous convergence guarantees .	0	4	2	4.389759	-3.95598	0
129-7-21	an important challenge is therefore to reconcile these probabilistic integrators with rigorous convergence guarantees .	in simulations , fwbq is competitive with state-of-the-art methods and out-performs alternatives based on frank-wolfe optimisation .	1	2	5	-5.9758306	5.131474	1
129-7-21	an important challenge is therefore to reconcile these probabilistic integrators with rigorous convergence guarantees .	our approach is applied to successfully quantify numerical error in the solution to a challenging bayesian model choice problem in cellular biology .	1	2	6	-5.928837	5.003763	1
129-7-21	under fwbq , convergence to the true value of the integral is shown to be up to exponential and posterior contraction rates are proven to be up to super-exponential .	in this paper , we present the first probabilistic integrator that admits such theoretical treatment , called frank-wolfe bayesian quadrature ( fwbq ) .	0	4	3	5.3159204	-4.717786	0
129-7-21	in this paper , we present the first probabilistic integrator that admits such theoretical treatment , called frank-wolfe bayesian quadrature ( fwbq ) .	in simulations , fwbq is competitive with state-of-the-art methods and out-performs alternatives based on frank-wolfe optimisation .	1	3	5	-5.912377	5.204988	1
129-7-21	in this paper , we present the first probabilistic integrator that admits such theoretical treatment , called frank-wolfe bayesian quadrature ( fwbq ) .	our approach is applied to successfully quantify numerical error in the solution to a challenging bayesian model choice problem in cellular biology .	1	3	6	-5.8266954	5.1572976	1
129-7-21	in simulations , fwbq is competitive with state-of-the-art methods and out-performs alternatives based on frank-wolfe optimisation .	under fwbq , convergence to the true value of the integral is shown to be up to exponential and posterior contraction rates are proven to be up to super-exponential .	0	5	4	4.2578716	-3.8367438	0
129-7-21	our approach is applied to successfully quantify numerical error in the solution to a challenging bayesian model choice problem in cellular biology .	under fwbq , convergence to the true value of the integral is shown to be up to exponential and posterior contraction rates are proven to be up to super-exponential .	0	6	4	3.0900803	-2.8979516	0
129-7-21	our approach is applied to successfully quantify numerical error in the solution to a challenging bayesian model choice problem in cellular biology .	in simulations , fwbq is competitive with state-of-the-art methods and out-performs alternatives based on frank-wolfe optimisation .	0	6	5	-3.6823308	3.4055378	1
130-7-21	recurrent neural networks can be trained to produce sequences of tokens given some input , as exemplified by recent results in machine translation and image captioning .	the current approach to training them consists of maximizing the likelihood of each token in the sequence given the current ( recurrent ) state and the previous token .	1	0	1	-4.056255	3.848061	1
130-7-21	recurrent neural networks can be trained to produce sequences of tokens given some input , as exemplified by recent results in machine translation and image captioning .	at inference , the unknown previous token is then replaced by a token generated by the model itself .	1	0	2	-2.2588677	2.266584	1
130-7-21	this discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence .	recurrent neural networks can be trained to produce sequences of tokens given some input , as exemplified by recent results in machine translation and image captioning .	0	3	0	4.2784176	-3.874218	0
130-7-21	recurrent neural networks can be trained to produce sequences of tokens given some input , as exemplified by recent results in machine translation and image captioning .	we propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token , towards a less guided scheme which mostly uses the generated token instead .	1	0	4	-5.4127264	4.949581	1
130-7-21	experiments on several sequence prediction tasks show that this approach yields significant improvements .	recurrent neural networks can be trained to produce sequences of tokens given some input , as exemplified by recent results in machine translation and image captioning .	0	5	0	5.4806185	-4.901254	0
130-7-21	recurrent neural networks can be trained to produce sequences of tokens given some input , as exemplified by recent results in machine translation and image captioning .	moreover , it was used succesfully in our winning entry to the mscoco image captioning challenge , 2015 .	1	0	6	-5.956336	5.164859	1
130-7-21	the current approach to training them consists of maximizing the likelihood of each token in the sequence given the current ( recurrent ) state and the previous token .	at inference , the unknown previous token is then replaced by a token generated by the model itself .	1	1	2	-3.401885	3.2950535	1
130-7-21	this discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence .	the current approach to training them consists of maximizing the likelihood of each token in the sequence given the current ( recurrent ) state and the previous token .	0	3	1	3.1212516	-2.950118	0
130-7-21	we propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token , towards a less guided scheme which mostly uses the generated token instead .	the current approach to training them consists of maximizing the likelihood of each token in the sequence given the current ( recurrent ) state and the previous token .	0	4	1	4.588099	-4.086981	0
130-7-21	the current approach to training them consists of maximizing the likelihood of each token in the sequence given the current ( recurrent ) state and the previous token .	experiments on several sequence prediction tasks show that this approach yields significant improvements .	1	1	5	-5.884197	4.906814	1
130-7-21	the current approach to training them consists of maximizing the likelihood of each token in the sequence given the current ( recurrent ) state and the previous token .	moreover , it was used succesfully in our winning entry to the mscoco image captioning challenge , 2015 .	1	1	6	-5.9506254	5.066272	1
130-7-21	at inference , the unknown previous token is then replaced by a token generated by the model itself .	this discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence .	1	2	3	-2.7448587	2.8236842	1
130-7-21	we propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token , towards a less guided scheme which mostly uses the generated token instead .	at inference , the unknown previous token is then replaced by a token generated by the model itself .	0	4	2	3.733756	-3.503077	0
130-7-21	at inference , the unknown previous token is then replaced by a token generated by the model itself .	experiments on several sequence prediction tasks show that this approach yields significant improvements .	1	2	5	-5.9517617	5.019979	1
130-7-21	at inference , the unknown previous token is then replaced by a token generated by the model itself .	moreover , it was used succesfully in our winning entry to the mscoco image captioning challenge , 2015 .	1	2	6	-5.764189	4.9171815	1
130-7-21	this discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence .	we propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token , towards a less guided scheme which mostly uses the generated token instead .	1	3	4	-5.4887953	5.0484996	1
130-7-21	this discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence .	experiments on several sequence prediction tasks show that this approach yields significant improvements .	1	3	5	-5.8493133	4.8854012	1
130-7-21	this discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence .	moreover , it was used succesfully in our winning entry to the mscoco image captioning challenge , 2015 .	1	3	6	-5.8860846	4.975169	1
130-7-21	we propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token , towards a less guided scheme which mostly uses the generated token instead .	experiments on several sequence prediction tasks show that this approach yields significant improvements .	1	4	5	-5.9561424	5.1853256	1
130-7-21	moreover , it was used succesfully in our winning entry to the mscoco image captioning challenge , 2015 .	we propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token , towards a less guided scheme which mostly uses the generated token instead .	0	6	4	4.557495	-4.1000576	0
130-7-21	moreover , it was used succesfully in our winning entry to the mscoco image captioning challenge , 2015 .	experiments on several sequence prediction tasks show that this approach yields significant improvements .	0	6	5	2.4040337	-2.2693956	0
131-4-6	in this paper , we present a unified analysis of matrix completion under general low-dimensional structural constraints induced by any norm regularization .	we consider two estimators for the general problem of structured matrix completion , and provide unified upper bounds on the sample complexity and the estimation error .	1	0	1	-3.3383527	3.2950132	1
131-4-6	[CLS] our analysis relies on results from generic chaining, and we establish two intermediate results of independent interest : ( a ) in characterizing the size or complexity of low dimensional subsets in high dimensional ambient space, a certain partial complexity measure encountered in the analysis of matrix completion problems is characterized in terms of a well understood complexity measure of gaussian widths,	in this paper , we present a unified analysis of matrix completion under general low-dimensional structural constraints induced by any norm regularization .	0	2	0	5.5338	-4.8730965	0
131-4-6	in this paper , we present a unified analysis of matrix completion under general low-dimensional structural constraints induced by any norm regularization .	further , we provide several non-trivial examples of structures included in our framework , notably the recently proposed spectral k-support norm .	1	0	3	-6.0109363	5.139525	1
131-4-6	[CLS] our analysis relies on results from generic chaining, and we establish two intermediate results of independent interest : ( a ) in characterizing the size or complexity of low dimensional subsets in high dimensional ambient space, a certain partial complexity measure encountered in the analysis of matrix completion problems is characterized in terms of a well understood complexity measure of gaussian	we consider two estimators for the general problem of structured matrix completion , and provide unified upper bounds on the sample complexity and the estimation error .	0	2	1	5.2551126	-4.6540413	0
131-4-6	further , we provide several non-trivial examples of structures included in our framework , notably the recently proposed spectral k-support norm .	we consider two estimators for the general problem of structured matrix completion , and provide unified upper bounds on the sample complexity and the estimation error .	0	3	1	5.341638	-4.758338	0
131-4-6	further , we provide several non-trivial examples of structures included in our framework , notably the recently proposed spectral k-support norm .	[CLS] our analysis relies on results from generic chaining, and we establish two intermediate results of independent interest : ( a ) in characterizing the size or complexity of low dimensional subsets in high dimensional ambient space, a certain partial complexity measure encountered in the analysis of matrix completion problems is characterized in terms of a well understood complexity measure of gaussian widths,	0	3	2	2.5578775	-2.4143782	0
132-6-15	while previously the relationship between tasks had to be user-defined in the form of an output kernel , recent approaches jointly learn the tasks and the output kernel .	the paradigm of multi-task learning is that one can achieve better generalization by learning tasks jointly and thus exploiting the similarity between the tasks rather than learning them independently of each other .	0	1	0	5.072585	-4.5287776	0
132-6-15	the paradigm of multi-task learning is that one can achieve better generalization by learning tasks jointly and thus exploiting the similarity between the tasks rather than learning them independently of each other .	as the output kernel is a positive semidefinite matrix , the resulting optimization problems are not scalable in the number of tasks as an eigendecomposition is required in each step .	1	0	2	-5.890066	5.2350674	1
132-6-15	the paradigm of multi-task learning is that one can achieve better generalization by learning tasks jointly and thus exploiting the similarity between the tasks rather than learning them independently of each other .	using the theory of positive semidefinite kernels we show in this paper that for a certain class of regularizers on the output kernel , the constraint of being positive semidefinite can be dropped as it is automatically satisfied for the relaxed problem .	1	0	3	-5.904602	5.2445173	1
132-6-15	the paradigm of multi-task learning is that one can achieve better generalization by learning tasks jointly and thus exploiting the similarity between the tasks rather than learning them independently of each other .	this leads to an unconstrained dual problem which can be solved efficiently .	1	0	4	-6.001693	5.09778	1
132-6-15	the paradigm of multi-task learning is that one can achieve better generalization by learning tasks jointly and thus exploiting the similarity between the tasks rather than learning them independently of each other .	experiments on several multi-task and multi-class data sets illustrate the efficacy of our approach in terms of computational efficiency as well as generalization performance .	1	0	5	-5.9767656	5.1460743	1
132-6-15	as the output kernel is a positive semidefinite matrix , the resulting optimization problems are not scalable in the number of tasks as an eigendecomposition is required in each step .	while previously the relationship between tasks had to be user-defined in the form of an output kernel , recent approaches jointly learn the tasks and the output kernel .	0	2	1	5.2449713	-4.637499	0
132-6-15	while previously the relationship between tasks had to be user-defined in the form of an output kernel , recent approaches jointly learn the tasks and the output kernel .	using the theory of positive semidefinite kernels we show in this paper that for a certain class of regularizers on the output kernel , the constraint of being positive semidefinite can be dropped as it is automatically satisfied for the relaxed problem .	1	1	3	-5.9614763	5.223249	1
132-6-15	this leads to an unconstrained dual problem which can be solved efficiently .	while previously the relationship between tasks had to be user-defined in the form of an output kernel , recent approaches jointly learn the tasks and the output kernel .	0	4	1	5.4218307	-4.7977085	0
132-6-15	while previously the relationship between tasks had to be user-defined in the form of an output kernel , recent approaches jointly learn the tasks and the output kernel .	experiments on several multi-task and multi-class data sets illustrate the efficacy of our approach in terms of computational efficiency as well as generalization performance .	1	1	5	-5.9589963	5.1042485	1
132-6-15	as the output kernel is a positive semidefinite matrix , the resulting optimization problems are not scalable in the number of tasks as an eigendecomposition is required in each step .	using the theory of positive semidefinite kernels we show in this paper that for a certain class of regularizers on the output kernel , the constraint of being positive semidefinite can be dropped as it is automatically satisfied for the relaxed problem .	1	2	3	-3.822236	3.6030254	1
132-6-15	this leads to an unconstrained dual problem which can be solved efficiently .	as the output kernel is a positive semidefinite matrix , the resulting optimization problems are not scalable in the number of tasks as an eigendecomposition is required in each step .	0	4	2	3.3297763	-3.1624846	0
132-6-15	experiments on several multi-task and multi-class data sets illustrate the efficacy of our approach in terms of computational efficiency as well as generalization performance .	as the output kernel is a positive semidefinite matrix , the resulting optimization problems are not scalable in the number of tasks as an eigendecomposition is required in each step .	0	5	2	5.427696	-4.8118706	0
132-6-15	using the theory of positive semidefinite kernels we show in this paper that for a certain class of regularizers on the output kernel , the constraint of being positive semidefinite can be dropped as it is automatically satisfied for the relaxed problem .	this leads to an unconstrained dual problem which can be solved efficiently .	1	3	4	-4.199001	4.0691805	1
132-6-15	experiments on several multi-task and multi-class data sets illustrate the efficacy of our approach in terms of computational efficiency as well as generalization performance .	using the theory of positive semidefinite kernels we show in this paper that for a certain class of regularizers on the output kernel , the constraint of being positive semidefinite can be dropped as it is automatically satisfied for the relaxed problem .	0	5	3	5.441128	-4.8025017	0
132-6-15	this leads to an unconstrained dual problem which can be solved efficiently .	experiments on several multi-task and multi-class data sets illustrate the efficacy of our approach in terms of computational efficiency as well as generalization performance .	1	4	5	-5.913283	5.0842156	1
133-7-21	we develop an inference algorithm for the sticky hierarchical dirichlet process hidden markov model that scales to big datasets by processing a few sequences at a time yet allows rapid adaptation of the state space cardinality .	bayesian nonparametric hidden markov models are typically learned via fixed truncations of the infinite state space or local monte carlo proposals that make small changes to the state space .	0	1	0	5.5244694	-4.903186	0
133-7-21	bayesian nonparametric hidden markov models are typically learned via fixed truncations of the infinite state space or local monte carlo proposals that make small changes to the state space .	unlike previous point-estimate methods , our novel variational bound penalizes redundant or irrelevant states and thus enables optimization of the state space .	1	0	2	-6.028673	5.2280664	1
133-7-21	our birth proposals use observed data statistics to create useful new states that escape local optima .	bayesian nonparametric hidden markov models are typically learned via fixed truncations of the infinite state space or local monte carlo proposals that make small changes to the state space .	0	3	0	5.649682	-5.010481	0
133-7-21	bayesian nonparametric hidden markov models are typically learned via fixed truncations of the infinite state space or local monte carlo proposals that make small changes to the state space .	merge and delete proposals remove ineffective states to yield simpler models with more affordable future computations .	1	0	4	-5.8036613	5.137046	1
133-7-21	bayesian nonparametric hidden markov models are typically learned via fixed truncations of the infinite state space or local monte carlo proposals that make small changes to the state space .	experiments on speaker diarization , motion capture , and epigenetic chromatin datasets discover models that are more compact , more interpretable , and better aligned to ground truth segmentations than competitors .	1	0	5	-5.987859	5.153279	1
133-7-21	we have released an open-source python implementation which can parallelize local inference steps across sequences .	bayesian nonparametric hidden markov models are typically learned via fixed truncations of the infinite state space or local monte carlo proposals that make small changes to the state space .	0	6	0	5.713417	-5.073492	0
133-7-21	unlike previous point-estimate methods , our novel variational bound penalizes redundant or irrelevant states and thus enables optimization of the state space .	we develop an inference algorithm for the sticky hierarchical dirichlet process hidden markov model that scales to big datasets by processing a few sequences at a time yet allows rapid adaptation of the state space cardinality .	0	2	1	0.5830274	-0.30934164	0
133-7-21	our birth proposals use observed data statistics to create useful new states that escape local optima .	we develop an inference algorithm for the sticky hierarchical dirichlet process hidden markov model that scales to big datasets by processing a few sequences at a time yet allows rapid adaptation of the state space cardinality .	0	3	1	-4.4786644	4.2451115	1
133-7-21	merge and delete proposals remove ineffective states to yield simpler models with more affordable future computations .	we develop an inference algorithm for the sticky hierarchical dirichlet process hidden markov model that scales to big datasets by processing a few sequences at a time yet allows rapid adaptation of the state space cardinality .	0	4	1	-5.0702553	4.7885675	1
133-7-21	we develop an inference algorithm for the sticky hierarchical dirichlet process hidden markov model that scales to big datasets by processing a few sequences at a time yet allows rapid adaptation of the state space cardinality .	experiments on speaker diarization , motion capture , and epigenetic chromatin datasets discover models that are more compact , more interpretable , and better aligned to ground truth segmentations than competitors .	1	1	5	-5.454404	5.000314	1
133-7-21	we develop an inference algorithm for the sticky hierarchical dirichlet process hidden markov model that scales to big datasets by processing a few sequences at a time yet allows rapid adaptation of the state space cardinality .	we have released an open-source python implementation which can parallelize local inference steps across sequences .	1	1	6	-3.48788	3.3258774	1
133-7-21	unlike previous point-estimate methods , our novel variational bound penalizes redundant or irrelevant states and thus enables optimization of the state space .	our birth proposals use observed data statistics to create useful new states that escape local optima .	1	2	3	3.7897582	-3.4456751	0
133-7-21	unlike previous point-estimate methods , our novel variational bound penalizes redundant or irrelevant states and thus enables optimization of the state space .	merge and delete proposals remove ineffective states to yield simpler models with more affordable future computations .	1	2	4	2.432928	-2.2571564	0
133-7-21	experiments on speaker diarization , motion capture , and epigenetic chromatin datasets discover models that are more compact , more interpretable , and better aligned to ground truth segmentations than competitors .	unlike previous point-estimate methods , our novel variational bound penalizes redundant or irrelevant states and thus enables optimization of the state space .	0	5	2	5.176916	-4.6318226	0
133-7-21	unlike previous point-estimate methods , our novel variational bound penalizes redundant or irrelevant states and thus enables optimization of the state space .	we have released an open-source python implementation which can parallelize local inference steps across sequences .	1	2	6	1.5970273	-1.3756015	0
133-7-21	merge and delete proposals remove ineffective states to yield simpler models with more affordable future computations .	our birth proposals use observed data statistics to create useful new states that escape local optima .	0	4	3	1.3045754	-0.9938966	0
133-7-21	experiments on speaker diarization , motion capture , and epigenetic chromatin datasets discover models that are more compact , more interpretable , and better aligned to ground truth segmentations than competitors .	our birth proposals use observed data statistics to create useful new states that escape local optima .	0	5	3	5.1491623	-4.6399374	0
133-7-21	our birth proposals use observed data statistics to create useful new states that escape local optima .	we have released an open-source python implementation which can parallelize local inference steps across sequences .	1	3	6	-3.1458263	3.1142015	1
133-7-21	merge and delete proposals remove ineffective states to yield simpler models with more affordable future computations .	experiments on speaker diarization , motion capture , and epigenetic chromatin datasets discover models that are more compact , more interpretable , and better aligned to ground truth segmentations than competitors .	1	4	5	-5.7499933	5.228194	1
133-7-21	merge and delete proposals remove ineffective states to yield simpler models with more affordable future computations .	we have released an open-source python implementation which can parallelize local inference steps across sequences .	1	4	6	-4.7821646	4.5294847	1
133-7-21	we have released an open-source python implementation which can parallelize local inference steps across sequences .	experiments on speaker diarization , motion capture , and epigenetic chromatin datasets discover models that are more compact , more interpretable , and better aligned to ground truth segmentations than competitors .	0	6	5	-5.3266726	4.9183683	1
134-9-36	unfortunately , mcmc algorithms are typically serial , and do not scale to the large datasets typical of modern machine learning .	practitioners of bayesian statistics have long depended on markov chain monte carlo ( mcmc ) to obtain samples from intractable posterior distributions .	0	1	0	5.613077	-5.061612	0
134-9-36	the recently proposed consensus monte carlo algorithm removes this limitation by partitioning the data and drawing samples conditional on each partition in parallel .	practitioners of bayesian statistics have long depended on markov chain monte carlo ( mcmc ) to obtain samples from intractable posterior distributions .	0	2	0	5.663928	-5.1056566	0
134-9-36	practitioners of bayesian statistics have long depended on markov chain monte carlo ( mcmc ) to obtain samples from intractable posterior distributions .	a fixed aggregation function then combines these samples , yielding approximate posterior samples .	1	0	3	-5.8232417	5.1299047	1
134-9-36	practitioners of bayesian statistics have long depended on markov chain monte carlo ( mcmc ) to obtain samples from intractable posterior distributions .	we introduce variational consensus monte carlo ( vcmc ) , a variational bayes algorithm that optimizes over aggregation functions to obtain samples from a distribution that better approximates the target .	1	0	4	-5.886907	5.184796	1
134-9-36	the resulting objective contains an intractable entropy term ; we therefore derive a relaxation of the objective and show that the relaxed problem is blockwise concave under mild conditions .	practitioners of bayesian statistics have long depended on markov chain monte carlo ( mcmc ) to obtain samples from intractable posterior distributions .	0	5	0	5.7076445	-5.0917788	0
134-9-36	practitioners of bayesian statistics have long depended on markov chain monte carlo ( mcmc ) to obtain samples from intractable posterior distributions .	we illustrate the advantages of our algorithm on three inference tasks from the literature , demonstrating both the superior quality of the posterior approximation and the moderate overhead of the optimization step .	1	0	6	-5.8743553	5.1747518	1
134-9-36	practitioners of bayesian statistics have long depended on markov chain monte carlo ( mcmc ) to obtain samples from intractable posterior distributions .	[CLS] our algorithm achieves a relative error reduction ( measured against serial mcmc ) of up to 39 % compared to consensus monte carlo on the task of estimating 300 - dimensional probit regression parameter expectations ; similarly, it achieves an error reduction of 92 % on the task of estimating cluster comembership probabilities in a gaussian mixture	1	0	7	-5.818738	5.186925	1
134-9-36	practitioners of bayesian statistics have long depended on markov chain monte carlo ( mcmc ) to obtain samples from intractable posterior distributions .	furthermore , these gains come at moderate cost compared to the runtime of serial mcmc -- achieving near-ideal speedup in some instances .	1	0	8	-5.8461194	5.153139	1
134-9-36	unfortunately , mcmc algorithms are typically serial , and do not scale to the large datasets typical of modern machine learning .	the recently proposed consensus monte carlo algorithm removes this limitation by partitioning the data and drawing samples conditional on each partition in parallel .	1	1	2	-3.0041387	3.024254	1
134-9-36	unfortunately , mcmc algorithms are typically serial , and do not scale to the large datasets typical of modern machine learning .	a fixed aggregation function then combines these samples , yielding approximate posterior samples .	1	1	3	-5.2256494	4.9370327	1
134-9-36	unfortunately , mcmc algorithms are typically serial , and do not scale to the large datasets typical of modern machine learning .	we introduce variational consensus monte carlo ( vcmc ) , a variational bayes algorithm that optimizes over aggregation functions to obtain samples from a distribution that better approximates the target .	1	1	4	-3.9408875	3.7937257	1
134-9-36	unfortunately , mcmc algorithms are typically serial , and do not scale to the large datasets typical of modern machine learning .	the resulting objective contains an intractable entropy term ; we therefore derive a relaxation of the objective and show that the relaxed problem is blockwise concave under mild conditions .	1	1	5	-5.895192	5.2397356	1
134-9-36	unfortunately , mcmc algorithms are typically serial , and do not scale to the large datasets typical of modern machine learning .	we illustrate the advantages of our algorithm on three inference tasks from the literature , demonstrating both the superior quality of the posterior approximation and the moderate overhead of the optimization step .	1	1	6	-5.952535	5.1787214	1
134-9-36	unfortunately , mcmc algorithms are typically serial , and do not scale to the large datasets typical of modern machine learning .	[CLS] our algorithm achieves a relative error reduction ( measured against serial mcmc ) of up to 39 % compared to consensus monte carlo on the task of estimating 300 - dimensional probit regression parameter expectations ; similarly, it achieves an error reduction of 92 % on the task of estimating cluster comembership probabilities in a gaussian mixture model with	1	1	7	-5.920305	5.1985154	1
134-9-36	furthermore , these gains come at moderate cost compared to the runtime of serial mcmc -- achieving near-ideal speedup in some instances .	unfortunately , mcmc algorithms are typically serial , and do not scale to the large datasets typical of modern machine learning .	0	8	1	5.5934997	-4.912804	0
134-9-36	the recently proposed consensus monte carlo algorithm removes this limitation by partitioning the data and drawing samples conditional on each partition in parallel .	a fixed aggregation function then combines these samples , yielding approximate posterior samples .	1	2	3	-4.9641542	4.698765	1
134-9-36	we introduce variational consensus monte carlo ( vcmc ) , a variational bayes algorithm that optimizes over aggregation functions to obtain samples from a distribution that better approximates the target .	the recently proposed consensus monte carlo algorithm removes this limitation by partitioning the data and drawing samples conditional on each partition in parallel .	0	4	2	-1.7645531	1.9328566	1
134-9-36	the recently proposed consensus monte carlo algorithm removes this limitation by partitioning the data and drawing samples conditional on each partition in parallel .	the resulting objective contains an intractable entropy term ; we therefore derive a relaxation of the objective and show that the relaxed problem is blockwise concave under mild conditions .	1	2	5	-5.8478413	5.2501063	1
134-9-36	the recently proposed consensus monte carlo algorithm removes this limitation by partitioning the data and drawing samples conditional on each partition in parallel .	we illustrate the advantages of our algorithm on three inference tasks from the literature , demonstrating both the superior quality of the posterior approximation and the moderate overhead of the optimization step .	1	2	6	-5.9942436	5.1734357	1
134-9-36	[CLS] our algorithm achieves a relative error reduction ( measured against serial mcmc ) of up to 39 % compared to consensus monte carlo on the task of estimating 300 - dimensional probit regression parameter expectations ; similarly, it achieves an error reduction of 92 % on the task of estimating cluster comembership probabilities in a gaussian mixture model with	the recently proposed consensus monte carlo algorithm removes this limitation by partitioning the data and drawing samples conditional on each partition in parallel .	0	7	2	5.491681	-4.8890085	0
134-9-36	furthermore , these gains come at moderate cost compared to the runtime of serial mcmc -- achieving near-ideal speedup in some instances .	the recently proposed consensus monte carlo algorithm removes this limitation by partitioning the data and drawing samples conditional on each partition in parallel .	0	8	2	5.3813553	-4.7576027	0
134-9-36	a fixed aggregation function then combines these samples , yielding approximate posterior samples .	we introduce variational consensus monte carlo ( vcmc ) , a variational bayes algorithm that optimizes over aggregation functions to obtain samples from a distribution that better approximates the target .	1	3	4	4.359159	-3.8825908	0
134-9-36	the resulting objective contains an intractable entropy term ; we therefore derive a relaxation of the objective and show that the relaxed problem is blockwise concave under mild conditions .	a fixed aggregation function then combines these samples , yielding approximate posterior samples .	0	5	3	0.3137531	-0.09247878	0
134-9-36	a fixed aggregation function then combines these samples , yielding approximate posterior samples .	we illustrate the advantages of our algorithm on three inference tasks from the literature , demonstrating both the superior quality of the posterior approximation and the moderate overhead of the optimization step .	1	3	6	-5.9482694	5.050958	1
134-9-36	a fixed aggregation function then combines these samples , yielding approximate posterior samples .	our algorithm achieves a relative error reduction ( measured against serial mcmc ) of up to 39 % compared to consensus monte carlo on the task of estimating 300-dimensional probit regression parameter expectations ; similarly , it achieves an error reduction of 92 % on the task of estimating cluster comembership probabilities in a gaussian mixture model with 8 components in 8 dimensions .	1	3	7	-5.5174427	5.0346537	1
134-9-36	furthermore , these gains come at moderate cost compared to the runtime of serial mcmc -- achieving near-ideal speedup in some instances .	a fixed aggregation function then combines these samples , yielding approximate posterior samples .	0	8	3	4.698052	-4.324429	0
134-9-36	we introduce variational consensus monte carlo ( vcmc ) , a variational bayes algorithm that optimizes over aggregation functions to obtain samples from a distribution that better approximates the target .	the resulting objective contains an intractable entropy term ; we therefore derive a relaxation of the objective and show that the relaxed problem is blockwise concave under mild conditions .	1	4	5	-5.7524214	5.180135	1
134-9-36	we illustrate the advantages of our algorithm on three inference tasks from the literature , demonstrating both the superior quality of the posterior approximation and the moderate overhead of the optimization step .	we introduce variational consensus monte carlo ( vcmc ) , a variational bayes algorithm that optimizes over aggregation functions to obtain samples from a distribution that better approximates the target .	0	6	4	5.5092335	-4.8296742	0
134-9-36	we introduce variational consensus monte carlo ( vcmc ) , a variational bayes algorithm that optimizes over aggregation functions to obtain samples from a distribution that better approximates the target .	[CLS] our algorithm achieves a relative error reduction ( measured against serial mcmc ) of up to 39 % compared to consensus monte carlo on the task of estimating 300 - dimensional probit regression parameter expectations ; similarly, it achieves an error reduction of 92 % on the task of estimating cluster come	1	4	7	-5.9785347	5.1955323	1
134-9-36	we introduce variational consensus monte carlo ( vcmc ) , a variational bayes algorithm that optimizes over aggregation functions to obtain samples from a distribution that better approximates the target .	furthermore , these gains come at moderate cost compared to the runtime of serial mcmc -- achieving near-ideal speedup in some instances .	1	4	8	-5.8481364	5.1833925	1
134-9-36	the resulting objective contains an intractable entropy term ; we therefore derive a relaxation of the objective and show that the relaxed problem is blockwise concave under mild conditions .	we illustrate the advantages of our algorithm on three inference tasks from the literature , demonstrating both the superior quality of the posterior approximation and the moderate overhead of the optimization step .	1	5	6	-5.9520254	5.114747	1
134-9-36	[CLS] our algorithm achieves a relative error reduction ( measured against serial mcmc ) of up to 39 % compared to consensus monte carlo on the task of estimating 300 - dimensional probit regression parameter expectations ; similarly, it achieves an error reduction of 92 % on the task of estimating cluster comembership probabilities	the resulting objective contains an intractable entropy term ; we therefore derive a relaxation of the objective and show that the relaxed problem is blockwise concave under mild conditions .	0	7	5	4.785411	-4.361187	0
134-9-36	furthermore , these gains come at moderate cost compared to the runtime of serial mcmc -- achieving near-ideal speedup in some instances .	the resulting objective contains an intractable entropy term ; we therefore derive a relaxation of the objective and show that the relaxed problem is blockwise concave under mild conditions .	0	8	5	4.426137	-4.0399313	0
134-9-36	we illustrate the advantages of our algorithm on three inference tasks from the literature , demonstrating both the superior quality of the posterior approximation and the moderate overhead of the optimization step .	[CLS] our algorithm achieves a relative error reduction ( measured against serial mcmc ) of up to 39 % compared to consensus monte carlo on the task of estimating 300 - dimensional probit regression parameter expectations ; similarly, it achieves an error reduction of 92 % on the task of estimating cluster comembership probabilities	1	6	7	-1.1695129	1.2628572	1
134-9-36	we illustrate the advantages of our algorithm on three inference tasks from the literature , demonstrating both the superior quality of the posterior approximation and the moderate overhead of the optimization step .	furthermore , these gains come at moderate cost compared to the runtime of serial mcmc -- achieving near-ideal speedup in some instances .	1	6	8	-0.47749758	0.66914415	1
134-9-36	[CLS] our algorithm achieves a relative error reduction ( measured against serial mcmc ) of up to 39 % compared to consensus monte carlo on the task of estimating 300 - dimensional probit regression parameter expectations ; similarly, it achieves an error reduction of 92 % on the task of estimating cluster comembership probabilities in a gauss	furthermore , these gains come at moderate cost compared to the runtime of serial mcmc -- achieving near-ideal speedup in some instances .	1	7	8	-0.62846327	0.82635987	1
135-8-28	in this regime , optimization algorithms can immensely benefit from approximate second order information .	we consider the problem of efficiently computing the maximum likelihood estimator in generalized linear models ( glms ) when the number of observations is much larger than the number of coefficients ( n p 1 ) .	0	1	0	5.354658	-4.8015833	0
135-8-28	we propose an alternative way of constructing the curvature information by formulating it as an estimation problem and applying a stein-type lemma , which allows further improvements through sub-sampling and eigenvalue thresholding .	we consider the problem of efficiently computing the maximum likelihood estimator in generalized linear models ( glms ) when the number of observations is much larger than the number of coefficients ( n p 1 ) .	0	2	0	5.593604	-4.9481697	0
135-8-28	our algorithm enjoys fast convergence rates , resembling that of second order methods , with modest per-iteration cost .	we consider the problem of efficiently computing the maximum likelihood estimator in generalized linear models ( glms ) when the number of observations is much larger than the number of coefficients ( n p 1 ) .	0	3	0	5.552926	-5.0070586	0
135-8-28	we consider the problem of efficiently computing the maximum likelihood estimator in generalized linear models ( glms ) when the number of observations is much larger than the number of coefficients ( n p 1 ) .	we provide its convergence analysis for the case where the rows of the design matrix are i.i.d .	1	0	4	-5.9263477	5.1994576	1
135-8-28	we consider the problem of efficiently computing the maximum likelihood estimator in generalized linear models ( glms ) when the number of observations is much larger than the number of coefficients ( n p 1 ) .	samples with bounded support .	1	0	5	-5.429331	4.93778	1
135-8-28	we consider the problem of efficiently computing the maximum likelihood estimator in generalized linear models ( glms ) when the number of observations is much larger than the number of coefficients ( n p 1 ) .	we show that the convergence has two phases , a quadratic phase followed by a linear phase .	1	0	6	-5.9308968	5.2134743	1
135-8-28	finally , we empirically demonstrate that our algorithm achieves the highest performance compared to various algorithms on several datasets .	we consider the problem of efficiently computing the maximum likelihood estimator in generalized linear models ( glms ) when the number of observations is much larger than the number of coefficients ( n p 1 ) .	0	7	0	5.551985	-4.992953	0
135-8-28	in this regime , optimization algorithms can immensely benefit from approximate second order information .	we propose an alternative way of constructing the curvature information by formulating it as an estimation problem and applying a stein-type lemma , which allows further improvements through sub-sampling and eigenvalue thresholding .	1	1	2	-5.9249563	5.12527	1
135-8-28	in this regime , optimization algorithms can immensely benefit from approximate second order information .	our algorithm enjoys fast convergence rates , resembling that of second order methods , with modest per-iteration cost .	1	1	3	-5.9257526	5.187031	1
135-8-28	in this regime , optimization algorithms can immensely benefit from approximate second order information .	we provide its convergence analysis for the case where the rows of the design matrix are i.i.d .	1	1	4	-5.887285	5.1861067	1
135-8-28	in this regime , optimization algorithms can immensely benefit from approximate second order information .	samples with bounded support .	1	1	5	-0.6768602	0.73797476	1
135-8-28	we show that the convergence has two phases , a quadratic phase followed by a linear phase .	in this regime , optimization algorithms can immensely benefit from approximate second order information .	0	6	1	4.026299	-3.6570108	0
135-8-28	in this regime , optimization algorithms can immensely benefit from approximate second order information .	finally , we empirically demonstrate that our algorithm achieves the highest performance compared to various algorithms on several datasets .	1	1	7	-5.9447207	5.0396185	1
135-8-28	our algorithm enjoys fast convergence rates , resembling that of second order methods , with modest per-iteration cost .	we propose an alternative way of constructing the curvature information by formulating it as an estimation problem and applying a stein-type lemma , which allows further improvements through sub-sampling and eigenvalue thresholding .	0	3	2	3.260902	-3.0079408	0
135-8-28	we propose an alternative way of constructing the curvature information by formulating it as an estimation problem and applying a stein-type lemma , which allows further improvements through sub-sampling and eigenvalue thresholding .	we provide its convergence analysis for the case where the rows of the design matrix are i.i.d .	1	2	4	3.1091275	-2.8417282	0
135-8-28	we propose an alternative way of constructing the curvature information by formulating it as an estimation problem and applying a stein-type lemma , which allows further improvements through sub-sampling and eigenvalue thresholding .	samples with bounded support .	1	2	5	4.0189342	-3.6565137	0
135-8-28	we show that the convergence has two phases , a quadratic phase followed by a linear phase .	we propose an alternative way of constructing the curvature information by formulating it as an estimation problem and applying a stein-type lemma , which allows further improvements through sub-sampling and eigenvalue thresholding .	0	6	2	-4.710067	4.4260483	1
135-8-28	we propose an alternative way of constructing the curvature information by formulating it as an estimation problem and applying a stein-type lemma , which allows further improvements through sub-sampling and eigenvalue thresholding .	finally , we empirically demonstrate that our algorithm achieves the highest performance compared to various algorithms on several datasets .	1	2	7	-5.9611855	5.1379223	1
135-8-28	we provide its convergence analysis for the case where the rows of the design matrix are i.i.d .	our algorithm enjoys fast convergence rates , resembling that of second order methods , with modest per-iteration cost .	0	4	3	-2.9843252	2.970635	1
135-8-28	our algorithm enjoys fast convergence rates , resembling that of second order methods , with modest per-iteration cost .	samples with bounded support .	1	3	5	3.481575	-3.248103	0
135-8-28	our algorithm enjoys fast convergence rates , resembling that of second order methods , with modest per-iteration cost .	we show that the convergence has two phases , a quadratic phase followed by a linear phase .	1	3	6	2.1133616	-1.91659	0
135-8-28	our algorithm enjoys fast convergence rates , resembling that of second order methods , with modest per-iteration cost .	finally , we empirically demonstrate that our algorithm achieves the highest performance compared to various algorithms on several datasets .	1	3	7	-5.8593497	5.200944	1
135-8-28	samples with bounded support .	we provide its convergence analysis for the case where the rows of the design matrix are i.i.d .	0	5	4	-1.6956239	1.8872309	1
135-8-28	we show that the convergence has two phases , a quadratic phase followed by a linear phase .	we provide its convergence analysis for the case where the rows of the design matrix are i.i.d .	0	6	4	-1.497393	1.7195457	1
135-8-28	we provide its convergence analysis for the case where the rows of the design matrix are i.i.d .	finally , we empirically demonstrate that our algorithm achieves the highest performance compared to various algorithms on several datasets .	1	4	7	-5.936734	5.087563	1
135-8-28	samples with bounded support .	we show that the convergence has two phases , a quadratic phase followed by a linear phase .	1	5	6	-0.14738357	0.32484055	1
135-8-28	finally , we empirically demonstrate that our algorithm achieves the highest performance compared to various algorithms on several datasets .	samples with bounded support .	0	7	5	5.2385936	-4.692671	0
135-8-28	finally , we empirically demonstrate that our algorithm achieves the highest performance compared to various algorithms on several datasets .	we show that the convergence has two phases , a quadratic phase followed by a linear phase .	0	7	6	5.47534	-4.850864	0
136-5-10	we show the existence of a locality-sensitive hashing ( lsh ) family for the angular distance that yields an approximate near neighbor search algorithm with the asymptotically optimal running time exponent .	unlike earlier algorithms with this property ( e.g. , spherical lsh ) , our algorithm is also practical , improving upon the well-studied hyperplane lsh in practice .	1	0	1	-5.974	5.1922803	1
136-5-10	we show the existence of a locality-sensitive hashing ( lsh ) family for the angular distance that yields an approximate near neighbor search algorithm with the asymptotically optimal running time exponent .	we also introduce a multiprobe version of this algorithm and conduct an experimental evaluation on real and synthetic data sets .	1	0	2	-5.979865	5.1753416	1
136-5-10	we show the existence of a locality-sensitive hashing ( lsh ) family for the angular distance that yields an approximate near neighbor search algorithm with the asymptotically optimal running time exponent .	we complement the above positive results with a fine-grained lower bound for the quality of any lsh family for angular distance .	1	0	3	-5.8648043	5.1874056	1
136-5-10	our lower bound implies that the above lsh family exhibits a trade-off between evaluation time and quality that is close to optimal for a natural class of lsh functions .	we show the existence of a locality-sensitive hashing ( lsh ) family for the angular distance that yields an approximate near neighbor search algorithm with the asymptotically optimal running time exponent .	0	4	0	5.5940833	-4.9091516	0
136-5-10	we also introduce a multiprobe version of this algorithm and conduct an experimental evaluation on real and synthetic data sets .	unlike earlier algorithms with this property ( e.g. , spherical lsh ) , our algorithm is also practical , improving upon the well-studied hyperplane lsh in practice .	0	2	1	4.3764935	-4.0077653	0
136-5-10	we complement the above positive results with a fine-grained lower bound for the quality of any lsh family for angular distance .	unlike earlier algorithms with this property ( e.g. , spherical lsh ) , our algorithm is also practical , improving upon the well-studied hyperplane lsh in practice .	0	3	1	2.5532231	-2.3992584	0
136-5-10	our lower bound implies that the above lsh family exhibits a trade-off between evaluation time and quality that is close to optimal for a natural class of lsh functions .	unlike earlier algorithms with this property ( e.g. , spherical lsh ) , our algorithm is also practical , improving upon the well-studied hyperplane lsh in practice .	0	4	1	-4.018955	3.8288965	1
136-5-10	we also introduce a multiprobe version of this algorithm and conduct an experimental evaluation on real and synthetic data sets .	we complement the above positive results with a fine-grained lower bound for the quality of any lsh family for angular distance .	1	2	3	1.9455295	-1.727139	0
136-5-10	our lower bound implies that the above lsh family exhibits a trade-off between evaluation time and quality that is close to optimal for a natural class of lsh functions .	we also introduce a multiprobe version of this algorithm and conduct an experimental evaluation on real and synthetic data sets .	0	4	2	-5.2307396	4.7312403	1
136-5-10	our lower bound implies that the above lsh family exhibits a trade-off between evaluation time and quality that is close to optimal for a natural class of lsh functions .	we complement the above positive results with a fine-grained lower bound for the quality of any lsh family for angular distance .	0	4	3	-2.5303073	2.5623062	1
137-5-10	however , a principled way in which to train such hierarchies in the unsupervised setting has remained elusive .	training deep feature hierarchies to solve supervised learning tasks has achieved state of the art performance on many problems in computer vision .	0	1	0	5.2484746	-4.627372	0
137-5-10	training deep feature hierarchies to solve supervised learning tasks has achieved state of the art performance on many problems in computer vision .	in this work we suggest a new architecture and loss for training deep feature hierarchies that linearize the transformations observed in unlabeled natural video sequences .	1	0	2	-5.8841467	5.2655015	1
137-5-10	training deep feature hierarchies to solve supervised learning tasks has achieved state of the art performance on many problems in computer vision .	this is done by training a generative model to predict video frames .	1	0	3	-5.925436	5.208962	1
137-5-10	training deep feature hierarchies to solve supervised learning tasks has achieved state of the art performance on many problems in computer vision .	we also address the problem of inherent uncertainty in prediction by introducing latent variables that are non-deterministic functions of the input into the network architecture .	1	0	4	-6.032313	5.194584	1
137-5-10	however , a principled way in which to train such hierarchies in the unsupervised setting has remained elusive .	in this work we suggest a new architecture and loss for training deep feature hierarchies that linearize the transformations observed in unlabeled natural video sequences .	1	1	2	-5.650324	5.16998	1
137-5-10	this is done by training a generative model to predict video frames .	however , a principled way in which to train such hierarchies in the unsupervised setting has remained elusive .	0	3	1	5.085	-4.518428	0
137-5-10	we also address the problem of inherent uncertainty in prediction by introducing latent variables that are non-deterministic functions of the input into the network architecture .	however , a principled way in which to train such hierarchies in the unsupervised setting has remained elusive .	0	4	1	4.335618	-3.8611703	0
137-5-10	in this work we suggest a new architecture and loss for training deep feature hierarchies that linearize the transformations observed in unlabeled natural video sequences .	this is done by training a generative model to predict video frames .	1	2	3	-4.720536	4.421764	1
137-5-10	we also address the problem of inherent uncertainty in prediction by introducing latent variables that are non-deterministic functions of the input into the network architecture .	in this work we suggest a new architecture and loss for training deep feature hierarchies that linearize the transformations observed in unlabeled natural video sequences .	0	4	2	3.640031	-3.3085408	0
137-5-10	this is done by training a generative model to predict video frames .	we also address the problem of inherent uncertainty in prediction by introducing latent variables that are non-deterministic functions of the input into the network architecture .	1	3	4	-2.597888	2.725931	1
138-4-6	we show that lmc allows to sample in polynomial time from a posterior distribution restricted to a convex body and with concave log-likelihood .	we analyze the projected langevin monte carlo ( lmc ) algorithm , a close cousin of projected stochastic gradient descent ( sgd ) .	0	1	0	5.6602407	-5.018888	0
138-4-6	we analyze the projected langevin monte carlo ( lmc ) algorithm , a close cousin of projected stochastic gradient descent ( sgd ) .	this gives the first markov chain to sample from a log-concave distribution with a first-order oracle , as the existing chains with provable guarantees ( lattice walk , ball walk and hit-and-run ) require a zerothorder oracle .	1	0	2	-5.8950105	5.1983376	1
138-4-6	our proof uses elementary concepts from stochastic calculus which could be useful more generally to understand sgd and its variants .	we analyze the projected langevin monte carlo ( lmc ) algorithm , a close cousin of projected stochastic gradient descent ( sgd ) .	0	3	0	5.677966	-5.007684	0
138-4-6	this gives the first markov chain to sample from a log-concave distribution with a first-order oracle , as the existing chains with provable guarantees ( lattice walk , ball walk and hit-and-run ) require a zerothorder oracle .	we show that lmc allows to sample in polynomial time from a posterior distribution restricted to a convex body and with concave log-likelihood .	0	2	1	2.1521485	-1.9542258	0
138-4-6	we show that lmc allows to sample in polynomial time from a posterior distribution restricted to a convex body and with concave log-likelihood .	our proof uses elementary concepts from stochastic calculus which could be useful more generally to understand sgd and its variants .	1	1	3	-3.975915	3.8050094	1
138-4-6	our proof uses elementary concepts from stochastic calculus which could be useful more generally to understand sgd and its variants .	this gives the first markov chain to sample from a log-concave distribution with a first-order oracle , as the existing chains with provable guarantees ( lattice walk , ball walk and hit-and-run ) require a zerothorder oracle .	0	3	2	3.3543851	-3.135071	0
139-6-15	in addition to identifying the content within a single image , relating images and generating related images are critical tasks for image understanding .	recently , deep convolutional networks have yielded breakthroughs in predicting image labels , annotations and captions , but have only just begun to be used for generating high-quality images .	1	0	1	-2.7165198	2.8433888	1
139-6-15	in addition to identifying the content within a single image , relating images and generating related images are critical tasks for image understanding .	in this paper we develop a novel deep network trained end-to-end to perform visual analogy making , which is the task of transforming a query image according to an example pair of related images .	1	0	2	-5.927464	5.191095	1
139-6-15	in addition to identifying the content within a single image , relating images and generating related images are critical tasks for image understanding .	solving this problem requires both accurately recognizing a visual relationship and generating a transformed query image accordingly .	1	0	3	-5.841074	5.18954	1
139-6-15	in addition to identifying the content within a single image , relating images and generating related images are critical tasks for image understanding .	inspired by recent advances in language modeling , we propose to solve visual analogies by learning to map images to a neural embedding in which analogical reasoning is simple , such as by vector subtraction and addition .	1	0	4	-5.948114	5.162355	1
139-6-15	in addition to identifying the content within a single image , relating images and generating related images are critical tasks for image understanding .	in experiments , our model effectively models visual analogies on several datasets : 2d shapes , animated video game sprites , and 3d car models .	1	0	5	-5.967659	5.1202273	1
139-6-15	recently , deep convolutional networks have yielded breakthroughs in predicting image labels , annotations and captions , but have only just begun to be used for generating high-quality images .	in this paper we develop a novel deep network trained end-to-end to perform visual analogy making , which is the task of transforming a query image according to an example pair of related images .	1	1	2	-5.7963076	5.247572	1
139-6-15	solving this problem requires both accurately recognizing a visual relationship and generating a transformed query image accordingly .	recently , deep convolutional networks have yielded breakthroughs in predicting image labels , annotations and captions , but have only just begun to be used for generating high-quality images .	0	3	1	5.1726513	-4.6143184	0
139-6-15	inspired by recent advances in language modeling , we propose to solve visual analogies by learning to map images to a neural embedding in which analogical reasoning is simple , such as by vector subtraction and addition .	recently , deep convolutional networks have yielded breakthroughs in predicting image labels , annotations and captions , but have only just begun to be used for generating high-quality images .	0	4	1	5.4387054	-4.8397565	0
139-6-15	in experiments , our model effectively models visual analogies on several datasets : 2d shapes , animated video game sprites , and 3d car models .	recently , deep convolutional networks have yielded breakthroughs in predicting image labels , annotations and captions , but have only just begun to be used for generating high-quality images .	0	5	1	5.702303	-5.0767612	0
139-6-15	in this paper we develop a novel deep network trained end-to-end to perform visual analogy making , which is the task of transforming a query image according to an example pair of related images .	solving this problem requires both accurately recognizing a visual relationship and generating a transformed query image accordingly .	1	2	3	1.3252015	-0.9563835	0
139-6-15	in this paper we develop a novel deep network trained end-to-end to perform visual analogy making , which is the task of transforming a query image according to an example pair of related images .	inspired by recent advances in language modeling , we propose to solve visual analogies by learning to map images to a neural embedding in which analogical reasoning is simple , such as by vector subtraction and addition .	1	2	4	-1.7245088	1.817996	1
139-6-15	in this paper we develop a novel deep network trained end-to-end to perform visual analogy making , which is the task of transforming a query image according to an example pair of related images .	in experiments , our model effectively models visual analogies on several datasets : 2d shapes , animated video game sprites , and 3d car models .	1	2	5	-5.9246593	5.201202	1
139-6-15	solving this problem requires both accurately recognizing a visual relationship and generating a transformed query image accordingly .	inspired by recent advances in language modeling , we propose to solve visual analogies by learning to map images to a neural embedding in which analogical reasoning is simple , such as by vector subtraction and addition .	1	3	4	-2.4741888	2.5382514	1
139-6-15	in experiments , our model effectively models visual analogies on several datasets : 2d shapes , animated video game sprites , and 3d car models .	solving this problem requires both accurately recognizing a visual relationship and generating a transformed query image accordingly .	0	5	3	5.436756	-4.8519936	0
139-6-15	inspired by recent advances in language modeling , we propose to solve visual analogies by learning to map images to a neural embedding in which analogical reasoning is simple , such as by vector subtraction and addition .	in experiments , our model effectively models visual analogies on several datasets : 2d shapes , animated video game sprites , and 3d car models .	1	4	5	-5.935473	5.1939034	1
140-7-21	the completion of low rank matrices from few entries is a task with many practical applications .	we consider here two aspects of this problem : detectability , i.e .	1	0	1	-5.930895	5.2049785	1
140-7-21	the ability to estimate the rank r reliably from the fewest possible random entries , and performance in achieving small reconstruction error .	the completion of low rank matrices from few entries is a task with many practical applications .	0	2	0	5.5974236	-4.9665165	0
140-7-21	the completion of low rank matrices from few entries is a task with many practical applications .	we propose a spectral algorithm for these two tasks called macbeth ( for matrix completion with the bethe hessian ) .	1	0	3	-5.8796864	5.1024246	1
140-7-21	the completion of low rank matrices from few entries is a task with many practical applications .	the rank is estimated as the number of negative eigenvalues of the bethe hessian matrix , and the corresponding eigenvectors are used as initial condition for the minimization of the discrepancy between the estimated matrix and the revealed entries .	1	0	4	-5.7375655	5.1597495	1
140-7-21	the completion of low rank matrices from few entries is a task with many practical applications .	we analyze the performance in a random matrix setting using results from the statistical mechanics of the hopfield neural network , and show in particular thatmacbeth efficiently detects the rank r of a large n x m matrix from c ( r ) r nm entries , where c ( r ) is a constant close to 1 .	1	0	5	-5.919137	5.211703	1
140-7-21	the completion of low rank matrices from few entries is a task with many practical applications .	we also evaluate the corresponding root-mean-square error empirically and show that macbeth compares favorably to other existing approaches .	1	0	6	-5.8755713	5.1767163	1
140-7-21	we consider here two aspects of this problem : detectability , i.e .	the ability to estimate the rank r reliably from the fewest possible random entries , and performance in achieving small reconstruction error .	1	1	2	-4.18318	3.84637	1
140-7-21	we consider here two aspects of this problem : detectability , i.e .	we propose a spectral algorithm for these two tasks called macbeth ( for matrix completion with the bethe hessian ) .	1	1	3	-5.9909225	5.133939	1
140-7-21	the rank is estimated as the number of negative eigenvalues of the bethe hessian matrix , and the corresponding eigenvectors are used as initial condition for the minimization of the discrepancy between the estimated matrix and the revealed entries .	we consider here two aspects of this problem : detectability , i.e .	0	4	1	4.4262543	-3.9993165	0
140-7-21	we consider here two aspects of this problem : detectability , i.e .	we analyze the performance in a random matrix setting using results from the statistical mechanics of the hopfield neural network , and show in particular thatmacbeth efficiently detects the rank r of a large n x m matrix from c ( r ) r nm entries , where c ( r ) is a constant close to 1 .	1	1	5	-5.9501705	5.2087717	1
140-7-21	we also evaluate the corresponding root-mean-square error empirically and show that macbeth compares favorably to other existing approaches .	we consider here two aspects of this problem : detectability , i.e .	0	6	1	5.670231	-5.0858026	0
140-7-21	the ability to estimate the rank r reliably from the fewest possible random entries , and performance in achieving small reconstruction error .	we propose a spectral algorithm for these two tasks called macbeth ( for matrix completion with the bethe hessian ) .	1	2	3	0.048270017	0.10656588	1
140-7-21	the ability to estimate the rank r reliably from the fewest possible random entries , and performance in achieving small reconstruction error .	the rank is estimated as the number of negative eigenvalues of the bethe hessian matrix , and the corresponding eigenvectors are used as initial condition for the minimization of the discrepancy between the estimated matrix and the revealed entries .	1	2	4	3.346272	-3.1246762	0
140-7-21	the ability to estimate the rank r reliably from the fewest possible random entries , and performance in achieving small reconstruction error .	we analyze the performance in a random matrix setting using results from the statistical mechanics of the hopfield neural network , and show in particular thatmacbeth efficiently detects the rank r of a large n x m matrix from c ( r ) r nm entries , where c ( r ) is a constant close to 1 .	1	2	5	3.7431726	-3.3893647	0
140-7-21	the ability to estimate the rank r reliably from the fewest possible random entries , and performance in achieving small reconstruction error .	we also evaluate the corresponding root-mean-square error empirically and show that macbeth compares favorably to other existing approaches .	1	2	6	-4.5102296	4.131493	1
140-7-21	we propose a spectral algorithm for these two tasks called macbeth ( for matrix completion with the bethe hessian ) .	the rank is estimated as the number of negative eigenvalues of the bethe hessian matrix , and the corresponding eigenvectors are used as initial condition for the minimization of the discrepancy between the estimated matrix and the revealed entries .	1	3	4	-2.6553993	2.666793	1
140-7-21	we propose a spectral algorithm for these two tasks called macbeth ( for matrix completion with the bethe hessian ) .	we analyze the performance in a random matrix setting using results from the statistical mechanics of the hopfield neural network , and show in particular thatmacbeth efficiently detects the rank r of a large n x m matrix from c ( r ) r nm entries , where c ( r ) is a constant close to 1 .	1	3	5	-4.761491	4.4183984	1
140-7-21	we propose a spectral algorithm for these two tasks called macbeth ( for matrix completion with the bethe hessian ) .	we also evaluate the corresponding root-mean-square error empirically and show that macbeth compares favorably to other existing approaches .	1	3	6	-5.99809	5.225721	1
140-7-21	[CLS] we analyze the performance in a random matrix setting using results from the statistical mechanics of the hopfield neural network, and show in particular thatmacbeth efficiently detects the rank r of a large n x m matrix from c ( r ) r	[CLS] the rank is estimated as the number of negative eigenvalues of the bethe hessian matrix, and the corresponding eigenvectors are used as initial condition for the minimization of the discrepancy between the estimated matrix and	0	5	4	-2.2494779	2.2588158	1
140-7-21	the rank is estimated as the number of negative eigenvalues of the bethe hessian matrix , and the corresponding eigenvectors are used as initial condition for the minimization of the discrepancy between the estimated matrix and the revealed entries .	we also evaluate the corresponding root-mean-square error empirically and show that macbeth compares favorably to other existing approaches .	1	4	6	-5.89599	5.176061	1
140-7-21	we analyze the performance in a random matrix setting using results from the statistical mechanics of the hopfield neural network , and show in particular thatmacbeth efficiently detects the rank r of a large n x m matrix from c ( r ) r nm entries , where c ( r ) is a constant close to 1 .	we also evaluate the corresponding root-mean-square error empirically and show that macbeth compares favorably to other existing approaches .	1	5	6	-5.649725	5.0925164	1
141-5-10	we study the performance of standard online learning algorithms when the feedback is delayed by an adversary .	we show that online-gradient-descent and follow-the-perturbed-leader achieve regret o ( d ) in the delayed setting , where d is the sum of delays of each round 's feedback .	1	0	1	-5.8887997	5.19219	1
141-5-10	we study the performance of standard online learning algorithms when the feedback is delayed by an adversary .	this bound collapses to an optimal o ( t ) bound in the usual setting of no delays ( where d = t ) .	1	0	2	-5.8260193	5.205021	1
141-5-10	we study the performance of standard online learning algorithms when the feedback is delayed by an adversary .	our main contribution is to show that standard algorithms for online learning already have simple regret bounds in the most general setting of delayed feedback , making adjustments to the analysis and not to the algorithms themselves .	1	0	3	-5.9167175	5.1840982	1
141-5-10	our results help affirm and clarify the success of recent algorithms in optimization and machine learning that operate in a delayed feedback model .	we study the performance of standard online learning algorithms when the feedback is delayed by an adversary .	0	4	0	5.563757	-5.039517	0
141-5-10	we show that online-gradient-descent and follow-the-perturbed-leader achieve regret o ( d ) in the delayed setting , where d is the sum of delays of each round 's feedback .	this bound collapses to an optimal o ( t ) bound in the usual setting of no delays ( where d = t ) .	1	1	2	-2.4041104	2.4884012	1
141-5-10	our main contribution is to show that standard algorithms for online learning already have simple regret bounds in the most general setting of delayed feedback , making adjustments to the analysis and not to the algorithms themselves .	we show that online-gradient-descent and follow-the-perturbed-leader achieve regret o ( d ) in the delayed setting , where d is the sum of delays of each round 's feedback .	0	3	1	-1.2802165	1.558279	1
141-5-10	our results help affirm and clarify the success of recent algorithms in optimization and machine learning that operate in a delayed feedback model .	we show that online-gradient-descent and follow-the-perturbed-leader achieve regret o ( d ) in the delayed setting , where d is the sum of delays of each round 's feedback .	0	4	1	4.941543	-4.381117	0
141-5-10	this bound collapses to an optimal o ( t ) bound in the usual setting of no delays ( where d = t ) .	our main contribution is to show that standard algorithms for online learning already have simple regret bounds in the most general setting of delayed feedback , making adjustments to the analysis and not to the algorithms themselves .	1	2	3	2.3211522	-2.1160855	0
141-5-10	this bound collapses to an optimal o ( t ) bound in the usual setting of no delays ( where d = t ) .	our results help affirm and clarify the success of recent algorithms in optimization and machine learning that operate in a delayed feedback model .	1	2	4	-5.7718863	4.8762765	1
141-5-10	our main contribution is to show that standard algorithms for online learning already have simple regret bounds in the most general setting of delayed feedback , making adjustments to the analysis and not to the algorithms themselves .	our results help affirm and clarify the success of recent algorithms in optimization and machine learning that operate in a delayed feedback model .	1	3	4	-5.887745	5.1608567	1
142-9-36	it has been applied successfully in many real-world applications .	tree structured group lasso ( tgl ) is a powerful technique in uncovering the tree structured sparsity over the features , where each node encodes a group of features .	0	1	0	-4.146711	4.0083294	1
142-9-36	tree structured group lasso ( tgl ) is a powerful technique in uncovering the tree structured sparsity over the features , where each node encodes a group of features .	however , with extremely large feature dimensions , solving tgl remains a significant challenge due to its highly complicated regularizer .	1	0	2	-5.9638014	5.131785	1
142-9-36	tree structured group lasso ( tgl ) is a powerful technique in uncovering the tree structured sparsity over the features , where each node encodes a group of features .	in this paper , we propose a novel multilayer feature reduction method ( mlfre ) to quickly identify the inactive nodes ( the groups of features with zero coefficients in the solution ) hierarchically in a top-down fashion , which are guaranteed to be irrelevant to the response .	1	0	3	-4.939166	4.597107	1
142-9-36	thus , we can remove the detected nodes from the optimization without sacrificing accuracy .	tree structured group lasso ( tgl ) is a powerful technique in uncovering the tree structured sparsity over the features , where each node encodes a group of features .	0	4	0	4.5792103	-4.056164	0
142-9-36	the major challenge in developing such testing rules is due to the overlaps between the parents and their children nodes .	tree structured group lasso ( tgl ) is a powerful technique in uncovering the tree structured sparsity over the features , where each node encodes a group of features .	0	5	0	-5.0850744	4.8115616	1
142-9-36	by a novel hierarchical projection algorithm , mlfre is able to test the nodes independently from any of their ancestor nodes .	tree structured group lasso ( tgl ) is a powerful technique in uncovering the tree structured sparsity over the features , where each node encodes a group of features .	0	6	0	4.6129894	-4.0618725	0
142-9-36	tree structured group lasso ( tgl ) is a powerful technique in uncovering the tree structured sparsity over the features , where each node encodes a group of features .	moreover , we can integrate mlfre -- that has a low computational cost -- with any existing solvers .	1	0	7	-5.8906746	5.2587986	1
142-9-36	tree structured group lasso ( tgl ) is a powerful technique in uncovering the tree structured sparsity over the features , where each node encodes a group of features .	experiments on both synthetic and real data sets demonstrate that the speedup gained by mlfre can be orders of magnitude .	1	0	8	-5.9060225	5.2643814	1
142-9-36	it has been applied successfully in many real-world applications .	however , with extremely large feature dimensions , solving tgl remains a significant challenge due to its highly complicated regularizer .	1	1	2	-5.587654	5.094935	1
142-9-36	in this paper , we propose a novel multilayer feature reduction method ( mlfre ) to quickly identify the inactive nodes ( the groups of features with zero coefficients in the solution ) hierarchically in a top-down fashion , which are guaranteed to be irrelevant to the response .	it has been applied successfully in many real-world applications .	0	3	1	5.2460685	-4.6907682	0
142-9-36	thus , we can remove the detected nodes from the optimization without sacrificing accuracy .	it has been applied successfully in many real-world applications .	0	4	1	5.407418	-4.779912	0
142-9-36	the major challenge in developing such testing rules is due to the overlaps between the parents and their children nodes .	it has been applied successfully in many real-world applications .	0	5	1	4.5338817	-4.0818725	0
142-9-36	by a novel hierarchical projection algorithm , mlfre is able to test the nodes independently from any of their ancestor nodes .	it has been applied successfully in many real-world applications .	0	6	1	5.4641128	-4.8697553	0
142-9-36	it has been applied successfully in many real-world applications .	moreover , we can integrate mlfre -- that has a low computational cost -- with any existing solvers .	1	1	7	-5.897714	5.192422	1
142-9-36	experiments on both synthetic and real data sets demonstrate that the speedup gained by mlfre can be orders of magnitude .	it has been applied successfully in many real-world applications .	0	8	1	5.6431704	-5.0635667	0
142-9-36	in this paper , we propose a novel multilayer feature reduction method ( mlfre ) to quickly identify the inactive nodes ( the groups of features with zero coefficients in the solution ) hierarchically in a top-down fashion , which are guaranteed to be irrelevant to the response .	however , with extremely large feature dimensions , solving tgl remains a significant challenge due to its highly complicated regularizer .	0	3	2	4.8933206	-4.4226594	0
142-9-36	thus , we can remove the detected nodes from the optimization without sacrificing accuracy .	however , with extremely large feature dimensions , solving tgl remains a significant challenge due to its highly complicated regularizer .	0	4	2	5.026272	-4.421913	0
142-9-36	the major challenge in developing such testing rules is due to the overlaps between the parents and their children nodes .	however , with extremely large feature dimensions , solving tgl remains a significant challenge due to its highly complicated regularizer .	0	5	2	0.86794037	-0.55834913	0
142-9-36	however , with extremely large feature dimensions , solving tgl remains a significant challenge due to its highly complicated regularizer .	by a novel hierarchical projection algorithm , mlfre is able to test the nodes independently from any of their ancestor nodes .	1	2	6	-5.950576	5.2254844	1
142-9-36	moreover , we can integrate mlfre -- that has a low computational cost -- with any existing solvers .	however , with extremely large feature dimensions , solving tgl remains a significant challenge due to its highly complicated regularizer .	0	7	2	5.3522234	-4.707201	0
142-9-36	experiments on both synthetic and real data sets demonstrate that the speedup gained by mlfre can be orders of magnitude .	however , with extremely large feature dimensions , solving tgl remains a significant challenge due to its highly complicated regularizer .	0	8	2	5.6511946	-5.087094	0
142-9-36	in this paper , we propose a novel multilayer feature reduction method ( mlfre ) to quickly identify the inactive nodes ( the groups of features with zero coefficients in the solution ) hierarchically in a top-down fashion , which are guaranteed to be irrelevant to the response .	thus , we can remove the detected nodes from the optimization without sacrificing accuracy .	1	3	4	-5.928258	5.1397657	1
142-9-36	in this paper , we propose a novel multilayer feature reduction method ( mlfre ) to quickly identify the inactive nodes ( the groups of features with zero coefficients in the solution ) hierarchically in a top-down fashion , which are guaranteed to be irrelevant to the response .	the major challenge in developing such testing rules is due to the overlaps between the parents and their children nodes .	1	3	5	4.785465	-4.2375927	0
142-9-36	in this paper , we propose a novel multilayer feature reduction method ( mlfre ) to quickly identify the inactive nodes ( the groups of features with zero coefficients in the solution ) hierarchically in a top-down fashion , which are guaranteed to be irrelevant to the response .	by a novel hierarchical projection algorithm , mlfre is able to test the nodes independently from any of their ancestor nodes .	1	3	6	-5.9203196	5.2057285	1
142-9-36	in this paper , we propose a novel multilayer feature reduction method ( mlfre ) to quickly identify the inactive nodes ( the groups of features with zero coefficients in the solution ) hierarchically in a top-down fashion , which are guaranteed to be irrelevant to the response .	moreover , we can integrate mlfre -- that has a low computational cost -- with any existing solvers .	1	3	7	-5.9917364	5.1714525	1
142-9-36	experiments on both synthetic and real data sets demonstrate that the speedup gained by mlfre can be orders of magnitude .	in this paper , we propose a novel multilayer feature reduction method ( mlfre ) to quickly identify the inactive nodes ( the groups of features with zero coefficients in the solution ) hierarchically in a top-down fashion , which are guaranteed to be irrelevant to the response .	0	8	3	5.689089	-5.0555496	0
142-9-36	thus , we can remove the detected nodes from the optimization without sacrificing accuracy .	the major challenge in developing such testing rules is due to the overlaps between the parents and their children nodes .	1	4	5	5.2412252	-4.5721054	0
142-9-36	by a novel hierarchical projection algorithm , mlfre is able to test the nodes independently from any of their ancestor nodes .	thus , we can remove the detected nodes from the optimization without sacrificing accuracy .	0	6	4	1.2970698	-1.1133747	0
142-9-36	moreover , we can integrate mlfre -- that has a low computational cost -- with any existing solvers .	thus , we can remove the detected nodes from the optimization without sacrificing accuracy .	0	7	4	2.9988866	-2.8390281	0
142-9-36	experiments on both synthetic and real data sets demonstrate that the speedup gained by mlfre can be orders of magnitude .	thus , we can remove the detected nodes from the optimization without sacrificing accuracy .	0	8	4	4.7091827	-4.286956	0
142-9-36	the major challenge in developing such testing rules is due to the overlaps between the parents and their children nodes .	by a novel hierarchical projection algorithm , mlfre is able to test the nodes independently from any of their ancestor nodes .	1	5	6	-5.9637356	5.222493	1
142-9-36	the major challenge in developing such testing rules is due to the overlaps between the parents and their children nodes .	moreover , we can integrate mlfre -- that has a low computational cost -- with any existing solvers .	1	5	7	-5.9927278	5.1800756	1
142-9-36	experiments on both synthetic and real data sets demonstrate that the speedup gained by mlfre can be orders of magnitude .	the major challenge in developing such testing rules is due to the overlaps between the parents and their children nodes .	0	8	5	5.567529	-4.933888	0
142-9-36	moreover , we can integrate mlfre -- that has a low computational cost -- with any existing solvers .	by a novel hierarchical projection algorithm , mlfre is able to test the nodes independently from any of their ancestor nodes .	0	7	6	3.7261243	-3.4004126	0
142-9-36	by a novel hierarchical projection algorithm , mlfre is able to test the nodes independently from any of their ancestor nodes .	experiments on both synthetic and real data sets demonstrate that the speedup gained by mlfre can be orders of magnitude .	1	6	8	-5.644818	5.1119146	1
142-9-36	moreover , we can integrate mlfre -- that has a low computational cost -- with any existing solvers .	experiments on both synthetic and real data sets demonstrate that the speedup gained by mlfre can be orders of magnitude .	1	7	8	-3.279668	3.10886	1
143-8-28	max-product belief propagation ( bp ) is a popular message-passing algorithm for computing a maximum-a-posteriori ( map ) assignment over a distribution represented by a graphical model ( gm ) .	[CLS] it has been shown that bp can solve a number of combinatorial optimization problems including minimum weight matching, shortest path, network flow and vertex cover under the following common assumption : the respective linear programming ( lp ) relaxation is tight, i. e., no integrality gap is present	1	0	1	-5.9299026	5.11536	1
143-8-28	however , when lp shows an integrality gap , no model has been known which can be solved systematically via sequential applications of bp .	max-product belief propagation ( bp ) is a popular message-passing algorithm for computing a maximum-a-posteriori ( map ) assignment over a distribution represented by a graphical model ( gm ) .	0	2	0	5.6640453	-5.011055	0
143-8-28	in this paper , we develop the first such algorithm , coined blossom-bp , for solving the minimum weight matching problem over arbitrary graphs .	max-product belief propagation ( bp ) is a popular message-passing algorithm for computing a maximum-a-posteriori ( map ) assignment over a distribution represented by a graphical model ( gm ) .	0	3	0	5.6007776	-5.023096	0
143-8-28	each step of the sequential algorithm requires applying bp over a modified graph constructed by contractions and expansions of blossoms , i.e. , odd sets of vertices .	max-product belief propagation ( bp ) is a popular message-passing algorithm for computing a maximum-a-posteriori ( map ) assignment over a distribution represented by a graphical model ( gm ) .	0	4	0	5.659517	-5.072642	0
143-8-28	max-product belief propagation ( bp ) is a popular message-passing algorithm for computing a maximum-a-posteriori ( map ) assignment over a distribution represented by a graphical model ( gm ) .	our scheme guarantees termination in o ( n2 ) of bp runs , where n is the number of vertices in the original graph .	1	0	5	-5.867982	5.123187	1
143-8-28	in essence , the blossom-bp offers a distributed version of the celebrated edmonds ' blossom algorithm by jumping at once over many sub-steps with a single bp .	max-product belief propagation ( bp ) is a popular message-passing algorithm for computing a maximum-a-posteriori ( map ) assignment over a distribution represented by a graphical model ( gm ) .	0	6	0	5.578206	-5.0112576	0
143-8-28	moreover , our result provides an interpretation of the edmonds ' algorithm as a sequence of lps .	max-product belief propagation ( bp ) is a popular message-passing algorithm for computing a maximum-a-posteriori ( map ) assignment over a distribution represented by a graphical model ( gm ) .	0	7	0	5.549139	-4.9673624	0
143-8-28	however , when lp shows an integrality gap , no model has been known which can be solved systematically via sequential applications of bp .	it has been shown that bp can solve a number of combinatorial optimization problems including minimum weight matching , shortest path , network flow and vertex cover under the following common assumption : the respective linear programming ( lp ) relaxation is tight , i.e. , no integrality gap is present .	0	2	1	5.4923573	-4.8761	0
143-8-28	it has been shown that bp can solve a number of combinatorial optimization problems including minimum weight matching , shortest path , network flow and vertex cover under the following common assumption : the respective linear programming ( lp ) relaxation is tight , i.e. , no integrality gap is present .	in this paper , we develop the first such algorithm , coined blossom-bp , for solving the minimum weight matching problem over arbitrary graphs .	1	1	3	-5.63214	5.1585207	1
143-8-28	each step of the sequential algorithm requires applying bp over a modified graph constructed by contractions and expansions of blossoms , i.e. , odd sets of vertices .	it has been shown that bp can solve a number of combinatorial optimization problems including minimum weight matching , shortest path , network flow and vertex cover under the following common assumption : the respective linear programming ( lp ) relaxation is tight , i.e. , no integrality gap is present .	0	4	1	3.0547562	-2.6566257	0
143-8-28	it has been shown that bp can solve a number of combinatorial optimization problems including minimum weight matching , shortest path , network flow and vertex cover under the following common assumption : the respective linear programming ( lp ) relaxation is tight , i.e. , no integrality gap is present .	our scheme guarantees termination in o ( n2 ) of bp runs , where n is the number of vertices in the original graph .	1	1	5	-5.929516	5.2262716	1
143-8-28	it has been shown that bp can solve a number of combinatorial optimization problems including minimum weight matching , shortest path , network flow and vertex cover under the following common assumption : the respective linear programming ( lp ) relaxation is tight , i.e. , no integrality gap is present .	in essence , the blossom-bp offers a distributed version of the celebrated edmonds ' blossom algorithm by jumping at once over many sub-steps with a single bp .	1	1	6	-5.8564434	5.2185936	1
143-8-28	moreover , our result provides an interpretation of the edmonds ' algorithm as a sequence of lps .	it has been shown that bp can solve a number of combinatorial optimization problems including minimum weight matching , shortest path , network flow and vertex cover under the following common assumption : the respective linear programming ( lp ) relaxation is tight , i.e. , no integrality gap is present .	0	7	1	5.552314	-4.943709	0
143-8-28	in this paper , we develop the first such algorithm , coined blossom-bp , for solving the minimum weight matching problem over arbitrary graphs .	however , when lp shows an integrality gap , no model has been known which can be solved systematically via sequential applications of bp .	0	3	2	4.8615294	-4.318365	0
143-8-28	each step of the sequential algorithm requires applying bp over a modified graph constructed by contractions and expansions of blossoms , i.e. , odd sets of vertices .	however , when lp shows an integrality gap , no model has been known which can be solved systematically via sequential applications of bp .	0	4	2	-0.5450373	0.8747474	1
143-8-28	our scheme guarantees termination in o ( n2 ) of bp runs , where n is the number of vertices in the original graph .	however , when lp shows an integrality gap , no model has been known which can be solved systematically via sequential applications of bp .	0	5	2	5.0355463	-4.514119	0
143-8-28	in essence , the blossom-bp offers a distributed version of the celebrated edmonds ' blossom algorithm by jumping at once over many sub-steps with a single bp .	however , when lp shows an integrality gap , no model has been known which can be solved systematically via sequential applications of bp .	0	6	2	2.5899544	-2.5173473	0
143-8-28	however , when lp shows an integrality gap , no model has been known which can be solved systematically via sequential applications of bp .	moreover , our result provides an interpretation of the edmonds ' algorithm as a sequence of lps .	1	2	7	-5.943165	5.21915	1
143-8-28	in this paper , we develop the first such algorithm , coined blossom-bp , for solving the minimum weight matching problem over arbitrary graphs .	each step of the sequential algorithm requires applying bp over a modified graph constructed by contractions and expansions of blossoms , i.e. , odd sets of vertices .	1	3	4	-2.5654745	2.5047717	1
143-8-28	our scheme guarantees termination in o ( n2 ) of bp runs , where n is the number of vertices in the original graph .	in this paper , we develop the first such algorithm , coined blossom-bp , for solving the minimum weight matching problem over arbitrary graphs .	0	5	3	5.167017	-4.5815077	0
143-8-28	in this paper , we develop the first such algorithm , coined blossom-bp , for solving the minimum weight matching problem over arbitrary graphs .	in essence , the blossom-bp offers a distributed version of the celebrated edmonds ' blossom algorithm by jumping at once over many sub-steps with a single bp .	1	3	6	-5.871316	5.1635227	1
143-8-28	moreover , our result provides an interpretation of the edmonds ' algorithm as a sequence of lps .	in this paper , we develop the first such algorithm , coined blossom-bp , for solving the minimum weight matching problem over arbitrary graphs .	0	7	3	5.151473	-4.4859734	0
143-8-28	each step of the sequential algorithm requires applying bp over a modified graph constructed by contractions and expansions of blossoms , i.e. , odd sets of vertices .	our scheme guarantees termination in o ( n2 ) of bp runs , where n is the number of vertices in the original graph .	1	4	5	-5.842247	5.2043376	1
143-8-28	each step of the sequential algorithm requires applying bp over a modified graph constructed by contractions and expansions of blossoms , i.e. , odd sets of vertices .	in essence , the blossom-bp offers a distributed version of the celebrated edmonds ' blossom algorithm by jumping at once over many sub-steps with a single bp .	1	4	6	-5.702738	5.1135902	1
143-8-28	moreover , our result provides an interpretation of the edmonds ' algorithm as a sequence of lps .	each step of the sequential algorithm requires applying bp over a modified graph constructed by contractions and expansions of blossoms , i.e. , odd sets of vertices .	0	7	4	4.1368766	-3.7794256	0
143-8-28	in essence , the blossom-bp offers a distributed version of the celebrated edmonds ' blossom algorithm by jumping at once over many sub-steps with a single bp .	our scheme guarantees termination in o ( n2 ) of bp runs , where n is the number of vertices in the original graph .	0	6	5	-2.8965073	2.888086	1
143-8-28	our scheme guarantees termination in o ( n2 ) of bp runs , where n is the number of vertices in the original graph .	moreover , our result provides an interpretation of the edmonds ' algorithm as a sequence of lps .	1	5	7	-4.4589453	4.18577	1
143-8-28	moreover , our result provides an interpretation of the edmonds ' algorithm as a sequence of lps .	in essence , the blossom-bp offers a distributed version of the celebrated edmonds ' blossom algorithm by jumping at once over many sub-steps with a single bp .	0	7	6	2.481784	-2.2260394	0
144-5-10	however , the problem of finding an optimal trade-off between exploration and exploitation ( otherwise known as the bandit problem ) , a crucial problem in collaborative filtering from cold-start , has not been previously addressed .	matrix factorization ( mf ) collaborative filtering is an effective and widely used method in recommendation systems .	0	1	0	5.3939643	-4.830269	0
144-5-10	matrix factorization ( mf ) collaborative filtering is an effective and widely used method in recommendation systems .	in this paper , we present a novel algorithm for online mf recommendation that automatically combines finding the most relevant items with exploring new or less-recommended items .	1	0	2	-5.8549566	5.107282	1
144-5-10	matrix factorization ( mf ) collaborative filtering is an effective and widely used method in recommendation systems .	our approach , called particle thompson sampling for mf ( pts ) , is based on the general thompson sampling framework , but augmented with a novel efficient online bayesian probabilistic matrix factorization method based on the rao-blackwellized particle filter .	1	0	3	-5.80237	5.124691	1
144-5-10	matrix factorization ( mf ) collaborative filtering is an effective and widely used method in recommendation systems .	extensive experiments in collaborative filtering using several real-world datasets demonstrate that pts significantly outperforms the current state-of-the-arts .	1	0	4	-5.9396205	5.174485	1
144-5-10	in this paper , we present a novel algorithm for online mf recommendation that automatically combines finding the most relevant items with exploring new or less-recommended items .	however , the problem of finding an optimal trade-off between exploration and exploitation ( otherwise known as the bandit problem ) , a crucial problem in collaborative filtering from cold-start , has not been previously addressed .	0	2	1	5.2552123	-4.596425	0
144-5-10	our approach , called particle thompson sampling for mf ( pts ) , is based on the general thompson sampling framework , but augmented with a novel efficient online bayesian probabilistic matrix factorization method based on the rao-blackwellized particle filter .	however , the problem of finding an optimal trade-off between exploration and exploitation ( otherwise known as the bandit problem ) , a crucial problem in collaborative filtering from cold-start , has not been previously addressed .	0	3	1	5.289357	-4.6819534	0
144-5-10	extensive experiments in collaborative filtering using several real-world datasets demonstrate that pts significantly outperforms the current state-of-the-arts .	however , the problem of finding an optimal trade-off between exploration and exploitation ( otherwise known as the bandit problem ) , a crucial problem in collaborative filtering from cold-start , has not been previously addressed .	0	4	1	5.5747976	-4.945175	0
144-5-10	our approach , called particle thompson sampling for mf ( pts ) , is based on the general thompson sampling framework , but augmented with a novel efficient online bayesian probabilistic matrix factorization method based on the rao-blackwellized particle filter .	in this paper , we present a novel algorithm for online mf recommendation that automatically combines finding the most relevant items with exploring new or less-recommended items .	0	3	2	3.9467402	-3.613371	0
144-5-10	extensive experiments in collaborative filtering using several real-world datasets demonstrate that pts significantly outperforms the current state-of-the-arts .	in this paper , we present a novel algorithm for online mf recommendation that automatically combines finding the most relevant items with exploring new or less-recommended items .	0	4	2	5.3337255	-4.754973	0
144-5-10	extensive experiments in collaborative filtering using several real-world datasets demonstrate that pts significantly outperforms the current state-of-the-arts .	our approach , called particle thompson sampling for mf ( pts ) , is based on the general thompson sampling framework , but augmented with a novel efficient online bayesian probabilistic matrix factorization method based on the rao-blackwellized particle filter .	0	4	3	5.507512	-4.873245	0
145-8-28	it was recently shown that for convex problems the classical cyclic bcgd ( block coordinate gradient descent ) achieves an o ( 1/r ) complexity ( r is the number of passes of all blocks ) .	the iteration complexity of the block-coordinate descent ( bcd ) type algorithm has been under extensive investigation .	0	1	0	-4.7696424	4.4558587	1
145-8-28	the iteration complexity of the block-coordinate descent ( bcd ) type algorithm has been under extensive investigation .	however , such bounds are at least linearly depend on k ( the number of variable blocks ) , and are at least k times worse than those of the gradient descent ( gd ) and proximal gradient ( pg ) methods .	1	0	2	-5.7284236	5.2041545	1
145-8-28	in this paper , we close such theoretical performance gap between cyclic bcd and gd/pg .	the iteration complexity of the block-coordinate descent ( bcd ) type algorithm has been under extensive investigation .	0	3	0	5.64779	-5.0294504	0
145-8-28	first we show that for a family of quadratic nonsmooth problems , the complexity bounds for cyclic block coordinate proximal gradient ( bcpg ) , a popular variant of bcd , can match those of the gd/pg in terms of dependency on k ( up to a log2 ( k ) factor ) .	the iteration complexity of the block-coordinate descent ( bcd ) type algorithm has been under extensive investigation .	0	4	0	5.6227107	-5.0139484	0
145-8-28	second , we establish an improved complexity bound for coordinate gradient descent ( cgd ) for general convex problems which can match that of gd in certain scenarios .	the iteration complexity of the block-coordinate descent ( bcd ) type algorithm has been under extensive investigation .	0	5	0	5.3841696	-4.749257	0
145-8-28	the iteration complexity of the block-coordinate descent ( bcd ) type algorithm has been under extensive investigation .	our bounds are sharper than the known bounds as they are always at least k times worse than gd .	1	0	6	-5.984643	5.212709	1
145-8-28	the iteration complexity of the block-coordinate descent ( bcd ) type algorithm has been under extensive investigation .	our analyses do not depend on the update order of block variables inside each cycle , thus our results also apply to bcd methods with random permutation ( random sampling without replacement , another popular variant ) .	1	0	7	-5.937273	5.1136885	1
145-8-28	however , such bounds are at least linearly depend on k ( the number of variable blocks ) , and are at least k times worse than those of the gradient descent ( gd ) and proximal gradient ( pg ) methods .	it was recently shown that for convex problems the classical cyclic bcgd ( block coordinate gradient descent ) achieves an o ( 1/r ) complexity ( r is the number of passes of all blocks ) .	0	2	1	5.0441227	-4.4632344	0
145-8-28	in this paper , we close such theoretical performance gap between cyclic bcd and gd/pg .	it was recently shown that for convex problems the classical cyclic bcgd ( block coordinate gradient descent ) achieves an o ( 1/r ) complexity ( r is the number of passes of all blocks ) .	0	3	1	5.194732	-4.6049824	0
145-8-28	it was recently shown that for convex problems the classical cyclic bcgd ( block coordinate gradient descent ) achieves an o ( 1/r ) complexity ( r is the number of passes of all blocks ) .	[CLS] first we show that for a family of quadratic nonsmooth problems, the complexity bounds for cyclic block coordinate proximal gradient ( bcpg ), a popular variant of bcd, can match those of the gd / pg in terms of dependency on k ( up	1	1	4	-6.011764	5.167318	1
145-8-28	second , we establish an improved complexity bound for coordinate gradient descent ( cgd ) for general convex problems which can match that of gd in certain scenarios .	it was recently shown that for convex problems the classical cyclic bcgd ( block coordinate gradient descent ) achieves an o ( 1/r ) complexity ( r is the number of passes of all blocks ) .	0	5	1	5.260337	-4.6479797	0
145-8-28	it was recently shown that for convex problems the classical cyclic bcgd ( block coordinate gradient descent ) achieves an o ( 1/r ) complexity ( r is the number of passes of all blocks ) .	our bounds are sharper than the known bounds as they are always at least k times worse than gd .	1	1	6	-6.0134335	5.2017436	1
145-8-28	it was recently shown that for convex problems the classical cyclic bcgd ( block coordinate gradient descent ) achieves an o ( 1/r ) complexity ( r is the number of passes of all blocks ) .	our analyses do not depend on the update order of block variables inside each cycle , thus our results also apply to bcd methods with random permutation ( random sampling without replacement , another popular variant ) .	1	1	7	-6.021938	5.1294117	1
145-8-28	however , such bounds are at least linearly depend on k ( the number of variable blocks ) , and are at least k times worse than those of the gradient descent ( gd ) and proximal gradient ( pg ) methods .	in this paper , we close such theoretical performance gap between cyclic bcd and gd/pg .	1	2	3	-5.9389963	5.0931764	1
145-8-28	[CLS] first we show that for a family of quadratic nonsmooth problems, the complexity bounds for cyclic block coordinate proximal gradient ( bcpg ), a popular variant of bcd, can match those of the gd / pg in terms of	however , such bounds are at least linearly depend on k ( the number of variable blocks ) , and are at least k times worse than those of the gradient descent ( gd ) and proximal gradient ( pg ) methods .	0	4	2	4.290633	-3.8651438	0
145-8-28	however , such bounds are at least linearly depend on k ( the number of variable blocks ) , and are at least k times worse than those of the gradient descent ( gd ) and proximal gradient ( pg ) methods .	second , we establish an improved complexity bound for coordinate gradient descent ( cgd ) for general convex problems which can match that of gd in certain scenarios .	1	2	5	-5.938911	5.1322703	1
145-8-28	our bounds are sharper than the known bounds as they are always at least k times worse than gd .	however , such bounds are at least linearly depend on k ( the number of variable blocks ) , and are at least k times worse than those of the gradient descent ( gd ) and proximal gradient ( pg ) methods .	0	6	2	4.3842826	-3.9283304	0
145-8-28	our analyses do not depend on the update order of block variables inside each cycle , thus our results also apply to bcd methods with random permutation ( random sampling without replacement , another popular variant ) .	however , such bounds are at least linearly depend on k ( the number of variable blocks ) , and are at least k times worse than those of the gradient descent ( gd ) and proximal gradient ( pg ) methods .	0	7	2	4.892888	-4.3490796	0
145-8-28	first we show that for a family of quadratic nonsmooth problems , the complexity bounds for cyclic block coordinate proximal gradient ( bcpg ) , a popular variant of bcd , can match those of the gd/pg in terms of dependency on k ( up to a log2 ( k ) factor ) .	in this paper , we close such theoretical performance gap between cyclic bcd and gd/pg .	0	4	3	-2.2805495	2.4215317	1
145-8-28	second , we establish an improved complexity bound for coordinate gradient descent ( cgd ) for general convex problems which can match that of gd in certain scenarios .	in this paper , we close such theoretical performance gap between cyclic bcd and gd/pg .	0	5	3	4.786166	-4.2529645	0
145-8-28	in this paper , we close such theoretical performance gap between cyclic bcd and gd/pg .	our bounds are sharper than the known bounds as they are always at least k times worse than gd .	1	3	6	-5.6605873	5.061199	1
145-8-28	in this paper , we close such theoretical performance gap between cyclic bcd and gd/pg .	our analyses do not depend on the update order of block variables inside each cycle , thus our results also apply to bcd methods with random permutation ( random sampling without replacement , another popular variant ) .	1	3	7	-5.9396586	5.151018	1
145-8-28	second , we establish an improved complexity bound for coordinate gradient descent ( cgd ) for general convex problems which can match that of gd in certain scenarios .	first we show that for a family of quadratic nonsmooth problems , the complexity bounds for cyclic block coordinate proximal gradient ( bcpg ) , a popular variant of bcd , can match those of the gd/pg in terms of dependency on k ( up to a log2 ( k ) factor ) .	0	5	4	5.2576933	-4.592651	0
145-8-28	our bounds are sharper than the known bounds as they are always at least k times worse than gd .	first we show that for a family of quadratic nonsmooth problems , the complexity bounds for cyclic block coordinate proximal gradient ( bcpg ) , a popular variant of bcd , can match those of the gd/pg in terms of dependency on k ( up to a log2 ( k ) factor ) .	0	6	4	5.0690427	-4.3755164	0
145-8-28	[CLS] first we show that for a family of quadratic nonsmooth problems, the complexity bounds for cyclic block coordinate proximal gradient ( bcpg ), a popular variant of bcd, can match those of the gd / pg in terms of dependency on k ( up to	our analyses do not depend on the update order of block variables inside each cycle , thus our results also apply to bcd methods with random permutation ( random sampling without replacement , another popular variant ) .	1	4	7	-5.9467816	5.0787315	1
145-8-28	our bounds are sharper than the known bounds as they are always at least k times worse than gd .	second , we establish an improved complexity bound for coordinate gradient descent ( cgd ) for general convex problems which can match that of gd in certain scenarios .	0	6	5	-2.3404932	2.474607	1
145-8-28	second , we establish an improved complexity bound for coordinate gradient descent ( cgd ) for general convex problems which can match that of gd in certain scenarios .	our analyses do not depend on the update order of block variables inside each cycle , thus our results also apply to bcd methods with random permutation ( random sampling without replacement , another popular variant ) .	1	5	7	-4.6624618	4.292693	1
145-8-28	our analyses do not depend on the update order of block variables inside each cycle , thus our results also apply to bcd methods with random permutation ( random sampling without replacement , another popular variant ) .	our bounds are sharper than the known bounds as they are always at least k times worse than gd .	0	7	6	3.8591573	-3.6015215	0
146-7-21	in this work , we extend symmetry breaking to the problem of model finding in weighted and unweighted relational theories , a class of problems that includes map inference in markov logic and similar statistical-relational languages .	symmetry breaking is a technique for speeding up propositional satisfiability testing by adding constraints to the theory that restrict the search space while preserving satisfiability .	0	1	0	4.985244	-4.3886547	0
146-7-21	symmetry breaking is a technique for speeding up propositional satisfiability testing by adding constraints to the theory that restrict the search space while preserving satisfiability .	we introduce term symmetries , which are induced by an evidence set and extend to symmetries over a relational theory .	1	0	2	-5.8889112	5.2645454	1
146-7-21	symmetry breaking is a technique for speeding up propositional satisfiability testing by adding constraints to the theory that restrict the search space while preserving satisfiability .	we provide the important special case of term equivalent symmetries , showing that such symmetries can be found in low-degree polynomial time .	1	0	3	-5.8827868	5.240344	1
146-7-21	symmetry breaking is a technique for speeding up propositional satisfiability testing by adding constraints to the theory that restrict the search space while preserving satisfiability .	we show how to break an exponential number of these symmetries with added constraints whose number is linear in the size of the domain .	1	0	4	-5.9399595	5.179734	1
146-7-21	symmetry breaking is a technique for speeding up propositional satisfiability testing by adding constraints to the theory that restrict the search space while preserving satisfiability .	we demonstrate the effectiveness of these techniques through experiments in two relational domains .	1	0	5	-5.9772577	5.1732492	1
146-7-21	we also discuss the connections between relational symmetry breaking and work on lifted inference in statistical-relational reasoning .	symmetry breaking is a technique for speeding up propositional satisfiability testing by adding constraints to the theory that restrict the search space while preserving satisfiability .	0	6	0	5.468121	-4.8813252	0
146-7-21	in this work , we extend symmetry breaking to the problem of model finding in weighted and unweighted relational theories , a class of problems that includes map inference in markov logic and similar statistical-relational languages .	we introduce term symmetries , which are induced by an evidence set and extend to symmetries over a relational theory .	1	1	2	-5.078796	4.7454777	1
146-7-21	we provide the important special case of term equivalent symmetries , showing that such symmetries can be found in low-degree polynomial time .	in this work , we extend symmetry breaking to the problem of model finding in weighted and unweighted relational theories , a class of problems that includes map inference in markov logic and similar statistical-relational languages .	0	3	1	4.720604	-4.206838	0
146-7-21	we show how to break an exponential number of these symmetries with added constraints whose number is linear in the size of the domain .	in this work , we extend symmetry breaking to the problem of model finding in weighted and unweighted relational theories , a class of problems that includes map inference in markov logic and similar statistical-relational languages .	0	4	1	1.9780574	-1.757988	0
146-7-21	in this work , we extend symmetry breaking to the problem of model finding in weighted and unweighted relational theories , a class of problems that includes map inference in markov logic and similar statistical-relational languages .	we demonstrate the effectiveness of these techniques through experiments in two relational domains .	1	1	5	-5.968102	5.257471	1
146-7-21	in this work , we extend symmetry breaking to the problem of model finding in weighted and unweighted relational theories , a class of problems that includes map inference in markov logic and similar statistical-relational languages .	we also discuss the connections between relational symmetry breaking and work on lifted inference in statistical-relational reasoning .	1	1	6	-5.7952557	5.102604	1
146-7-21	we introduce term symmetries , which are induced by an evidence set and extend to symmetries over a relational theory .	we provide the important special case of term equivalent symmetries , showing that such symmetries can be found in low-degree polynomial time .	1	2	3	-3.5017483	3.3347654	1
146-7-21	we show how to break an exponential number of these symmetries with added constraints whose number is linear in the size of the domain .	we introduce term symmetries , which are induced by an evidence set and extend to symmetries over a relational theory .	0	4	2	0.8229245	-0.56935567	0
146-7-21	we introduce term symmetries , which are induced by an evidence set and extend to symmetries over a relational theory .	we demonstrate the effectiveness of these techniques through experiments in two relational domains .	1	2	5	-5.9473357	5.162097	1
146-7-21	we also discuss the connections between relational symmetry breaking and work on lifted inference in statistical-relational reasoning .	we introduce term symmetries , which are induced by an evidence set and extend to symmetries over a relational theory .	0	6	2	5.182541	-4.572977	0
146-7-21	we provide the important special case of term equivalent symmetries , showing that such symmetries can be found in low-degree polynomial time .	we show how to break an exponential number of these symmetries with added constraints whose number is linear in the size of the domain .	1	3	4	1.2691948	-0.97693753	0
146-7-21	we provide the important special case of term equivalent symmetries , showing that such symmetries can be found in low-degree polynomial time .	we demonstrate the effectiveness of these techniques through experiments in two relational domains .	1	3	5	-5.7086825	5.0842957	1
146-7-21	we provide the important special case of term equivalent symmetries , showing that such symmetries can be found in low-degree polynomial time .	we also discuss the connections between relational symmetry breaking and work on lifted inference in statistical-relational reasoning .	1	3	6	-5.895398	5.0398884	1
146-7-21	we show how to break an exponential number of these symmetries with added constraints whose number is linear in the size of the domain .	we demonstrate the effectiveness of these techniques through experiments in two relational domains .	1	4	5	-5.8611526	5.1761885	1
146-7-21	we show how to break an exponential number of these symmetries with added constraints whose number is linear in the size of the domain .	we also discuss the connections between relational symmetry breaking and work on lifted inference in statistical-relational reasoning .	1	4	6	-5.8541684	4.9159966	1
146-7-21	we demonstrate the effectiveness of these techniques through experiments in two relational domains .	we also discuss the connections between relational symmetry breaking and work on lifted inference in statistical-relational reasoning .	1	5	6	-5.5705776	5.0121803	1
147-4-6	we develop a framework for performing statistical inference on biclusters found by score-based algorithms .	biclustering ( also known as submatrix localization ) is a problem of high practical relevance in exploratory analysis of high-dimensional data .	0	1	0	5.565918	-4.9440846	0
147-4-6	since the bicluster was selected in a data dependent manner by a biclustering or localization algorithm , this is a form of selective inference .	biclustering ( also known as submatrix localization ) is a problem of high practical relevance in exploratory analysis of high-dimensional data .	0	2	0	5.532363	-4.965995	0
147-4-6	our framework gives exact ( non-asymptotic ) confidence intervals and p-values for the significance of the selected biclusters .	biclustering ( also known as submatrix localization ) is a problem of high practical relevance in exploratory analysis of high-dimensional data .	0	3	0	5.6393924	-5.0475273	0
147-4-6	since the bicluster was selected in a data dependent manner by a biclustering or localization algorithm , this is a form of selective inference .	we develop a framework for performing statistical inference on biclusters found by score-based algorithms .	0	2	1	2.7009377	-2.39261	0
147-4-6	we develop a framework for performing statistical inference on biclusters found by score-based algorithms .	our framework gives exact ( non-asymptotic ) confidence intervals and p-values for the significance of the selected biclusters .	1	1	3	-5.923396	5.2371264	1
147-4-6	since the bicluster was selected in a data dependent manner by a biclustering or localization algorithm , this is a form of selective inference .	our framework gives exact ( non-asymptotic ) confidence intervals and p-values for the significance of the selected biclusters .	1	2	3	-3.5696597	3.410242	1
148-6-15	the framework makes it possible to tradeoff the discriminative value of learned features against the generalization error of the learning algorithm .	this paper proposes a framework for learning features that are robust to data variation , which is particularly important when only a limited number of training samples are available .	0	1	0	5.610441	-4.950717	0
148-6-15	robustness is achieved by encouraging the transform that maps data to features to be a local isometry .	this paper proposes a framework for learning features that are robust to data variation , which is particularly important when only a limited number of training samples are available .	0	2	0	5.303282	-4.693837	0
148-6-15	this paper proposes a framework for learning features that are robust to data variation , which is particularly important when only a limited number of training samples are available .	this geometric property is shown to improve ( k , ) -robustness , thereby providing theoretical justification for reductions in generalization error observed in experiments .	1	0	3	-5.94692	5.208425	1
148-6-15	the proposed optimization framework is used to train standard learning algorithms such as deep neural networks .	this paper proposes a framework for learning features that are robust to data variation , which is particularly important when only a limited number of training samples are available .	0	4	0	5.6143847	-4.9805155	0
148-6-15	this paper proposes a framework for learning features that are robust to data variation , which is particularly important when only a limited number of training samples are available .	experimental results obtained on benchmark datasets , such as labeled faces in the wild , demonstrate the value of being able to balance discrimination and robustness .	1	0	5	-5.9559727	5.2134027	1
148-6-15	the framework makes it possible to tradeoff the discriminative value of learned features against the generalization error of the learning algorithm .	robustness is achieved by encouraging the transform that maps data to features to be a local isometry .	1	1	2	-3.1703072	3.138475	1
148-6-15	the framework makes it possible to tradeoff the discriminative value of learned features against the generalization error of the learning algorithm .	this geometric property is shown to improve ( k , ) -robustness , thereby providing theoretical justification for reductions in generalization error observed in experiments .	1	1	3	-5.5868444	5.0700693	1
148-6-15	the framework makes it possible to tradeoff the discriminative value of learned features against the generalization error of the learning algorithm .	the proposed optimization framework is used to train standard learning algorithms such as deep neural networks .	1	1	4	-0.71438426	0.9789227	1
148-6-15	experimental results obtained on benchmark datasets , such as labeled faces in the wild , demonstrate the value of being able to balance discrimination and robustness .	the framework makes it possible to tradeoff the discriminative value of learned features against the generalization error of the learning algorithm .	0	5	1	5.131858	-4.582299	0
148-6-15	this geometric property is shown to improve ( k , ) -robustness , thereby providing theoretical justification for reductions in generalization error observed in experiments .	robustness is achieved by encouraging the transform that maps data to features to be a local isometry .	0	3	2	4.0895205	-3.8268735	0
148-6-15	the proposed optimization framework is used to train standard learning algorithms such as deep neural networks .	robustness is achieved by encouraging the transform that maps data to features to be a local isometry .	0	4	2	-0.81091654	0.9783595	1
148-6-15	robustness is achieved by encouraging the transform that maps data to features to be a local isometry .	experimental results obtained on benchmark datasets , such as labeled faces in the wild , demonstrate the value of being able to balance discrimination and robustness .	1	2	5	-5.763686	5.094917	1
148-6-15	this geometric property is shown to improve ( k , ) -robustness , thereby providing theoretical justification for reductions in generalization error observed in experiments .	the proposed optimization framework is used to train standard learning algorithms such as deep neural networks .	1	3	4	-0.18053415	0.39013717	1
148-6-15	experimental results obtained on benchmark datasets , such as labeled faces in the wild , demonstrate the value of being able to balance discrimination and robustness .	this geometric property is shown to improve ( k , ) -robustness , thereby providing theoretical justification for reductions in generalization error observed in experiments .	0	5	3	2.157243	-1.9211533	0
148-6-15	experimental results obtained on benchmark datasets , such as labeled faces in the wild , demonstrate the value of being able to balance discrimination and robustness .	the proposed optimization framework is used to train standard learning algorithms such as deep neural networks .	0	5	4	4.159155	-3.825965	0
149-7-21	one of the hallmarks of a bandit setting is the agent 's capacity to explore its environment through active intervention , which contrasts with the ability to collect passive data by estimating associational relationships between actions and payouts .	the multi-armed bandit problem constitutes an archetypal setting for sequential decision-making , permeating multiple domains including engineering , business , and medicine .	0	1	0	5.48066	-4.857079	0
149-7-21	the existence of unobserved confounders , namely unmeasured variables affecting both the action and the outcome variables , implies that these two data-collection modes will in general not coincide .	the multi-armed bandit problem constitutes an archetypal setting for sequential decision-making , permeating multiple domains including engineering , business , and medicine .	0	2	0	5.5792427	-4.9702597	0
149-7-21	in this paper , we show that formalizing this distinction has conceptual and algorithmic implications to the bandit setting .	the multi-armed bandit problem constitutes an archetypal setting for sequential decision-making , permeating multiple domains including engineering , business , and medicine .	0	3	0	5.6621175	-5.10131	0
149-7-21	the multi-armed bandit problem constitutes an archetypal setting for sequential decision-making , permeating multiple domains including engineering , business , and medicine .	the current generation of bandit algorithms implicitly try to maximize rewards based on estimation of the experimental distribution , which we show is not always the best strategy to pursue .	1	0	4	-5.931002	5.219076	1
149-7-21	the multi-armed bandit problem constitutes an archetypal setting for sequential decision-making , permeating multiple domains including engineering , business , and medicine .	indeed , to achieve low regret in certain realistic classes of bandit problems ( namely , in the face of unobserved confounders ) , both experimental and observational quantities are required by the rational agent .	1	0	5	-5.600383	5.036866	1
149-7-21	after this realization , we propose an optimization metric ( employing both experimental and observational distributions ) that bandit agents should pursue , and illustrate its benefits over traditional algorithms .	the multi-armed bandit problem constitutes an archetypal setting for sequential decision-making , permeating multiple domains including engineering , business , and medicine .	0	6	0	5.6456156	-5.0598764	0
149-7-21	the existence of unobserved confounders , namely unmeasured variables affecting both the action and the outcome variables , implies that these two data-collection modes will in general not coincide .	one of the hallmarks of a bandit setting is the agent 's capacity to explore its environment through active intervention , which contrasts with the ability to collect passive data by estimating associational relationships between actions and payouts .	0	2	1	4.876768	-4.333992	0
149-7-21	in this paper , we show that formalizing this distinction has conceptual and algorithmic implications to the bandit setting .	one of the hallmarks of a bandit setting is the agent 's capacity to explore its environment through active intervention , which contrasts with the ability to collect passive data by estimating associational relationships between actions and payouts .	0	3	1	3.1496394	-2.9268713	0
149-7-21	the current generation of bandit algorithms implicitly try to maximize rewards based on estimation of the experimental distribution , which we show is not always the best strategy to pursue .	one of the hallmarks of a bandit setting is the agent 's capacity to explore its environment through active intervention , which contrasts with the ability to collect passive data by estimating associational relationships between actions and payouts .	0	4	1	4.504078	-4.002986	0
149-7-21	one of the hallmarks of a bandit setting is the agent 's capacity to explore its environment through active intervention , which contrasts with the ability to collect passive data by estimating associational relationships between actions and payouts .	indeed , to achieve low regret in certain realistic classes of bandit problems ( namely , in the face of unobserved confounders ) , both experimental and observational quantities are required by the rational agent .	1	1	5	-4.1048417	3.7837899	1
149-7-21	one of the hallmarks of a bandit setting is the agent 's capacity to explore its environment through active intervention , which contrasts with the ability to collect passive data by estimating associational relationships between actions and payouts .	after this realization , we propose an optimization metric ( employing both experimental and observational distributions ) that bandit agents should pursue , and illustrate its benefits over traditional algorithms .	1	1	6	-5.958904	5.197919	1
149-7-21	in this paper , we show that formalizing this distinction has conceptual and algorithmic implications to the bandit setting .	the existence of unobserved confounders , namely unmeasured variables affecting both the action and the outcome variables , implies that these two data-collection modes will in general not coincide .	0	3	2	-1.8084853	1.9969808	1
149-7-21	the current generation of bandit algorithms implicitly try to maximize rewards based on estimation of the experimental distribution , which we show is not always the best strategy to pursue .	the existence of unobserved confounders , namely unmeasured variables affecting both the action and the outcome variables , implies that these two data-collection modes will in general not coincide .	0	4	2	-1.9552372	2.1173368	1
149-7-21	indeed , to achieve low regret in certain realistic classes of bandit problems ( namely , in the face of unobserved confounders ) , both experimental and observational quantities are required by the rational agent .	the existence of unobserved confounders , namely unmeasured variables affecting both the action and the outcome variables , implies that these two data-collection modes will in general not coincide .	0	5	2	-1.8046827	2.013107	1
149-7-21	the existence of unobserved confounders , namely unmeasured variables affecting both the action and the outcome variables , implies that these two data-collection modes will in general not coincide .	after this realization , we propose an optimization metric ( employing both experimental and observational distributions ) that bandit agents should pursue , and illustrate its benefits over traditional algorithms .	1	2	6	-5.6382675	5.138405	1
149-7-21	the current generation of bandit algorithms implicitly try to maximize rewards based on estimation of the experimental distribution , which we show is not always the best strategy to pursue .	in this paper , we show that formalizing this distinction has conceptual and algorithmic implications to the bandit setting .	0	4	3	4.006117	-3.54609	0
149-7-21	indeed , to achieve low regret in certain realistic classes of bandit problems ( namely , in the face of unobserved confounders ) , both experimental and observational quantities are required by the rational agent .	in this paper , we show that formalizing this distinction has conceptual and algorithmic implications to the bandit setting .	0	5	3	-1.807751	1.9825081	1
149-7-21	after this realization , we propose an optimization metric ( employing both experimental and observational distributions ) that bandit agents should pursue , and illustrate its benefits over traditional algorithms .	in this paper , we show that formalizing this distinction has conceptual and algorithmic implications to the bandit setting .	0	6	3	5.5729313	-4.923538	0
149-7-21	the current generation of bandit algorithms implicitly try to maximize rewards based on estimation of the experimental distribution , which we show is not always the best strategy to pursue .	indeed , to achieve low regret in certain realistic classes of bandit problems ( namely , in the face of unobserved confounders ) , both experimental and observational quantities are required by the rational agent .	1	4	5	0.056063708	0.15910932	1
149-7-21	the current generation of bandit algorithms implicitly try to maximize rewards based on estimation of the experimental distribution , which we show is not always the best strategy to pursue .	after this realization , we propose an optimization metric ( employing both experimental and observational distributions ) that bandit agents should pursue , and illustrate its benefits over traditional algorithms .	1	4	6	-5.917189	5.213048	1
149-7-21	indeed , to achieve low regret in certain realistic classes of bandit problems ( namely , in the face of unobserved confounders ) , both experimental and observational quantities are required by the rational agent .	after this realization , we propose an optimization metric ( employing both experimental and observational distributions ) that bandit agents should pursue , and illustrate its benefits over traditional algorithms .	1	5	6	-5.9183383	5.1494327	1
150-4-6	the algorithm uses the structure of the ensemble predictions on unlabeled data to yield significant performance improvements .	we present and empirically evaluate an efficient algorithm that learns to aggregate the predictions of an ensemble of binary classifiers .	0	1	0	5.5907507	-5.018209	0
150-4-6	we present and empirically evaluate an efficient algorithm that learns to aggregate the predictions of an ensemble of binary classifiers .	it does this without making assumptions on the structure or origin of the ensemble , without parameters , and as scalably as linear learning .	1	0	2	-5.685006	5.14955	1
150-4-6	we present and empirically evaluate an efficient algorithm that learns to aggregate the predictions of an ensemble of binary classifiers .	we empirically demonstrate these performance gains with random forests .	1	0	3	-6.013006	5.1673527	1
150-4-6	the algorithm uses the structure of the ensemble predictions on unlabeled data to yield significant performance improvements .	it does this without making assumptions on the structure or origin of the ensemble , without parameters , and as scalably as linear learning .	1	1	2	2.922318	-2.7143135	0
150-4-6	the algorithm uses the structure of the ensemble predictions on unlabeled data to yield significant performance improvements .	we empirically demonstrate these performance gains with random forests .	1	1	3	-4.294286	4.0410585	1
150-4-6	it does this without making assumptions on the structure or origin of the ensemble , without parameters , and as scalably as linear learning .	we empirically demonstrate these performance gains with random forests .	1	2	3	-5.749975	5.102674	1
151-4-6	[CLS] we consider a sequential learning problem with gaussian payoffs and side observations : after selecting an action i, the learner receives information about the payoff of every action j in the form of gaussian observations whose mean is the same as the mean payoff, but the variance depends on the pair ( i, j ) (	the setup allows a more refined information transfer from one action to another than previous partial monitoring setups , including the recently introduced graph-structured feedback case .	1	0	1	-5.947274	5.179316	1
151-4-6	[CLS] for the first time in the literature, we provide non - asymptotic problem - dependent lower bounds on the regret of any algorithm, which recover existing asymptotic problem - dependent lower bounds and finitetime minimax lower bounds available in	[CLS] we consider a sequential learning problem with gaussian payoffs and side observations : after selecting an action i, the learner receives information about the payoff of every action j in the form of gaussian observations whose mean is the same as	0	2	0	5.4554405	-4.841193	0
151-4-6	[CLS] we consider a sequential learning problem with gaussian payoffs and side observations : after selecting an action i, the learner receives information about the payoff of every action j in the form of gaussian observations whose mean is the same as the mean payoff, but the variance depends on the pair	we also provide algorithms that achieve the problem-dependent lower bound ( up to some universal constant factor ) or the minimax lower bounds ( up to logarithmic factors ) .	1	0	3	-5.94436	5.188058	1
151-4-6	the setup allows a more refined information transfer from one action to another than previous partial monitoring setups , including the recently introduced graph-structured feedback case .	for the first time in the literature , we provide non-asymptotic problem-dependent lower bounds on the regret of any algorithm , which recover existing asymptotic problem-dependent lower bounds and finitetime minimax lower bounds available in the literature .	1	1	2	3.082779	-2.8270729	0
151-4-6	we also provide algorithms that achieve the problem-dependent lower bound ( up to some universal constant factor ) or the minimax lower bounds ( up to logarithmic factors ) .	the setup allows a more refined information transfer from one action to another than previous partial monitoring setups , including the recently introduced graph-structured feedback case .	0	3	1	3.029121	-2.8777752	0
151-4-6	we also provide algorithms that achieve the problem-dependent lower bound ( up to some universal constant factor ) or the minimax lower bounds ( up to logarithmic factors ) .	for the first time in the literature , we provide non-asymptotic problem-dependent lower bounds on the regret of any algorithm , which recover existing asymptotic problem-dependent lower bounds and finitetime minimax lower bounds available in the literature .	0	3	2	4.6596413	-4.1369123	0
152-6-15	we design algorithms for fitting a high-dimensional statistical model to a large , sparse network without revealing sensitive information of individual members .	given a sparse input graph g , our algorithms output a node-differentially private nonparametric block model approximation .	1	0	1	-5.7419677	5.226226	1
152-6-15	we design algorithms for fitting a high-dimensional statistical model to a large , sparse network without revealing sensitive information of individual members .	by node-differentially private , we mean that our output hides the insertion or removal of a vertex and all its adjacent edges .	1	0	2	-2.844292	2.8747516	1
152-6-15	if g is an instance of the network obtained from a generative nonparametric model defined in terms of a graphon w , our model guarantees consistency : as the number of vertices tends to infinity , the output of our algorithm converges to w in an appropriate version of the l2 norm .	we design algorithms for fitting a high-dimensional statistical model to a large , sparse network without revealing sensitive information of individual members .	0	3	0	5.623518	-4.9739227	0
152-6-15	we design algorithms for fitting a high-dimensional statistical model to a large , sparse network without revealing sensitive information of individual members .	in particular , this means we can estimate the sizes of all multi-way cuts in g. our results hold as long as w is bounded , the average degree of g grows at least like the log of the number of vertices , and the number of blocks goes to infinity at an appropriate rate .	1	0	4	-5.9015293	5.2343364	1
152-6-15	we give explicit error bounds in terms of the parameters of the model ; in several settings , our bounds improve on or match known nonprivate results .	we design algorithms for fitting a high-dimensional statistical model to a large , sparse network without revealing sensitive information of individual members .	0	5	0	5.2712383	-4.5917826	0
152-6-15	by node-differentially private , we mean that our output hides the insertion or removal of a vertex and all its adjacent edges .	given a sparse input graph g , our algorithms output a node-differentially private nonparametric block model approximation .	0	2	1	3.1886554	-2.9090767	0
152-6-15	if g is an instance of the network obtained from a generative nonparametric model defined in terms of a graphon w , our model guarantees consistency : as the number of vertices tends to infinity , the output of our algorithm converges to w in an appropriate version of the l2 norm .	given a sparse input graph g , our algorithms output a node-differentially private nonparametric block model approximation .	0	3	1	4.806225	-4.26355	0
152-6-15	given a sparse input graph g , our algorithms output a node-differentially private nonparametric block model approximation .	in particular , this means we can estimate the sizes of all multi-way cuts in g. our results hold as long as w is bounded , the average degree of g grows at least like the log of the number of vertices , and the number of blocks goes to infinity at an appropriate rate .	1	1	4	-5.8971395	5.184297	1
152-6-15	we give explicit error bounds in terms of the parameters of the model ; in several settings , our bounds improve on or match known nonprivate results .	given a sparse input graph g , our algorithms output a node-differentially private nonparametric block model approximation .	0	5	1	4.972602	-4.3985634	0
152-6-15	if g is an instance of the network obtained from a generative nonparametric model defined in terms of a graphon w , our model guarantees consistency : as the number of vertices tends to infinity , the output of our algorithm converges to w in an appropriate version of the l2 norm .	by node-differentially private , we mean that our output hides the insertion or removal of a vertex and all its adjacent edges .	0	3	2	3.7918806	-3.4494927	0
152-6-15	by node-differentially private , we mean that our output hides the insertion or removal of a vertex and all its adjacent edges .	in particular , this means we can estimate the sizes of all multi-way cuts in g. our results hold as long as w is bounded , the average degree of g grows at least like the log of the number of vertices , and the number of blocks goes to infinity at an appropriate rate .	1	2	4	-5.646792	5.0469804	1
152-6-15	we give explicit error bounds in terms of the parameters of the model ; in several settings , our bounds improve on or match known nonprivate results .	by node-differentially private , we mean that our output hides the insertion or removal of a vertex and all its adjacent edges .	0	5	2	5.0521784	-4.4552794	0
152-6-15	[CLS] if g is an instance of the network obtained from a generative nonparametric model defined in terms of a graphon w, our model guarantees consistency : as the number of vertices tends to infinity, the output of our algorithm converges to w	[CLS] in particular, this means we can estimate the sizes of all multi - way cuts in g. our results hold as long as w is bounded, the average degree of g grows at least like the log of the number of vertices, and the number	1	3	4	-3.3683581	3.2658706	1
152-6-15	we give explicit error bounds in terms of the parameters of the model ; in several settings , our bounds improve on or match known nonprivate results .	if g is an instance of the network obtained from a generative nonparametric model defined in terms of a graphon w , our model guarantees consistency : as the number of vertices tends to infinity , the output of our algorithm converges to w in an appropriate version of the l2 norm .	0	5	3	4.748169	-4.2444124	0
152-6-15	in particular , this means we can estimate the sizes of all multi-way cuts in g. our results hold as long as w is bounded , the average degree of g grows at least like the log of the number of vertices , and the number of blocks goes to infinity at an appropriate rate .	we give explicit error bounds in terms of the parameters of the model ; in several settings , our bounds improve on or match known nonprivate results .	1	4	5	-2.8893862	2.8893967	1
153-6-15	since the number of possible bounding boxes in an image is very large o ( # pixels2 ) , even a single linear scan to perform the greedy augmentation for submodular maximization is intractable .	this paper formulates the search for a set of bounding boxes ( as needed in object proposal generation ) as a monotone submodular maximization problem over the space of all possible bounding boxes in an image .	0	1	0	2.8430486	-2.2659197	0
153-6-15	thus , we formulate the greedy augmentation step as a branch-and-bound scheme .	this paper formulates the search for a set of bounding boxes ( as needed in object proposal generation ) as a monotone submodular maximization problem over the space of all possible bounding boxes in an image .	0	2	0	5.403861	-4.7783594	0
153-6-15	in order to speed up repeated application of b & b , we propose a novel generalization of minoux 's `lazy greedy ' algorithm to the b & b tree .	this paper formulates the search for a set of bounding boxes ( as needed in object proposal generation ) as a monotone submodular maximization problem over the space of all possible bounding boxes in an image .	0	3	0	5.311968	-4.65063	0
153-6-15	theoretically , our proposed formulation provides a new understanding to the problem , and contains classic heuristic approaches such as sliding window+non-maximal suppression ( nms ) and and efficient subwindow search ( ess ) as special cases .	this paper formulates the search for a set of bounding boxes ( as needed in object proposal generation ) as a monotone submodular maximization problem over the space of all possible bounding boxes in an image .	0	4	0	5.5259867	-4.9171257	0
153-6-15	this paper formulates the search for a set of bounding boxes ( as needed in object proposal generation ) as a monotone submodular maximization problem over the space of all possible bounding boxes in an image .	empirically , we show that our approach leads to a state-of-art performance on object proposal generation via a novel diversity measure .	1	0	5	-5.9734793	5.089116	1
153-6-15	thus , we formulate the greedy augmentation step as a branch-and-bound scheme .	since the number of possible bounding boxes in an image is very large o ( # pixels2 ) , even a single linear scan to perform the greedy augmentation for submodular maximization is intractable .	0	2	1	3.9054017	-3.5766563	0
153-6-15	in order to speed up repeated application of b & b , we propose a novel generalization of minoux 's `lazy greedy ' algorithm to the b & b tree .	since the number of possible bounding boxes in an image is very large o ( # pixels2 ) , even a single linear scan to perform the greedy augmentation for submodular maximization is intractable .	0	3	1	4.217125	-3.848526	0
153-6-15	since the number of possible bounding boxes in an image is very large o ( # pixels2 ) , even a single linear scan to perform the greedy augmentation for submodular maximization is intractable .	theoretically , our proposed formulation provides a new understanding to the problem , and contains classic heuristic approaches such as sliding window+non-maximal suppression ( nms ) and and efficient subwindow search ( ess ) as special cases .	1	1	4	-5.9815083	5.079544	1
153-6-15	empirically , we show that our approach leads to a state-of-art performance on object proposal generation via a novel diversity measure .	since the number of possible bounding boxes in an image is very large o ( # pixels2 ) , even a single linear scan to perform the greedy augmentation for submodular maximization is intractable .	0	5	1	5.5756006	-4.943696	0
153-6-15	in order to speed up repeated application of b & b , we propose a novel generalization of minoux 's `lazy greedy ' algorithm to the b & b tree .	thus , we formulate the greedy augmentation step as a branch-and-bound scheme .	0	3	2	-4.5931377	4.353773	1
153-6-15	theoretically , our proposed formulation provides a new understanding to the problem , and contains classic heuristic approaches such as sliding window+non-maximal suppression ( nms ) and and efficient subwindow search ( ess ) as special cases .	thus , we formulate the greedy augmentation step as a branch-and-bound scheme .	0	4	2	2.8563657	-2.6822429	0
153-6-15	empirically , we show that our approach leads to a state-of-art performance on object proposal generation via a novel diversity measure .	thus , we formulate the greedy augmentation step as a branch-and-bound scheme .	0	5	2	3.9820995	-3.6816792	0
153-6-15	theoretically , our proposed formulation provides a new understanding to the problem , and contains classic heuristic approaches such as sliding window+non-maximal suppression ( nms ) and and efficient subwindow search ( ess ) as special cases .	in order to speed up repeated application of b & b , we propose a novel generalization of minoux 's `lazy greedy ' algorithm to the b & b tree .	0	4	3	4.0027533	-3.6042085	0
153-6-15	in order to speed up repeated application of b & b , we propose a novel generalization of minoux 's `lazy greedy ' algorithm to the b & b tree .	empirically , we show that our approach leads to a state-of-art performance on object proposal generation via a novel diversity measure .	1	3	5	-5.9228506	5.197051	1
153-6-15	empirically , we show that our approach leads to a state-of-art performance on object proposal generation via a novel diversity measure .	theoretically , our proposed formulation provides a new understanding to the problem , and contains classic heuristic approaches such as sliding window+non-maximal suppression ( nms ) and and efficient subwindow search ( ess ) as special cases .	0	5	4	3.1260502	-2.9586172	0
154-6-15	this is accomplished by generalizing the gradient computation in stochastic backpropagation via a reparametrization trick with lower complexity .	we propose a second-order ( hessian or hessian-free ) based optimization method for variational inference inspired by gaussian backpropagation , and argue that quasi-newton optimization can be developed as well .	0	1	0	-1.6839039	1.8715723	1
154-6-15	as an illustrative example , we apply this approach to the problems of bayesian logistic regression and variational auto-encoder ( vae ) .	we propose a second-order ( hessian or hessian-free ) based optimization method for variational inference inspired by gaussian backpropagation , and argue that quasi-newton optimization can be developed as well .	0	2	0	4.007688	-3.657493	0
154-6-15	additionally , we compute bounds on the estimator variance of intractable expectations for the family of lipschitz continuous function .	we propose a second-order ( hessian or hessian-free ) based optimization method for variational inference inspired by gaussian backpropagation , and argue that quasi-newton optimization can be developed as well .	0	3	0	1.2426472	-1.0491321	0
154-6-15	we propose a second-order ( hessian or hessian-free ) based optimization method for variational inference inspired by gaussian backpropagation , and argue that quasi-newton optimization can be developed as well .	our method is practical , scalable and model free .	1	0	4	-2.9115791	2.8191059	1
154-6-15	we propose a second-order ( hessian or hessian-free ) based optimization method for variational inference inspired by gaussian backpropagation , and argue that quasi-newton optimization can be developed as well .	we demonstrate our method on several real-world datasets and provide comparisons with other stochastic gradient methods to show substantial enhancement in convergence rates .	1	0	5	-5.8632984	5.1870975	1
154-6-15	as an illustrative example , we apply this approach to the problems of bayesian logistic regression and variational auto-encoder ( vae ) .	this is accomplished by generalizing the gradient computation in stochastic backpropagation via a reparametrization trick with lower complexity .	0	2	1	4.0741186	-3.7817826	0
154-6-15	this is accomplished by generalizing the gradient computation in stochastic backpropagation via a reparametrization trick with lower complexity .	additionally , we compute bounds on the estimator variance of intractable expectations for the family of lipschitz continuous function .	1	1	3	-1.8698277	2.037489	1
154-6-15	this is accomplished by generalizing the gradient computation in stochastic backpropagation via a reparametrization trick with lower complexity .	our method is practical , scalable and model free .	1	1	4	-2.4505236	2.4919057	1
154-6-15	we demonstrate our method on several real-world datasets and provide comparisons with other stochastic gradient methods to show substantial enhancement in convergence rates .	this is accomplished by generalizing the gradient computation in stochastic backpropagation via a reparametrization trick with lower complexity .	0	5	1	5.347353	-4.751054	0
154-6-15	additionally , we compute bounds on the estimator variance of intractable expectations for the family of lipschitz continuous function .	as an illustrative example , we apply this approach to the problems of bayesian logistic regression and variational auto-encoder ( vae ) .	0	3	2	-5.1650076	4.639841	1
154-6-15	our method is practical , scalable and model free .	as an illustrative example , we apply this approach to the problems of bayesian logistic regression and variational auto-encoder ( vae ) .	0	4	2	-5.680087	4.9392567	1
154-6-15	as an illustrative example , we apply this approach to the problems of bayesian logistic regression and variational auto-encoder ( vae ) .	we demonstrate our method on several real-world datasets and provide comparisons with other stochastic gradient methods to show substantial enhancement in convergence rates .	1	2	5	-2.474709	2.4494085	1
154-6-15	our method is practical , scalable and model free .	additionally , we compute bounds on the estimator variance of intractable expectations for the family of lipschitz continuous function .	0	4	3	-2.6778398	2.6667228	1
154-6-15	additionally , we compute bounds on the estimator variance of intractable expectations for the family of lipschitz continuous function .	we demonstrate our method on several real-world datasets and provide comparisons with other stochastic gradient methods to show substantial enhancement in convergence rates .	1	3	5	-5.339527	4.842096	1
154-6-15	our method is practical , scalable and model free .	we demonstrate our method on several real-world datasets and provide comparisons with other stochastic gradient methods to show substantial enhancement in convergence rates .	1	4	5	-5.651333	5.0707617	1
155-8-28	since being analyzed by rokhlin , szlam , and tygert and popularized by halko , martinsson , and tropp , randomized simultaneous power iteration has become the method of choice for approximate singular value decomposition .	it is more accurate than simpler sketching algorithms , yet still converges quickly for any matrix , independently of singular value gaps .	1	0	1	-5.9711366	5.211033	1
155-8-28	after o ( 1/ ) iterations , it gives a low-rank approximation within ( 1 + ) of optimal for spectral norm error .	since being analyzed by rokhlin , szlam , and tygert and popularized by halko , martinsson , and tropp , randomized simultaneous power iteration has become the method of choice for approximate singular value decomposition .	0	2	0	5.3892756	-4.76967	0
155-8-28	[CLS] we give the first provable runtime improvement on simultaneous iteration : a randomized block krylov method, closely related to the classic block lanczos algo ) iterations and performs substanrithm, gives the same guarantees in just o	since being analyzed by rokhlin , szlam , and tygert and popularized by halko , martinsson , and tropp , randomized simultaneous power iteration has become the method of choice for approximate singular value decomposition .	0	3	0	5.55149	-4.9743347	0
155-8-28	our analysis is the first of a krylov subspace method that does not depend on singular value gaps , which are unreliable in practice .	since being analyzed by rokhlin , szlam , and tygert and popularized by halko , martinsson , and tropp , randomized simultaneous power iteration has become the method of choice for approximate singular value decomposition .	0	4	0	5.480989	-4.8675137	0
155-8-28	furthermore , while it is a simple accuracy benchmark , even ( 1 + ) error for spectral norm low-rank approximation does not imply that an algorithm returns high quality principal components , a major issue for data applications .	since being analyzed by rokhlin , szlam , and tygert and popularized by halko , martinsson , and tropp , randomized simultaneous power iteration has become the method of choice for approximate singular value decomposition .	0	5	0	5.5017414	-4.800584	0
155-8-28	since being analyzed by rokhlin , szlam , and tygert and popularized by halko , martinsson , and tropp , randomized simultaneous power iteration has become the method of choice for approximate singular value decomposition .	we address this problem for the first time by showing that both block krylov iteration and simultaneous iteration give nearly optimal pca for any matrix .	1	0	6	-6.022262	5.179029	1
155-8-28	this result further justifies their strength over non-iterative sketching methods .	since being analyzed by rokhlin , szlam , and tygert and popularized by halko , martinsson , and tropp , randomized simultaneous power iteration has become the method of choice for approximate singular value decomposition .	0	7	0	5.586765	-4.986062	0
155-8-28	it is more accurate than simpler sketching algorithms , yet still converges quickly for any matrix , independently of singular value gaps .	after o ( 1/ ) iterations , it gives a low-rank approximation within ( 1 + ) of optimal for spectral norm error .	1	1	2	2.6396875	-2.488799	0
155-8-28	we give the first provable runtime improvement on simultaneous iteration : a randomized block krylov method , closely related to the classic block lanczos algo ) iterations and performs substanrithm , gives the same guarantees in just o ( 1/ tially better experimentally .	it is more accurate than simpler sketching algorithms , yet still converges quickly for any matrix , independently of singular value gaps .	0	3	1	-2.1973205	2.243969	1
155-8-28	it is more accurate than simpler sketching algorithms , yet still converges quickly for any matrix , independently of singular value gaps .	our analysis is the first of a krylov subspace method that does not depend on singular value gaps , which are unreliable in practice .	1	1	4	2.3896518	-2.1636686	0
155-8-28	it is more accurate than simpler sketching algorithms , yet still converges quickly for any matrix , independently of singular value gaps .	furthermore , while it is a simple accuracy benchmark , even ( 1 + ) error for spectral norm low-rank approximation does not imply that an algorithm returns high quality principal components , a major issue for data applications .	1	1	5	3.5391142	-3.2792299	0
155-8-28	we address this problem for the first time by showing that both block krylov iteration and simultaneous iteration give nearly optimal pca for any matrix .	it is more accurate than simpler sketching algorithms , yet still converges quickly for any matrix , independently of singular value gaps .	0	6	1	-4.9094715	4.5712337	1
155-8-28	it is more accurate than simpler sketching algorithms , yet still converges quickly for any matrix , independently of singular value gaps .	this result further justifies their strength over non-iterative sketching methods .	1	1	7	-4.441063	4.213521	1
155-8-28	we give the first provable runtime improvement on simultaneous iteration : a randomized block krylov method , closely related to the classic block lanczos algo ) iterations and performs substanrithm , gives the same guarantees in just o ( 1/ tially better experimentally .	after o ( 1/ ) iterations , it gives a low-rank approximation within ( 1 + ) of optimal for spectral norm error .	0	3	2	-0.31209266	0.5136651	1
155-8-28	our analysis is the first of a krylov subspace method that does not depend on singular value gaps , which are unreliable in practice .	after o ( 1/ ) iterations , it gives a low-rank approximation within ( 1 + ) of optimal for spectral norm error .	0	4	2	1.296322	-1.1272914	0
155-8-28	furthermore , while it is a simple accuracy benchmark , even ( 1 + ) error for spectral norm low-rank approximation does not imply that an algorithm returns high quality principal components , a major issue for data applications .	after o ( 1/ ) iterations , it gives a low-rank approximation within ( 1 + ) of optimal for spectral norm error .	0	5	2	-0.8679195	1.042137	1
155-8-28	we address this problem for the first time by showing that both block krylov iteration and simultaneous iteration give nearly optimal pca for any matrix .	after o ( 1/ ) iterations , it gives a low-rank approximation within ( 1 + ) of optimal for spectral norm error .	0	6	2	-3.8294697	3.7159722	1
155-8-28	this result further justifies their strength over non-iterative sketching methods .	after o ( 1/ ) iterations , it gives a low-rank approximation within ( 1 + ) of optimal for spectral norm error .	0	7	2	4.6428466	-4.2339473	0
155-8-28	we give the first provable runtime improvement on simultaneous iteration : a randomized block krylov method , closely related to the classic block lanczos algo ) iterations and performs substanrithm , gives the same guarantees in just o ( 1/ tially better experimentally .	our analysis is the first of a krylov subspace method that does not depend on singular value gaps , which are unreliable in practice .	1	3	4	1.4149358	-1.1741077	0
155-8-28	furthermore , while it is a simple accuracy benchmark , even ( 1 + ) error for spectral norm low-rank approximation does not imply that an algorithm returns high quality principal components , a major issue for data applications .	[CLS] we give the first provable runtime improvement on simultaneous iteration : a randomized block krylov method, closely related to the classic block lanczos algo ) iterations and performs substanrithm, gives the same guarantees in just o ( 1 / tia	0	5	3	-3.337004	3.3023703	1
155-8-28	we give the first provable runtime improvement on simultaneous iteration : a randomized block krylov method , closely related to the classic block lanczos algo ) iterations and performs substanrithm , gives the same guarantees in just o ( 1/ tially better experimentally .	we address this problem for the first time by showing that both block krylov iteration and simultaneous iteration give nearly optimal pca for any matrix .	1	3	6	4.1908712	-3.7582967	0
155-8-28	we give the first provable runtime improvement on simultaneous iteration : a randomized block krylov method , closely related to the classic block lanczos algo ) iterations and performs substanrithm , gives the same guarantees in just o ( 1/ tially better experimentally .	this result further justifies their strength over non-iterative sketching methods .	1	3	7	-5.3264723	4.829876	1
155-8-28	furthermore , while it is a simple accuracy benchmark , even ( 1 + ) error for spectral norm low-rank approximation does not imply that an algorithm returns high quality principal components , a major issue for data applications .	our analysis is the first of a krylov subspace method that does not depend on singular value gaps , which are unreliable in practice .	0	5	4	-4.850217	4.596006	1
155-8-28	our analysis is the first of a krylov subspace method that does not depend on singular value gaps , which are unreliable in practice .	we address this problem for the first time by showing that both block krylov iteration and simultaneous iteration give nearly optimal pca for any matrix .	1	4	6	4.283677	-3.845049	0
155-8-28	our analysis is the first of a krylov subspace method that does not depend on singular value gaps , which are unreliable in practice .	this result further justifies their strength over non-iterative sketching methods .	1	4	7	-5.4280243	4.918763	1
155-8-28	furthermore , while it is a simple accuracy benchmark , even ( 1 + ) error for spectral norm low-rank approximation does not imply that an algorithm returns high quality principal components , a major issue for data applications .	we address this problem for the first time by showing that both block krylov iteration and simultaneous iteration give nearly optimal pca for any matrix .	1	5	6	-2.4129333	2.5259728	1
155-8-28	furthermore , while it is a simple accuracy benchmark , even ( 1 + ) error for spectral norm low-rank approximation does not imply that an algorithm returns high quality principal components , a major issue for data applications .	this result further justifies their strength over non-iterative sketching methods .	1	5	7	-5.891943	5.1931252	1
155-8-28	this result further justifies their strength over non-iterative sketching methods .	we address this problem for the first time by showing that both block krylov iteration and simultaneous iteration give nearly optimal pca for any matrix .	0	7	6	5.3735294	-4.763872	0
156-7-21	we propose a kernel-based method for finding matching between instances across different domains , such as multilingual documents and images with annotations .	each instance is assumed to be represented as a multiset of features , e.g. , a bag-ofwords representation for documents .	1	0	1	-5.0114427	4.589093	1
156-7-21	we propose a kernel-based method for finding matching between instances across different domains , such as multilingual documents and images with annotations .	the major difficulty in finding cross-domain relationships is that the similarity between instances in different domains can not be directly measured .	1	0	2	-3.3372931	3.2032404	1
156-7-21	we propose a kernel-based method for finding matching between instances across different domains , such as multilingual documents and images with annotations .	to overcome this difficulty , the proposed method embeds all the features of different domains in a shared latent space , and regards each instance as a distribution of its own features in the shared latent space .	1	0	3	-5.9729457	5.185013	1
156-7-21	to represent the distributions efficiently and nonparametrically , we employ the framework of the kernel embeddings of distributions .	we propose a kernel-based method for finding matching between instances across different domains , such as multilingual documents and images with annotations .	0	4	0	5.235891	-4.625782	0
156-7-21	we propose a kernel-based method for finding matching between instances across different domains , such as multilingual documents and images with annotations .	the embedding is estimated so as to minimize the difference between distributions of paired instances while keeping unpaired instances apart .	1	0	5	-5.90177	5.2013717	1
156-7-21	we propose a kernel-based method for finding matching between instances across different domains , such as multilingual documents and images with annotations .	in our experiments , we show that the proposed method can achieve high performance on finding correspondence between multi-lingual wikipedia articles , between documents and tags , and between images and tags .	1	0	6	-5.9689765	5.1837606	1
156-7-21	each instance is assumed to be represented as a multiset of features , e.g. , a bag-ofwords representation for documents .	the major difficulty in finding cross-domain relationships is that the similarity between instances in different domains can not be directly measured .	1	1	2	3.4560657	-3.2170374	0
156-7-21	to overcome this difficulty , the proposed method embeds all the features of different domains in a shared latent space , and regards each instance as a distribution of its own features in the shared latent space .	each instance is assumed to be represented as a multiset of features , e.g. , a bag-ofwords representation for documents .	0	3	1	4.6523323	-4.165175	0
156-7-21	each instance is assumed to be represented as a multiset of features , e.g. , a bag-ofwords representation for documents .	to represent the distributions efficiently and nonparametrically , we employ the framework of the kernel embeddings of distributions .	1	1	4	-2.7830238	2.8435373	1
156-7-21	the embedding is estimated so as to minimize the difference between distributions of paired instances while keeping unpaired instances apart .	each instance is assumed to be represented as a multiset of features , e.g. , a bag-ofwords representation for documents .	0	5	1	5.181806	-4.647994	0
156-7-21	in our experiments , we show that the proposed method can achieve high performance on finding correspondence between multi-lingual wikipedia articles , between documents and tags , and between images and tags .	each instance is assumed to be represented as a multiset of features , e.g. , a bag-ofwords representation for documents .	0	6	1	5.494099	-4.893449	0
156-7-21	to overcome this difficulty , the proposed method embeds all the features of different domains in a shared latent space , and regards each instance as a distribution of its own features in the shared latent space .	the major difficulty in finding cross-domain relationships is that the similarity between instances in different domains can not be directly measured .	0	3	2	4.619025	-4.1090345	0
156-7-21	the major difficulty in finding cross-domain relationships is that the similarity between instances in different domains can not be directly measured .	to represent the distributions efficiently and nonparametrically , we employ the framework of the kernel embeddings of distributions .	1	2	4	-5.0477867	4.786652	1
156-7-21	the embedding is estimated so as to minimize the difference between distributions of paired instances while keeping unpaired instances apart .	the major difficulty in finding cross-domain relationships is that the similarity between instances in different domains can not be directly measured .	0	5	2	5.3087783	-4.756774	0
156-7-21	in our experiments , we show that the proposed method can achieve high performance on finding correspondence between multi-lingual wikipedia articles , between documents and tags , and between images and tags .	the major difficulty in finding cross-domain relationships is that the similarity between instances in different domains can not be directly measured .	0	6	2	5.5978413	-5.0000286	0
156-7-21	to represent the distributions efficiently and nonparametrically , we employ the framework of the kernel embeddings of distributions .	to overcome this difficulty , the proposed method embeds all the features of different domains in a shared latent space , and regards each instance as a distribution of its own features in the shared latent space .	0	4	3	-1.6536644	1.7647331	1
156-7-21	to overcome this difficulty , the proposed method embeds all the features of different domains in a shared latent space , and regards each instance as a distribution of its own features in the shared latent space .	the embedding is estimated so as to minimize the difference between distributions of paired instances while keeping unpaired instances apart .	1	3	5	-5.5022798	5.024074	1
156-7-21	to overcome this difficulty , the proposed method embeds all the features of different domains in a shared latent space , and regards each instance as a distribution of its own features in the shared latent space .	in our experiments , we show that the proposed method can achieve high performance on finding correspondence between multi-lingual wikipedia articles , between documents and tags , and between images and tags .	1	3	6	-5.973312	5.090049	1
156-7-21	to represent the distributions efficiently and nonparametrically , we employ the framework of the kernel embeddings of distributions .	the embedding is estimated so as to minimize the difference between distributions of paired instances while keeping unpaired instances apart .	1	4	5	-5.520481	5.00896	1
156-7-21	in our experiments , we show that the proposed method can achieve high performance on finding correspondence between multi-lingual wikipedia articles , between documents and tags , and between images and tags .	to represent the distributions efficiently and nonparametrically , we employ the framework of the kernel embeddings of distributions .	0	6	4	5.497079	-4.885332	0
156-7-21	the embedding is estimated so as to minimize the difference between distributions of paired instances while keeping unpaired instances apart .	in our experiments , we show that the proposed method can achieve high performance on finding correspondence between multi-lingual wikipedia articles , between documents and tags , and between images and tags .	1	5	6	-5.993124	5.180369	1
157-4-6	we propose a sparse method for scalable automated variational inference ( avi ) in a large class of models with gaussian process ( gp ) priors , multiple latent functions , multiple outputs and non-linear likelihoods .	our approach maintains the statistical efficiency property of the original avi method , requiring only expectations over univariate gaussian distributions to approximate the posterior with a mixture of gaussians .	1	0	1	-5.857458	5.185845	1
157-4-6	experiments on small datasets for various problems including regression , classification , log gaussian cox processes , and warped gps show that our method can perform as well as the full method under high sparsity levels .	we propose a sparse method for scalable automated variational inference ( avi ) in a large class of models with gaussian process ( gp ) priors , multiple latent functions , multiple outputs and non-linear likelihoods .	0	2	0	5.5813046	-4.937957	0
157-4-6	we propose a sparse method for scalable automated variational inference ( avi ) in a large class of models with gaussian process ( gp ) priors , multiple latent functions , multiple outputs and non-linear likelihoods .	on larger experiments using the mnist and the sarcos datasets we show that our method can provide superior performance to previously published scalable approaches that have been handcrafted to specific likelihood models .	1	0	3	-5.945028	5.079873	1
157-4-6	experiments on small datasets for various problems including regression , classification , log gaussian cox processes , and warped gps show that our method can perform as well as the full method under high sparsity levels .	our approach maintains the statistical efficiency property of the original avi method , requiring only expectations over univariate gaussian distributions to approximate the posterior with a mixture of gaussians .	0	2	1	5.367219	-4.733907	0
157-4-6	on larger experiments using the mnist and the sarcos datasets we show that our method can provide superior performance to previously published scalable approaches that have been handcrafted to specific likelihood models .	our approach maintains the statistical efficiency property of the original avi method , requiring only expectations over univariate gaussian distributions to approximate the posterior with a mixture of gaussians .	0	3	1	5.2535768	-4.6443253	0
157-4-6	on larger experiments using the mnist and the sarcos datasets we show that our method can provide superior performance to previously published scalable approaches that have been handcrafted to specific likelihood models .	experiments on small datasets for various problems including regression , classification , log gaussian cox processes , and warped gps show that our method can perform as well as the full method under high sparsity levels .	0	3	2	2.7914224	-2.5796986	0
158-4-6	[CLS] given the target state t, we use a ( reverse ) local power iteration to construct an ` expanded target distribution ', which has the same mean as the quantity we want to estimate, but a smaller variance - this can then be sampled efficiently by	[CLS] we develop a new bidirectional algorithm for estimating markov chain multi - step transition probabilities : given a markov chain, we want to estimate the probability of hitting a given target state in steps after starting from a given source	0	1	0	5.3095417	-4.6270933	0
158-4-6	[CLS] our method extends to any markov chain on a discrete ( finite or countable ) state - space, and can be extended to compute functions of multi - step transition probabilities such as pagerank, graph diffusions, hitting / return times	[CLS] we develop a new bidirectional algorithm for estimating markov chain multi - step transition probabilities : given a markov chain, we want to estimate the probability of hitting a given target state in steps after starting from a given source	0	2	0	5.3810987	-4.6580806	0
158-4-6	[CLS] our main result is that in ` sparse'markov chains - wherein the number of transitions between states is comparable to the number of states - the running time of our algorithm for a uniform - random target node is order - wise smaller than monte carlo	[CLS] we develop a new bidirectional algorithm for estimating markov chain multi - step transition probabilities : given a markov chain, we want to estimate the probability of hitting a given target state in steps after starting from a given source	0	3	0	4.8012033	-4.082131	0
158-4-6	[CLS] our method extends to any markov chain on a discrete ( finite or countable ) state - space, and can be extended to compute functions of multi - step transition probabilities such as pagerank, graph diffusions, hitting / return times	[CLS] given the target state t, we use a ( reverse ) local power iteration to construct an ` expanded target distribution ', which has the same mean as the quantity we want to estimate, but a smaller variance - this can then be sampled efficiently by	0	2	1	4.3751307	-3.9433768	0
158-4-6	[CLS] our main result is that in ` sparse'markov chains - wherein the number of transitions between states is comparable to the number of states - the running time of our algorithm for a uniform - random target node is order - wise smaller than monte carlo	[CLS] given the target state t, we use a ( reverse ) local power iteration to construct an ` expanded target distribution ', which has the same mean as the quantity we want to estimate, but a smaller variance - this can then be sampled efficiently by	0	3	1	2.5016088	-2.1269612	0
158-4-6	[CLS] our main result is that in ` sparse'markov chains - wherein the number of transitions between states is comparable to the number of states - the running time of our algorithm for a uniform - random target node is order - wise smaller than monte carlo	[CLS] our method extends to any markov chain on a discrete ( finite or countable ) state - space, and can be extended to compute functions of multi - step transition probabilities such as pagerank, graph diffusions, hitting / return times	0	3	2	-1.4484272	1.5866678	1
159-5-10	on the other hand , monte carlo estimators such as importance sampling have excellent any-time behavior , but depend critically on the proposal distribution .	variational algorithms such as tree-reweighted belief propagation can provide deterministic bounds on the partition function , but are often loose and difficult to use in an `` any-time '' fashion , expending more computation for tighter bounds .	0	1	0	-0.8803781	1.1962571	1
159-5-10	we propose a simple monte carlo based inference method that augments convex variational bounds by adding importance sampling ( is ) .	variational algorithms such as tree-reweighted belief propagation can provide deterministic bounds on the partition function , but are often loose and difficult to use in an `` any-time '' fashion , expending more computation for tighter bounds .	0	2	0	4.974514	-4.3710256	0
159-5-10	[CLS] variational algorithms such as tree - reweighted belief propagation can provide deterministic bounds on the partition function, but are often loose and difficult to use in an ` ` any - time'' fashion, expending more computation for tighter	[CLS] we argue that convex variational methods naturally provide good is proposals that ` ` cover'' the target probability, and reinterpret the variational optimization as designing a proposal to minimize an upper bound on the variance of our is estimator	1	0	3	-4.881432	4.5746965	1
159-5-10	[CLS] variational algorithms such as tree - reweighted belief propagation can provide deterministic bounds on the partition function, but are often loose and difficult to use in an ` ` any - time'' fashion, expending more computation for tighter	[CLS] this both provides an accurate estimator and enables construction of any - time probabilistic bounds that improve quickly and directly on state - of - the - art variational bounds, and provide certificates of accuracy given enough samples relative to the error	1	0	4	-5.878455	5.116369	1
159-5-10	on the other hand , monte carlo estimators such as importance sampling have excellent any-time behavior , but depend critically on the proposal distribution .	we propose a simple monte carlo based inference method that augments convex variational bounds by adding importance sampling ( is ) .	1	1	2	-3.876092	3.736652	1
159-5-10	we argue that convex variational methods naturally provide good is proposals that `` cover '' the target probability , and reinterpret the variational optimization as designing a proposal to minimize an upper bound on the variance of our is estimator .	on the other hand , monte carlo estimators such as importance sampling have excellent any-time behavior , but depend critically on the proposal distribution .	0	3	1	4.556661	-4.049565	0
159-5-10	on the other hand , monte carlo estimators such as importance sampling have excellent any-time behavior , but depend critically on the proposal distribution .	this both provides an accurate estimator and enables construction of any-time probabilistic bounds that improve quickly and directly on state-of-the-art variational bounds , and provide certificates of accuracy given enough samples relative to the error in the initial bound .	1	1	4	-5.9978056	5.210283	1
159-5-10	we propose a simple monte carlo based inference method that augments convex variational bounds by adding importance sampling ( is ) .	we argue that convex variational methods naturally provide good is proposals that `` cover '' the target probability , and reinterpret the variational optimization as designing a proposal to minimize an upper bound on the variance of our is estimator .	1	2	3	-3.6144943	3.5575829	1
159-5-10	this both provides an accurate estimator and enables construction of any-time probabilistic bounds that improve quickly and directly on state-of-the-art variational bounds , and provide certificates of accuracy given enough samples relative to the error in the initial bound .	we propose a simple monte carlo based inference method that augments convex variational bounds by adding importance sampling ( is ) .	0	4	2	5.3681293	-4.764967	0
159-5-10	[CLS] this both provides an accurate estimator and enables construction of any - time probabilistic bounds that improve quickly and directly on state - of - the - art variational bounds, and provide certificates of accuracy given enough samples relative to the error	[CLS] we argue that convex variational methods naturally provide good is proposals that ` ` cover'' the target probability, and reinterpret the variational optimization as designing a proposal to minimize an upper bound on the variance of our is estimator	0	4	3	4.379899	-3.9308348	0
160-7-21	mean field variational bayes ( mfvb ) is a popular posterior approximation method due to its fast runtime on large-scale data sets .	however , a well known major failing of mfvb is that it underestimates the uncertainty of model variables ( sometimes severely ) and provides no information about model variable covariance .	1	0	1	-5.897043	5.180909	1
160-7-21	we generalize linear response methods from statistical physics to deliver accurate uncertainty estimates for model variables -- both for individual variables and coherently across variables .	mean field variational bayes ( mfvb ) is a popular posterior approximation method due to its fast runtime on large-scale data sets .	0	2	0	0.12669769	0.3257193	1
160-7-21	mean field variational bayes ( mfvb ) is a popular posterior approximation method due to its fast runtime on large-scale data sets .	we call our method linear response variational bayes ( lrvb ) .	1	0	3	-5.8235474	5.2201223	1
160-7-21	when the mfvb posterior approximation is in the exponential family , lrvb has a simple , analytic form , even for non-conjugate models .	mean field variational bayes ( mfvb ) is a popular posterior approximation method due to its fast runtime on large-scale data sets .	0	4	0	5.572618	-4.9854603	0
160-7-21	mean field variational bayes ( mfvb ) is a popular posterior approximation method due to its fast runtime on large-scale data sets .	indeed , we make no assumptions about the form of the true posterior .	1	0	5	-5.6285915	5.09976	1
160-7-21	we demonstrate the accuracy and scalability of our method on a range of models for both simulated and real data .	mean field variational bayes ( mfvb ) is a popular posterior approximation method due to its fast runtime on large-scale data sets .	0	6	0	5.55484	-4.9623146	0
160-7-21	we generalize linear response methods from statistical physics to deliver accurate uncertainty estimates for model variables -- both for individual variables and coherently across variables .	however , a well known major failing of mfvb is that it underestimates the uncertainty of model variables ( sometimes severely ) and provides no information about model variable covariance .	0	2	1	3.484064	-3.2928567	0
160-7-21	we call our method linear response variational bayes ( lrvb ) .	however , a well known major failing of mfvb is that it underestimates the uncertainty of model variables ( sometimes severely ) and provides no information about model variable covariance .	0	3	1	4.3679085	-3.9967847	0
160-7-21	however , a well known major failing of mfvb is that it underestimates the uncertainty of model variables ( sometimes severely ) and provides no information about model variable covariance .	when the mfvb posterior approximation is in the exponential family , lrvb has a simple , analytic form , even for non-conjugate models .	1	1	4	-5.574513	5.1373215	1
160-7-21	however , a well known major failing of mfvb is that it underestimates the uncertainty of model variables ( sometimes severely ) and provides no information about model variable covariance .	indeed , we make no assumptions about the form of the true posterior .	1	1	5	-5.837164	5.2163477	1
160-7-21	we demonstrate the accuracy and scalability of our method on a range of models for both simulated and real data .	however , a well known major failing of mfvb is that it underestimates the uncertainty of model variables ( sometimes severely ) and provides no information about model variable covariance .	0	6	1	5.6082115	-4.9857855	0
160-7-21	we generalize linear response methods from statistical physics to deliver accurate uncertainty estimates for model variables -- both for individual variables and coherently across variables .	we call our method linear response variational bayes ( lrvb ) .	1	2	3	-5.035716	4.7014523	1
160-7-21	we generalize linear response methods from statistical physics to deliver accurate uncertainty estimates for model variables -- both for individual variables and coherently across variables .	when the mfvb posterior approximation is in the exponential family , lrvb has a simple , analytic form , even for non-conjugate models .	1	2	4	-5.468219	5.000155	1
160-7-21	we generalize linear response methods from statistical physics to deliver accurate uncertainty estimates for model variables -- both for individual variables and coherently across variables .	indeed , we make no assumptions about the form of the true posterior .	1	2	5	-3.366953	3.2737155	1
160-7-21	we generalize linear response methods from statistical physics to deliver accurate uncertainty estimates for model variables -- both for individual variables and coherently across variables .	we demonstrate the accuracy and scalability of our method on a range of models for both simulated and real data .	1	2	6	-6.016465	5.196374	1
160-7-21	we call our method linear response variational bayes ( lrvb ) .	when the mfvb posterior approximation is in the exponential family , lrvb has a simple , analytic form , even for non-conjugate models .	1	3	4	-5.7589626	5.2209783	1
160-7-21	indeed , we make no assumptions about the form of the true posterior .	we call our method linear response variational bayes ( lrvb ) .	0	5	3	-2.3484735	2.4082904	1
160-7-21	we call our method linear response variational bayes ( lrvb ) .	we demonstrate the accuracy and scalability of our method on a range of models for both simulated and real data .	1	3	6	-5.9475307	5.178976	1
160-7-21	when the mfvb posterior approximation is in the exponential family , lrvb has a simple , analytic form , even for non-conjugate models .	indeed , we make no assumptions about the form of the true posterior .	1	4	5	-2.6018121	2.6655486	1
160-7-21	we demonstrate the accuracy and scalability of our method on a range of models for both simulated and real data .	when the mfvb posterior approximation is in the exponential family , lrvb has a simple , analytic form , even for non-conjugate models .	0	6	4	5.326335	-4.728258	0
160-7-21	indeed , we make no assumptions about the form of the true posterior .	we demonstrate the accuracy and scalability of our method on a range of models for both simulated and real data .	1	5	6	-5.9561253	5.094942	1
161-8-28	the weights of the items are binary , stochastic , and drawn independently of each other .	we propose combinatorial cascading bandits , a class of partial monitoring problems where at each step a learning agent chooses a tuple of ground items subject to constraints and receives a reward if and only if the weights of all chosen items are one .	0	1	0	4.9256086	-4.343729	0
161-8-28	the agent observes the index of the first chosen item whose weight is zero .	we propose combinatorial cascading bandits , a class of partial monitoring problems where at each step a learning agent chooses a tuple of ground items subject to constraints and receives a reward if and only if the weights of all chosen items are one .	0	2	0	5.4269857	-4.83451	0
161-8-28	we propose combinatorial cascading bandits , a class of partial monitoring problems where at each step a learning agent chooses a tuple of ground items subject to constraints and receives a reward if and only if the weights of all chosen items are one .	this observation model arises in network routing , for instance , where the learning agent may only observe the first link in the routing path which is down , and blocks the path .	1	0	3	-4.8576446	4.44395	1
161-8-28	we propose a ucb-like algorithm for solving our problems , combcascade ; and prove gap-dependent and gap-free upper bounds on its n-step regret .	we propose combinatorial cascading bandits , a class of partial monitoring problems where at each step a learning agent chooses a tuple of ground items subject to constraints and receives a reward if and only if the weights of all chosen items are one .	0	4	0	5.4495363	-4.821618	0
161-8-28	our proofs build on recent work in stochastic combinatorial semi-bandits but also address two novel challenges of our setting , a non-linear reward function and partial observability .	we propose combinatorial cascading bandits , a class of partial monitoring problems where at each step a learning agent chooses a tuple of ground items subject to constraints and receives a reward if and only if the weights of all chosen items are one .	0	5	0	5.441413	-4.7266674	0
161-8-28	we evaluate combcascade on two real-world problems and show that it performs well even when our modeling assumptions are violated .	we propose combinatorial cascading bandits , a class of partial monitoring problems where at each step a learning agent chooses a tuple of ground items subject to constraints and receives a reward if and only if the weights of all chosen items are one .	0	6	0	5.634541	-5.034945	0
161-8-28	we also demonstrate that our setting requires a new learning algorithm .	we propose combinatorial cascading bandits , a class of partial monitoring problems where at each step a learning agent chooses a tuple of ground items subject to constraints and receives a reward if and only if the weights of all chosen items are one .	0	7	0	5.4402113	-4.7880955	0
161-8-28	the weights of the items are binary , stochastic , and drawn independently of each other .	the agent observes the index of the first chosen item whose weight is zero .	1	1	2	-2.6828203	2.6092668	1
161-8-28	this observation model arises in network routing , for instance , where the learning agent may only observe the first link in the routing path which is down , and blocks the path .	the weights of the items are binary , stochastic , and drawn independently of each other .	0	3	1	3.6019812	-3.2991726	0
161-8-28	we propose a ucb-like algorithm for solving our problems , combcascade ; and prove gap-dependent and gap-free upper bounds on its n-step regret .	the weights of the items are binary , stochastic , and drawn independently of each other .	0	4	1	4.769106	-4.310956	0
161-8-28	the weights of the items are binary , stochastic , and drawn independently of each other .	our proofs build on recent work in stochastic combinatorial semi-bandits but also address two novel challenges of our setting , a non-linear reward function and partial observability .	1	1	5	-5.8988175	4.9019356	1
161-8-28	we evaluate combcascade on two real-world problems and show that it performs well even when our modeling assumptions are violated .	the weights of the items are binary , stochastic , and drawn independently of each other .	0	6	1	5.591578	-5.030058	0
161-8-28	the weights of the items are binary , stochastic , and drawn independently of each other .	we also demonstrate that our setting requires a new learning algorithm .	1	1	7	-5.7267575	4.7644806	1
161-8-28	this observation model arises in network routing , for instance , where the learning agent may only observe the first link in the routing path which is down , and blocks the path .	the agent observes the index of the first chosen item whose weight is zero .	0	3	2	3.1800342	-2.9138784	0
161-8-28	we propose a ucb-like algorithm for solving our problems , combcascade ; and prove gap-dependent and gap-free upper bounds on its n-step regret .	the agent observes the index of the first chosen item whose weight is zero .	0	4	2	4.9794183	-4.522087	0
161-8-28	the agent observes the index of the first chosen item whose weight is zero .	our proofs build on recent work in stochastic combinatorial semi-bandits but also address two novel challenges of our setting , a non-linear reward function and partial observability .	1	2	5	-5.991144	5.019946	1
161-8-28	the agent observes the index of the first chosen item whose weight is zero .	we evaluate combcascade on two real-world problems and show that it performs well even when our modeling assumptions are violated .	1	2	6	-6.015602	5.1559505	1
161-8-28	the agent observes the index of the first chosen item whose weight is zero .	we also demonstrate that our setting requires a new learning algorithm .	1	2	7	-5.8547583	4.8891277	1
161-8-28	we propose a ucb-like algorithm for solving our problems , combcascade ; and prove gap-dependent and gap-free upper bounds on its n-step regret .	this observation model arises in network routing , for instance , where the learning agent may only observe the first link in the routing path which is down , and blocks the path .	0	4	3	4.3280497	-3.9415088	0
161-8-28	this observation model arises in network routing , for instance , where the learning agent may only observe the first link in the routing path which is down , and blocks the path .	our proofs build on recent work in stochastic combinatorial semi-bandits but also address two novel challenges of our setting , a non-linear reward function and partial observability .	1	3	5	-5.890603	5.19269	1
161-8-28	we evaluate combcascade on two real-world problems and show that it performs well even when our modeling assumptions are violated .	this observation model arises in network routing , for instance , where the learning agent may only observe the first link in the routing path which is down , and blocks the path .	0	6	3	5.4810095	-4.9306917	0
161-8-28	this observation model arises in network routing , for instance , where the learning agent may only observe the first link in the routing path which is down , and blocks the path .	we also demonstrate that our setting requires a new learning algorithm .	1	3	7	-5.923751	5.0114536	1
161-8-28	we propose a ucb-like algorithm for solving our problems , combcascade ; and prove gap-dependent and gap-free upper bounds on its n-step regret .	our proofs build on recent work in stochastic combinatorial semi-bandits but also address two novel challenges of our setting , a non-linear reward function and partial observability .	1	4	5	-4.946089	4.513885	1
161-8-28	we evaluate combcascade on two real-world problems and show that it performs well even when our modeling assumptions are violated .	we propose a ucb-like algorithm for solving our problems , combcascade ; and prove gap-dependent and gap-free upper bounds on its n-step regret .	0	6	4	5.550128	-5.0212927	0
161-8-28	we propose a ucb-like algorithm for solving our problems , combcascade ; and prove gap-dependent and gap-free upper bounds on its n-step regret .	we also demonstrate that our setting requires a new learning algorithm .	1	4	7	-5.8656607	4.9942856	1
161-8-28	we evaluate combcascade on two real-world problems and show that it performs well even when our modeling assumptions are violated .	our proofs build on recent work in stochastic combinatorial semi-bandits but also address two novel challenges of our setting , a non-linear reward function and partial observability .	0	6	5	4.360797	-4.016245	0
161-8-28	our proofs build on recent work in stochastic combinatorial semi-bandits but also address two novel challenges of our setting , a non-linear reward function and partial observability .	we also demonstrate that our setting requires a new learning algorithm .	1	5	7	-5.5184765	4.9310465	1
161-8-28	we also demonstrate that our setting requires a new learning algorithm .	we evaluate combcascade on two real-world problems and show that it performs well even when our modeling assumptions are violated .	0	7	6	2.9792304	-2.853364	0
162-7-21	the interval is computed from a single finite-length sample path from the markov chain , and does not require the knowledge of any parameters of the chain .	this article provides the first procedure for computing a fully data-dependent interval that traps the mixing time tmix of a finite reversible ergodic markov chain at a prescribed confidence level .	0	1	0	4.304329	-3.8413494	0
162-7-21	this article provides the first procedure for computing a fully data-dependent interval that traps the mixing time tmix of a finite reversible ergodic markov chain at a prescribed confidence level .	this stands in contrast to previous approaches , which either only provide point estimates , or require a reset mechanism , or additional prior knowledge .	1	0	2	-5.24859	4.832795	1
162-7-21	this article provides the first procedure for computing a fully data-dependent interval that traps the mixing time tmix of a finite reversible ergodic markov chain at a prescribed confidence level .	the interval is constructed around the relaxation time trelax , which is strongly related to the mixing time , and the width of the interval converges to zero roughly at a n rate , where n is the length of the sample path .	1	0	3	-5.3996134	4.9582577	1
162-7-21	this article provides the first procedure for computing a fully data-dependent interval that traps the mixing time tmix of a finite reversible ergodic markov chain at a prescribed confidence level .	upper and lower bounds are given on the number of samples required to achieve constant-factor multiplicative accuracy .	1	0	4	-3.3405552	3.2707515	1
162-7-21	the lower bounds indicate that , unless further restrictions are placed on the chain , no procedure can achieve this accuracy level before seeing each state at least ( trelax ) times on the average .	this article provides the first procedure for computing a fully data-dependent interval that traps the mixing time tmix of a finite reversible ergodic markov chain at a prescribed confidence level .	0	5	0	4.1956615	-3.7625682	0
162-7-21	this article provides the first procedure for computing a fully data-dependent interval that traps the mixing time tmix of a finite reversible ergodic markov chain at a prescribed confidence level .	finally , future directions of research are identified .	1	0	6	-5.878398	4.9719577	1
162-7-21	the interval is computed from a single finite-length sample path from the markov chain , and does not require the knowledge of any parameters of the chain .	this stands in contrast to previous approaches , which either only provide point estimates , or require a reset mechanism , or additional prior knowledge .	1	1	2	-2.2986116	2.320357	1
162-7-21	the interval is constructed around the relaxation time trelax , which is strongly related to the mixing time , and the width of the interval converges to zero roughly at a n rate , where n is the length of the sample path .	the interval is computed from a single finite-length sample path from the markov chain , and does not require the knowledge of any parameters of the chain .	0	3	1	2.382452	-2.1860423	0
162-7-21	upper and lower bounds are given on the number of samples required to achieve constant-factor multiplicative accuracy .	the interval is computed from a single finite-length sample path from the markov chain , and does not require the knowledge of any parameters of the chain .	0	4	1	-3.2875054	3.238646	1
162-7-21	the lower bounds indicate that , unless further restrictions are placed on the chain , no procedure can achieve this accuracy level before seeing each state at least ( trelax ) times on the average .	the interval is computed from a single finite-length sample path from the markov chain , and does not require the knowledge of any parameters of the chain .	0	5	1	0.8304822	-0.57999	0
162-7-21	finally , future directions of research are identified .	the interval is computed from a single finite-length sample path from the markov chain , and does not require the knowledge of any parameters of the chain .	0	6	1	4.9266834	-4.4330845	0
162-7-21	the interval is constructed around the relaxation time trelax , which is strongly related to the mixing time , and the width of the interval converges to zero roughly at a n rate , where n is the length of the sample path .	this stands in contrast to previous approaches , which either only provide point estimates , or require a reset mechanism , or additional prior knowledge .	0	3	2	-1.5217304	1.6488178	1
162-7-21	this stands in contrast to previous approaches , which either only provide point estimates , or require a reset mechanism , or additional prior knowledge .	upper and lower bounds are given on the number of samples required to achieve constant-factor multiplicative accuracy .	1	2	4	2.2166393	-2.0332172	0
162-7-21	the lower bounds indicate that , unless further restrictions are placed on the chain , no procedure can achieve this accuracy level before seeing each state at least ( trelax ) times on the average .	this stands in contrast to previous approaches , which either only provide point estimates , or require a reset mechanism , or additional prior knowledge .	0	5	2	-2.3002024	2.3481104	1
162-7-21	finally , future directions of research are identified .	this stands in contrast to previous approaches , which either only provide point estimates , or require a reset mechanism , or additional prior knowledge .	0	6	2	4.861969	-4.389475	0
162-7-21	upper and lower bounds are given on the number of samples required to achieve constant-factor multiplicative accuracy .	the interval is constructed around the relaxation time trelax , which is strongly related to the mixing time , and the width of the interval converges to zero roughly at a n rate , where n is the length of the sample path .	0	4	3	-4.4424477	4.220192	1
162-7-21	the interval is constructed around the relaxation time trelax , which is strongly related to the mixing time , and the width of the interval converges to zero roughly at a n rate , where n is the length of the sample path .	the lower bounds indicate that , unless further restrictions are placed on the chain , no procedure can achieve this accuracy level before seeing each state at least ( trelax ) times on the average .	1	3	5	-2.9200478	2.8906965	1
162-7-21	the interval is constructed around the relaxation time trelax , which is strongly related to the mixing time , and the width of the interval converges to zero roughly at a n rate , where n is the length of the sample path .	finally , future directions of research are identified .	1	3	6	-5.651742	4.778062	1
162-7-21	the lower bounds indicate that , unless further restrictions are placed on the chain , no procedure can achieve this accuracy level before seeing each state at least ( trelax ) times on the average .	upper and lower bounds are given on the number of samples required to achieve constant-factor multiplicative accuracy .	0	5	4	4.3618574	-4.0276384	0
162-7-21	upper and lower bounds are given on the number of samples required to achieve constant-factor multiplicative accuracy .	finally , future directions of research are identified .	1	4	6	-5.856674	4.9421444	1
162-7-21	the lower bounds indicate that , unless further restrictions are placed on the chain , no procedure can achieve this accuracy level before seeing each state at least ( trelax ) times on the average .	finally , future directions of research are identified .	1	5	6	-5.8835754	4.9998493	1
163-7-21	these studies have focused on specific risk-measures , such as the variance or conditional value at risk ( cvar ) .	several authors have recently developed risk-sensitive policy gradient methods that augment the standard expected cost minimization problem with a measure of variability in cost .	0	1	0	4.341744	-3.6395094	0
163-7-21	in this work , we extend the policy gradient method to the whole class of coherent risk measures , which is widely accepted in finance and operations research , among other fields .	several authors have recently developed risk-sensitive policy gradient methods that augment the standard expected cost minimization problem with a measure of variability in cost .	0	2	0	5.4912643	-4.8793344	0
163-7-21	we consider both static and time-consistent dynamic risk measures .	several authors have recently developed risk-sensitive policy gradient methods that augment the standard expected cost minimization problem with a measure of variability in cost .	0	3	0	5.44681	-4.84653	0
163-7-21	for static risk measures , our approach is in the spirit of policy gradient algorithms and combines a standard sampling approach with convex programming .	several authors have recently developed risk-sensitive policy gradient methods that augment the standard expected cost minimization problem with a measure of variability in cost .	0	4	0	5.352664	-4.7871485	0
163-7-21	several authors have recently developed risk-sensitive policy gradient methods that augment the standard expected cost minimization problem with a measure of variability in cost .	for dynamic risk measures , our approach is actor-critic style and involves explicit approximation of value function .	1	0	5	-5.99234	5.1649084	1
163-7-21	several authors have recently developed risk-sensitive policy gradient methods that augment the standard expected cost minimization problem with a measure of variability in cost .	most importantly , our contribution presents a unified approach to risk-sensitive reinforcement learning that generalizes and extends previous results .	1	0	6	-5.972533	5.102539	1
163-7-21	these studies have focused on specific risk-measures , such as the variance or conditional value at risk ( cvar ) .	in this work , we extend the policy gradient method to the whole class of coherent risk measures , which is widely accepted in finance and operations research , among other fields .	1	1	2	-5.9899983	5.2081738	1
163-7-21	these studies have focused on specific risk-measures , such as the variance or conditional value at risk ( cvar ) .	we consider both static and time-consistent dynamic risk measures .	1	1	3	-5.8634996	5.1861954	1
163-7-21	for static risk measures , our approach is in the spirit of policy gradient algorithms and combines a standard sampling approach with convex programming .	these studies have focused on specific risk-measures , such as the variance or conditional value at risk ( cvar ) .	0	4	1	5.469576	-4.7991953	0
163-7-21	for dynamic risk measures , our approach is actor-critic style and involves explicit approximation of value function .	these studies have focused on specific risk-measures , such as the variance or conditional value at risk ( cvar ) .	0	5	1	5.3357615	-4.715481	0
163-7-21	these studies have focused on specific risk-measures , such as the variance or conditional value at risk ( cvar ) .	most importantly , our contribution presents a unified approach to risk-sensitive reinforcement learning that generalizes and extends previous results .	1	1	6	-5.986703	5.100437	1
163-7-21	we consider both static and time-consistent dynamic risk measures .	in this work , we extend the policy gradient method to the whole class of coherent risk measures , which is widely accepted in finance and operations research , among other fields .	0	3	2	-1.180061	1.3287416	1
163-7-21	in this work , we extend the policy gradient method to the whole class of coherent risk measures , which is widely accepted in finance and operations research , among other fields .	for static risk measures , our approach is in the spirit of policy gradient algorithms and combines a standard sampling approach with convex programming .	1	2	4	-3.48288	3.3249843	1
163-7-21	in this work , we extend the policy gradient method to the whole class of coherent risk measures , which is widely accepted in finance and operations research , among other fields .	for dynamic risk measures , our approach is actor-critic style and involves explicit approximation of value function .	1	2	5	-2.9841962	2.934493	1
163-7-21	most importantly , our contribution presents a unified approach to risk-sensitive reinforcement learning that generalizes and extends previous results .	in this work , we extend the policy gradient method to the whole class of coherent risk measures , which is widely accepted in finance and operations research , among other fields .	0	6	2	4.634166	-4.054749	0
163-7-21	we consider both static and time-consistent dynamic risk measures .	for static risk measures , our approach is in the spirit of policy gradient algorithms and combines a standard sampling approach with convex programming .	1	3	4	-5.730706	5.1562886	1
163-7-21	for dynamic risk measures , our approach is actor-critic style and involves explicit approximation of value function .	we consider both static and time-consistent dynamic risk measures .	0	5	3	3.1227846	-2.881009	0
163-7-21	we consider both static and time-consistent dynamic risk measures .	most importantly , our contribution presents a unified approach to risk-sensitive reinforcement learning that generalizes and extends previous results .	1	3	6	-4.516202	4.19685	1
163-7-21	for dynamic risk measures , our approach is actor-critic style and involves explicit approximation of value function .	for static risk measures , our approach is in the spirit of policy gradient algorithms and combines a standard sampling approach with convex programming .	0	5	4	-2.515215	2.5592844	1
163-7-21	for static risk measures , our approach is in the spirit of policy gradient algorithms and combines a standard sampling approach with convex programming .	most importantly , our contribution presents a unified approach to risk-sensitive reinforcement learning that generalizes and extends previous results .	1	4	6	-2.988944	2.9750276	1
163-7-21	most importantly , our contribution presents a unified approach to risk-sensitive reinforcement learning that generalizes and extends previous results .	for dynamic risk measures , our approach is actor-critic style and involves explicit approximation of value function .	0	6	5	3.1015074	-2.9379869	0
164-3-3	[CLS] we consider empirical risk minimization ( erm ) in the context of stochastic optimization with exp - concave and smooth losses - - a general optimization framework that captures several important learning problems including linear and logistic regression, learning sv	[CLS] in this setting, we establish the first evidence that erm is able to attain fast generalization rates, and show that the expected loss of the erm solution in d dimensions converges to the optimal expected loss in a rate of d / n	1	0	1	-5.7917414	5.224565	1
164-3-3	[CLS] this rate matches existing lower bounds up to constants and improves by a log n factor upon the state - of - the - art, which is only known to be attained by an online - to - batch conversion of computationally expensive online algorithms.	[CLS] we consider empirical risk minimization ( erm ) in the context of stochastic optimization with exp - concave and smooth losses - - a general optimization framework that captures several important learning problems including linear and logistic regression, learning sv	0	2	0	4.60339	-4.0016866	0
164-3-3	[CLS] this rate matches existing lower bounds up to constants and improves by a log n factor upon the state - of - the - art, which is only known to be attained by an online - to - batch conversion of computationally expensive online algorithms.	[CLS] in this setting, we establish the first evidence that erm is able to attain fast generalization rates, and show that the expected loss of the erm solution in d dimensions converges to the optimal expected loss in a rate of d / n	0	2	1	4.462145	-3.8204181	0
165-6-15	in this paper we introduce a generative parametric model capable of producing high quality samples of natural images .	our approach uses a cascade of convolutional networks within a laplacian pyramid framework to generate images in a coarse-to-fine fashion .	1	0	1	-5.756893	5.1619463	1
165-6-15	at each level of the pyramid , a separate generative convnet model is trained using the generative adversarial nets ( gan ) approach .	in this paper we introduce a generative parametric model capable of producing high quality samples of natural images .	0	2	0	4.7716465	-4.282465	0
165-6-15	in this paper we introduce a generative parametric model capable of producing high quality samples of natural images .	samples drawn from our model are of significantly higher quality than alternate approaches .	1	0	3	-6.030368	5.1399393	1
165-6-15	in a quantitative assessment by human evaluators , our cifar10 samples were mistaken for real images around 40 % of the time , compared to 10 % for samples drawn from a gan baseline model .	in this paper we introduce a generative parametric model capable of producing high quality samples of natural images .	0	4	0	5.572176	-4.879816	0
165-6-15	in this paper we introduce a generative parametric model capable of producing high quality samples of natural images .	we also show samples from models trained on the higher resolution images of the lsun scene dataset .	1	0	5	-5.989946	5.181861	1
165-6-15	our approach uses a cascade of convolutional networks within a laplacian pyramid framework to generate images in a coarse-to-fine fashion .	at each level of the pyramid , a separate generative convnet model is trained using the generative adversarial nets ( gan ) approach .	1	1	2	-3.9229472	3.7422993	1
165-6-15	our approach uses a cascade of convolutional networks within a laplacian pyramid framework to generate images in a coarse-to-fine fashion .	samples drawn from our model are of significantly higher quality than alternate approaches .	1	1	3	-4.6714363	4.243696	1
165-6-15	in a quantitative assessment by human evaluators , our cifar10 samples were mistaken for real images around 40 % of the time , compared to 10 % for samples drawn from a gan baseline model .	our approach uses a cascade of convolutional networks within a laplacian pyramid framework to generate images in a coarse-to-fine fashion .	0	4	1	5.24859	-4.6451373	0
165-6-15	we also show samples from models trained on the higher resolution images of the lsun scene dataset .	our approach uses a cascade of convolutional networks within a laplacian pyramid framework to generate images in a coarse-to-fine fashion .	0	5	1	5.491516	-4.835476	0
165-6-15	samples drawn from our model are of significantly higher quality than alternate approaches .	at each level of the pyramid , a separate generative convnet model is trained using the generative adversarial nets ( gan ) approach .	0	3	2	4.2466955	-3.803276	0
165-6-15	at each level of the pyramid , a separate generative convnet model is trained using the generative adversarial nets ( gan ) approach .	in a quantitative assessment by human evaluators , our cifar10 samples were mistaken for real images around 40 % of the time , compared to 10 % for samples drawn from a gan baseline model .	1	2	4	-6.0077114	5.1715593	1
165-6-15	at each level of the pyramid , a separate generative convnet model is trained using the generative adversarial nets ( gan ) approach .	we also show samples from models trained on the higher resolution images of the lsun scene dataset .	1	2	5	-5.9565687	5.0623136	1
165-6-15	in a quantitative assessment by human evaluators , our cifar10 samples were mistaken for real images around 40 % of the time , compared to 10 % for samples drawn from a gan baseline model .	samples drawn from our model are of significantly higher quality than alternate approaches .	0	4	3	4.31789	-3.9417806	0
165-6-15	we also show samples from models trained on the higher resolution images of the lsun scene dataset .	samples drawn from our model are of significantly higher quality than alternate approaches .	0	5	3	4.469324	-4.0317483	0
165-6-15	we also show samples from models trained on the higher resolution images of the lsun scene dataset .	in a quantitative assessment by human evaluators , our cifar10 samples were mistaken for real images around 40 % of the time , compared to 10 % for samples drawn from a gan baseline model .	0	5	4	-1.4102452	1.5813723	1
166-6-15	contrary to existing approaches posing semantic segmentation as a single task of region-based classification , our algorithm decouples classification and segmentation , and learns a separate network for each task .	we propose a novel deep neural network architecture for semi-supervised semantic segmentation using heterogeneous annotations .	0	1	0	5.4596796	-4.778167	0
166-6-15	in this architecture , labels associated with an image are identified by classification network , and binary segmentation is subsequently performed for each identified label in segmentation network .	we propose a novel deep neural network architecture for semi-supervised semantic segmentation using heterogeneous annotations .	0	2	0	5.426028	-4.826234	0
166-6-15	we propose a novel deep neural network architecture for semi-supervised semantic segmentation using heterogeneous annotations .	the decoupled architecture enables us to learn classification and segmentation networks separately based on the training data with image-level and pixel-wise class labels , respectively .	1	0	3	-5.9495153	5.138549	1
166-6-15	it facilitates to reduce search space for segmentation effectively by exploiting class-specific activation maps obtained from bridging layers .	we propose a novel deep neural network architecture for semi-supervised semantic segmentation using heterogeneous annotations .	0	4	0	5.551699	-4.8783636	0
166-6-15	our algorithm shows outstanding performance compared to other semi-supervised approaches with much less training images with strong annotations in pascal voc dataset .	we propose a novel deep neural network architecture for semi-supervised semantic segmentation using heterogeneous annotations .	0	5	0	5.588294	-4.9471865	0
166-6-15	in this architecture , labels associated with an image are identified by classification network , and binary segmentation is subsequently performed for each identified label in segmentation network .	contrary to existing approaches posing semantic segmentation as a single task of region-based classification , our algorithm decouples classification and segmentation , and learns a separate network for each task .	0	2	1	-5.350672	4.9662457	1
166-6-15	the decoupled architecture enables us to learn classification and segmentation networks separately based on the training data with image-level and pixel-wise class labels , respectively .	contrary to existing approaches posing semantic segmentation as a single task of region-based classification , our algorithm decouples classification and segmentation , and learns a separate network for each task .	0	3	1	3.9166565	-3.581058	0
166-6-15	it facilitates to reduce search space for segmentation effectively by exploiting class-specific activation maps obtained from bridging layers .	contrary to existing approaches posing semantic segmentation as a single task of region-based classification , our algorithm decouples classification and segmentation , and learns a separate network for each task .	0	4	1	2.5855913	-2.4273129	0
166-6-15	contrary to existing approaches posing semantic segmentation as a single task of region-based classification , our algorithm decouples classification and segmentation , and learns a separate network for each task .	our algorithm shows outstanding performance compared to other semi-supervised approaches with much less training images with strong annotations in pascal voc dataset .	1	1	5	-5.835541	5.139034	1
166-6-15	the decoupled architecture enables us to learn classification and segmentation networks separately based on the training data with image-level and pixel-wise class labels , respectively .	in this architecture , labels associated with an image are identified by classification network , and binary segmentation is subsequently performed for each identified label in segmentation network .	0	3	2	5.3848333	-4.718708	0
166-6-15	in this architecture , labels associated with an image are identified by classification network , and binary segmentation is subsequently performed for each identified label in segmentation network .	it facilitates to reduce search space for segmentation effectively by exploiting class-specific activation maps obtained from bridging layers .	1	2	4	-5.758071	5.031413	1
166-6-15	in this architecture , labels associated with an image are identified by classification network , and binary segmentation is subsequently performed for each identified label in segmentation network .	our algorithm shows outstanding performance compared to other semi-supervised approaches with much less training images with strong annotations in pascal voc dataset .	1	2	5	-5.9591227	5.0253	1
166-6-15	it facilitates to reduce search space for segmentation effectively by exploiting class-specific activation maps obtained from bridging layers .	the decoupled architecture enables us to learn classification and segmentation networks separately based on the training data with image-level and pixel-wise class labels , respectively .	0	4	3	-4.7199864	4.4146347	1
166-6-15	the decoupled architecture enables us to learn classification and segmentation networks separately based on the training data with image-level and pixel-wise class labels , respectively .	our algorithm shows outstanding performance compared to other semi-supervised approaches with much less training images with strong annotations in pascal voc dataset .	1	3	5	-5.5782127	4.9705667	1
166-6-15	our algorithm shows outstanding performance compared to other semi-supervised approaches with much less training images with strong annotations in pascal voc dataset .	it facilitates to reduce search space for segmentation effectively by exploiting class-specific activation maps obtained from bridging layers .	0	5	4	4.890846	-4.474042	0
167-5-10	following recent work that strongly suggests that most of the critical points encountered when training such networks are saddle points , we find how considering the presence of negative eigenvalues of the hessian could help us design better suited adaptive learning rate schemes .	parameter-specific adaptive learning rate methods are computationally efficient ways to reduce the ill-conditioning problems encountered when training large deep networks .	0	1	0	5.593916	-4.9686713	0
167-5-10	parameter-specific adaptive learning rate methods are computationally efficient ways to reduce the ill-conditioning problems encountered when training large deep networks .	we show that the popular jacobi preconditioner has undesirable behavior in the presence of both positive and negative curvature , and present theoretical and empirical evidence that the socalled equilibration preconditioner is comparatively better suited to non-convex problems .	1	0	2	-5.962266	5.239748	1
167-5-10	parameter-specific adaptive learning rate methods are computationally efficient ways to reduce the ill-conditioning problems encountered when training large deep networks .	we introduce a novel adaptive learning rate scheme , called esgd , based on the equilibration preconditioner .	1	0	3	-5.9005604	5.2676973	1
167-5-10	our experiments show that esgd performs as well or better than rmsprop in terms of convergence speed , always clearly improving over plain stochastic gradient descent .	parameter-specific adaptive learning rate methods are computationally efficient ways to reduce the ill-conditioning problems encountered when training large deep networks .	0	4	0	5.534621	-4.930066	0
167-5-10	[CLS] we show that the popular jacobi preconditioner has undesirable behavior in the presence of both positive and negative curvature, and present theoretical and empirical evidence that the socalled equilibration preconditioner is comparatively better	[CLS] following recent work that strongly suggests that most of the critical points encountered when training such networks are saddle points, we find how considering the presence of negative eigenvalues of the hessian could help us design better suited adaptive learning rate schemes. [SEP]	0	2	1	-2.6549432	2.6708007	1
167-5-10	we introduce a novel adaptive learning rate scheme , called esgd , based on the equilibration preconditioner .	following recent work that strongly suggests that most of the critical points encountered when training such networks are saddle points , we find how considering the presence of negative eigenvalues of the hessian could help us design better suited adaptive learning rate schemes .	0	3	1	-4.348593	3.9794083	1
167-5-10	following recent work that strongly suggests that most of the critical points encountered when training such networks are saddle points , we find how considering the presence of negative eigenvalues of the hessian could help us design better suited adaptive learning rate schemes .	our experiments show that esgd performs as well or better than rmsprop in terms of convergence speed , always clearly improving over plain stochastic gradient descent .	1	1	4	-5.2912436	4.818551	1
167-5-10	we show that the popular jacobi preconditioner has undesirable behavior in the presence of both positive and negative curvature , and present theoretical and empirical evidence that the socalled equilibration preconditioner is comparatively better suited to non-convex problems .	we introduce a novel adaptive learning rate scheme , called esgd , based on the equilibration preconditioner .	1	2	3	2.742733	-2.4237678	0
167-5-10	we show that the popular jacobi preconditioner has undesirable behavior in the presence of both positive and negative curvature , and present theoretical and empirical evidence that the socalled equilibration preconditioner is comparatively better suited to non-convex problems .	our experiments show that esgd performs as well or better than rmsprop in terms of convergence speed , always clearly improving over plain stochastic gradient descent .	1	2	4	-5.8454227	5.0591164	1
167-5-10	our experiments show that esgd performs as well or better than rmsprop in terms of convergence speed , always clearly improving over plain stochastic gradient descent .	we introduce a novel adaptive learning rate scheme , called esgd , based on the equilibration preconditioner .	0	4	3	5.569711	-4.9337893	0
168-6-15	we propose a simple method to learn linear causal cyclic models in the presence of latent variables .	the method relies on equilibrium data of the model recorded under a specific kind of interventions ( `` shift interventions '' ) .	1	0	1	-5.888566	5.2483172	1
168-6-15	we propose a simple method to learn linear causal cyclic models in the presence of latent variables .	the location and strength of these interventions do not have to be known and can be estimated from the data .	1	0	2	-2.2017236	2.2902703	1
168-6-15	our method , called back s hift , only uses second moments of the data and performs simple joint matrix diagonalization , applied to differences between covariance matrices .	we propose a simple method to learn linear causal cyclic models in the presence of latent variables .	0	3	0	5.4678793	-4.8396254	0
168-6-15	we give a sufficient and necessary condition for identifiability of the system , which is fulfilled almost surely under some quite general assumptions if and only if there are at least three distinct experimental settings , one of which can be pure observational data .	we propose a simple method to learn linear causal cyclic models in the presence of latent variables .	0	4	0	2.939409	-2.6953106	0
168-6-15	we demonstrate the performance on some simulated data and applications in flow cytometry and financial time series .	we propose a simple method to learn linear causal cyclic models in the presence of latent variables .	0	5	0	5.577161	-4.9080634	0
168-6-15	the method relies on equilibrium data of the model recorded under a specific kind of interventions ( `` shift interventions '' ) .	the location and strength of these interventions do not have to be known and can be estimated from the data .	1	1	2	-5.5342526	4.9505405	1
168-6-15	the method relies on equilibrium data of the model recorded under a specific kind of interventions ( `` shift interventions '' ) .	our method , called back s hift , only uses second moments of the data and performs simple joint matrix diagonalization , applied to differences between covariance matrices .	1	1	3	-4.984054	4.606187	1
168-6-15	we give a sufficient and necessary condition for identifiability of the system , which is fulfilled almost surely under some quite general assumptions if and only if there are at least three distinct experimental settings , one of which can be pure observational data .	the method relies on equilibrium data of the model recorded under a specific kind of interventions ( `` shift interventions '' ) .	0	4	1	2.078078	-1.8452424	0
168-6-15	the method relies on equilibrium data of the model recorded under a specific kind of interventions ( `` shift interventions '' ) .	we demonstrate the performance on some simulated data and applications in flow cytometry and financial time series .	1	1	5	-5.99137	5.167905	1
168-6-15	our method , called back s hift , only uses second moments of the data and performs simple joint matrix diagonalization , applied to differences between covariance matrices .	the location and strength of these interventions do not have to be known and can be estimated from the data .	0	3	2	-1.2494694	1.4812338	1
168-6-15	we give a sufficient and necessary condition for identifiability of the system , which is fulfilled almost surely under some quite general assumptions if and only if there are at least three distinct experimental settings , one of which can be pure observational data .	the location and strength of these interventions do not have to be known and can be estimated from the data .	0	4	2	-3.0458434	3.0670521	1
168-6-15	we demonstrate the performance on some simulated data and applications in flow cytometry and financial time series .	the location and strength of these interventions do not have to be known and can be estimated from the data .	0	5	2	5.0664353	-4.5522537	0
168-6-15	our method , called back s hift , only uses second moments of the data and performs simple joint matrix diagonalization , applied to differences between covariance matrices .	we give a sufficient and necessary condition for identifiability of the system , which is fulfilled almost surely under some quite general assumptions if and only if there are at least three distinct experimental settings , one of which can be pure observational data .	1	3	4	2.507101	-2.3402095	0
168-6-15	our method , called back s hift , only uses second moments of the data and performs simple joint matrix diagonalization , applied to differences between covariance matrices .	we demonstrate the performance on some simulated data and applications in flow cytometry and financial time series .	1	3	5	-5.8104095	5.132055	1
168-6-15	we give a sufficient and necessary condition for identifiability of the system , which is fulfilled almost surely under some quite general assumptions if and only if there are at least three distinct experimental settings , one of which can be pure observational data .	we demonstrate the performance on some simulated data and applications in flow cytometry and financial time series .	1	4	5	-5.9374223	5.195005	1
169-8-28	in this paper we address the problem of decision making within a markov decision process ( mdp ) framework where risk and modeling errors are taken into account .	our approach is to minimize a risk-sensitive conditional-value-at-risk ( cvar ) objective , as opposed to a standard risk-neutral expectation .	1	0	1	-5.8359275	5.198276	1
169-8-28	we refer to such problem as cvar mdp .	in this paper we address the problem of decision making within a markov decision process ( mdp ) framework where risk and modeling errors are taken into account .	0	2	0	5.613023	-4.96822	0
169-8-28	our first contribution is to show that a cvar objective , besides capturing risk sensitivity , has an alternative interpretation as expected cost under worst-case modeling errors , for a given error budget .	in this paper we address the problem of decision making within a markov decision process ( mdp ) framework where risk and modeling errors are taken into account .	0	3	0	5.641139	-5.054472	0
169-8-28	in this paper we address the problem of decision making within a markov decision process ( mdp ) framework where risk and modeling errors are taken into account .	this result , which is of independent interest , motivates cvar mdps as a unifying framework for risk-sensitive and robust decision making .	1	0	4	-5.943349	5.139014	1
169-8-28	our second contribution is to present an approximate value-iteration algorithm for cvar mdps and analyze its convergence rate .	in this paper we address the problem of decision making within a markov decision process ( mdp ) framework where risk and modeling errors are taken into account .	0	5	0	5.669329	-5.1018505	0
169-8-28	to our knowledge , this is the first solution algorithm for cvar mdps that enjoys error guarantees .	in this paper we address the problem of decision making within a markov decision process ( mdp ) framework where risk and modeling errors are taken into account .	0	6	0	5.658867	-4.9989796	0
169-8-28	in this paper we address the problem of decision making within a markov decision process ( mdp ) framework where risk and modeling errors are taken into account .	finally , we present results from numerical experiments that corroborate our theoretical findings and show the practicality of our approach .	1	0	7	-5.958766	5.1533246	1
169-8-28	our approach is to minimize a risk-sensitive conditional-value-at-risk ( cvar ) objective , as opposed to a standard risk-neutral expectation .	we refer to such problem as cvar mdp .	1	1	2	-5.2071543	4.8132277	1
169-8-28	our first contribution is to show that a cvar objective , besides capturing risk sensitivity , has an alternative interpretation as expected cost under worst-case modeling errors , for a given error budget .	our approach is to minimize a risk-sensitive conditional-value-at-risk ( cvar ) objective , as opposed to a standard risk-neutral expectation .	0	3	1	5.2668843	-4.548212	0
169-8-28	our approach is to minimize a risk-sensitive conditional-value-at-risk ( cvar ) objective , as opposed to a standard risk-neutral expectation .	this result , which is of independent interest , motivates cvar mdps as a unifying framework for risk-sensitive and robust decision making .	1	1	4	-5.932192	5.201327	1
169-8-28	our second contribution is to present an approximate value-iteration algorithm for cvar mdps and analyze its convergence rate .	our approach is to minimize a risk-sensitive conditional-value-at-risk ( cvar ) objective , as opposed to a standard risk-neutral expectation .	0	5	1	5.4795265	-4.8260174	0
169-8-28	our approach is to minimize a risk-sensitive conditional-value-at-risk ( cvar ) objective , as opposed to a standard risk-neutral expectation .	to our knowledge , this is the first solution algorithm for cvar mdps that enjoys error guarantees .	1	1	6	-5.935172	5.2030106	1
169-8-28	finally , we present results from numerical experiments that corroborate our theoretical findings and show the practicality of our approach .	our approach is to minimize a risk-sensitive conditional-value-at-risk ( cvar ) objective , as opposed to a standard risk-neutral expectation .	0	7	1	5.3750095	-4.6992326	0
169-8-28	our first contribution is to show that a cvar objective , besides capturing risk sensitivity , has an alternative interpretation as expected cost under worst-case modeling errors , for a given error budget .	we refer to such problem as cvar mdp .	0	3	2	4.03886	-3.6539183	0
169-8-28	this result , which is of independent interest , motivates cvar mdps as a unifying framework for risk-sensitive and robust decision making .	we refer to such problem as cvar mdp .	0	4	2	5.344124	-4.7245417	0
169-8-28	our second contribution is to present an approximate value-iteration algorithm for cvar mdps and analyze its convergence rate .	we refer to such problem as cvar mdp .	0	5	2	5.439811	-4.8349023	0
169-8-28	to our knowledge , this is the first solution algorithm for cvar mdps that enjoys error guarantees .	we refer to such problem as cvar mdp .	0	6	2	5.2586317	-4.5848494	0
169-8-28	we refer to such problem as cvar mdp .	finally , we present results from numerical experiments that corroborate our theoretical findings and show the practicality of our approach .	1	2	7	-5.966316	5.089821	1
169-8-28	our first contribution is to show that a cvar objective , besides capturing risk sensitivity , has an alternative interpretation as expected cost under worst-case modeling errors , for a given error budget .	this result , which is of independent interest , motivates cvar mdps as a unifying framework for risk-sensitive and robust decision making .	1	3	4	-5.52524	5.062604	1
169-8-28	our first contribution is to show that a cvar objective , besides capturing risk sensitivity , has an alternative interpretation as expected cost under worst-case modeling errors , for a given error budget .	our second contribution is to present an approximate value-iteration algorithm for cvar mdps and analyze its convergence rate .	1	3	5	-5.9533176	5.2309895	1
169-8-28	to our knowledge , this is the first solution algorithm for cvar mdps that enjoys error guarantees .	our first contribution is to show that a cvar objective , besides capturing risk sensitivity , has an alternative interpretation as expected cost under worst-case modeling errors , for a given error budget .	0	6	3	3.9729855	-3.627468	0
169-8-28	our first contribution is to show that a cvar objective , besides capturing risk sensitivity , has an alternative interpretation as expected cost under worst-case modeling errors , for a given error budget .	finally , we present results from numerical experiments that corroborate our theoretical findings and show the practicality of our approach .	1	3	7	-5.975068	5.119665	1
169-8-28	our second contribution is to present an approximate value-iteration algorithm for cvar mdps and analyze its convergence rate .	this result , which is of independent interest , motivates cvar mdps as a unifying framework for risk-sensitive and robust decision making .	0	5	4	-2.497519	2.5162034	1
169-8-28	to our knowledge , this is the first solution algorithm for cvar mdps that enjoys error guarantees .	this result , which is of independent interest , motivates cvar mdps as a unifying framework for risk-sensitive and robust decision making .	0	6	4	-2.174392	2.2531178	1
169-8-28	finally , we present results from numerical experiments that corroborate our theoretical findings and show the practicality of our approach .	this result , which is of independent interest , motivates cvar mdps as a unifying framework for risk-sensitive and robust decision making .	0	7	4	4.2662373	-3.9147456	0
169-8-28	our second contribution is to present an approximate value-iteration algorithm for cvar mdps and analyze its convergence rate .	to our knowledge , this is the first solution algorithm for cvar mdps that enjoys error guarantees .	1	5	6	-2.395649	2.5119267	1
169-8-28	finally , we present results from numerical experiments that corroborate our theoretical findings and show the practicality of our approach .	our second contribution is to present an approximate value-iteration algorithm for cvar mdps and analyze its convergence rate .	0	7	5	5.0735407	-4.433816	0
169-8-28	to our knowledge , this is the first solution algorithm for cvar mdps that enjoys error guarantees .	finally , we present results from numerical experiments that corroborate our theoretical findings and show the practicality of our approach .	1	6	7	-5.3835707	4.886362	1
170-4-6	roughly , the noise inherent to the stochastic approximation scheme dominates any noise from asynchrony .	we show that asymptotically , completely asynchronous stochastic gradient procedures achieve optimal ( even to constant factors ) convergence rates for the solution of convex optimization problems under nearly the same conditions required for asymptotic optimality of standard stochastic gradient procedures .	0	1	0	-3.9791617	3.7809196	1
170-4-6	[CLS] we also give empirical evidence demonstrating the strong performance of asynchronous, parallel stochastic optimization schemes, demonstrating that the robustness inherent to stochastic approximation problems allows substantially faster parallel and asynchronous solution methods. [SEP]	[CLS] we show that asymptotically, completely asynchronous stochastic gradient procedures achieve optimal ( even to constant factors ) convergence rates for the solution of convex optimization problems under nearly the same conditions required for asymptotic optimal	0	2	0	5.113282	-4.497616	0
170-4-6	in short , we show that for many stochastic approximation problems , as freddie mercury sings in queen 's bohemian rhapsody , `` nothing really matters . ''	we show that asymptotically , completely asynchronous stochastic gradient procedures achieve optimal ( even to constant factors ) convergence rates for the solution of convex optimization problems under nearly the same conditions required for asymptotic optimality of standard stochastic gradient procedures .	0	3	0	-3.2559905	3.2888734	1
170-4-6	roughly , the noise inherent to the stochastic approximation scheme dominates any noise from asynchrony .	we also give empirical evidence demonstrating the strong performance of asynchronous , parallel stochastic optimization schemes , demonstrating that the robustness inherent to stochastic approximation problems allows substantially faster parallel and asynchronous solution methods .	1	1	2	-5.912928	5.1898093	1
170-4-6	in short , we show that for many stochastic approximation problems , as freddie mercury sings in queen 's bohemian rhapsody , `` nothing really matters . ''	roughly , the noise inherent to the stochastic approximation scheme dominates any noise from asynchrony .	0	3	1	-3.994632	3.8782775	1
170-4-6	we also give empirical evidence demonstrating the strong performance of asynchronous , parallel stochastic optimization schemes , demonstrating that the robustness inherent to stochastic approximation problems allows substantially faster parallel and asynchronous solution methods .	in short , we show that for many stochastic approximation problems , as freddie mercury sings in queen 's bohemian rhapsody , `` nothing really matters . ''	1	2	3	5.3052263	-4.699215	0
171-9-36	previous work analyzing this scenario is based on the assumption that learning tasks are sampled i.i.d .	in this work we aim at extending the theoretical foundations of lifelong learning .	0	1	0	3.566711	-3.1926632	0
171-9-36	from a task environment or limited to strongly constrained data distributions .	in this work we aim at extending the theoretical foundations of lifelong learning .	0	2	0	4.3089685	-3.895425	0
171-9-36	in this work we aim at extending the theoretical foundations of lifelong learning .	instead , we study two scenarios when lifelong learning is possible , even though the observed tasks do not form an i.i.d .	1	0	3	-5.983263	5.1057043	1
171-9-36	sample : first , when they are sampled from the same environment , but possibly with dependencies , and second , when the task environment is allowed to change over time in a consistent way .	in this work we aim at extending the theoretical foundations of lifelong learning .	0	4	0	5.3926907	-4.8495007	0
171-9-36	in this work we aim at extending the theoretical foundations of lifelong learning .	in the first case we prove a pac-bayesian theorem that can be seen as a direct generalization of the analogous previous result for the i.i.d .	1	0	5	-5.998348	5.171177	1
171-9-36	case .	in this work we aim at extending the theoretical foundations of lifelong learning .	0	6	0	5.129763	-4.616391	0
171-9-36	in this work we aim at extending the theoretical foundations of lifelong learning .	for the second scenario we propose to learn an inductive bias in form of a transfer procedure .	1	0	7	-5.969739	5.087364	1
171-9-36	in this work we aim at extending the theoretical foundations of lifelong learning .	we present a generalization bound and show on a toy example how it can be used to identify a beneficial transfer algorithm .	1	0	8	-5.955432	5.085815	1
171-9-36	previous work analyzing this scenario is based on the assumption that learning tasks are sampled i.i.d .	from a task environment or limited to strongly constrained data distributions .	1	1	2	-3.5893059	3.3914428	1
171-9-36	previous work analyzing this scenario is based on the assumption that learning tasks are sampled i.i.d .	instead , we study two scenarios when lifelong learning is possible , even though the observed tasks do not form an i.i.d .	1	1	3	-4.387595	4.245063	1
171-9-36	previous work analyzing this scenario is based on the assumption that learning tasks are sampled i.i.d .	sample : first , when they are sampled from the same environment , but possibly with dependencies , and second , when the task environment is allowed to change over time in a consistent way .	1	1	4	-5.718975	5.224845	1
171-9-36	in the first case we prove a pac-bayesian theorem that can be seen as a direct generalization of the analogous previous result for the i.i.d .	previous work analyzing this scenario is based on the assumption that learning tasks are sampled i.i.d .	0	5	1	5.1582246	-4.5351806	0
171-9-36	previous work analyzing this scenario is based on the assumption that learning tasks are sampled i.i.d .	case .	1	1	6	-4.57213	4.345078	1
171-9-36	for the second scenario we propose to learn an inductive bias in form of a transfer procedure .	previous work analyzing this scenario is based on the assumption that learning tasks are sampled i.i.d .	0	7	1	4.979428	-4.434202	0
171-9-36	we present a generalization bound and show on a toy example how it can be used to identify a beneficial transfer algorithm .	previous work analyzing this scenario is based on the assumption that learning tasks are sampled i.i.d .	0	8	1	5.29889	-4.691023	0
171-9-36	instead , we study two scenarios when lifelong learning is possible , even though the observed tasks do not form an i.i.d .	from a task environment or limited to strongly constrained data distributions .	0	3	2	-2.0431657	2.0090318	1
171-9-36	from a task environment or limited to strongly constrained data distributions .	sample : first , when they are sampled from the same environment , but possibly with dependencies , and second , when the task environment is allowed to change over time in a consistent way .	1	2	4	-1.3400794	1.4433746	1
171-9-36	in the first case we prove a pac-bayesian theorem that can be seen as a direct generalization of the analogous previous result for the i.i.d .	from a task environment or limited to strongly constrained data distributions .	0	5	2	3.0945392	-2.8964925	0
171-9-36	from a task environment or limited to strongly constrained data distributions .	case .	1	2	6	-2.494132	2.4909692	1
171-9-36	for the second scenario we propose to learn an inductive bias in form of a transfer procedure .	from a task environment or limited to strongly constrained data distributions .	0	7	2	4.0150237	-3.7207096	0
171-9-36	we present a generalization bound and show on a toy example how it can be used to identify a beneficial transfer algorithm .	from a task environment or limited to strongly constrained data distributions .	0	8	2	4.610969	-4.1844716	0
171-9-36	instead , we study two scenarios when lifelong learning is possible , even though the observed tasks do not form an i.i.d .	sample : first , when they are sampled from the same environment , but possibly with dependencies , and second , when the task environment is allowed to change over time in a consistent way .	1	3	4	-4.6010027	4.231331	1
171-9-36	in the first case we prove a pac-bayesian theorem that can be seen as a direct generalization of the analogous previous result for the i.i.d .	instead , we study two scenarios when lifelong learning is possible , even though the observed tasks do not form an i.i.d .	0	5	3	4.3919134	-3.8550017	0
171-9-36	instead , we study two scenarios when lifelong learning is possible , even though the observed tasks do not form an i.i.d .	case .	1	3	6	-3.0823526	2.901078	1
171-9-36	for the second scenario we propose to learn an inductive bias in form of a transfer procedure .	instead , we study two scenarios when lifelong learning is possible , even though the observed tasks do not form an i.i.d .	0	7	3	5.1138473	-4.539152	0
171-9-36	we present a generalization bound and show on a toy example how it can be used to identify a beneficial transfer algorithm .	instead , we study two scenarios when lifelong learning is possible , even though the observed tasks do not form an i.i.d .	0	8	3	4.2435	-3.8749795	0
171-9-36	in the first case we prove a pac-bayesian theorem that can be seen as a direct generalization of the analogous previous result for the i.i.d .	sample : first , when they are sampled from the same environment , but possibly with dependencies , and second , when the task environment is allowed to change over time in a consistent way .	0	5	4	2.9341173	-2.7462473	0
171-9-36	sample : first , when they are sampled from the same environment , but possibly with dependencies , and second , when the task environment is allowed to change over time in a consistent way .	case .	1	4	6	-1.9789687	2.0974553	1
171-9-36	sample : first , when they are sampled from the same environment , but possibly with dependencies , and second , when the task environment is allowed to change over time in a consistent way .	for the second scenario we propose to learn an inductive bias in form of a transfer procedure .	1	4	7	-5.838793	5.118121	1
171-9-36	we present a generalization bound and show on a toy example how it can be used to identify a beneficial transfer algorithm .	sample : first , when they are sampled from the same environment , but possibly with dependencies , and second , when the task environment is allowed to change over time in a consistent way .	0	8	4	4.6814537	-4.194173	0
171-9-36	case .	in the first case we prove a pac-bayesian theorem that can be seen as a direct generalization of the analogous previous result for the i.i.d .	0	6	5	2.48316	-2.1376264	0
171-9-36	for the second scenario we propose to learn an inductive bias in form of a transfer procedure .	in the first case we prove a pac-bayesian theorem that can be seen as a direct generalization of the analogous previous result for the i.i.d .	0	7	5	4.7231894	-4.1653614	0
171-9-36	we present a generalization bound and show on a toy example how it can be used to identify a beneficial transfer algorithm .	in the first case we prove a pac-bayesian theorem that can be seen as a direct generalization of the analogous previous result for the i.i.d .	0	8	5	3.432252	-3.2245507	0
171-9-36	for the second scenario we propose to learn an inductive bias in form of a transfer procedure .	case .	0	7	6	2.9665985	-2.7785497	0
171-9-36	case .	we present a generalization bound and show on a toy example how it can be used to identify a beneficial transfer algorithm .	1	6	8	-3.9811938	3.6635432	1
171-9-36	for the second scenario we propose to learn an inductive bias in form of a transfer procedure .	we present a generalization bound and show on a toy example how it can be used to identify a beneficial transfer algorithm .	1	7	8	-3.5675704	3.4656172	1
172-6-15	linear regression studies the problem of estimating a model parameter rp , from n observations { ( yi , xi ) } ni=1 from linear model yi = xi , + i .	we consider a significant generalization in which the relationship between xi , and yi is noisy , quantized to a single bit , potentially nonlinear , noninvertible , as well as unknown .	1	0	1	-5.786359	5.2240806	1
172-6-15	linear regression studies the problem of estimating a model parameter rp , from n observations { ( yi , xi ) } ni=1 from linear model yi = xi , + i .	this model is known as the single-index model in statistics , and , among other things , it represents a significant generalization of one-bit compressed sensing .	1	0	2	-5.807345	5.2459784	1
172-6-15	linear regression studies the problem of estimating a model parameter rp , from n observations { ( yi , xi ) } ni=1 from linear model yi = xi , + i .	we propose a novel spectral-based estimation procedure and show that we can recover in settings ( i.e. , classes of link function f ) where previous algorithms fail .	1	0	3	-5.91064	5.244479	1
172-6-15	linear regression studies the problem of estimating a model parameter rp , from n observations { ( yi , xi ) } ni=1 from linear model yi = xi , + i .	in general , our algorithm requires only very mild restrictions on the ( unknown ) functional relationship between yi and xi , .	1	0	4	-5.8872523	5.1930766	1
172-6-15	[CLS] we also consider the high dimensional setting where is sparse, and introduce a two - stage nonconvex framework that addresses estimation challenges in high dimensional regimes where p n. for a broad class of link functions between xi, and yi, we establish minimax lower bounds that demonstrate the optimality of our	linear regression studies the problem of estimating a model parameter rp , from n observations { ( yi , xi ) } ni=1 from linear model yi = xi , + i .	0	5	0	5.5939126	-4.961286	0
172-6-15	we consider a significant generalization in which the relationship between xi , and yi is noisy , quantized to a single bit , potentially nonlinear , noninvertible , as well as unknown .	this model is known as the single-index model in statistics , and , among other things , it represents a significant generalization of one-bit compressed sensing .	1	1	2	-5.7330017	5.1232414	1
172-6-15	we consider a significant generalization in which the relationship between xi , and yi is noisy , quantized to a single bit , potentially nonlinear , noninvertible , as well as unknown .	we propose a novel spectral-based estimation procedure and show that we can recover in settings ( i.e. , classes of link function f ) where previous algorithms fail .	1	1	3	-5.984708	5.165948	1
172-6-15	in general , our algorithm requires only very mild restrictions on the ( unknown ) functional relationship between yi and xi , .	we consider a significant generalization in which the relationship between xi , and yi is noisy , quantized to a single bit , potentially nonlinear , noninvertible , as well as unknown .	0	4	1	3.5487285	-3.254137	0
172-6-15	we consider a significant generalization in which the relationship between xi , and yi is noisy , quantized to a single bit , potentially nonlinear , noninvertible , as well as unknown .	[CLS] we also consider the high dimensional setting where is sparse, and introduce a two - stage nonconvex framework that addresses estimation challenges in high dimensional regimes where p n. for a broad class of link functions between xi, and yi, we establish minimax lower bounds that demonstrate the optimality of	1	1	5	-5.963887	5.1258	1
172-6-15	this model is known as the single-index model in statistics , and , among other things , it represents a significant generalization of one-bit compressed sensing .	we propose a novel spectral-based estimation procedure and show that we can recover in settings ( i.e. , classes of link function f ) where previous algorithms fail .	1	2	3	-3.1366792	3.1532803	1
172-6-15	this model is known as the single-index model in statistics , and , among other things , it represents a significant generalization of one-bit compressed sensing .	in general , our algorithm requires only very mild restrictions on the ( unknown ) functional relationship between yi and xi , .	1	2	4	-3.0721285	3.1068807	1
172-6-15	this model is known as the single-index model in statistics , and , among other things , it represents a significant generalization of one-bit compressed sensing .	[CLS] we also consider the high dimensional setting where is sparse, and introduce a two - stage nonconvex framework that addresses estimation challenges in high dimensional regimes where p n. for a broad class of link functions between xi, and yi, we establish minimax lower bounds that demonstrate the optimality of our estimators in both	1	2	5	-5.635812	5.0325522	1
172-6-15	in general , our algorithm requires only very mild restrictions on the ( unknown ) functional relationship between yi and xi , .	we propose a novel spectral-based estimation procedure and show that we can recover in settings ( i.e. , classes of link function f ) where previous algorithms fail .	0	4	3	-0.18611932	0.40640038	1
172-6-15	[CLS] we also consider the high dimensional setting where is sparse, and introduce a two - stage nonconvex framework that addresses estimation challenges in high dimensional regimes where p n. for a broad class of link functions between xi, and yi, we establish minimax lower bounds that demonstrate the optimality of our estimators	we propose a novel spectral-based estimation procedure and show that we can recover in settings ( i.e. , classes of link function f ) where previous algorithms fail .	0	5	3	4.226269	-3.771941	0
172-6-15	in general , our algorithm requires only very mild restrictions on the ( unknown ) functional relationship between yi and xi , .	we also consider the high dimensional setting where is sparse , and introduce a two-stage nonconvex framework that addresses estimation challenges in high dimensional regimes where p n. for a broad class of link functions between xi , and yi , we establish minimax lower bounds that demonstrate the optimality of our estimators in both the classical and high dimensional regimes .	1	4	5	-5.5581884	5.04984	1
173-6-15	more specifically , a group invariant signal signature is obtained through cumulative distributions of group-transformed random projections .	we analyze in this paper a random feature map based on a theory of invariance ( i-theory ) introduced in .	0	1	0	3.7420466	-3.3687823	0
173-6-15	our analysis bridges invariant feature learning with kernel methods , as we show that this feature map defines an expected haar-integration kernel that is invariant to the specified group action .	we analyze in this paper a random feature map based on a theory of invariance ( i-theory ) introduced in .	0	2	0	5.5851436	-4.9684405	0
173-6-15	we analyze in this paper a random feature map based on a theory of invariance ( i-theory ) introduced in .	we show how this non-linear random feature map approximates this group invariant kernel uniformly on a set of n points .	1	0	3	-5.5474215	5.0916405	1
173-6-15	we analyze in this paper a random feature map based on a theory of invariance ( i-theory ) introduced in .	moreover , we show that it defines a function space that is dense in the equivalent invariant reproducing kernel hilbert space .	1	0	4	-5.9154396	5.196247	1
173-6-15	finally , we quantify error rates of the convergence of the empirical risk minimization , as well as the reduction in the sample complexity of a learning algorithm using such an invariant representation for signal classification , in a classical supervised learning setting .	we analyze in this paper a random feature map based on a theory of invariance ( i-theory ) introduced in .	0	5	0	5.561781	-4.915106	0
173-6-15	more specifically , a group invariant signal signature is obtained through cumulative distributions of group-transformed random projections .	our analysis bridges invariant feature learning with kernel methods , as we show that this feature map defines an expected haar-integration kernel that is invariant to the specified group action .	1	1	2	-2.1124346	2.242774	1
173-6-15	more specifically , a group invariant signal signature is obtained through cumulative distributions of group-transformed random projections .	we show how this non-linear random feature map approximates this group invariant kernel uniformly on a set of n points .	1	1	3	-2.2592385	2.342085	1
173-6-15	more specifically , a group invariant signal signature is obtained through cumulative distributions of group-transformed random projections .	moreover , we show that it defines a function space that is dense in the equivalent invariant reproducing kernel hilbert space .	1	1	4	-2.6009583	2.71142	1
173-6-15	finally , we quantify error rates of the convergence of the empirical risk minimization , as well as the reduction in the sample complexity of a learning algorithm using such an invariant representation for signal classification , in a classical supervised learning setting .	more specifically , a group invariant signal signature is obtained through cumulative distributions of group-transformed random projections .	0	5	1	5.200733	-4.6233754	0
173-6-15	we show how this non-linear random feature map approximates this group invariant kernel uniformly on a set of n points .	our analysis bridges invariant feature learning with kernel methods , as we show that this feature map defines an expected haar-integration kernel that is invariant to the specified group action .	0	3	2	-3.9784777	3.763022	1
173-6-15	moreover , we show that it defines a function space that is dense in the equivalent invariant reproducing kernel hilbert space .	our analysis bridges invariant feature learning with kernel methods , as we show that this feature map defines an expected haar-integration kernel that is invariant to the specified group action .	0	4	2	2.980491	-2.7768607	0
173-6-15	our analysis bridges invariant feature learning with kernel methods , as we show that this feature map defines an expected haar-integration kernel that is invariant to the specified group action .	finally , we quantify error rates of the convergence of the empirical risk minimization , as well as the reduction in the sample complexity of a learning algorithm using such an invariant representation for signal classification , in a classical supervised learning setting .	1	2	5	-5.899081	5.0413613	1
173-6-15	we show how this non-linear random feature map approximates this group invariant kernel uniformly on a set of n points .	moreover , we show that it defines a function space that is dense in the equivalent invariant reproducing kernel hilbert space .	1	3	4	-4.017606	3.8883405	1
173-6-15	we show how this non-linear random feature map approximates this group invariant kernel uniformly on a set of n points .	finally , we quantify error rates of the convergence of the empirical risk minimization , as well as the reduction in the sample complexity of a learning algorithm using such an invariant representation for signal classification , in a classical supervised learning setting .	1	3	5	-5.923832	5.0723763	1
173-6-15	moreover , we show that it defines a function space that is dense in the equivalent invariant reproducing kernel hilbert space .	finally , we quantify error rates of the convergence of the empirical risk minimization , as well as the reduction in the sample complexity of a learning algorithm using such an invariant representation for signal classification , in a classical supervised learning setting .	1	4	5	-5.5605025	5.0334463	1
174-9-36	the popular em algorithm and its variants , is a much used algorithmic tool ; yet our rigorous understanding of its performance is highly incomplete .	latent models are a fundamental modeling tool in machine learning applications , but they present significant computational and analytical challenges .	0	1	0	5.486348	-4.871333	0
174-9-36	recently , work in has demonstrated that for an important class of problems , em exhibits linear local convergence .	latent models are a fundamental modeling tool in machine learning applications , but they present significant computational and analytical challenges .	0	2	0	5.645057	-5.046829	0
174-9-36	in the high-dimensional setting , however , the m -step may not be well defined .	latent models are a fundamental modeling tool in machine learning applications , but they present significant computational and analytical challenges .	0	3	0	5.565749	-4.89214	0
174-9-36	latent models are a fundamental modeling tool in machine learning applications , but they present significant computational and analytical challenges .	we address precisely this setting through a unified treatment using regularization .	1	0	4	-5.967696	5.212862	1
174-9-36	latent models are a fundamental modeling tool in machine learning applications , but they present significant computational and analytical challenges .	while regularization for high-dimensional problems is by now well understood , the iterative em algorithm requires a careful balancing of making progress towards the solution while identifying the right structure ( e.g. , sparsity or low-rank ) .	1	0	5	-5.836688	5.2601004	1
174-9-36	latent models are a fundamental modeling tool in machine learning applications , but they present significant computational and analytical challenges .	in particular , regularizing the m -step using the state-of-the-art highdimensional prescriptions ( e.g. , a la ) is not guaranteed to provide this balance .	1	0	6	-5.9214697	5.2218475	1
174-9-36	latent models are a fundamental modeling tool in machine learning applications , but they present significant computational and analytical challenges .	our algorithm and analysis are linked in a way that reveals the balance between optimization and statistical errors .	1	0	7	-5.943205	5.123088	1
174-9-36	latent models are a fundamental modeling tool in machine learning applications , but they present significant computational and analytical challenges .	we specialize our general framework to sparse gaussian mixture models , high-dimensional mixed regression , and regression with missing variables , obtaining statistical guarantees for each of these examples .	1	0	8	-5.9979186	5.1531477	1
174-9-36	recently , work in has demonstrated that for an important class of problems , em exhibits linear local convergence .	the popular em algorithm and its variants , is a much used algorithmic tool ; yet our rigorous understanding of its performance is highly incomplete .	0	2	1	2.38707	-1.9717865	0
174-9-36	the popular em algorithm and its variants , is a much used algorithmic tool ; yet our rigorous understanding of its performance is highly incomplete .	in the high-dimensional setting , however , the m -step may not be well defined .	1	1	3	-4.0843463	3.8532598	1
174-9-36	we address precisely this setting through a unified treatment using regularization .	the popular em algorithm and its variants , is a much used algorithmic tool ; yet our rigorous understanding of its performance is highly incomplete .	0	4	1	5.113518	-4.551256	0
174-9-36	while regularization for high-dimensional problems is by now well understood , the iterative em algorithm requires a careful balancing of making progress towards the solution while identifying the right structure ( e.g. , sparsity or low-rank ) .	the popular em algorithm and its variants , is a much used algorithmic tool ; yet our rigorous understanding of its performance is highly incomplete .	0	5	1	3.7363434	-3.2520642	0
174-9-36	the popular em algorithm and its variants , is a much used algorithmic tool ; yet our rigorous understanding of its performance is highly incomplete .	in particular , regularizing the m -step using the state-of-the-art highdimensional prescriptions ( e.g. , a la ) is not guaranteed to provide this balance .	1	1	6	-5.6222653	5.150035	1
174-9-36	the popular em algorithm and its variants , is a much used algorithmic tool ; yet our rigorous understanding of its performance is highly incomplete .	our algorithm and analysis are linked in a way that reveals the balance between optimization and statistical errors .	1	1	7	-5.9860525	5.232932	1
174-9-36	we specialize our general framework to sparse gaussian mixture models , high-dimensional mixed regression , and regression with missing variables , obtaining statistical guarantees for each of these examples .	the popular em algorithm and its variants , is a much used algorithmic tool ; yet our rigorous understanding of its performance is highly incomplete .	0	8	1	5.037485	-4.469763	0
174-9-36	recently , work in has demonstrated that for an important class of problems , em exhibits linear local convergence .	in the high-dimensional setting , however , the m -step may not be well defined .	1	2	3	-4.9485955	4.677472	1
174-9-36	we address precisely this setting through a unified treatment using regularization .	recently , work in has demonstrated that for an important class of problems , em exhibits linear local convergence .	0	4	2	5.093047	-4.5187826	0
174-9-36	recently , work in has demonstrated that for an important class of problems , em exhibits linear local convergence .	while regularization for high-dimensional problems is by now well understood , the iterative em algorithm requires a careful balancing of making progress towards the solution while identifying the right structure ( e.g. , sparsity or low-rank ) .	1	2	5	-3.3690195	3.340932	1
174-9-36	in particular , regularizing the m -step using the state-of-the-art highdimensional prescriptions ( e.g. , a la ) is not guaranteed to provide this balance .	recently , work in has demonstrated that for an important class of problems , em exhibits linear local convergence .	0	6	2	4.612134	-4.102723	0
174-9-36	our algorithm and analysis are linked in a way that reveals the balance between optimization and statistical errors .	recently , work in has demonstrated that for an important class of problems , em exhibits linear local convergence .	0	7	2	5.381526	-4.761638	0
174-9-36	we specialize our general framework to sparse gaussian mixture models , high-dimensional mixed regression , and regression with missing variables , obtaining statistical guarantees for each of these examples .	recently , work in has demonstrated that for an important class of problems , em exhibits linear local convergence .	0	8	2	5.1479216	-4.5612926	0
174-9-36	in the high-dimensional setting , however , the m -step may not be well defined .	we address precisely this setting through a unified treatment using regularization .	1	3	4	-5.924748	5.155022	1
174-9-36	in the high-dimensional setting , however , the m -step may not be well defined .	while regularization for high-dimensional problems is by now well understood , the iterative em algorithm requires a careful balancing of making progress towards the solution while identifying the right structure ( e.g. , sparsity or low-rank ) .	1	3	5	2.856551	-2.5749032	0
174-9-36	in the high-dimensional setting , however , the m -step may not be well defined .	in particular , regularizing the m -step using the state-of-the-art highdimensional prescriptions ( e.g. , a la ) is not guaranteed to provide this balance .	1	3	6	-4.9662304	4.588375	1
174-9-36	in the high-dimensional setting , however , the m -step may not be well defined .	our algorithm and analysis are linked in a way that reveals the balance between optimization and statistical errors .	1	3	7	-5.9728665	5.1966143	1
174-9-36	in the high-dimensional setting , however , the m -step may not be well defined .	we specialize our general framework to sparse gaussian mixture models , high-dimensional mixed regression , and regression with missing variables , obtaining statistical guarantees for each of these examples .	1	3	8	-5.9481406	5.2105637	1
174-9-36	we address precisely this setting through a unified treatment using regularization .	while regularization for high-dimensional problems is by now well understood , the iterative em algorithm requires a careful balancing of making progress towards the solution while identifying the right structure ( e.g. , sparsity or low-rank ) .	1	4	5	3.3517158	-3.0730996	0
174-9-36	in particular , regularizing the m -step using the state-of-the-art highdimensional prescriptions ( e.g. , a la ) is not guaranteed to provide this balance .	we address precisely this setting through a unified treatment using regularization .	0	6	4	-3.8973043	3.7925034	1
174-9-36	we address precisely this setting through a unified treatment using regularization .	our algorithm and analysis are linked in a way that reveals the balance between optimization and statistical errors .	1	4	7	-5.668805	5.041295	1
174-9-36	we specialize our general framework to sparse gaussian mixture models , high-dimensional mixed regression , and regression with missing variables , obtaining statistical guarantees for each of these examples .	we address precisely this setting through a unified treatment using regularization .	0	8	4	3.2303572	-3.0227547	0
174-9-36	in particular , regularizing the m -step using the state-of-the-art highdimensional prescriptions ( e.g. , a la ) is not guaranteed to provide this balance .	while regularization for high-dimensional problems is by now well understood , the iterative em algorithm requires a careful balancing of making progress towards the solution while identifying the right structure ( e.g. , sparsity or low-rank ) .	0	6	5	4.3989897	-3.9066107	0
174-9-36	while regularization for high-dimensional problems is by now well understood , the iterative em algorithm requires a careful balancing of making progress towards the solution while identifying the right structure ( e.g. , sparsity or low-rank ) .	our algorithm and analysis are linked in a way that reveals the balance between optimization and statistical errors .	1	5	7	-5.9123907	5.0208044	1
174-9-36	while regularization for high-dimensional problems is by now well understood , the iterative em algorithm requires a careful balancing of making progress towards the solution while identifying the right structure ( e.g. , sparsity or low-rank ) .	we specialize our general framework to sparse gaussian mixture models , high-dimensional mixed regression , and regression with missing variables , obtaining statistical guarantees for each of these examples .	1	5	8	-5.9277525	5.1254654	1
174-9-36	our algorithm and analysis are linked in a way that reveals the balance between optimization and statistical errors .	in particular , regularizing the m -step using the state-of-the-art highdimensional prescriptions ( e.g. , a la ) is not guaranteed to provide this balance .	0	7	6	4.4554424	-4.024989	0
174-9-36	we specialize our general framework to sparse gaussian mixture models , high-dimensional mixed regression , and regression with missing variables , obtaining statistical guarantees for each of these examples .	in particular , regularizing the m -step using the state-of-the-art highdimensional prescriptions ( e.g. , a la ) is not guaranteed to provide this balance .	0	8	6	3.8134017	-3.5267012	0
174-9-36	our algorithm and analysis are linked in a way that reveals the balance between optimization and statistical errors .	we specialize our general framework to sparse gaussian mixture models , high-dimensional mixed regression , and regression with missing variables , obtaining statistical guarantees for each of these examples .	1	7	8	3.4040031	-3.1472523	0
175-8-28	this paper proposes a distributionally robust approach to logistic regression .	we use the wasserstein distance to construct a ball in the space of probability distributions centered at the uniform distribution on the training samples .	1	0	1	-5.87481	5.152602	1
175-8-28	if the radius of this ball is chosen judiciously , we can guarantee that it contains the unknown datagenerating distribution with high confidence .	this paper proposes a distributionally robust approach to logistic regression .	0	2	0	5.1680675	-4.673707	0
175-8-28	we then formulate a distributionally robust logistic regression model that minimizes a worst-case expected logloss function , where the worst case is taken over all distributions in the wasserstein ball .	this paper proposes a distributionally robust approach to logistic regression .	0	3	0	5.605262	-5.015882	0
175-8-28	we prove that this optimization problem admits a tractable reformulation and encapsulates the classical as well as the popular regularized logistic regression problems as special cases .	this paper proposes a distributionally robust approach to logistic regression .	0	4	0	5.4959636	-4.915559	0
175-8-28	this paper proposes a distributionally robust approach to logistic regression .	we further propose a distributionally robust approach based on wasserstein balls to compute upper and lower confidence bounds on the misclassification probability of the resulting classifier .	1	0	5	-5.9622755	5.1255007	1
175-8-28	these bounds are given by the optimal values of two highly tractable linear programs .	this paper proposes a distributionally robust approach to logistic regression .	0	6	0	5.1088133	-4.5775576	0
175-8-28	this paper proposes a distributionally robust approach to logistic regression .	we validate our theoretical out-of-sample guarantees through simulated and empirical experiments .	1	0	7	-5.9461193	5.0740395	1
175-8-28	we use the wasserstein distance to construct a ball in the space of probability distributions centered at the uniform distribution on the training samples .	if the radius of this ball is chosen judiciously , we can guarantee that it contains the unknown datagenerating distribution with high confidence .	1	1	2	-5.631197	5.090864	1
175-8-28	we use the wasserstein distance to construct a ball in the space of probability distributions centered at the uniform distribution on the training samples .	we then formulate a distributionally robust logistic regression model that minimizes a worst-case expected logloss function , where the worst case is taken over all distributions in the wasserstein ball .	1	1	3	-5.2680273	4.8680477	1
175-8-28	we prove that this optimization problem admits a tractable reformulation and encapsulates the classical as well as the popular regularized logistic regression problems as special cases .	we use the wasserstein distance to construct a ball in the space of probability distributions centered at the uniform distribution on the training samples .	0	4	1	2.6841123	-2.4843683	0
175-8-28	we use the wasserstein distance to construct a ball in the space of probability distributions centered at the uniform distribution on the training samples .	we further propose a distributionally robust approach based on wasserstein balls to compute upper and lower confidence bounds on the misclassification probability of the resulting classifier .	1	1	5	-5.7228546	5.0854387	1
175-8-28	these bounds are given by the optimal values of two highly tractable linear programs .	we use the wasserstein distance to construct a ball in the space of probability distributions centered at the uniform distribution on the training samples .	0	6	1	0.07171275	0.16804059	1
175-8-28	we use the wasserstein distance to construct a ball in the space of probability distributions centered at the uniform distribution on the training samples .	we validate our theoretical out-of-sample guarantees through simulated and empirical experiments .	1	1	7	-5.8333406	5.1638775	1
175-8-28	if the radius of this ball is chosen judiciously , we can guarantee that it contains the unknown datagenerating distribution with high confidence .	we then formulate a distributionally robust logistic regression model that minimizes a worst-case expected logloss function , where the worst case is taken over all distributions in the wasserstein ball .	1	2	3	-3.4146109	3.4419503	1
175-8-28	if the radius of this ball is chosen judiciously , we can guarantee that it contains the unknown datagenerating distribution with high confidence .	we prove that this optimization problem admits a tractable reformulation and encapsulates the classical as well as the popular regularized logistic regression problems as special cases .	1	2	4	-4.802497	4.5084157	1
175-8-28	we further propose a distributionally robust approach based on wasserstein balls to compute upper and lower confidence bounds on the misclassification probability of the resulting classifier .	if the radius of this ball is chosen judiciously , we can guarantee that it contains the unknown datagenerating distribution with high confidence .	0	5	2	3.8889093	-3.6099665	0
175-8-28	if the radius of this ball is chosen judiciously , we can guarantee that it contains the unknown datagenerating distribution with high confidence .	these bounds are given by the optimal values of two highly tractable linear programs .	1	2	6	-2.898786	2.9616876	1
175-8-28	if the radius of this ball is chosen judiciously , we can guarantee that it contains the unknown datagenerating distribution with high confidence .	we validate our theoretical out-of-sample guarantees through simulated and empirical experiments .	1	2	7	-5.948764	5.162039	1
175-8-28	we then formulate a distributionally robust logistic regression model that minimizes a worst-case expected logloss function , where the worst case is taken over all distributions in the wasserstein ball .	we prove that this optimization problem admits a tractable reformulation and encapsulates the classical as well as the popular regularized logistic regression problems as special cases .	1	3	4	-2.8273926	2.8417037	1
175-8-28	we then formulate a distributionally robust logistic regression model that minimizes a worst-case expected logloss function , where the worst case is taken over all distributions in the wasserstein ball .	we further propose a distributionally robust approach based on wasserstein balls to compute upper and lower confidence bounds on the misclassification probability of the resulting classifier .	1	3	5	-4.1738796	3.9955678	1
175-8-28	these bounds are given by the optimal values of two highly tractable linear programs .	we then formulate a distributionally robust logistic regression model that minimizes a worst-case expected logloss function , where the worst case is taken over all distributions in the wasserstein ball .	0	6	3	-2.4614177	2.6879754	1
175-8-28	we then formulate a distributionally robust logistic regression model that minimizes a worst-case expected logloss function , where the worst case is taken over all distributions in the wasserstein ball .	we validate our theoretical out-of-sample guarantees through simulated and empirical experiments .	1	3	7	-5.7315893	5.124743	1
175-8-28	we further propose a distributionally robust approach based on wasserstein balls to compute upper and lower confidence bounds on the misclassification probability of the resulting classifier .	we prove that this optimization problem admits a tractable reformulation and encapsulates the classical as well as the popular regularized logistic regression problems as special cases .	0	5	4	4.1564503	-3.7946966	0
175-8-28	we prove that this optimization problem admits a tractable reformulation and encapsulates the classical as well as the popular regularized logistic regression problems as special cases .	these bounds are given by the optimal values of two highly tractable linear programs .	1	4	6	3.2343445	-3.0090551	0
175-8-28	we validate our theoretical out-of-sample guarantees through simulated and empirical experiments .	we prove that this optimization problem admits a tractable reformulation and encapsulates the classical as well as the popular regularized logistic regression problems as special cases .	0	7	4	5.121373	-4.577233	0
175-8-28	these bounds are given by the optimal values of two highly tractable linear programs .	we further propose a distributionally robust approach based on wasserstein balls to compute upper and lower confidence bounds on the misclassification probability of the resulting classifier .	0	6	5	-4.262582	4.1270175	1
175-8-28	we validate our theoretical out-of-sample guarantees through simulated and empirical experiments .	we further propose a distributionally robust approach based on wasserstein balls to compute upper and lower confidence bounds on the misclassification probability of the resulting classifier .	0	7	5	4.4766293	-4.024193	0
175-8-28	we validate our theoretical out-of-sample guarantees through simulated and empirical experiments .	these bounds are given by the optimal values of two highly tractable linear programs .	0	7	6	5.308408	-4.7725835	0
176-6-15	adaptive stochastic optimization ( aso ) optimizes an objective function adaptively under uncertainty .	it plays a crucial role in planning and learning under uncertainty , but is , unfortunately , computationally intractable in general .	1	0	1	-3.696991	3.5927927	1
176-6-15	this paper introduces two conditions on the objective function , the marginal likelihood rate bound and the marginal likelihood bound , which , together with pointwise submodularity , enable efficient approximate solution of aso .	adaptive stochastic optimization ( aso ) optimizes an objective function adaptively under uncertainty .	0	2	0	5.7032185	-5.060884	0
176-6-15	adaptive stochastic optimization ( aso ) optimizes an objective function adaptively under uncertainty .	several interesting classes of functions satisfy these conditions naturally , e.g. , the version space reduction function for hypothesis learning .	1	0	3	-5.943533	5.217437	1
176-6-15	we describe recursive adaptive coverage , a new aso algorithm that exploits these conditions , and apply the algorithm to two robot planning tasks under uncertainty .	adaptive stochastic optimization ( aso ) optimizes an objective function adaptively under uncertainty .	0	4	0	5.669484	-5.087762	0
176-6-15	in contrast to the earlier submodular optimization approach , our algorithm applies to aso over both sets and paths .	adaptive stochastic optimization ( aso ) optimizes an objective function adaptively under uncertainty .	0	5	0	5.652819	-5.005931	0
176-6-15	it plays a crucial role in planning and learning under uncertainty , but is , unfortunately , computationally intractable in general .	this paper introduces two conditions on the objective function , the marginal likelihood rate bound and the marginal likelihood bound , which , together with pointwise submodularity , enable efficient approximate solution of aso .	1	1	2	-5.8506107	5.2450533	1
176-6-15	several interesting classes of functions satisfy these conditions naturally , e.g. , the version space reduction function for hypothesis learning .	it plays a crucial role in planning and learning under uncertainty , but is , unfortunately , computationally intractable in general .	0	3	1	5.266615	-4.6872663	0
176-6-15	it plays a crucial role in planning and learning under uncertainty , but is , unfortunately , computationally intractable in general .	we describe recursive adaptive coverage , a new aso algorithm that exploits these conditions , and apply the algorithm to two robot planning tasks under uncertainty .	1	1	4	-5.9111404	5.25029	1
176-6-15	in contrast to the earlier submodular optimization approach , our algorithm applies to aso over both sets and paths .	it plays a crucial role in planning and learning under uncertainty , but is , unfortunately , computationally intractable in general .	0	5	1	5.4493537	-4.849543	0
176-6-15	this paper introduces two conditions on the objective function , the marginal likelihood rate bound and the marginal likelihood bound , which , together with pointwise submodularity , enable efficient approximate solution of aso .	several interesting classes of functions satisfy these conditions naturally , e.g. , the version space reduction function for hypothesis learning .	1	2	3	-2.425004	2.522985	1
176-6-15	we describe recursive adaptive coverage , a new aso algorithm that exploits these conditions , and apply the algorithm to two robot planning tasks under uncertainty .	this paper introduces two conditions on the objective function , the marginal likelihood rate bound and the marginal likelihood bound , which , together with pointwise submodularity , enable efficient approximate solution of aso .	0	4	2	2.847405	-2.7241518	0
176-6-15	this paper introduces two conditions on the objective function , the marginal likelihood rate bound and the marginal likelihood bound , which , together with pointwise submodularity , enable efficient approximate solution of aso .	in contrast to the earlier submodular optimization approach , our algorithm applies to aso over both sets and paths .	1	2	5	-4.5814056	4.216301	1
176-6-15	several interesting classes of functions satisfy these conditions naturally , e.g. , the version space reduction function for hypothesis learning .	we describe recursive adaptive coverage , a new aso algorithm that exploits these conditions , and apply the algorithm to two robot planning tasks under uncertainty .	1	3	4	-3.8410687	3.6652641	1
176-6-15	in contrast to the earlier submodular optimization approach , our algorithm applies to aso over both sets and paths .	several interesting classes of functions satisfy these conditions naturally , e.g. , the version space reduction function for hypothesis learning .	0	5	3	3.364044	-3.1512706	0
176-6-15	we describe recursive adaptive coverage , a new aso algorithm that exploits these conditions , and apply the algorithm to two robot planning tasks under uncertainty .	in contrast to the earlier submodular optimization approach , our algorithm applies to aso over both sets and paths .	1	4	5	-4.6228285	4.2426577	1
177-8-28	stochastic convex optimization is a basic and well studied primitive in machine learning .	it is well known that convex and lipschitz functions can be minimized efficiently using stochastic gradient descent ( sgd ) .	1	0	1	-5.626048	5.1590033	1
177-8-28	stochastic convex optimization is a basic and well studied primitive in machine learning .	the normalized gradient descent ( ngd ) algorithm , is an adaptation of gradient descent , which updates according to the direction of the gradients , rather than the gradients themselves .	1	0	2	-5.8326793	5.205261	1
177-8-28	in this paper we analyze a stochastic version of ngd and prove its convergence to a global minimum for a wider class of functions : we require the functions to be quasi-convex and locally-lipschitz .	stochastic convex optimization is a basic and well studied primitive in machine learning .	0	3	0	5.7165976	-5.116051	0
177-8-28	stochastic convex optimization is a basic and well studied primitive in machine learning .	quasi-convexity broadens the concept of unimodality to multidimensions and allows for certain types of saddle points , which are a known hurdle for first-order optimization methods such as gradient descent .	1	0	4	-5.786952	5.1718116	1
177-8-28	stochastic convex optimization is a basic and well studied primitive in machine learning .	locally-lipschitz functions are only required to be lipschitz in a small region around the optimum .	1	0	5	-5.864023	5.147037	1
177-8-28	this assumption circumvents gradient explosion , which is another known hurdle for gradient descent variants .	stochastic convex optimization is a basic and well studied primitive in machine learning .	0	6	0	5.6488657	-5.0569415	0
177-8-28	interestingly , unlike the vanilla sgd algorithm , the stochastic normalized gradient descent algorithm provably requires a minimal minibatch size .	stochastic convex optimization is a basic and well studied primitive in machine learning .	0	7	0	5.6433244	-5.0247016	0
177-8-28	the normalized gradient descent ( ngd ) algorithm , is an adaptation of gradient descent , which updates according to the direction of the gradients , rather than the gradients themselves .	it is well known that convex and lipschitz functions can be minimized efficiently using stochastic gradient descent ( sgd ) .	0	2	1	5.3442945	-4.7780495	0
177-8-28	it is well known that convex and lipschitz functions can be minimized efficiently using stochastic gradient descent ( sgd ) .	in this paper we analyze a stochastic version of ngd and prove its convergence to a global minimum for a wider class of functions : we require the functions to be quasi-convex and locally-lipschitz .	1	1	3	-5.9644804	5.1521063	1
177-8-28	quasi-convexity broadens the concept of unimodality to multidimensions and allows for certain types of saddle points , which are a known hurdle for first-order optimization methods such as gradient descent .	it is well known that convex and lipschitz functions can be minimized efficiently using stochastic gradient descent ( sgd ) .	0	4	1	5.3131943	-4.7004776	0
177-8-28	it is well known that convex and lipschitz functions can be minimized efficiently using stochastic gradient descent ( sgd ) .	locally-lipschitz functions are only required to be lipschitz in a small region around the optimum .	1	1	5	-5.919898	5.204338	1
177-8-28	it is well known that convex and lipschitz functions can be minimized efficiently using stochastic gradient descent ( sgd ) .	this assumption circumvents gradient explosion , which is another known hurdle for gradient descent variants .	1	1	6	-5.9471436	5.1363626	1
177-8-28	it is well known that convex and lipschitz functions can be minimized efficiently using stochastic gradient descent ( sgd ) .	interestingly , unlike the vanilla sgd algorithm , the stochastic normalized gradient descent algorithm provably requires a minimal minibatch size .	1	1	7	-5.7720013	5.085283	1
177-8-28	the normalized gradient descent ( ngd ) algorithm , is an adaptation of gradient descent , which updates according to the direction of the gradients , rather than the gradients themselves .	in this paper we analyze a stochastic version of ngd and prove its convergence to a global minimum for a wider class of functions : we require the functions to be quasi-convex and locally-lipschitz .	1	2	3	-6.034393	5.1704903	1
177-8-28	quasi-convexity broadens the concept of unimodality to multidimensions and allows for certain types of saddle points , which are a known hurdle for first-order optimization methods such as gradient descent .	the normalized gradient descent ( ngd ) algorithm , is an adaptation of gradient descent , which updates according to the direction of the gradients , rather than the gradients themselves .	0	4	2	-2.2751691	2.432498	1
177-8-28	the normalized gradient descent ( ngd ) algorithm , is an adaptation of gradient descent , which updates according to the direction of the gradients , rather than the gradients themselves .	locally-lipschitz functions are only required to be lipschitz in a small region around the optimum .	1	2	5	-3.795094	3.5873518	1
177-8-28	the normalized gradient descent ( ngd ) algorithm , is an adaptation of gradient descent , which updates according to the direction of the gradients , rather than the gradients themselves .	this assumption circumvents gradient explosion , which is another known hurdle for gradient descent variants .	1	2	6	-5.936774	5.1635494	1
177-8-28	the normalized gradient descent ( ngd ) algorithm , is an adaptation of gradient descent , which updates according to the direction of the gradients , rather than the gradients themselves .	interestingly , unlike the vanilla sgd algorithm , the stochastic normalized gradient descent algorithm provably requires a minimal minibatch size .	1	2	7	-6.0006895	5.116811	1
177-8-28	quasi-convexity broadens the concept of unimodality to multidimensions and allows for certain types of saddle points , which are a known hurdle for first-order optimization methods such as gradient descent .	in this paper we analyze a stochastic version of ngd and prove its convergence to a global minimum for a wider class of functions : we require the functions to be quasi-convex and locally-lipschitz .	0	4	3	2.5832844	-2.388966	0
177-8-28	locally-lipschitz functions are only required to be lipschitz in a small region around the optimum .	in this paper we analyze a stochastic version of ngd and prove its convergence to a global minimum for a wider class of functions : we require the functions to be quasi-convex and locally-lipschitz .	0	5	3	1.3151875	-1.0974946	0
177-8-28	in this paper we analyze a stochastic version of ngd and prove its convergence to a global minimum for a wider class of functions : we require the functions to be quasi-convex and locally-lipschitz .	this assumption circumvents gradient explosion , which is another known hurdle for gradient descent variants .	1	3	6	-5.3008137	4.870327	1
177-8-28	interestingly , unlike the vanilla sgd algorithm , the stochastic normalized gradient descent algorithm provably requires a minimal minibatch size .	in this paper we analyze a stochastic version of ngd and prove its convergence to a global minimum for a wider class of functions : we require the functions to be quasi-convex and locally-lipschitz .	0	7	3	4.679448	-4.161481	0
177-8-28	locally-lipschitz functions are only required to be lipschitz in a small region around the optimum .	quasi-convexity broadens the concept of unimodality to multidimensions and allows for certain types of saddle points , which are a known hurdle for first-order optimization methods such as gradient descent .	0	5	4	1.5334759	-1.3573035	0
177-8-28	this assumption circumvents gradient explosion , which is another known hurdle for gradient descent variants .	quasi-convexity broadens the concept of unimodality to multidimensions and allows for certain types of saddle points , which are a known hurdle for first-order optimization methods such as gradient descent .	0	6	4	4.6554174	-4.192253	0
177-8-28	interestingly , unlike the vanilla sgd algorithm , the stochastic normalized gradient descent algorithm provably requires a minimal minibatch size .	quasi-convexity broadens the concept of unimodality to multidimensions and allows for certain types of saddle points , which are a known hurdle for first-order optimization methods such as gradient descent .	0	7	4	4.4947834	-4.1259184	0
177-8-28	locally-lipschitz functions are only required to be lipschitz in a small region around the optimum .	this assumption circumvents gradient explosion , which is another known hurdle for gradient descent variants .	1	5	6	-2.4223516	2.530367	1
177-8-28	interestingly , unlike the vanilla sgd algorithm , the stochastic normalized gradient descent algorithm provably requires a minimal minibatch size .	locally-lipschitz functions are only required to be lipschitz in a small region around the optimum .	0	7	5	3.777964	-3.4977918	0
177-8-28	this assumption circumvents gradient explosion , which is another known hurdle for gradient descent variants .	interestingly , unlike the vanilla sgd algorithm , the stochastic normalized gradient descent algorithm provably requires a minimal minibatch size .	1	6	7	-4.9386797	4.588143	1
178-5-10	numerical sampling techniques are often practically useful , but lead to limited conceptual insight about optimal encoding/decoding strategies , which are of significant relevance to computational neuroscience .	the process of dynamic state estimation ( filtering ) based on point process observations is in general intractable .	0	1	0	3.4414878	-3.1595838	0
178-5-10	we develop an analytically tractable bayesian approximation to optimal filtering based on point process observations , which allows us to introduce distributional assumptions about sensory cell properties , that greatly facilitate the analysis of optimal encoding in situations deviating from common assumptions of uniform coding .	the process of dynamic state estimation ( filtering ) based on point process observations is in general intractable .	0	2	0	5.5422864	-4.921326	0
178-5-10	the process of dynamic state estimation ( filtering ) based on point process observations is in general intractable .	the analytic framework leads to insights which are difficult to obtain from numerical algorithms , and is consistent with experiments about the distribution of tuning curve centers .	1	0	3	-5.9334974	5.149812	1
178-5-10	interestingly , we find that the information gained from the absence of spikes may be crucial to performance .	the process of dynamic state estimation ( filtering ) based on point process observations is in general intractable .	0	4	0	5.491105	-4.904978	0
178-5-10	numerical sampling techniques are often practically useful , but lead to limited conceptual insight about optimal encoding/decoding strategies , which are of significant relevance to computational neuroscience .	we develop an analytically tractable bayesian approximation to optimal filtering based on point process observations , which allows us to introduce distributional assumptions about sensory cell properties , that greatly facilitate the analysis of optimal encoding in situations deviating from common assumptions of uniform coding .	1	1	2	-5.9636784	5.182291	1
178-5-10	numerical sampling techniques are often practically useful , but lead to limited conceptual insight about optimal encoding/decoding strategies , which are of significant relevance to computational neuroscience .	the analytic framework leads to insights which are difficult to obtain from numerical algorithms , and is consistent with experiments about the distribution of tuning curve centers .	1	1	3	-5.8968267	5.1784487	1
178-5-10	numerical sampling techniques are often practically useful , but lead to limited conceptual insight about optimal encoding/decoding strategies , which are of significant relevance to computational neuroscience .	interestingly , we find that the information gained from the absence of spikes may be crucial to performance .	1	1	4	-5.9642835	5.200589	1
178-5-10	the analytic framework leads to insights which are difficult to obtain from numerical algorithms , and is consistent with experiments about the distribution of tuning curve centers .	we develop an analytically tractable bayesian approximation to optimal filtering based on point process observations , which allows us to introduce distributional assumptions about sensory cell properties , that greatly facilitate the analysis of optimal encoding in situations deviating from common assumptions of uniform coding .	0	3	2	4.6120653	-4.1089497	0
178-5-10	we develop an analytically tractable bayesian approximation to optimal filtering based on point process observations , which allows us to introduce distributional assumptions about sensory cell properties , that greatly facilitate the analysis of optimal encoding in situations deviating from common assumptions of uniform coding .	interestingly , we find that the information gained from the absence of spikes may be crucial to performance .	1	2	4	-5.360174	4.8827214	1
178-5-10	interestingly , we find that the information gained from the absence of spikes may be crucial to performance .	the analytic framework leads to insights which are difficult to obtain from numerical algorithms , and is consistent with experiments about the distribution of tuning curve centers .	0	4	3	0.3781328	-0.31670332	0
179-8-28	specifically , we consider the sparse principal component analysis ( sparse pca ) problem , and the family of sum-of-squares ( sos , aka lasserre/parillo ) convex relaxations .	this paper establishes a statistical versus computational trade-off for solving a basic high-dimensional machine learning problem via a basic convex relaxation method .	0	1	0	3.748558	-3.3624885	0
179-8-28	this paper establishes a statistical versus computational trade-off for solving a basic high-dimensional machine learning problem via a basic convex relaxation method .	it was well known that in large dimension p , a planted k-sparse unit vector can be in principle detected using only n k log p ( gaussian or bernoulli ) samples , but all efficient ( polynomial time ) algorithms known require n k 2 samples .	1	0	2	5.214039	-4.5786715	0
179-8-28	it was also known that this quadratic gap can not be improved by the the most basic semi-definite ( sdp , aka spectral ) relaxation , equivalent to a degree-2 sos algorithms .	this paper establishes a statistical versus computational trade-off for solving a basic high-dimensional machine learning problem via a basic convex relaxation method .	0	3	0	-0.4489547	0.7961671	1
179-8-28	here we prove that also degree-4 sos algorithms can not improve this quadratic gap .	this paper establishes a statistical versus computational trade-off for solving a basic high-dimensional machine learning problem via a basic convex relaxation method .	0	4	0	-4.9610214	4.5736732	1
179-8-28	this average-case lower bound adds to the small collection of hardness results in machine learning for this powerful family of convex relaxation algorithms .	this paper establishes a statistical versus computational trade-off for solving a basic high-dimensional machine learning problem via a basic convex relaxation method .	0	5	0	5.493204	-4.8845973	0
179-8-28	this paper establishes a statistical versus computational trade-off for solving a basic high-dimensional machine learning problem via a basic convex relaxation method .	moreover , our design of moments ( or `` pseudo-expectations '' ) for this lower bound is quite different than previous lower bounds .	1	0	6	-3.2342622	3.3073382	1
179-8-28	this paper establishes a statistical versus computational trade-off for solving a basic high-dimensional machine learning problem via a basic convex relaxation method .	establishing lower bounds for higher degree sos algorithms for remains a challenging problem .	1	0	7	5.177466	-4.5394626	0
179-8-28	specifically , we consider the sparse principal component analysis ( sparse pca ) problem , and the family of sum-of-squares ( sos , aka lasserre/parillo ) convex relaxations .	it was well known that in large dimension p , a planted k-sparse unit vector can be in principle detected using only n k log p ( gaussian or bernoulli ) samples , but all efficient ( polynomial time ) algorithms known require n k 2 samples .	1	1	2	-0.18021809	0.4918313	1
179-8-28	specifically , we consider the sparse principal component analysis ( sparse pca ) problem , and the family of sum-of-squares ( sos , aka lasserre/parillo ) convex relaxations .	it was also known that this quadratic gap can not be improved by the the most basic semi-definite ( sdp , aka spectral ) relaxation , equivalent to a degree-2 sos algorithms .	1	1	3	-5.0471845	4.632757	1
179-8-28	here we prove that also degree-4 sos algorithms can not improve this quadratic gap .	specifically , we consider the sparse principal component analysis ( sparse pca ) problem , and the family of sum-of-squares ( sos , aka lasserre/parillo ) convex relaxations .	0	4	1	4.306185	-3.8288212	0
179-8-28	specifically , we consider the sparse principal component analysis ( sparse pca ) problem , and the family of sum-of-squares ( sos , aka lasserre/parillo ) convex relaxations .	this average-case lower bound adds to the small collection of hardness results in machine learning for this powerful family of convex relaxation algorithms .	1	1	5	-5.977631	5.072128	1
179-8-28	moreover , our design of moments ( or `` pseudo-expectations '' ) for this lower bound is quite different than previous lower bounds .	specifically , we consider the sparse principal component analysis ( sparse pca ) problem , and the family of sum-of-squares ( sos , aka lasserre/parillo ) convex relaxations .	0	6	1	4.3561277	-3.900732	0
179-8-28	specifically , we consider the sparse principal component analysis ( sparse pca ) problem , and the family of sum-of-squares ( sos , aka lasserre/parillo ) convex relaxations .	establishing lower bounds for higher degree sos algorithms for remains a challenging problem .	1	1	7	-3.8706765	3.7752376	1
179-8-28	it was also known that this quadratic gap can not be improved by the the most basic semi-definite ( sdp , aka spectral ) relaxation , equivalent to a degree-2 sos algorithms .	it was well known that in large dimension p , a planted k-sparse unit vector can be in principle detected using only n k log p ( gaussian or bernoulli ) samples , but all efficient ( polynomial time ) algorithms known require n k 2 samples .	0	3	2	5.3185434	-4.6853113	0
179-8-28	it was well known that in large dimension p , a planted k-sparse unit vector can be in principle detected using only n k log p ( gaussian or bernoulli ) samples , but all efficient ( polynomial time ) algorithms known require n k 2 samples .	here we prove that also degree-4 sos algorithms can not improve this quadratic gap .	1	2	4	-5.9522996	5.197995	1
179-8-28	this average-case lower bound adds to the small collection of hardness results in machine learning for this powerful family of convex relaxation algorithms .	it was well known that in large dimension p , a planted k-sparse unit vector can be in principle detected using only n k log p ( gaussian or bernoulli ) samples , but all efficient ( polynomial time ) algorithms known require n k 2 samples .	0	5	2	5.450321	-4.8095884	0
179-8-28	moreover , our design of moments ( or `` pseudo-expectations '' ) for this lower bound is quite different than previous lower bounds .	it was well known that in large dimension p , a planted k-sparse unit vector can be in principle detected using only n k log p ( gaussian or bernoulli ) samples , but all efficient ( polynomial time ) algorithms known require n k 2 samples .	0	6	2	5.4183755	-4.8287673	0
179-8-28	it was well known that in large dimension p , a planted k-sparse unit vector can be in principle detected using only n k log p ( gaussian or bernoulli ) samples , but all efficient ( polynomial time ) algorithms known require n k 2 samples .	establishing lower bounds for higher degree sos algorithms for remains a challenging problem .	1	2	7	-4.0407553	3.8593903	1
179-8-28	here we prove that also degree-4 sos algorithms can not improve this quadratic gap .	it was also known that this quadratic gap can not be improved by the the most basic semi-definite ( sdp , aka spectral ) relaxation , equivalent to a degree-2 sos algorithms .	0	4	3	3.5809577	-3.1980696	0
179-8-28	it was also known that this quadratic gap can not be improved by the the most basic semi-definite ( sdp , aka spectral ) relaxation , equivalent to a degree-2 sos algorithms .	this average-case lower bound adds to the small collection of hardness results in machine learning for this powerful family of convex relaxation algorithms .	1	3	5	-5.527331	5.0561614	1
179-8-28	moreover , our design of moments ( or `` pseudo-expectations '' ) for this lower bound is quite different than previous lower bounds .	it was also known that this quadratic gap can not be improved by the the most basic semi-definite ( sdp , aka spectral ) relaxation , equivalent to a degree-2 sos algorithms .	0	6	3	4.252652	-3.798028	0
179-8-28	it was also known that this quadratic gap can not be improved by the the most basic semi-definite ( sdp , aka spectral ) relaxation , equivalent to a degree-2 sos algorithms .	establishing lower bounds for higher degree sos algorithms for remains a challenging problem .	1	3	7	2.5521362	-2.033343	0
179-8-28	here we prove that also degree-4 sos algorithms can not improve this quadratic gap .	this average-case lower bound adds to the small collection of hardness results in machine learning for this powerful family of convex relaxation algorithms .	1	4	5	-5.567086	5.008728	1
179-8-28	here we prove that also degree-4 sos algorithms can not improve this quadratic gap .	moreover , our design of moments ( or `` pseudo-expectations '' ) for this lower bound is quite different than previous lower bounds .	1	4	6	-5.2080483	4.8332176	1
179-8-28	here we prove that also degree-4 sos algorithms can not improve this quadratic gap .	establishing lower bounds for higher degree sos algorithms for remains a challenging problem .	1	4	7	4.3523226	-3.8625474	0
179-8-28	moreover , our design of moments ( or `` pseudo-expectations '' ) for this lower bound is quite different than previous lower bounds .	this average-case lower bound adds to the small collection of hardness results in machine learning for this powerful family of convex relaxation algorithms .	0	6	5	-1.7103527	1.8966236	1
179-8-28	this average-case lower bound adds to the small collection of hardness results in machine learning for this powerful family of convex relaxation algorithms .	establishing lower bounds for higher degree sos algorithms for remains a challenging problem .	1	5	7	4.552828	-3.947012	0
179-8-28	establishing lower bounds for higher degree sos algorithms for remains a challenging problem .	moreover , our design of moments ( or `` pseudo-expectations '' ) for this lower bound is quite different than previous lower bounds .	0	7	6	-5.950478	5.178995	1
180-4-6	learning-from-crowds aims to design proper aggregation strategies to infer the unknown true labels from the noisy labels provided by ordinary web workers .	this paper presents max-margin majority voting ( m3 v ) to improve the discriminative ability of majority voting and further presents a bayesian generalization to incorporate the flexibility of generative methods on modeling noisy observations with worker confusion matrices .	1	0	1	-5.927849	5.2340846	1
180-4-6	we formulate the joint learning as a regularized bayesian inference problem , where the posterior regularization is derived by maximizing the margin between the aggregated score of a potential true label and that of any alternative label .	learning-from-crowds aims to design proper aggregation strategies to infer the unknown true labels from the noisy labels provided by ordinary web workers .	0	2	0	5.58399	-4.897125	0
180-4-6	our bayesian model naturally covers the dawid-skene estimator and m3 v. empirical results demonstrate that our methods are competitive , often achieving better results than state-of-the-art estimators .	learning-from-crowds aims to design proper aggregation strategies to infer the unknown true labels from the noisy labels provided by ordinary web workers .	0	3	0	5.623219	-4.999322	0
180-4-6	this paper presents max-margin majority voting ( m3 v ) to improve the discriminative ability of majority voting and further presents a bayesian generalization to incorporate the flexibility of generative methods on modeling noisy observations with worker confusion matrices .	we formulate the joint learning as a regularized bayesian inference problem , where the posterior regularization is derived by maximizing the margin between the aggregated score of a potential true label and that of any alternative label .	1	1	2	-0.90798825	1.0721616	1
180-4-6	our bayesian model naturally covers the dawid-skene estimator and m3 v. empirical results demonstrate that our methods are competitive , often achieving better results than state-of-the-art estimators .	this paper presents max-margin majority voting ( m3 v ) to improve the discriminative ability of majority voting and further presents a bayesian generalization to incorporate the flexibility of generative methods on modeling noisy observations with worker confusion matrices .	0	3	1	5.6452293	-5.011709	0
180-4-6	our bayesian model naturally covers the dawid-skene estimator and m3 v. empirical results demonstrate that our methods are competitive , often achieving better results than state-of-the-art estimators .	we formulate the joint learning as a regularized bayesian inference problem , where the posterior regularization is derived by maximizing the margin between the aggregated score of a potential true label and that of any alternative label .	0	3	2	5.446061	-4.812111	0
181-4-6	in particular , we show that , if all other parameters are fixed a priori , the number of passes over the data ( epochs ) acts as a regularization parameter , and prove strong universal consistency , i.e .	within a statistical learning setting , we propose and study an iterative regularization algorithm for least squares defined by an incremental gradient method .	0	1	0	5.316726	-4.715582	0
181-4-6	almost sure convergence of the risk , as well as sharp finite sample bounds for the iterates .	within a statistical learning setting , we propose and study an iterative regularization algorithm for least squares defined by an incremental gradient method .	0	2	0	5.4633694	-4.888996	0
181-4-6	our results are a step towards understanding the effect of multiple epochs in stochastic gradient techniques in machine learning and rely on integrating statistical and optimization results .	within a statistical learning setting , we propose and study an iterative regularization algorithm for least squares defined by an incremental gradient method .	0	3	0	5.468617	-4.862158	0
181-4-6	in particular , we show that , if all other parameters are fixed a priori , the number of passes over the data ( epochs ) acts as a regularization parameter , and prove strong universal consistency , i.e .	almost sure convergence of the risk , as well as sharp finite sample bounds for the iterates .	1	1	2	-5.157628	4.7175436	1
181-4-6	in particular , we show that , if all other parameters are fixed a priori , the number of passes over the data ( epochs ) acts as a regularization parameter , and prove strong universal consistency , i.e .	our results are a step towards understanding the effect of multiple epochs in stochastic gradient techniques in machine learning and rely on integrating statistical and optimization results .	1	1	3	-5.4451776	4.5426664	1
181-4-6	our results are a step towards understanding the effect of multiple epochs in stochastic gradient techniques in machine learning and rely on integrating statistical and optimization results .	almost sure convergence of the risk , as well as sharp finite sample bounds for the iterates .	0	3	2	4.4338517	-4.0952444	0
182-7-21	random walk kernels measure graph similarity by counting matching walks in two graphs .	in their most popular form of geometric random walk kernels , longer walks of length k are downweighted by a factor of k ( < 1 ) to ensure convergence of the corresponding geometric series .	1	0	1	-5.662913	5.1175423	1
182-7-21	we know from the field of link prediction that this downweighting often leads to a phenomenon referred to as halting : longer walks are downweighted so much that the similarity score is completely dominated by the comparison of walks of length 1 .	random walk kernels measure graph similarity by counting matching walks in two graphs .	0	2	0	2.0862355	-1.8805691	0
182-7-21	this is a naive kernel between edges and vertices .	random walk kernels measure graph similarity by counting matching walks in two graphs .	0	3	0	3.9258132	-3.571858	0
182-7-21	we theoretically show that halting may occur in geometric random walk kernels .	random walk kernels measure graph similarity by counting matching walks in two graphs .	0	4	0	1.1969885	-0.9202727	0
182-7-21	we also empirically quantify its impact in simulated datasets and popular graph classification benchmark datasets .	random walk kernels measure graph similarity by counting matching walks in two graphs .	0	5	0	5.6205187	-4.990283	0
182-7-21	our findings promise to be instrumental in future graph kernel development and applications of random walk kernels .	random walk kernels measure graph similarity by counting matching walks in two graphs .	0	6	0	5.421528	-4.821087	0
182-7-21	we know from the field of link prediction that this downweighting often leads to a phenomenon referred to as halting : longer walks are downweighted so much that the similarity score is completely dominated by the comparison of walks of length 1 .	in their most popular form of geometric random walk kernels , longer walks of length k are downweighted by a factor of k ( < 1 ) to ensure convergence of the corresponding geometric series .	0	2	1	-3.3020186	3.215166	1
182-7-21	this is a naive kernel between edges and vertices .	in their most popular form of geometric random walk kernels , longer walks of length k are downweighted by a factor of k ( < 1 ) to ensure convergence of the corresponding geometric series .	0	3	1	2.1559856	-2.0707483	0
182-7-21	we theoretically show that halting may occur in geometric random walk kernels .	in their most popular form of geometric random walk kernels , longer walks of length k are downweighted by a factor of k ( < 1 ) to ensure convergence of the corresponding geometric series .	0	4	1	-3.1936243	3.1917896	1
182-7-21	we also empirically quantify its impact in simulated datasets and popular graph classification benchmark datasets .	in their most popular form of geometric random walk kernels , longer walks of length k are downweighted by a factor of k ( < 1 ) to ensure convergence of the corresponding geometric series .	0	5	1	5.551392	-4.917921	0
182-7-21	in their most popular form of geometric random walk kernels , longer walks of length k are downweighted by a factor of k ( < 1 ) to ensure convergence of the corresponding geometric series .	our findings promise to be instrumental in future graph kernel development and applications of random walk kernels .	1	1	6	-5.7297525	4.750716	1
182-7-21	this is a naive kernel between edges and vertices .	we know from the field of link prediction that this downweighting often leads to a phenomenon referred to as halting : longer walks are downweighted so much that the similarity score is completely dominated by the comparison of walks of length 1 .	0	3	2	1.3616203	-1.0571718	0
182-7-21	we theoretically show that halting may occur in geometric random walk kernels .	we know from the field of link prediction that this downweighting often leads to a phenomenon referred to as halting : longer walks are downweighted so much that the similarity score is completely dominated by the comparison of walks of length 1 .	0	4	2	3.0964222	-2.825756	0
182-7-21	we know from the field of link prediction that this downweighting often leads to a phenomenon referred to as halting : longer walks are downweighted so much that the similarity score is completely dominated by the comparison of walks of length 1 .	we also empirically quantify its impact in simulated datasets and popular graph classification benchmark datasets .	1	2	5	-5.913076	5.0279427	1
182-7-21	our findings promise to be instrumental in future graph kernel development and applications of random walk kernels .	we know from the field of link prediction that this downweighting often leads to a phenomenon referred to as halting : longer walks are downweighted so much that the similarity score is completely dominated by the comparison of walks of length 1 .	0	6	2	5.261236	-4.722963	0
182-7-21	we theoretically show that halting may occur in geometric random walk kernels .	this is a naive kernel between edges and vertices .	0	4	3	-0.8031211	1.0023688	1
182-7-21	we also empirically quantify its impact in simulated datasets and popular graph classification benchmark datasets .	this is a naive kernel between edges and vertices .	0	5	3	5.506601	-4.873851	0
182-7-21	this is a naive kernel between edges and vertices .	our findings promise to be instrumental in future graph kernel development and applications of random walk kernels .	1	3	6	-5.7455716	4.803699	1
182-7-21	we also empirically quantify its impact in simulated datasets and popular graph classification benchmark datasets .	we theoretically show that halting may occur in geometric random walk kernels .	0	5	4	5.494122	-4.8739443	0
182-7-21	our findings promise to be instrumental in future graph kernel development and applications of random walk kernels .	we theoretically show that halting may occur in geometric random walk kernels .	0	6	4	5.3100586	-4.7136383	0
182-7-21	our findings promise to be instrumental in future graph kernel development and applications of random walk kernels .	we also empirically quantify its impact in simulated datasets and popular graph classification benchmark datasets .	0	6	5	3.2208853	-3.0154328	0
183-5-10	gaussian process ( gp ) models form a core part of probabilistic machine learning .	considerable research effort has been made into attacking three issues with gp models : how to compute efficiently when the number of data is large ; how to approximate the posterior when the likelihood is not gaussian and how to estimate covariance function parameter posteriors .	1	0	1	-5.72821	5.116332	1
183-5-10	this paper simultaneously addresses these , using a variational approximation to the posterior which is sparse in support of the function but otherwise free-form .	gaussian process ( gp ) models form a core part of probabilistic machine learning .	0	2	0	5.6935205	-5.078948	0
183-5-10	gaussian process ( gp ) models form a core part of probabilistic machine learning .	the result is a hybrid monte-carlo sampling scheme which allows for a non-gaussian approximation over the function values and covariance parameters simultaneously , with efficient computations based on inducing-point sparse gps .	1	0	3	-5.8643045	5.1970778	1
183-5-10	code to replicate each experiment in this paper is available at github.com/sparsemcmc .	gaussian process ( gp ) models form a core part of probabilistic machine learning .	0	4	0	5.650903	-5.0625257	0
183-5-10	considerable research effort has been made into attacking three issues with gp models : how to compute efficiently when the number of data is large ; how to approximate the posterior when the likelihood is not gaussian and how to estimate covariance function parameter posteriors .	this paper simultaneously addresses these , using a variational approximation to the posterior which is sparse in support of the function but otherwise free-form .	1	1	2	-5.976261	5.2112184	1
183-5-10	the result is a hybrid monte-carlo sampling scheme which allows for a non-gaussian approximation over the function values and covariance parameters simultaneously , with efficient computations based on inducing-point sparse gps .	considerable research effort has been made into attacking three issues with gp models : how to compute efficiently when the number of data is large ; how to approximate the posterior when the likelihood is not gaussian and how to estimate covariance function parameter posteriors .	0	3	1	5.302664	-4.785353	0
183-5-10	considerable research effort has been made into attacking three issues with gp models : how to compute efficiently when the number of data is large ; how to approximate the posterior when the likelihood is not gaussian and how to estimate covariance function parameter posteriors .	code to replicate each experiment in this paper is available at github.com/sparsemcmc .	1	1	4	-5.9870596	5.1768856	1
183-5-10	this paper simultaneously addresses these , using a variational approximation to the posterior which is sparse in support of the function but otherwise free-form .	the result is a hybrid monte-carlo sampling scheme which allows for a non-gaussian approximation over the function values and covariance parameters simultaneously , with efficient computations based on inducing-point sparse gps .	1	2	3	-2.8061192	2.8695	1
183-5-10	this paper simultaneously addresses these , using a variational approximation to the posterior which is sparse in support of the function but otherwise free-form .	code to replicate each experiment in this paper is available at github.com/sparsemcmc .	1	2	4	-5.6536384	4.8823876	1
183-5-10	code to replicate each experiment in this paper is available at github.com/sparsemcmc .	the result is a hybrid monte-carlo sampling scheme which allows for a non-gaussian approximation over the function values and covariance parameters simultaneously , with efficient computations based on inducing-point sparse gps .	0	4	3	4.559375	-4.150929	0
184-4-6	we study nystrom type subsampling approaches to large scale kernel methods , and prove learning bounds in the statistical learning setting , where random sampling and high probability estimates are considered .	in particular , we prove that these approaches can achieve optimal learning bounds , provided the subsampling level is suitably chosen .	1	0	1	-5.9875593	5.2069445	1
184-4-6	we study nystrom type subsampling approaches to large scale kernel methods , and prove learning bounds in the statistical learning setting , where random sampling and high probability estimates are considered .	these results suggest a simple incremental variant of nystrom kernel regularized least squares , where the subsampling level implements a form of computational regularization , in the sense that it controls at the same time regularization and computations .	1	0	2	-5.887476	5.236865	1
184-4-6	we study nystrom type subsampling approaches to large scale kernel methods , and prove learning bounds in the statistical learning setting , where random sampling and high probability estimates are considered .	extensive experimental analysis shows that the considered approach achieves state of the art performances on benchmark large scale datasets .	1	0	3	-6.002967	5.104425	1
184-4-6	in particular , we prove that these approaches can achieve optimal learning bounds , provided the subsampling level is suitably chosen .	these results suggest a simple incremental variant of nystrom kernel regularized least squares , where the subsampling level implements a form of computational regularization , in the sense that it controls at the same time regularization and computations .	1	1	2	-4.8008256	4.474639	1
184-4-6	extensive experimental analysis shows that the considered approach achieves state of the art performances on benchmark large scale datasets .	in particular , we prove that these approaches can achieve optimal learning bounds , provided the subsampling level is suitably chosen .	0	3	1	4.9242315	-4.407835	0
184-4-6	extensive experimental analysis shows that the considered approach achieves state of the art performances on benchmark large scale datasets .	these results suggest a simple incremental variant of nystrom kernel regularized least squares , where the subsampling level implements a form of computational regularization , in the sense that it controls at the same time regularization and computations .	0	3	2	3.7399566	-3.460832	0
185-5-10	we propose the infinite factorial dynamic model ( ifdm ) , a general bayesian nonparametric model for source separation .	our model builds on the markov indian buffet process to consider a potentially unbounded number of hidden markov chains ( sources ) that evolve independently according to some dynamics , in which the state space can be either discrete or continuous .	1	0	1	-4.429235	4.153452	1
185-5-10	for posterior inference , we develop an algorithm based on particle gibbs with ancestor sampling that can be efficiently applied to a wide range of source separation problems .	we propose the infinite factorial dynamic model ( ifdm ) , a general bayesian nonparametric model for source separation .	0	2	0	5.560212	-4.844692	0
185-5-10	we propose the infinite factorial dynamic model ( ifdm ) , a general bayesian nonparametric model for source separation .	we evaluate the performance of our ifdm on four well-known applications : multitarget tracking , cocktail party , power disaggregation , and multiuser detection .	1	0	3	-5.915845	5.175882	1
185-5-10	our experimental results show that our approach for source separation does not only outperform previous approaches , but it can also handle problems that were computationally intractable for existing approaches .	we propose the infinite factorial dynamic model ( ifdm ) , a general bayesian nonparametric model for source separation .	0	4	0	5.4499197	-4.798824	0
185-5-10	for posterior inference , we develop an algorithm based on particle gibbs with ancestor sampling that can be efficiently applied to a wide range of source separation problems .	our model builds on the markov indian buffet process to consider a potentially unbounded number of hidden markov chains ( sources ) that evolve independently according to some dynamics , in which the state space can be either discrete or continuous .	0	2	1	5.2718935	-4.6149197	0
185-5-10	our model builds on the markov indian buffet process to consider a potentially unbounded number of hidden markov chains ( sources ) that evolve independently according to some dynamics , in which the state space can be either discrete or continuous .	we evaluate the performance of our ifdm on four well-known applications : multitarget tracking , cocktail party , power disaggregation , and multiuser detection .	1	1	3	-5.9935417	5.211521	1
185-5-10	our experimental results show that our approach for source separation does not only outperform previous approaches , but it can also handle problems that were computationally intractable for existing approaches .	our model builds on the markov indian buffet process to consider a potentially unbounded number of hidden markov chains ( sources ) that evolve independently according to some dynamics , in which the state space can be either discrete or continuous .	0	4	1	5.433559	-4.7567163	0
185-5-10	we evaluate the performance of our ifdm on four well-known applications : multitarget tracking , cocktail party , power disaggregation , and multiuser detection .	for posterior inference , we develop an algorithm based on particle gibbs with ancestor sampling that can be efficiently applied to a wide range of source separation problems .	0	3	2	4.570797	-4.109702	0
185-5-10	for posterior inference , we develop an algorithm based on particle gibbs with ancestor sampling that can be efficiently applied to a wide range of source separation problems .	our experimental results show that our approach for source separation does not only outperform previous approaches , but it can also handle problems that were computationally intractable for existing approaches .	1	2	4	-5.438312	4.904601	1
185-5-10	we evaluate the performance of our ifdm on four well-known applications : multitarget tracking , cocktail party , power disaggregation , and multiuser detection .	our experimental results show that our approach for source separation does not only outperform previous approaches , but it can also handle problems that were computationally intractable for existing approaches .	1	3	4	-4.3987446	4.068986	1
186-5-10	careful tuning of a regularization parameter is indispensable in many machine learning tasks because it has a significant impact on generalization performances .	nevertheless , current practice of regularization parameter tuning is more of an art than a science , e.g. , it is hard to tell how many grid-points would be needed in cross-validation ( cv ) for obtaining a solution with sufficiently small cv error .	1	0	1	-5.4189944	5.0129476	1
186-5-10	in this paper we propose a novel framework for computing a lower bound of the cv errors as a function of the regularization parameter , which we call regularization path of cv error lower bounds .	careful tuning of a regularization parameter is indispensable in many machine learning tasks because it has a significant impact on generalization performances .	0	2	0	5.4341717	-4.8901806	0
186-5-10	careful tuning of a regularization parameter is indispensable in many machine learning tasks because it has a significant impact on generalization performances .	the proposed framework can be used for providing a theoretical approximation guarantee on a set of solutions in the sense that how far the cv error of the current best solution could be away from best possible cv error in the entire range of the regularization parameters .	1	0	3	-5.913025	5.2235584	1
186-5-10	our numerical experiments demonstrate that a theoretically guaranteed choice of a regularization parameter in the above sense is possible with reasonable computational costs .	careful tuning of a regularization parameter is indispensable in many machine learning tasks because it has a significant impact on generalization performances .	0	4	0	5.6164227	-5.0583715	0
186-5-10	nevertheless , current practice of regularization parameter tuning is more of an art than a science , e.g. , it is hard to tell how many grid-points would be needed in cross-validation ( cv ) for obtaining a solution with sufficiently small cv error .	in this paper we propose a novel framework for computing a lower bound of the cv errors as a function of the regularization parameter , which we call regularization path of cv error lower bounds .	1	1	2	-5.9269676	5.2187734	1
186-5-10	[CLS] the proposed framework can be used for providing a theoretical approximation guarantee on a set of solutions in the sense that how far the cv error of the current best solution could be away from best possible cv error in the entire range of the regularization parameters.	[CLS] nevertheless, current practice of regularization parameter tuning is more of an art than a science, e. g., it is hard to tell how many grid - points would be needed in cross - validation ( cv ) for obtaining a solution with sufficiently	0	3	1	4.8523664	-4.331221	0
186-5-10	our numerical experiments demonstrate that a theoretically guaranteed choice of a regularization parameter in the above sense is possible with reasonable computational costs .	nevertheless , current practice of regularization parameter tuning is more of an art than a science , e.g. , it is hard to tell how many grid-points would be needed in cross-validation ( cv ) for obtaining a solution with sufficiently small cv error .	0	4	1	5.549261	-4.9305964	0
186-5-10	the proposed framework can be used for providing a theoretical approximation guarantee on a set of solutions in the sense that how far the cv error of the current best solution could be away from best possible cv error in the entire range of the regularization parameters .	in this paper we propose a novel framework for computing a lower bound of the cv errors as a function of the regularization parameter , which we call regularization path of cv error lower bounds .	0	3	2	5.492963	-4.8892913	0
186-5-10	our numerical experiments demonstrate that a theoretically guaranteed choice of a regularization parameter in the above sense is possible with reasonable computational costs .	in this paper we propose a novel framework for computing a lower bound of the cv errors as a function of the regularization parameter , which we call regularization path of cv error lower bounds .	0	4	2	5.487813	-4.895007	0
186-5-10	our numerical experiments demonstrate that a theoretically guaranteed choice of a regularization parameter in the above sense is possible with reasonable computational costs .	the proposed framework can be used for providing a theoretical approximation guarantee on a set of solutions in the sense that how far the cv error of the current best solution could be away from best possible cv error in the entire range of the regularization parameters .	0	4	3	3.609006	-3.3696523	0
187-9-36	while the mechanism for rapid path planning is unknown , the ca3 region in the hippocampus plays an important role , and emerging evidence suggests that place cell activity during hippocampal `` preplay '' periods may trace out future goal-directed trajectories .	rodents navigating in a well-known environment can rapidly learn and revisit observed reward locations , often after a single trial .	0	1	0	5.5437827	-4.7448115	0
187-9-36	rodents navigating in a well-known environment can rapidly learn and revisit observed reward locations , often after a single trial .	here , we show how a particular mapping of space allows for the immediate generation of trajectories between arbitrary start and goal locations in an environment , based only on the mapped representation of the goal .	1	0	2	-5.960912	5.220982	1
187-9-36	we show that this representation can be implemented in a neural attractor network model , resulting in bump-like activity profiles resembling those of the ca3 region of hippocampus .	rodents navigating in a well-known environment can rapidly learn and revisit observed reward locations , often after a single trial .	0	3	0	5.7071447	-5.113326	0
187-9-36	neurons tend to locally excite neurons with similar place field centers , while inhibiting other neurons with distant place field centers , such that stable bumps of activity can form at arbitrary locations in the environment .	rodents navigating in a well-known environment can rapidly learn and revisit observed reward locations , often after a single trial .	0	4	0	5.2746716	-4.618915	0
187-9-36	rodents navigating in a well-known environment can rapidly learn and revisit observed reward locations , often after a single trial .	the network is initialized to represent a point in the environment , then weakly stimulated with an input corresponding to an arbitrary goal location .	1	0	5	-5.806818	5.1624546	1
187-9-36	we show that the resulting activity can be interpreted as a gradient ascent on the value function induced by a reward at the goal location .	rodents navigating in a well-known environment can rapidly learn and revisit observed reward locations , often after a single trial .	0	6	0	5.7447996	-5.1733694	0
187-9-36	rodents navigating in a well-known environment can rapidly learn and revisit observed reward locations , often after a single trial .	indeed , in networks with large place fields , we show that the network properties cause the bump to move smoothly from its initial location to the goal , around obstacles or walls .	1	0	7	-5.9113092	5.2191353	1
187-9-36	rodents navigating in a well-known environment can rapidly learn and revisit observed reward locations , often after a single trial .	our results illustrate that an attractor network with hippocampal-like attributes may be important for rapid path planning .	1	0	8	-5.9782276	5.10305	1
187-9-36	while the mechanism for rapid path planning is unknown , the ca3 region in the hippocampus plays an important role , and emerging evidence suggests that place cell activity during hippocampal `` preplay '' periods may trace out future goal-directed trajectories .	here , we show how a particular mapping of space allows for the immediate generation of trajectories between arbitrary start and goal locations in an environment , based only on the mapped representation of the goal .	1	1	2	-5.613511	5.088647	1
187-9-36	we show that this representation can be implemented in a neural attractor network model , resulting in bump-like activity profiles resembling those of the ca3 region of hippocampus .	while the mechanism for rapid path planning is unknown , the ca3 region in the hippocampus plays an important role , and emerging evidence suggests that place cell activity during hippocampal `` preplay '' periods may trace out future goal-directed trajectories .	0	3	1	5.231721	-4.7498736	0
187-9-36	while the mechanism for rapid path planning is unknown , the ca3 region in the hippocampus plays an important role , and emerging evidence suggests that place cell activity during hippocampal `` preplay '' periods may trace out future goal-directed trajectories .	neurons tend to locally excite neurons with similar place field centers , while inhibiting other neurons with distant place field centers , such that stable bumps of activity can form at arbitrary locations in the environment .	1	1	4	-1.4034294	1.5526245	1
187-9-36	the network is initialized to represent a point in the environment , then weakly stimulated with an input corresponding to an arbitrary goal location .	while the mechanism for rapid path planning is unknown , the ca3 region in the hippocampus plays an important role , and emerging evidence suggests that place cell activity during hippocampal `` preplay '' periods may trace out future goal-directed trajectories .	0	5	1	5.324504	-4.8532963	0
187-9-36	while the mechanism for rapid path planning is unknown , the ca3 region in the hippocampus plays an important role , and emerging evidence suggests that place cell activity during hippocampal `` preplay '' periods may trace out future goal-directed trajectories .	we show that the resulting activity can be interpreted as a gradient ascent on the value function induced by a reward at the goal location .	1	1	6	-5.890669	5.140354	1
187-9-36	indeed , in networks with large place fields , we show that the network properties cause the bump to move smoothly from its initial location to the goal , around obstacles or walls .	while the mechanism for rapid path planning is unknown , the ca3 region in the hippocampus plays an important role , and emerging evidence suggests that place cell activity during hippocampal `` preplay '' periods may trace out future goal-directed trajectories .	0	7	1	5.175501	-4.649745	0
187-9-36	while the mechanism for rapid path planning is unknown , the ca3 region in the hippocampus plays an important role , and emerging evidence suggests that place cell activity during hippocampal `` preplay '' periods may trace out future goal-directed trajectories .	our results illustrate that an attractor network with hippocampal-like attributes may be important for rapid path planning .	1	1	8	-5.9874954	5.1215806	1
187-9-36	here , we show how a particular mapping of space allows for the immediate generation of trajectories between arbitrary start and goal locations in an environment , based only on the mapped representation of the goal .	we show that this representation can be implemented in a neural attractor network model , resulting in bump-like activity profiles resembling those of the ca3 region of hippocampus .	1	2	3	-5.9849234	5.1631813	1
187-9-36	here , we show how a particular mapping of space allows for the immediate generation of trajectories between arbitrary start and goal locations in an environment , based only on the mapped representation of the goal .	neurons tend to locally excite neurons with similar place field centers , while inhibiting other neurons with distant place field centers , such that stable bumps of activity can form at arbitrary locations in the environment .	1	2	4	3.9580002	-3.6109917	0
187-9-36	here , we show how a particular mapping of space allows for the immediate generation of trajectories between arbitrary start and goal locations in an environment , based only on the mapped representation of the goal .	the network is initialized to represent a point in the environment , then weakly stimulated with an input corresponding to an arbitrary goal location .	1	2	5	-5.5318975	5.0352182	1
187-9-36	here , we show how a particular mapping of space allows for the immediate generation of trajectories between arbitrary start and goal locations in an environment , based only on the mapped representation of the goal .	we show that the resulting activity can be interpreted as a gradient ascent on the value function induced by a reward at the goal location .	1	2	6	-5.594407	5.073536	1
187-9-36	here , we show how a particular mapping of space allows for the immediate generation of trajectories between arbitrary start and goal locations in an environment , based only on the mapped representation of the goal .	indeed , in networks with large place fields , we show that the network properties cause the bump to move smoothly from its initial location to the goal , around obstacles or walls .	1	2	7	-5.542973	5.0369954	1
187-9-36	here , we show how a particular mapping of space allows for the immediate generation of trajectories between arbitrary start and goal locations in an environment , based only on the mapped representation of the goal .	our results illustrate that an attractor network with hippocampal-like attributes may be important for rapid path planning .	1	2	8	-5.980891	5.128588	1
187-9-36	we show that this representation can be implemented in a neural attractor network model , resulting in bump-like activity profiles resembling those of the ca3 region of hippocampus .	neurons tend to locally excite neurons with similar place field centers , while inhibiting other neurons with distant place field centers , such that stable bumps of activity can form at arbitrary locations in the environment .	1	3	4	4.988871	-4.4588823	0
187-9-36	the network is initialized to represent a point in the environment , then weakly stimulated with an input corresponding to an arbitrary goal location .	we show that this representation can be implemented in a neural attractor network model , resulting in bump-like activity profiles resembling those of the ca3 region of hippocampus .	0	5	3	3.865456	-3.524722	0
187-9-36	we show that the resulting activity can be interpreted as a gradient ascent on the value function induced by a reward at the goal location .	we show that this representation can be implemented in a neural attractor network model , resulting in bump-like activity profiles resembling those of the ca3 region of hippocampus .	0	6	3	0.30549884	0.02723582	0
187-9-36	indeed , in networks with large place fields , we show that the network properties cause the bump to move smoothly from its initial location to the goal , around obstacles or walls .	we show that this representation can be implemented in a neural attractor network model , resulting in bump-like activity profiles resembling those of the ca3 region of hippocampus .	0	7	3	0.83674145	-0.4463049	0
187-9-36	our results illustrate that an attractor network with hippocampal-like attributes may be important for rapid path planning .	we show that this representation can be implemented in a neural attractor network model , resulting in bump-like activity profiles resembling those of the ca3 region of hippocampus .	0	8	3	5.3598485	-4.7411757	0
187-9-36	the network is initialized to represent a point in the environment , then weakly stimulated with an input corresponding to an arbitrary goal location .	neurons tend to locally excite neurons with similar place field centers , while inhibiting other neurons with distant place field centers , such that stable bumps of activity can form at arbitrary locations in the environment .	0	5	4	5.097941	-4.56491	0
187-9-36	neurons tend to locally excite neurons with similar place field centers , while inhibiting other neurons with distant place field centers , such that stable bumps of activity can form at arbitrary locations in the environment .	we show that the resulting activity can be interpreted as a gradient ascent on the value function induced by a reward at the goal location .	1	4	6	-5.9806795	5.093625	1
187-9-36	neurons tend to locally excite neurons with similar place field centers , while inhibiting other neurons with distant place field centers , such that stable bumps of activity can form at arbitrary locations in the environment .	indeed , in networks with large place fields , we show that the network properties cause the bump to move smoothly from its initial location to the goal , around obstacles or walls .	1	4	7	-5.9270983	5.202648	1
187-9-36	neurons tend to locally excite neurons with similar place field centers , while inhibiting other neurons with distant place field centers , such that stable bumps of activity can form at arbitrary locations in the environment .	our results illustrate that an attractor network with hippocampal-like attributes may be important for rapid path planning .	1	4	8	-5.9673195	5.011261	1
187-9-36	we show that the resulting activity can be interpreted as a gradient ascent on the value function induced by a reward at the goal location .	the network is initialized to represent a point in the environment , then weakly stimulated with an input corresponding to an arbitrary goal location .	0	6	5	-0.68908113	0.88619214	1
187-9-36	the network is initialized to represent a point in the environment , then weakly stimulated with an input corresponding to an arbitrary goal location .	indeed , in networks with large place fields , we show that the network properties cause the bump to move smoothly from its initial location to the goal , around obstacles or walls .	1	5	7	0.6673907	-0.56170535	0
187-9-36	our results illustrate that an attractor network with hippocampal-like attributes may be important for rapid path planning .	the network is initialized to represent a point in the environment , then weakly stimulated with an input corresponding to an arbitrary goal location .	0	8	5	4.80391	-4.3534503	0
187-9-36	indeed , in networks with large place fields , we show that the network properties cause the bump to move smoothly from its initial location to the goal , around obstacles or walls .	we show that the resulting activity can be interpreted as a gradient ascent on the value function induced by a reward at the goal location .	0	7	6	-2.5896602	2.6896405	1
187-9-36	our results illustrate that an attractor network with hippocampal-like attributes may be important for rapid path planning .	we show that the resulting activity can be interpreted as a gradient ascent on the value function induced by a reward at the goal location .	0	8	6	5.280591	-4.7229476	0
187-9-36	our results illustrate that an attractor network with hippocampal-like attributes may be important for rapid path planning .	indeed , in networks with large place fields , we show that the network properties cause the bump to move smoothly from its initial location to the goal , around obstacles or walls .	0	8	7	4.4888034	-4.0527754	0
188-4-6	teaching machines to read natural language documents remains an elusive challenge .	machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen , but until now large scale training and test datasets have been missing for this type of evaluation .	1	0	1	-5.273804	4.8452587	1
188-4-6	in this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data .	teaching machines to read natural language documents remains an elusive challenge .	0	2	0	5.7094836	-5.0988836	0
188-4-6	this allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure .	teaching machines to read natural language documents remains an elusive challenge .	0	3	0	5.64999	-5.0631113	0
188-4-6	machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen , but until now large scale training and test datasets have been missing for this type of evaluation .	in this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data .	1	1	2	-5.965038	5.1957045	1
188-4-6	this allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure .	machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen , but until now large scale training and test datasets have been missing for this type of evaluation .	0	3	1	5.341503	-4.720188	0
188-4-6	this allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure .	in this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data .	0	3	2	3.7183828	-3.3550935	0
189-7-21	we introduce principal differences analysis ( pda ) for analyzing differences between high-dimensional distributions .	the method operates by finding the projection that maximizes the wasserstein divergence between the resulting univariate populations .	1	0	1	-5.8869095	5.2050433	1
189-7-21	we introduce principal differences analysis ( pda ) for analyzing differences between high-dimensional distributions .	relying on the cramer-wold device , it requires no assumptions about the form of the underlying distributions , nor the nature of their inter-class differences .	1	0	2	-5.8725276	5.2269	1
189-7-21	we introduce principal differences analysis ( pda ) for analyzing differences between high-dimensional distributions .	a sparse variant of the method is introduced to identify features responsible for the differences .	1	0	3	-5.95335	5.1883054	1
189-7-21	we provide algorithms for both the original minimax formulation as well as its semidefinite relaxation .	we introduce principal differences analysis ( pda ) for analyzing differences between high-dimensional distributions .	0	4	0	5.5017624	-4.827565	0
189-7-21	we introduce principal differences analysis ( pda ) for analyzing differences between high-dimensional distributions .	in addition to deriving some convergence results , we illustrate how the approach may be applied to identify differences between cell populations in the somatosensory cortex and hippocampus as manifested by single cell rna-seq .	1	0	5	-5.9948435	5.1447573	1
189-7-21	our broader framework extends beyond the specific choice of wasserstein divergence .	we introduce principal differences analysis ( pda ) for analyzing differences between high-dimensional distributions .	0	6	0	5.2498493	-4.61234	0
189-7-21	relying on the cramer-wold device , it requires no assumptions about the form of the underlying distributions , nor the nature of their inter-class differences .	the method operates by finding the projection that maximizes the wasserstein divergence between the resulting univariate populations .	0	2	1	-2.108003	2.2703524	1
189-7-21	a sparse variant of the method is introduced to identify features responsible for the differences .	the method operates by finding the projection that maximizes the wasserstein divergence between the resulting univariate populations .	0	3	1	3.5660255	-3.310958	0
189-7-21	we provide algorithms for both the original minimax formulation as well as its semidefinite relaxation .	the method operates by finding the projection that maximizes the wasserstein divergence between the resulting univariate populations .	0	4	1	3.782016	-3.4872048	0
189-7-21	the method operates by finding the projection that maximizes the wasserstein divergence between the resulting univariate populations .	in addition to deriving some convergence results , we illustrate how the approach may be applied to identify differences between cell populations in the somatosensory cortex and hippocampus as manifested by single cell rna-seq .	1	1	5	-5.9479666	5.1242013	1
189-7-21	the method operates by finding the projection that maximizes the wasserstein divergence between the resulting univariate populations .	our broader framework extends beyond the specific choice of wasserstein divergence .	1	1	6	-1.6845045	1.9225459	1
189-7-21	relying on the cramer-wold device , it requires no assumptions about the form of the underlying distributions , nor the nature of their inter-class differences .	a sparse variant of the method is introduced to identify features responsible for the differences .	1	2	3	-4.1294656	3.9276183	1
189-7-21	we provide algorithms for both the original minimax formulation as well as its semidefinite relaxation .	relying on the cramer-wold device , it requires no assumptions about the form of the underlying distributions , nor the nature of their inter-class differences .	0	4	2	4.433811	-4.0893087	0
189-7-21	relying on the cramer-wold device , it requires no assumptions about the form of the underlying distributions , nor the nature of their inter-class differences .	in addition to deriving some convergence results , we illustrate how the approach may be applied to identify differences between cell populations in the somatosensory cortex and hippocampus as manifested by single cell rna-seq .	1	2	5	-5.9816694	5.198615	1
189-7-21	our broader framework extends beyond the specific choice of wasserstein divergence .	relying on the cramer-wold device , it requires no assumptions about the form of the underlying distributions , nor the nature of their inter-class differences .	0	6	2	-1.1804475	1.3828472	1
189-7-21	we provide algorithms for both the original minimax formulation as well as its semidefinite relaxation .	a sparse variant of the method is introduced to identify features responsible for the differences .	0	4	3	2.6134953	-2.4538388	0
189-7-21	a sparse variant of the method is introduced to identify features responsible for the differences .	in addition to deriving some convergence results , we illustrate how the approach may be applied to identify differences between cell populations in the somatosensory cortex and hippocampus as manifested by single cell rna-seq .	1	3	5	-5.8156404	5.0767117	1
189-7-21	our broader framework extends beyond the specific choice of wasserstein divergence .	a sparse variant of the method is introduced to identify features responsible for the differences .	0	6	3	-3.3702507	3.3927717	1
189-7-21	in addition to deriving some convergence results , we illustrate how the approach may be applied to identify differences between cell populations in the somatosensory cortex and hippocampus as manifested by single cell rna-seq .	we provide algorithms for both the original minimax formulation as well as its semidefinite relaxation .	0	5	4	4.3300476	-3.9027166	0
189-7-21	our broader framework extends beyond the specific choice of wasserstein divergence .	we provide algorithms for both the original minimax formulation as well as its semidefinite relaxation .	0	6	4	-5.0134845	4.5987906	1
189-7-21	our broader framework extends beyond the specific choice of wasserstein divergence .	in addition to deriving some convergence results , we illustrate how the approach may be applied to identify differences between cell populations in the somatosensory cortex and hippocampus as manifested by single cell rna-seq .	0	6	5	-5.9006767	5.083982	1
190-4-6	under certain assumptions , we prove that the problem is indexable in the sense that the whittle index is a non-decreasing function of the relevant belief state .	we study the restless bandit associated with an extremely simple scalar kalman filter model in discrete time .	0	1	0	5.197556	-4.674685	0
190-4-6	in spite of the long history of this problem , this appears to be the first such proof .	we study the restless bandit associated with an extremely simple scalar kalman filter model in discrete time .	0	2	0	5.302356	-4.767637	0
190-4-6	we study the restless bandit associated with an extremely simple scalar kalman filter model in discrete time .	we use results about schur-convexity and mechanical words , which are particular binary strings intimately related to palindromes .	1	0	3	-6.003494	5.2040057	1
190-4-6	under certain assumptions , we prove that the problem is indexable in the sense that the whittle index is a non-decreasing function of the relevant belief state .	in spite of the long history of this problem , this appears to be the first such proof .	1	1	2	-3.3206055	3.3118234	1
190-4-6	under certain assumptions , we prove that the problem is indexable in the sense that the whittle index is a non-decreasing function of the relevant belief state .	we use results about schur-convexity and mechanical words , which are particular binary strings intimately related to palindromes .	1	1	3	-4.3878307	4.1377783	1
190-4-6	in spite of the long history of this problem , this appears to be the first such proof .	we use results about schur-convexity and mechanical words , which are particular binary strings intimately related to palindromes .	1	2	3	-2.7856736	2.9099126	1
191-10-45	markov random fields ( mrfs ) are a complementary model of symmetric relationships used in computer vision , spatial modeling , and social and gene expression networks .	bayesian networks are a popular representation of asymmetric ( for example causal ) relationships between random variables .	0	1	0	3.5481782	-2.9014792	0
191-10-45	a chain graph model under the lauritzen-wermuth-frydenberg interpretation ( hereafter a chain graph model ) generalizes both bayesian networks and mrfs , and can represent asymmetric and symmetric relationships together .	bayesian networks are a popular representation of asymmetric ( for example causal ) relationships between random variables .	0	2	0	5.6403885	-4.994747	0
191-10-45	as in other graphical models , the set of marginals from distributions in a chain graph model induced by the presence of hidden variables forms a complex model .	bayesian networks are a popular representation of asymmetric ( for example causal ) relationships between random variables .	0	3	0	5.638859	-4.993641	0
191-10-45	one recent approach to the study of marginal graphical models is to consider a well-behaved supermodel .	bayesian networks are a popular representation of asymmetric ( for example causal ) relationships between random variables .	0	4	0	5.5112796	-4.8871307	0
191-10-45	bayesian networks are a popular representation of asymmetric ( for example causal ) relationships between random variables .	such a supermodel of marginals of bayesian networks , defined only by conditional independences , and termed the ordinary markov model , was studied at length in .	1	0	5	-5.877227	5.1733027	1
191-10-45	bayesian networks are a popular representation of asymmetric ( for example causal ) relationships between random variables .	in this paper , we show that special mixed graphs which we call segregated graphs can be associated , via a markov property , with supermodels of marginals of chain graphs defined only by conditional independences .	1	0	6	-5.944841	5.1901207	1
191-10-45	bayesian networks are a popular representation of asymmetric ( for example causal ) relationships between random variables .	special features of segregated graphs imply the existence of a very natural factorization for these supermodels , and imply many existing results on the chain graph model , and the ordinary markov model carry over .	1	0	7	-5.863453	5.1664443	1
191-10-45	bayesian networks are a popular representation of asymmetric ( for example causal ) relationships between random variables .	our results suggest that segregated graphs define an analogue of the ordinary markov model for marginals of chain graph models .	1	0	8	-5.9514103	5.1146235	1
191-10-45	we illustrate the utility of segregated graphs for analyzing outcome interference in causal inference via simulated datasets .	bayesian networks are a popular representation of asymmetric ( for example causal ) relationships between random variables .	0	9	0	5.6322575	-5.025784	0
191-10-45	a chain graph model under the lauritzen-wermuth-frydenberg interpretation ( hereafter a chain graph model ) generalizes both bayesian networks and mrfs , and can represent asymmetric and symmetric relationships together .	markov random fields ( mrfs ) are a complementary model of symmetric relationships used in computer vision , spatial modeling , and social and gene expression networks .	0	2	1	5.560046	-4.9476085	0
191-10-45	markov random fields ( mrfs ) are a complementary model of symmetric relationships used in computer vision , spatial modeling , and social and gene expression networks .	as in other graphical models , the set of marginals from distributions in a chain graph model induced by the presence of hidden variables forms a complex model .	1	1	3	-5.7774844	5.2405653	1
191-10-45	one recent approach to the study of marginal graphical models is to consider a well-behaved supermodel .	markov random fields ( mrfs ) are a complementary model of symmetric relationships used in computer vision , spatial modeling , and social and gene expression networks .	0	4	1	5.150693	-4.5576878	0
191-10-45	such a supermodel of marginals of bayesian networks , defined only by conditional independences , and termed the ordinary markov model , was studied at length in .	markov random fields ( mrfs ) are a complementary model of symmetric relationships used in computer vision , spatial modeling , and social and gene expression networks .	0	5	1	5.347152	-4.727878	0
191-10-45	in this paper , we show that special mixed graphs which we call segregated graphs can be associated , via a markov property , with supermodels of marginals of chain graphs defined only by conditional independences .	markov random fields ( mrfs ) are a complementary model of symmetric relationships used in computer vision , spatial modeling , and social and gene expression networks .	0	6	1	5.3691435	-4.8011975	0
191-10-45	markov random fields ( mrfs ) are a complementary model of symmetric relationships used in computer vision , spatial modeling , and social and gene expression networks .	special features of segregated graphs imply the existence of a very natural factorization for these supermodels , and imply many existing results on the chain graph model , and the ordinary markov model carry over .	1	1	7	-5.8823566	5.239593	1
191-10-45	markov random fields ( mrfs ) are a complementary model of symmetric relationships used in computer vision , spatial modeling , and social and gene expression networks .	our results suggest that segregated graphs define an analogue of the ordinary markov model for marginals of chain graph models .	1	1	8	-6.0067453	5.21784	1
191-10-45	we illustrate the utility of segregated graphs for analyzing outcome interference in causal inference via simulated datasets .	markov random fields ( mrfs ) are a complementary model of symmetric relationships used in computer vision , spatial modeling , and social and gene expression networks .	0	9	1	5.6309814	-5.0333986	0
191-10-45	a chain graph model under the lauritzen-wermuth-frydenberg interpretation ( hereafter a chain graph model ) generalizes both bayesian networks and mrfs , and can represent asymmetric and symmetric relationships together .	as in other graphical models , the set of marginals from distributions in a chain graph model induced by the presence of hidden variables forms a complex model .	1	2	3	-0.441629	0.69967926	1
191-10-45	a chain graph model under the lauritzen-wermuth-frydenberg interpretation ( hereafter a chain graph model ) generalizes both bayesian networks and mrfs , and can represent asymmetric and symmetric relationships together .	one recent approach to the study of marginal graphical models is to consider a well-behaved supermodel .	1	2	4	4.206048	-3.8338766	0
191-10-45	such a supermodel of marginals of bayesian networks , defined only by conditional independences , and termed the ordinary markov model , was studied at length in .	a chain graph model under the lauritzen-wermuth-frydenberg interpretation ( hereafter a chain graph model ) generalizes both bayesian networks and mrfs , and can represent asymmetric and symmetric relationships together .	0	5	2	2.1943777	-1.8501263	0
191-10-45	a chain graph model under the lauritzen-wermuth-frydenberg interpretation ( hereafter a chain graph model ) generalizes both bayesian networks and mrfs , and can represent asymmetric and symmetric relationships together .	in this paper , we show that special mixed graphs which we call segregated graphs can be associated , via a markov property , with supermodels of marginals of chain graphs defined only by conditional independences .	1	2	6	-2.5239348	2.5480552	1
191-10-45	a chain graph model under the lauritzen-wermuth-frydenberg interpretation ( hereafter a chain graph model ) generalizes both bayesian networks and mrfs , and can represent asymmetric and symmetric relationships together .	special features of segregated graphs imply the existence of a very natural factorization for these supermodels , and imply many existing results on the chain graph model , and the ordinary markov model carry over .	1	2	7	-5.766842	5.1720266	1
191-10-45	our results suggest that segregated graphs define an analogue of the ordinary markov model for marginals of chain graph models .	a chain graph model under the lauritzen-wermuth-frydenberg interpretation ( hereafter a chain graph model ) generalizes both bayesian networks and mrfs , and can represent asymmetric and symmetric relationships together .	0	8	2	5.38221	-4.780404	0
191-10-45	we illustrate the utility of segregated graphs for analyzing outcome interference in causal inference via simulated datasets .	a chain graph model under the lauritzen-wermuth-frydenberg interpretation ( hereafter a chain graph model ) generalizes both bayesian networks and mrfs , and can represent asymmetric and symmetric relationships together .	0	9	2	5.457007	-4.797287	0
191-10-45	one recent approach to the study of marginal graphical models is to consider a well-behaved supermodel .	as in other graphical models , the set of marginals from distributions in a chain graph model induced by the presence of hidden variables forms a complex model .	0	4	3	-5.1180162	4.7326827	1
191-10-45	such a supermodel of marginals of bayesian networks , defined only by conditional independences , and termed the ordinary markov model , was studied at length in .	as in other graphical models , the set of marginals from distributions in a chain graph model induced by the presence of hidden variables forms a complex model .	0	5	3	1.8594303	-1.7056386	0
191-10-45	in this paper , we show that special mixed graphs which we call segregated graphs can be associated , via a markov property , with supermodels of marginals of chain graphs defined only by conditional independences .	as in other graphical models , the set of marginals from distributions in a chain graph model induced by the presence of hidden variables forms a complex model .	0	6	3	2.6017275	-2.3540053	0
191-10-45	special features of segregated graphs imply the existence of a very natural factorization for these supermodels , and imply many existing results on the chain graph model , and the ordinary markov model carry over .	as in other graphical models , the set of marginals from distributions in a chain graph model induced by the presence of hidden variables forms a complex model .	0	7	3	4.4646688	-4.010424	0
191-10-45	as in other graphical models , the set of marginals from distributions in a chain graph model induced by the presence of hidden variables forms a complex model .	our results suggest that segregated graphs define an analogue of the ordinary markov model for marginals of chain graph models .	1	3	8	-5.958032	5.1289935	1
191-10-45	we illustrate the utility of segregated graphs for analyzing outcome interference in causal inference via simulated datasets .	as in other graphical models , the set of marginals from distributions in a chain graph model induced by the presence of hidden variables forms a complex model .	0	9	3	5.3373604	-4.735653	0
191-10-45	one recent approach to the study of marginal graphical models is to consider a well-behaved supermodel .	such a supermodel of marginals of bayesian networks , defined only by conditional independences , and termed the ordinary markov model , was studied at length in .	1	4	5	-5.8560896	5.224355	1
191-10-45	one recent approach to the study of marginal graphical models is to consider a well-behaved supermodel .	in this paper , we show that special mixed graphs which we call segregated graphs can be associated , via a markov property , with supermodels of marginals of chain graphs defined only by conditional independences .	1	4	6	-5.8747826	5.238477	1
191-10-45	special features of segregated graphs imply the existence of a very natural factorization for these supermodels , and imply many existing results on the chain graph model , and the ordinary markov model carry over .	one recent approach to the study of marginal graphical models is to consider a well-behaved supermodel .	0	7	4	5.4183345	-4.772951	0
191-10-45	one recent approach to the study of marginal graphical models is to consider a well-behaved supermodel .	our results suggest that segregated graphs define an analogue of the ordinary markov model for marginals of chain graph models .	1	4	8	-6.023897	5.1708336	1
191-10-45	we illustrate the utility of segregated graphs for analyzing outcome interference in causal inference via simulated datasets .	one recent approach to the study of marginal graphical models is to consider a well-behaved supermodel .	0	9	4	5.537307	-4.9308276	0
191-10-45	in this paper , we show that special mixed graphs which we call segregated graphs can be associated , via a markov property , with supermodels of marginals of chain graphs defined only by conditional independences .	such a supermodel of marginals of bayesian networks , defined only by conditional independences , and termed the ordinary markov model , was studied at length in .	0	6	5	-5.1629057	4.809656	1
191-10-45	such a supermodel of marginals of bayesian networks , defined only by conditional independences , and termed the ordinary markov model , was studied at length in .	special features of segregated graphs imply the existence of a very natural factorization for these supermodels , and imply many existing results on the chain graph model , and the ordinary markov model carry over .	1	5	7	-5.323359	4.8645973	1
191-10-45	such a supermodel of marginals of bayesian networks , defined only by conditional independences , and termed the ordinary markov model , was studied at length in .	our results suggest that segregated graphs define an analogue of the ordinary markov model for marginals of chain graph models .	1	5	8	-5.993414	5.1034513	1
191-10-45	we illustrate the utility of segregated graphs for analyzing outcome interference in causal inference via simulated datasets .	such a supermodel of marginals of bayesian networks , defined only by conditional independences , and termed the ordinary markov model , was studied at length in .	0	9	5	5.4399385	-4.8175826	0
191-10-45	in this paper , we show that special mixed graphs which we call segregated graphs can be associated , via a markov property , with supermodels of marginals of chain graphs defined only by conditional independences .	special features of segregated graphs imply the existence of a very natural factorization for these supermodels , and imply many existing results on the chain graph model , and the ordinary markov model carry over .	1	6	7	-5.8517933	5.168271	1
191-10-45	our results suggest that segregated graphs define an analogue of the ordinary markov model for marginals of chain graph models .	in this paper , we show that special mixed graphs which we call segregated graphs can be associated , via a markov property , with supermodels of marginals of chain graphs defined only by conditional independences .	0	8	6	5.5958133	-5.0009704	0
191-10-45	we illustrate the utility of segregated graphs for analyzing outcome interference in causal inference via simulated datasets .	in this paper , we show that special mixed graphs which we call segregated graphs can be associated , via a markov property , with supermodels of marginals of chain graphs defined only by conditional independences .	0	9	6	5.512967	-4.867811	0
191-10-45	our results suggest that segregated graphs define an analogue of the ordinary markov model for marginals of chain graph models .	special features of segregated graphs imply the existence of a very natural factorization for these supermodels , and imply many existing results on the chain graph model , and the ordinary markov model carry over .	0	8	7	1.8192644	-1.422732	0
191-10-45	special features of segregated graphs imply the existence of a very natural factorization for these supermodels , and imply many existing results on the chain graph model , and the ordinary markov model carry over .	we illustrate the utility of segregated graphs for analyzing outcome interference in causal inference via simulated datasets .	1	7	9	-5.895771	5.0907035	1
191-10-45	our results suggest that segregated graphs define an analogue of the ordinary markov model for marginals of chain graph models .	we illustrate the utility of segregated graphs for analyzing outcome interference in causal inference via simulated datasets .	1	8	9	-3.8757842	3.768258	1
192-8-28	standard algorithms for decision tree induction optimize the split functions one node at a time according to some splitting criteria .	decision trees and randomized forests are widely used in computer vision and machine learning .	0	1	0	5.3675795	-4.7768345	0
192-8-28	decision trees and randomized forests are widely used in computer vision and machine learning .	this greedy procedure often leads to suboptimal trees .	1	0	2	-5.8533616	5.1459436	1
192-8-28	decision trees and randomized forests are widely used in computer vision and machine learning .	in this paper , we present an algorithm for optimizing the split functions at all levels of the tree jointly with the leaf parameters , based on a global objective .	1	0	3	-5.8810015	5.1589365	1
192-8-28	we show that the problem of finding optimal linear-combination ( oblique ) splits for decision trees is related to structured prediction with latent variables , and we formulate a convex-concave upper bound on the tree 's empirical loss .	decision trees and randomized forests are widely used in computer vision and machine learning .	0	4	0	5.6838493	-5.003566	0
192-8-28	decision trees and randomized forests are widely used in computer vision and machine learning .	computing the gradient of the proposed surrogate objective with respect to each training exemplar is o ( d2 ) , where d is the tree depth , and thus training deep trees is feasible .	1	0	5	-5.824397	5.195737	1
192-8-28	the use of stochastic gradient descent for optimization enables effective training with large datasets .	decision trees and randomized forests are widely used in computer vision and machine learning .	0	6	0	5.657781	-5.090384	0
192-8-28	decision trees and randomized forests are widely used in computer vision and machine learning .	experiments on several classification benchmarks demonstrate that the resulting non-greedy decision trees outperform greedy decision tree baselines .	1	0	7	-5.8789015	5.142708	1
192-8-28	this greedy procedure often leads to suboptimal trees .	standard algorithms for decision tree induction optimize the split functions one node at a time according to some splitting criteria .	0	2	1	5.6249933	-5.0769234	0
192-8-28	standard algorithms for decision tree induction optimize the split functions one node at a time according to some splitting criteria .	in this paper , we present an algorithm for optimizing the split functions at all levels of the tree jointly with the leaf parameters , based on a global objective .	1	1	3	-5.994933	5.1179285	1
192-8-28	we show that the problem of finding optimal linear-combination ( oblique ) splits for decision trees is related to structured prediction with latent variables , and we formulate a convex-concave upper bound on the tree 's empirical loss .	standard algorithms for decision tree induction optimize the split functions one node at a time according to some splitting criteria .	0	4	1	5.000835	-4.392112	0
192-8-28	computing the gradient of the proposed surrogate objective with respect to each training exemplar is o ( d2 ) , where d is the tree depth , and thus training deep trees is feasible .	standard algorithms for decision tree induction optimize the split functions one node at a time according to some splitting criteria .	0	5	1	5.652955	-5.112277	0
192-8-28	the use of stochastic gradient descent for optimization enables effective training with large datasets .	standard algorithms for decision tree induction optimize the split functions one node at a time according to some splitting criteria .	0	6	1	5.1838174	-4.6698575	0
192-8-28	standard algorithms for decision tree induction optimize the split functions one node at a time according to some splitting criteria .	experiments on several classification benchmarks demonstrate that the resulting non-greedy decision trees outperform greedy decision tree baselines .	1	1	7	-5.959756	5.13221	1
192-8-28	this greedy procedure often leads to suboptimal trees .	in this paper , we present an algorithm for optimizing the split functions at all levels of the tree jointly with the leaf parameters , based on a global objective .	1	2	3	-0.53224736	0.64911664	1
192-8-28	we show that the problem of finding optimal linear-combination ( oblique ) splits for decision trees is related to structured prediction with latent variables , and we formulate a convex-concave upper bound on the tree 's empirical loss .	this greedy procedure often leads to suboptimal trees .	0	4	2	-3.8745687	3.6701548	1
192-8-28	computing the gradient of the proposed surrogate objective with respect to each training exemplar is o ( d2 ) , where d is the tree depth , and thus training deep trees is feasible .	this greedy procedure often leads to suboptimal trees .	0	5	2	0.5427557	-0.383332	0
192-8-28	this greedy procedure often leads to suboptimal trees .	the use of stochastic gradient descent for optimization enables effective training with large datasets .	1	2	6	2.3525977	-2.262751	0
192-8-28	this greedy procedure often leads to suboptimal trees .	experiments on several classification benchmarks demonstrate that the resulting non-greedy decision trees outperform greedy decision tree baselines .	1	2	7	-6.0023108	5.1186414	1
192-8-28	we show that the problem of finding optimal linear-combination ( oblique ) splits for decision trees is related to structured prediction with latent variables , and we formulate a convex-concave upper bound on the tree 's empirical loss .	in this paper , we present an algorithm for optimizing the split functions at all levels of the tree jointly with the leaf parameters , based on a global objective .	0	4	3	-0.10581833	0.41251856	1
192-8-28	in this paper , we present an algorithm for optimizing the split functions at all levels of the tree jointly with the leaf parameters , based on a global objective .	computing the gradient of the proposed surrogate objective with respect to each training exemplar is o ( d2 ) , where d is the tree depth , and thus training deep trees is feasible .	1	3	5	-5.4649444	4.976105	1
192-8-28	the use of stochastic gradient descent for optimization enables effective training with large datasets .	in this paper , we present an algorithm for optimizing the split functions at all levels of the tree jointly with the leaf parameters , based on a global objective .	0	6	3	-2.9344788	3.0109284	1
192-8-28	experiments on several classification benchmarks demonstrate that the resulting non-greedy decision trees outperform greedy decision tree baselines .	in this paper , we present an algorithm for optimizing the split functions at all levels of the tree jointly with the leaf parameters , based on a global objective .	0	7	3	5.5643873	-4.932914	0
192-8-28	we show that the problem of finding optimal linear-combination ( oblique ) splits for decision trees is related to structured prediction with latent variables , and we formulate a convex-concave upper bound on the tree 's empirical loss .	computing the gradient of the proposed surrogate objective with respect to each training exemplar is o ( d2 ) , where d is the tree depth , and thus training deep trees is feasible .	1	4	5	-3.4276502	3.3023171	1
192-8-28	we show that the problem of finding optimal linear-combination ( oblique ) splits for decision trees is related to structured prediction with latent variables , and we formulate a convex-concave upper bound on the tree 's empirical loss .	the use of stochastic gradient descent for optimization enables effective training with large datasets .	1	4	6	-4.929408	4.5885825	1
192-8-28	experiments on several classification benchmarks demonstrate that the resulting non-greedy decision trees outperform greedy decision tree baselines .	we show that the problem of finding optimal linear-combination ( oblique ) splits for decision trees is related to structured prediction with latent variables , and we formulate a convex-concave upper bound on the tree 's empirical loss .	0	7	4	5.584196	-4.987111	0
192-8-28	computing the gradient of the proposed surrogate objective with respect to each training exemplar is o ( d2 ) , where d is the tree depth , and thus training deep trees is feasible .	the use of stochastic gradient descent for optimization enables effective training with large datasets .	1	5	6	1.5751835	-1.3849239	0
192-8-28	experiments on several classification benchmarks demonstrate that the resulting non-greedy decision trees outperform greedy decision tree baselines .	computing the gradient of the proposed surrogate objective with respect to each training exemplar is o ( d2 ) , where d is the tree depth , and thus training deep trees is feasible .	0	7	5	5.239574	-4.74178	0
192-8-28	experiments on several classification benchmarks demonstrate that the resulting non-greedy decision trees outperform greedy decision tree baselines .	the use of stochastic gradient descent for optimization enables effective training with large datasets .	0	7	6	5.1404133	-4.6368294	0
193-8-28	one common approach is to suppose that the observed data are close to a lower-dimensional smooth manifold .	learning of low dimensional structure in multidimensional data is a canonical problem in machine learning .	0	1	0	5.599488	-5.0296845	0
193-8-28	learning of low dimensional structure in multidimensional data is a canonical problem in machine learning .	there are a rich variety of manifold learning methods available , which allow mapping of data points to the manifold .	1	0	2	-5.388311	4.897911	1
193-8-28	learning of low dimensional structure in multidimensional data is a canonical problem in machine learning .	however , there is a clear lack of probabilistic methods that allow learning of the manifold along with the generative distribution of the observed data .	1	0	3	-5.9387026	5.193388	1
193-8-28	the best attempt is the gaussian process latent variable model ( gp-lvm ) , but identifiability issues lead to poor performance .	learning of low dimensional structure in multidimensional data is a canonical problem in machine learning .	0	4	0	5.649893	-5.088262	0
193-8-28	learning of low dimensional structure in multidimensional data is a canonical problem in machine learning .	we solve these issues by proposing a novel coulomb repulsive process ( corp ) for locations of points on the manifold , inspired by physical models of electrostatic interactions among particles .	1	0	5	-5.9818935	5.1477537	1
193-8-28	learning of low dimensional structure in multidimensional data is a canonical problem in machine learning .	combining this process with a gp prior for the mapping function yields a novel electrostatic gp ( electrogp ) process .	1	0	6	-5.9003057	5.187678	1
193-8-28	learning of low dimensional structure in multidimensional data is a canonical problem in machine learning .	focusing on the simple case of a one-dimensional manifold , we develop efficient inference algorithms , and illustrate substantially improved performance in a variety of experiments including filling in missing frames in video .	1	0	7	-5.919128	5.156369	1
193-8-28	there are a rich variety of manifold learning methods available , which allow mapping of data points to the manifold .	one common approach is to suppose that the observed data are close to a lower-dimensional smooth manifold .	0	2	1	-4.8761334	4.5155716	1
193-8-28	one common approach is to suppose that the observed data are close to a lower-dimensional smooth manifold .	however , there is a clear lack of probabilistic methods that allow learning of the manifold along with the generative distribution of the observed data .	1	1	3	-4.785323	4.3720245	1
193-8-28	one common approach is to suppose that the observed data are close to a lower-dimensional smooth manifold .	the best attempt is the gaussian process latent variable model ( gp-lvm ) , but identifiability issues lead to poor performance .	1	1	4	-5.758781	5.140113	1
193-8-28	we solve these issues by proposing a novel coulomb repulsive process ( corp ) for locations of points on the manifold , inspired by physical models of electrostatic interactions among particles .	one common approach is to suppose that the observed data are close to a lower-dimensional smooth manifold .	0	5	1	5.528076	-4.78168	0
193-8-28	combining this process with a gp prior for the mapping function yields a novel electrostatic gp ( electrogp ) process .	one common approach is to suppose that the observed data are close to a lower-dimensional smooth manifold .	0	6	1	5.460058	-4.7998705	0
193-8-28	one common approach is to suppose that the observed data are close to a lower-dimensional smooth manifold .	focusing on the simple case of a one-dimensional manifold , we develop efficient inference algorithms , and illustrate substantially improved performance in a variety of experiments including filling in missing frames in video .	1	1	7	-5.992006	5.1418066	1
193-8-28	however , there is a clear lack of probabilistic methods that allow learning of the manifold along with the generative distribution of the observed data .	there are a rich variety of manifold learning methods available , which allow mapping of data points to the manifold .	0	3	2	5.370711	-4.786894	0
193-8-28	the best attempt is the gaussian process latent variable model ( gp-lvm ) , but identifiability issues lead to poor performance .	there are a rich variety of manifold learning methods available , which allow mapping of data points to the manifold .	0	4	2	5.3452673	-4.795451	0
193-8-28	there are a rich variety of manifold learning methods available , which allow mapping of data points to the manifold .	we solve these issues by proposing a novel coulomb repulsive process ( corp ) for locations of points on the manifold , inspired by physical models of electrostatic interactions among particles .	1	2	5	-5.944454	5.137623	1
193-8-28	there are a rich variety of manifold learning methods available , which allow mapping of data points to the manifold .	combining this process with a gp prior for the mapping function yields a novel electrostatic gp ( electrogp ) process .	1	2	6	-5.949483	5.2358766	1
193-8-28	focusing on the simple case of a one-dimensional manifold , we develop efficient inference algorithms , and illustrate substantially improved performance in a variety of experiments including filling in missing frames in video .	there are a rich variety of manifold learning methods available , which allow mapping of data points to the manifold .	0	7	2	5.5101	-4.8572726	0
193-8-28	however , there is a clear lack of probabilistic methods that allow learning of the manifold along with the generative distribution of the observed data .	the best attempt is the gaussian process latent variable model ( gp-lvm ) , but identifiability issues lead to poor performance .	1	3	4	-3.773756	3.6326714	1
193-8-28	we solve these issues by proposing a novel coulomb repulsive process ( corp ) for locations of points on the manifold , inspired by physical models of electrostatic interactions among particles .	however , there is a clear lack of probabilistic methods that allow learning of the manifold along with the generative distribution of the observed data .	0	5	3	5.1141853	-4.3909135	0
193-8-28	however , there is a clear lack of probabilistic methods that allow learning of the manifold along with the generative distribution of the observed data .	combining this process with a gp prior for the mapping function yields a novel electrostatic gp ( electrogp ) process .	1	3	6	-5.723381	5.173588	1
193-8-28	focusing on the simple case of a one-dimensional manifold , we develop efficient inference algorithms , and illustrate substantially improved performance in a variety of experiments including filling in missing frames in video .	however , there is a clear lack of probabilistic methods that allow learning of the manifold along with the generative distribution of the observed data .	0	7	3	5.4046307	-4.7775135	0
193-8-28	we solve these issues by proposing a novel coulomb repulsive process ( corp ) for locations of points on the manifold , inspired by physical models of electrostatic interactions among particles .	the best attempt is the gaussian process latent variable model ( gp-lvm ) , but identifiability issues lead to poor performance .	0	5	4	2.323752	-2.1270921	0
193-8-28	the best attempt is the gaussian process latent variable model ( gp-lvm ) , but identifiability issues lead to poor performance .	combining this process with a gp prior for the mapping function yields a novel electrostatic gp ( electrogp ) process .	1	4	6	-4.8276253	4.59294	1
193-8-28	the best attempt is the gaussian process latent variable model ( gp-lvm ) , but identifiability issues lead to poor performance .	focusing on the simple case of a one-dimensional manifold , we develop efficient inference algorithms , and illustrate substantially improved performance in a variety of experiments including filling in missing frames in video .	1	4	7	-5.98843	5.158952	1
193-8-28	combining this process with a gp prior for the mapping function yields a novel electrostatic gp ( electrogp ) process .	we solve these issues by proposing a novel coulomb repulsive process ( corp ) for locations of points on the manifold , inspired by physical models of electrostatic interactions among particles .	0	6	5	2.9315646	-2.5396085	0
193-8-28	focusing on the simple case of a one-dimensional manifold , we develop efficient inference algorithms , and illustrate substantially improved performance in a variety of experiments including filling in missing frames in video .	we solve these issues by proposing a novel coulomb repulsive process ( corp ) for locations of points on the manifold , inspired by physical models of electrostatic interactions among particles .	0	7	5	4.6209183	-4.092907	0
193-8-28	focusing on the simple case of a one-dimensional manifold , we develop efficient inference algorithms , and illustrate substantially improved performance in a variety of experiments including filling in missing frames in video .	combining this process with a gp prior for the mapping function yields a novel electrostatic gp ( electrogp ) process .	0	7	6	4.658706	-4.2490716	0
194-6-15	this paper presents a novel generalization of the irl problem that allows each trajectory to be generated by multiple locally consistent reward functions , hence catering to more realistic and complex experts ' behaviors .	existing inverse reinforcement learning ( irl ) algorithms have assumed each expert 's demonstrated trajectory to be produced by only a single reward function .	0	1	0	5.5128775	-4.8289175	0
194-6-15	solving our generalized irl problem thus involves not only learning these reward functions but also the stochastic transitions between them at any state ( including unvisited states ) .	existing inverse reinforcement learning ( irl ) algorithms have assumed each expert 's demonstrated trajectory to be produced by only a single reward function .	0	2	0	5.6673455	-5.040094	0
194-6-15	existing inverse reinforcement learning ( irl ) algorithms have assumed each expert 's demonstrated trajectory to be produced by only a single reward function .	by representing our irl problem with a probabilistic graphical model , an expectation-maximization ( em ) algorithm can be devised to iteratively learn the different reward functions and the stochastic transitions between them in order to jointly improve the likelihood of the expert 's demonstrated trajectories .	1	0	3	-5.8794823	5.1345625	1
194-6-15	existing inverse reinforcement learning ( irl ) algorithms have assumed each expert 's demonstrated trajectory to be produced by only a single reward function .	as a result , the most likely partition of a trajectory into segments that are generated from different locally consistent reward functions selected by em can be derived .	1	0	4	-5.9959846	5.1599503	1
194-6-15	empirical evaluation on synthetic and real-world datasets shows that our irl algorithm outperforms the state-of-the-art em clustering with maximum likelihood irl , which is , interestingly , a reduced variant of our approach .	existing inverse reinforcement learning ( irl ) algorithms have assumed each expert 's demonstrated trajectory to be produced by only a single reward function .	0	5	0	5.662203	-5.0310116	0
194-6-15	this paper presents a novel generalization of the irl problem that allows each trajectory to be generated by multiple locally consistent reward functions , hence catering to more realistic and complex experts ' behaviors .	solving our generalized irl problem thus involves not only learning these reward functions but also the stochastic transitions between them at any state ( including unvisited states ) .	1	1	2	-5.920059	5.185753	1
194-6-15	this paper presents a novel generalization of the irl problem that allows each trajectory to be generated by multiple locally consistent reward functions , hence catering to more realistic and complex experts ' behaviors .	[CLS] by representing our irl problem with a probabilistic graphical model, an expectation - maximization ( em ) algorithm can be devised to iteratively learn the different reward functions and the stochastic transitions between them in order to jointly improve the likelihood of the expert's demonstrated traject	1	1	3	-5.991935	5.1423454	1
194-6-15	this paper presents a novel generalization of the irl problem that allows each trajectory to be generated by multiple locally consistent reward functions , hence catering to more realistic and complex experts ' behaviors .	as a result , the most likely partition of a trajectory into segments that are generated from different locally consistent reward functions selected by em can be derived .	1	1	4	-5.4479623	4.9538784	1
194-6-15	empirical evaluation on synthetic and real-world datasets shows that our irl algorithm outperforms the state-of-the-art em clustering with maximum likelihood irl , which is , interestingly , a reduced variant of our approach .	this paper presents a novel generalization of the irl problem that allows each trajectory to be generated by multiple locally consistent reward functions , hence catering to more realistic and complex experts ' behaviors .	0	5	1	5.593691	-4.962886	0
194-6-15	by representing our irl problem with a probabilistic graphical model , an expectation-maximization ( em ) algorithm can be devised to iteratively learn the different reward functions and the stochastic transitions between them in order to jointly improve the likelihood of the expert 's demonstrated trajectories .	solving our generalized irl problem thus involves not only learning these reward functions but also the stochastic transitions between them at any state ( including unvisited states ) .	0	3	2	1.674767	-1.5622216	0
194-6-15	as a result , the most likely partition of a trajectory into segments that are generated from different locally consistent reward functions selected by em can be derived .	solving our generalized irl problem thus involves not only learning these reward functions but also the stochastic transitions between them at any state ( including unvisited states ) .	0	4	2	1.6641371	-1.5294976	0
194-6-15	empirical evaluation on synthetic and real-world datasets shows that our irl algorithm outperforms the state-of-the-art em clustering with maximum likelihood irl , which is , interestingly , a reduced variant of our approach .	solving our generalized irl problem thus involves not only learning these reward functions but also the stochastic transitions between them at any state ( including unvisited states ) .	0	5	2	5.3177257	-4.7295766	0
194-6-15	as a result , the most likely partition of a trajectory into segments that are generated from different locally consistent reward functions selected by em can be derived .	by representing our irl problem with a probabilistic graphical model , an expectation-maximization ( em ) algorithm can be devised to iteratively learn the different reward functions and the stochastic transitions between them in order to jointly improve the likelihood of the expert 's demonstrated trajectories .	0	4	3	5.173	-4.5398765	0
194-6-15	[CLS] by representing our irl problem with a probabilistic graphical model, an expectation - maximization ( em ) algorithm can be devised to iteratively learn the different reward functions and the stochastic transitions between them in order to jointly improve	[CLS] empirical evaluation on synthetic and real - world datasets shows that our irl algorithm outperforms the state - of - the - art em clustering with maximum likelihood irl, which is, interestingly, a reduced variant of our approach	1	3	5	-4.716155	4.2194796	1
194-6-15	empirical evaluation on synthetic and real-world datasets shows that our irl algorithm outperforms the state-of-the-art em clustering with maximum likelihood irl , which is , interestingly , a reduced variant of our approach .	as a result , the most likely partition of a trajectory into segments that are generated from different locally consistent reward functions selected by em can be derived .	0	5	4	5.0347767	-4.5274563	0
195-3-3	we study the fundamental limits to communication-efficient distributed methods for convex learning and optimization , under different assumptions on the information available to individual machines , and the types of functions considered .	we identify cases where existing algorithms are already worst-case optimal , as well as cases where room for further improvement is still possible .	1	0	1	-5.9622393	5.2060966	1
195-3-3	among other things , our results indicate that without similarity between the local objective functions ( due to statistical data similarity or otherwise ) many communication rounds may be required , even if the machines have unbounded computational power .	we study the fundamental limits to communication-efficient distributed methods for convex learning and optimization , under different assumptions on the information available to individual machines , and the types of functions considered .	0	2	0	5.5706396	-4.97843	0
195-3-3	among other things , our results indicate that without similarity between the local objective functions ( due to statistical data similarity or otherwise ) many communication rounds may be required , even if the machines have unbounded computational power .	we identify cases where existing algorithms are already worst-case optimal , as well as cases where room for further improvement is still possible .	0	2	1	4.5292664	-4.076226	0
196-4-6	[CLS] different from traditional variational learning or gibbs sampling approaches, the proposed learning method applies ( i ) the mirror descent algorithm for maximum a posterior inference and ( ii ) back propagation over a deep architecture together with stochastic gradient / mirror descent for model	[CLS] we develop a fully discriminative learning approach for supervised latent dirichlet allocation ( lda ) model using back propagation ( i. e., bp - slda ), which maximizes the posterior probability of the prediction variable given	0	1	0	4.827614	-4.266554	0
196-4-6	as a byproduct , we also apply this technique to develop a new learning method for the traditional unsupervised lda model ( i.e. , bp-lda ) .	we develop a fully discriminative learning approach for supervised latent dirichlet allocation ( lda ) model using back propagation ( i.e. , bp-slda ) , which maximizes the posterior probability of the prediction variable given the input document .	0	2	0	5.540657	-4.9201245	0
196-4-6	we develop a fully discriminative learning approach for supervised latent dirichlet allocation ( lda ) model using back propagation ( i.e. , bp-slda ) , which maximizes the posterior probability of the prediction variable given the input document .	experimental results on three real-world regression and classification tasks show that the proposed methods significantly outperform the previous supervised topic models , neural networks , and is on par with deep neural networks .	1	0	3	-5.995469	5.2085376	1
196-4-6	as a byproduct , we also apply this technique to develop a new learning method for the traditional unsupervised lda model ( i.e. , bp-lda ) .	[CLS] different from traditional variational learning or gibbs sampling approaches, the proposed learning method applies ( i ) the mirror descent algorithm for maximum a posterior inference and ( ii ) back propagation over a deep architecture together with stochastic gradient / mirror descent for model parameter estimation, leading to scalable	0	2	1	4.192069	-3.863791	0
196-4-6	experimental results on three real-world regression and classification tasks show that the proposed methods significantly outperform the previous supervised topic models , neural networks , and is on par with deep neural networks .	[CLS] different from traditional variational learning or gibbs sampling approaches, the proposed learning method applies ( i ) the mirror descent algorithm for maximum a posterior inference and ( ii ) back propagation over a deep architecture together with stochastic gradient / mirror descent for model parameter estimation, leading to scalable and end -	0	3	1	4.526668	-4.123848	0
196-4-6	experimental results on three real-world regression and classification tasks show that the proposed methods significantly outperform the previous supervised topic models , neural networks , and is on par with deep neural networks .	as a byproduct , we also apply this technique to develop a new learning method for the traditional unsupervised lda model ( i.e. , bp-lda ) .	0	3	2	-1.8742335	2.0322309	1
197-5-10	selecting the optimal subset from a large set of variables is a fundamental problem in various learning tasks such as feature selection , sparse regression , dictionary learning , etc .	in this paper , we propose the poss approach which employs evolutionary pareto optimization to find a small-sized subset with good performance .	1	0	1	-5.885608	5.2427034	1
197-5-10	selecting the optimal subset from a large set of variables is a fundamental problem in various learning tasks such as feature selection , sparse regression , dictionary learning , etc .	we prove that for sparse regression , poss is able to achieve the best-so-far theoretically guaranteed approximation performance efficiently .	1	0	2	-5.966539	5.158338	1
197-5-10	selecting the optimal subset from a large set of variables is a fundamental problem in various learning tasks such as feature selection , sparse regression , dictionary learning , etc .	particularly , for the exponential decay subclass , poss is proven to achieve an optimal solution .	1	0	3	-5.9423394	5.223744	1
197-5-10	empirical study verifies the theoretical results , and exhibits the superior performance of poss to greedy and convex relaxation methods .	selecting the optimal subset from a large set of variables is a fundamental problem in various learning tasks such as feature selection , sparse regression , dictionary learning , etc .	0	4	0	5.6250286	-5.0919604	0
197-5-10	we prove that for sparse regression , poss is able to achieve the best-so-far theoretically guaranteed approximation performance efficiently .	in this paper , we propose the poss approach which employs evolutionary pareto optimization to find a small-sized subset with good performance .	0	2	1	5.4979362	-4.8500447	0
197-5-10	in this paper , we propose the poss approach which employs evolutionary pareto optimization to find a small-sized subset with good performance .	particularly , for the exponential decay subclass , poss is proven to achieve an optimal solution .	1	1	3	-5.3115196	4.8294187	1
197-5-10	in this paper , we propose the poss approach which employs evolutionary pareto optimization to find a small-sized subset with good performance .	empirical study verifies the theoretical results , and exhibits the superior performance of poss to greedy and convex relaxation methods .	1	1	4	-5.9611907	5.0842075	1
197-5-10	we prove that for sparse regression , poss is able to achieve the best-so-far theoretically guaranteed approximation performance efficiently .	particularly , for the exponential decay subclass , poss is proven to achieve an optimal solution .	1	2	3	3.102481	-2.9495864	0
197-5-10	empirical study verifies the theoretical results , and exhibits the superior performance of poss to greedy and convex relaxation methods .	we prove that for sparse regression , poss is able to achieve the best-so-far theoretically guaranteed approximation performance efficiently .	0	4	2	3.3419976	-3.1529865	0
197-5-10	empirical study verifies the theoretical results , and exhibits the superior performance of poss to greedy and convex relaxation methods .	particularly , for the exponential decay subclass , poss is proven to achieve an optimal solution .	0	4	3	4.367981	-3.986824	0
198-7-21	the problem of fast normalizer computation has therefore attracted significant attention in the theoretical and applied machine learning literature .	calculation of the log-normalizer is a major computational obstacle in applications of log-linear models with large output spaces .	0	1	0	1.9014803	-1.4821727	0
198-7-21	in this paper , we analyze a recently proposed technique known as `` self-normalization '' , which introduces a regularization term in training to penalize log normalizers for deviating from zero .	calculation of the log-normalizer is a major computational obstacle in applications of log-linear models with large output spaces .	0	2	0	5.154468	-4.6159954	0
198-7-21	this makes it possible to use unnormalized model scores as approximate probabilities .	calculation of the log-normalizer is a major computational obstacle in applications of log-linear models with large output spaces .	0	3	0	5.122155	-4.542387	0
198-7-21	calculation of the log-normalizer is a major computational obstacle in applications of log-linear models with large output spaces .	empirical evidence suggests that self-normalization is extremely effective , but a theoretical understanding of why it should work , and how generally it can be applied , is largely lacking .	1	0	4	-5.6123915	5.0808125	1
198-7-21	we prove upper bounds on the loss in accuracy due to self-normalization , describe classes of input distributions that self-normalize easily , and construct explicit examples of high-variance input distributions .	calculation of the log-normalizer is a major computational obstacle in applications of log-linear models with large output spaces .	0	5	0	4.2975388	-3.8714654	0
198-7-21	calculation of the log-normalizer is a major computational obstacle in applications of log-linear models with large output spaces .	our theoretical results make predictions about the difficulty of fitting self-normalized models to several classes of distributions , and we conclude with empirical validation of these predictions .	1	0	6	-5.9371495	5.1837025	1
198-7-21	the problem of fast normalizer computation has therefore attracted significant attention in the theoretical and applied machine learning literature .	in this paper , we analyze a recently proposed technique known as `` self-normalization '' , which introduces a regularization term in training to penalize log normalizers for deviating from zero .	1	1	2	-5.984583	5.2127447	1
198-7-21	the problem of fast normalizer computation has therefore attracted significant attention in the theoretical and applied machine learning literature .	this makes it possible to use unnormalized model scores as approximate probabilities .	1	1	3	-5.9313173	5.206861	1
198-7-21	empirical evidence suggests that self-normalization is extremely effective , but a theoretical understanding of why it should work , and how generally it can be applied , is largely lacking .	the problem of fast normalizer computation has therefore attracted significant attention in the theoretical and applied machine learning literature .	0	4	1	4.786661	-4.279375	0
198-7-21	we prove upper bounds on the loss in accuracy due to self-normalization , describe classes of input distributions that self-normalize easily , and construct explicit examples of high-variance input distributions .	the problem of fast normalizer computation has therefore attracted significant attention in the theoretical and applied machine learning literature .	0	5	1	5.5754423	-5.019287	0
198-7-21	the problem of fast normalizer computation has therefore attracted significant attention in the theoretical and applied machine learning literature .	our theoretical results make predictions about the difficulty of fitting self-normalized models to several classes of distributions , and we conclude with empirical validation of these predictions .	1	1	6	-5.97851	5.1334343	1
198-7-21	in this paper , we analyze a recently proposed technique known as `` self-normalization '' , which introduces a regularization term in training to penalize log normalizers for deviating from zero .	this makes it possible to use unnormalized model scores as approximate probabilities .	1	2	3	-4.8139563	4.536232	1
198-7-21	in this paper , we analyze a recently proposed technique known as `` self-normalization '' , which introduces a regularization term in training to penalize log normalizers for deviating from zero .	empirical evidence suggests that self-normalization is extremely effective , but a theoretical understanding of why it should work , and how generally it can be applied , is largely lacking .	1	2	4	4.001424	-3.6973753	0
198-7-21	we prove upper bounds on the loss in accuracy due to self-normalization , describe classes of input distributions that self-normalize easily , and construct explicit examples of high-variance input distributions .	in this paper , we analyze a recently proposed technique known as `` self-normalization '' , which introduces a regularization term in training to penalize log normalizers for deviating from zero .	0	5	2	5.515098	-4.8632894	0
198-7-21	in this paper , we analyze a recently proposed technique known as `` self-normalization '' , which introduces a regularization term in training to penalize log normalizers for deviating from zero .	our theoretical results make predictions about the difficulty of fitting self-normalized models to several classes of distributions , and we conclude with empirical validation of these predictions .	1	2	6	-5.9238563	5.2153387	1
198-7-21	this makes it possible to use unnormalized model scores as approximate probabilities .	empirical evidence suggests that self-normalization is extremely effective , but a theoretical understanding of why it should work , and how generally it can be applied , is largely lacking .	1	3	4	4.135535	-3.7590525	0
198-7-21	this makes it possible to use unnormalized model scores as approximate probabilities .	we prove upper bounds on the loss in accuracy due to self-normalization , describe classes of input distributions that self-normalize easily , and construct explicit examples of high-variance input distributions .	1	3	5	-2.627692	2.7220678	1
198-7-21	our theoretical results make predictions about the difficulty of fitting self-normalized models to several classes of distributions , and we conclude with empirical validation of these predictions .	this makes it possible to use unnormalized model scores as approximate probabilities .	0	6	3	4.8836718	-4.471468	0
198-7-21	we prove upper bounds on the loss in accuracy due to self-normalization , describe classes of input distributions that self-normalize easily , and construct explicit examples of high-variance input distributions .	empirical evidence suggests that self-normalization is extremely effective , but a theoretical understanding of why it should work , and how generally it can be applied , is largely lacking .	0	5	4	4.329324	-3.9094312	0
198-7-21	empirical evidence suggests that self-normalization is extremely effective , but a theoretical understanding of why it should work , and how generally it can be applied , is largely lacking .	our theoretical results make predictions about the difficulty of fitting self-normalized models to several classes of distributions , and we conclude with empirical validation of these predictions .	1	4	6	-5.96076	5.2023153	1
198-7-21	we prove upper bounds on the loss in accuracy due to self-normalization , describe classes of input distributions that self-normalize easily , and construct explicit examples of high-variance input distributions .	our theoretical results make predictions about the difficulty of fitting self-normalized models to several classes of distributions , and we conclude with empirical validation of these predictions .	1	5	6	-5.6868677	5.09104	1
199-9-36	in this game , the learner chooses an action and at the same time the opponent chooses an outcome , then the learner suffers a loss and receives a feedback signal .	partial monitoring is a general model for sequential learning with limited feedback formalized as a game between two players .	0	1	0	5.420716	-4.906844	0
199-9-36	partial monitoring is a general model for sequential learning with limited feedback formalized as a game between two players .	the goal of the learner is to minimize the total loss .	1	0	2	-5.7941313	5.2145176	1
199-9-36	in this paper , we study partial monitoring with finite actions and stochastic outcomes .	partial monitoring is a general model for sequential learning with limited feedback formalized as a game between two players .	0	3	0	4.3729286	-3.895473	0
199-9-36	partial monitoring is a general model for sequential learning with limited feedback formalized as a game between two players .	we derive a logarithmic distribution-dependent regret lower bound that defines the hardness of the problem .	1	0	4	-5.9924603	5.1969967	1
199-9-36	partial monitoring is a general model for sequential learning with limited feedback formalized as a game between two players .	inspired by the dmed algorithm ( honda and takemura , 2010 ) for the multi-armed bandit problem , we propose pm-dmed , an algorithm that minimizes the distribution-dependent regret .	1	0	5	-5.957758	5.214717	1
199-9-36	partial monitoring is a general model for sequential learning with limited feedback formalized as a game between two players .	pm-dmed significantly outperforms state-of-the-art algorithms in numerical experiments .	1	0	6	-5.963446	5.19717	1
199-9-36	partial monitoring is a general model for sequential learning with limited feedback formalized as a game between two players .	to show the optimality of pm-dmed with respect to the regret bound , we slightly modify the algorithm by introducing a hinge function ( pmdmed-hinge ) .	1	0	7	-5.9074287	5.183471	1
199-9-36	then , we derive an asymptotically optimal regret upper bound of pm-dmed-hinge that matches the lower bound .	partial monitoring is a general model for sequential learning with limited feedback formalized as a game between two players .	0	8	0	5.4414873	-4.8370147	0
199-9-36	the goal of the learner is to minimize the total loss .	in this game , the learner chooses an action and at the same time the opponent chooses an outcome , then the learner suffers a loss and receives a feedback signal .	0	2	1	3.0442095	-2.8943567	0
199-9-36	in this paper , we study partial monitoring with finite actions and stochastic outcomes .	in this game , the learner chooses an action and at the same time the opponent chooses an outcome , then the learner suffers a loss and receives a feedback signal .	0	3	1	-1.2934127	1.36534	1
199-9-36	in this game , the learner chooses an action and at the same time the opponent chooses an outcome , then the learner suffers a loss and receives a feedback signal .	we derive a logarithmic distribution-dependent regret lower bound that defines the hardness of the problem .	1	1	4	-6.009887	5.2031126	1
199-9-36	inspired by the dmed algorithm ( honda and takemura , 2010 ) for the multi-armed bandit problem , we propose pm-dmed , an algorithm that minimizes the distribution-dependent regret .	in this game , the learner chooses an action and at the same time the opponent chooses an outcome , then the learner suffers a loss and receives a feedback signal .	0	5	1	4.3472743	-3.9926362	0
199-9-36	in this game , the learner chooses an action and at the same time the opponent chooses an outcome , then the learner suffers a loss and receives a feedback signal .	pm-dmed significantly outperforms state-of-the-art algorithms in numerical experiments .	1	1	6	-6.009072	5.157421	1
199-9-36	to show the optimality of pm-dmed with respect to the regret bound , we slightly modify the algorithm by introducing a hinge function ( pmdmed-hinge ) .	in this game , the learner chooses an action and at the same time the opponent chooses an outcome , then the learner suffers a loss and receives a feedback signal .	0	7	1	5.349781	-4.7537184	0
199-9-36	then , we derive an asymptotically optimal regret upper bound of pm-dmed-hinge that matches the lower bound .	in this game , the learner chooses an action and at the same time the opponent chooses an outcome , then the learner suffers a loss and receives a feedback signal .	0	8	1	5.3624153	-4.7768354	0
199-9-36	in this paper , we study partial monitoring with finite actions and stochastic outcomes .	the goal of the learner is to minimize the total loss .	0	3	2	-2.9189873	2.8382456	1
199-9-36	the goal of the learner is to minimize the total loss .	we derive a logarithmic distribution-dependent regret lower bound that defines the hardness of the problem .	1	2	4	-5.8402233	5.2174454	1
199-9-36	the goal of the learner is to minimize the total loss .	inspired by the dmed algorithm ( honda and takemura , 2010 ) for the multi-armed bandit problem , we propose pm-dmed , an algorithm that minimizes the distribution-dependent regret .	1	2	5	-5.0703807	4.6533957	1
199-9-36	the goal of the learner is to minimize the total loss .	pm-dmed significantly outperforms state-of-the-art algorithms in numerical experiments .	1	2	6	-6.010125	5.160139	1
199-9-36	the goal of the learner is to minimize the total loss .	to show the optimality of pm-dmed with respect to the regret bound , we slightly modify the algorithm by introducing a hinge function ( pmdmed-hinge ) .	1	2	7	-5.925453	5.1739426	1
199-9-36	the goal of the learner is to minimize the total loss .	then , we derive an asymptotically optimal regret upper bound of pm-dmed-hinge that matches the lower bound .	1	2	8	-5.873846	5.220158	1
199-9-36	in this paper , we study partial monitoring with finite actions and stochastic outcomes .	we derive a logarithmic distribution-dependent regret lower bound that defines the hardness of the problem .	1	3	4	-5.924038	5.236002	1
199-9-36	in this paper , we study partial monitoring with finite actions and stochastic outcomes .	inspired by the dmed algorithm ( honda and takemura , 2010 ) for the multi-armed bandit problem , we propose pm-dmed , an algorithm that minimizes the distribution-dependent regret .	1	3	5	-5.894658	5.2355046	1
199-9-36	in this paper , we study partial monitoring with finite actions and stochastic outcomes .	pm-dmed significantly outperforms state-of-the-art algorithms in numerical experiments .	1	3	6	-6.020013	5.1762466	1
199-9-36	in this paper , we study partial monitoring with finite actions and stochastic outcomes .	to show the optimality of pm-dmed with respect to the regret bound , we slightly modify the algorithm by introducing a hinge function ( pmdmed-hinge ) .	1	3	7	-5.911714	5.2270346	1
199-9-36	then , we derive an asymptotically optimal regret upper bound of pm-dmed-hinge that matches the lower bound .	in this paper , we study partial monitoring with finite actions and stochastic outcomes .	0	8	3	5.642158	-5.0153594	0
199-9-36	we derive a logarithmic distribution-dependent regret lower bound that defines the hardness of the problem .	inspired by the dmed algorithm ( honda and takemura , 2010 ) for the multi-armed bandit problem , we propose pm-dmed , an algorithm that minimizes the distribution-dependent regret .	1	4	5	4.6255517	-4.061906	0
199-9-36	pm-dmed significantly outperforms state-of-the-art algorithms in numerical experiments .	we derive a logarithmic distribution-dependent regret lower bound that defines the hardness of the problem .	0	6	4	4.9826956	-4.582275	0
199-9-36	we derive a logarithmic distribution-dependent regret lower bound that defines the hardness of the problem .	to show the optimality of pm-dmed with respect to the regret bound , we slightly modify the algorithm by introducing a hinge function ( pmdmed-hinge ) .	1	4	7	-4.9825406	4.5541897	1
199-9-36	we derive a logarithmic distribution-dependent regret lower bound that defines the hardness of the problem .	then , we derive an asymptotically optimal regret upper bound of pm-dmed-hinge that matches the lower bound .	1	4	8	-5.179311	4.7570825	1
199-9-36	pm-dmed significantly outperforms state-of-the-art algorithms in numerical experiments .	inspired by the dmed algorithm ( honda and takemura , 2010 ) for the multi-armed bandit problem , we propose pm-dmed , an algorithm that minimizes the distribution-dependent regret .	0	6	5	5.5132194	-4.890219	0
199-9-36	inspired by the dmed algorithm ( honda and takemura , 2010 ) for the multi-armed bandit problem , we propose pm-dmed , an algorithm that minimizes the distribution-dependent regret .	to show the optimality of pm-dmed with respect to the regret bound , we slightly modify the algorithm by introducing a hinge function ( pmdmed-hinge ) .	1	5	7	-5.9379807	5.215718	1
199-9-36	inspired by the dmed algorithm ( honda and takemura , 2010 ) for the multi-armed bandit problem , we propose pm-dmed , an algorithm that minimizes the distribution-dependent regret .	then , we derive an asymptotically optimal regret upper bound of pm-dmed-hinge that matches the lower bound .	1	5	8	-5.8912826	5.20528	1
199-9-36	pm-dmed significantly outperforms state-of-the-art algorithms in numerical experiments .	to show the optimality of pm-dmed with respect to the regret bound , we slightly modify the algorithm by introducing a hinge function ( pmdmed-hinge ) .	1	6	7	4.6722	-4.2471113	0
199-9-36	pm-dmed significantly outperforms state-of-the-art algorithms in numerical experiments .	then , we derive an asymptotically optimal regret upper bound of pm-dmed-hinge that matches the lower bound .	1	6	8	4.788831	-4.382047	0
199-9-36	to show the optimality of pm-dmed with respect to the regret bound , we slightly modify the algorithm by introducing a hinge function ( pmdmed-hinge ) .	then , we derive an asymptotically optimal regret upper bound of pm-dmed-hinge that matches the lower bound .	1	7	8	-3.5488238	3.4771142	1
200-4-6	it seems that the only reasonable way to aggregate these k-approval votes is the approval voting rule , which simply counts the number of times each alternative was approved .	some crowdsourcing platforms ask workers to express their opinions by approving a set of k good alternatives .	0	1	0	5.6337304	-5.06536	0
200-4-6	we challenge this assertion by proposing a probabilistic framework of noisy voting , and asking whether approval voting yields an alternative that is most likely to be the best alternative , given k-approval votes .	some crowdsourcing platforms ask workers to express their opinions by approving a set of k good alternatives .	0	2	0	5.6240764	-5.043159	0
200-4-6	some crowdsourcing platforms ask workers to express their opinions by approving a set of k good alternatives .	while the answer is generally positive , our theoretical and empirical results call attention to situations where approval voting is suboptimal .	1	0	3	-5.974332	5.121034	1
200-4-6	we challenge this assertion by proposing a probabilistic framework of noisy voting , and asking whether approval voting yields an alternative that is most likely to be the best alternative , given k-approval votes .	it seems that the only reasonable way to aggregate these k-approval votes is the approval voting rule , which simply counts the number of times each alternative was approved .	0	2	1	1.3157678	-0.9854019	0
200-4-6	while the answer is generally positive , our theoretical and empirical results call attention to situations where approval voting is suboptimal .	it seems that the only reasonable way to aggregate these k-approval votes is the approval voting rule , which simply counts the number of times each alternative was approved .	0	3	1	4.875529	-4.309305	0
200-4-6	we challenge this assertion by proposing a probabilistic framework of noisy voting , and asking whether approval voting yields an alternative that is most likely to be the best alternative , given k-approval votes .	while the answer is generally positive , our theoretical and empirical results call attention to situations where approval voting is suboptimal .	1	2	3	-5.489756	4.9510365	1
201-8-28	we are interested in supervised metric learning of mahalanobis like distances .	existing approaches mainly focus on learning a new distance using similarity and dissimilarity constraints between examples .	1	0	1	-5.091135	4.6334076	1
201-8-28	in this paper , instead of bringing closer examples of the same class and pushing far away examples of different classes we propose to move the examples with respect to virtual points .	we are interested in supervised metric learning of mahalanobis like distances .	0	2	0	5.288212	-4.7508707	0
201-8-28	we are interested in supervised metric learning of mahalanobis like distances .	hence , each example is brought closer to a a priori defined virtual point reducing the number of constraints to satisfy .	1	0	3	-5.934577	5.229718	1
201-8-28	we show that our approach admits a closed form solution which can be kernelized .	we are interested in supervised metric learning of mahalanobis like distances .	0	4	0	5.4004145	-4.880229	0
201-8-28	we are interested in supervised metric learning of mahalanobis like distances .	we provide a theoretical analysis showing the consistency of the approach and establishing some links with other classical metric learning methods .	1	0	5	-5.936624	5.098858	1
201-8-28	furthermore we propose an efficient solution to the difficult problem of selecting virtual points based in part on recent works in optimal transport .	we are interested in supervised metric learning of mahalanobis like distances .	0	6	0	5.349052	-4.798641	0
201-8-28	lastly , we evaluate our approach on several state of the art datasets .	we are interested in supervised metric learning of mahalanobis like distances .	0	7	0	5.535317	-4.9220266	0
201-8-28	existing approaches mainly focus on learning a new distance using similarity and dissimilarity constraints between examples .	in this paper , instead of bringing closer examples of the same class and pushing far away examples of different classes we propose to move the examples with respect to virtual points .	1	1	2	-5.886296	5.2345247	1
201-8-28	existing approaches mainly focus on learning a new distance using similarity and dissimilarity constraints between examples .	hence , each example is brought closer to a a priori defined virtual point reducing the number of constraints to satisfy .	1	1	3	-5.9348164	5.2107506	1
201-8-28	we show that our approach admits a closed form solution which can be kernelized .	existing approaches mainly focus on learning a new distance using similarity and dissimilarity constraints between examples .	0	4	1	5.6341734	-5.013052	0
201-8-28	we provide a theoretical analysis showing the consistency of the approach and establishing some links with other classical metric learning methods .	existing approaches mainly focus on learning a new distance using similarity and dissimilarity constraints between examples .	0	5	1	5.601902	-4.931996	0
201-8-28	furthermore we propose an efficient solution to the difficult problem of selecting virtual points based in part on recent works in optimal transport .	existing approaches mainly focus on learning a new distance using similarity and dissimilarity constraints between examples .	0	6	1	5.499014	-4.8639336	0
201-8-28	lastly , we evaluate our approach on several state of the art datasets .	existing approaches mainly focus on learning a new distance using similarity and dissimilarity constraints between examples .	0	7	1	5.679029	-5.0727706	0
201-8-28	in this paper , instead of bringing closer examples of the same class and pushing far away examples of different classes we propose to move the examples with respect to virtual points .	hence , each example is brought closer to a a priori defined virtual point reducing the number of constraints to satisfy .	1	2	3	-5.5033455	4.9544044	1
201-8-28	we show that our approach admits a closed form solution which can be kernelized .	in this paper , instead of bringing closer examples of the same class and pushing far away examples of different classes we propose to move the examples with respect to virtual points .	0	4	2	5.280667	-4.7390223	0
201-8-28	in this paper , instead of bringing closer examples of the same class and pushing far away examples of different classes we propose to move the examples with respect to virtual points .	we provide a theoretical analysis showing the consistency of the approach and establishing some links with other classical metric learning methods .	1	2	5	-5.9813356	5.108757	1
201-8-28	furthermore we propose an efficient solution to the difficult problem of selecting virtual points based in part on recent works in optimal transport .	in this paper , instead of bringing closer examples of the same class and pushing far away examples of different classes we propose to move the examples with respect to virtual points .	0	6	2	5.361378	-4.698177	0
201-8-28	lastly , we evaluate our approach on several state of the art datasets .	in this paper , instead of bringing closer examples of the same class and pushing far away examples of different classes we propose to move the examples with respect to virtual points .	0	7	2	5.6030164	-4.9539857	0
201-8-28	hence , each example is brought closer to a a priori defined virtual point reducing the number of constraints to satisfy .	we show that our approach admits a closed form solution which can be kernelized .	1	3	4	2.368823	-2.1850731	0
201-8-28	we provide a theoretical analysis showing the consistency of the approach and establishing some links with other classical metric learning methods .	hence , each example is brought closer to a a priori defined virtual point reducing the number of constraints to satisfy .	0	5	3	4.512559	-4.166333	0
201-8-28	furthermore we propose an efficient solution to the difficult problem of selecting virtual points based in part on recent works in optimal transport .	hence , each example is brought closer to a a priori defined virtual point reducing the number of constraints to satisfy .	0	6	3	4.549816	-4.1662827	0
201-8-28	hence , each example is brought closer to a a priori defined virtual point reducing the number of constraints to satisfy .	lastly , we evaluate our approach on several state of the art datasets .	1	3	7	-5.878945	5.0691133	1
201-8-28	we provide a theoretical analysis showing the consistency of the approach and establishing some links with other classical metric learning methods .	we show that our approach admits a closed form solution which can be kernelized .	0	5	4	4.562603	-4.076349	0
201-8-28	we show that our approach admits a closed form solution which can be kernelized .	furthermore we propose an efficient solution to the difficult problem of selecting virtual points based in part on recent works in optimal transport .	1	4	6	-5.875316	5.090129	1
201-8-28	we show that our approach admits a closed form solution which can be kernelized .	lastly , we evaluate our approach on several state of the art datasets .	1	4	7	-5.789549	5.0874777	1
201-8-28	we provide a theoretical analysis showing the consistency of the approach and establishing some links with other classical metric learning methods .	furthermore we propose an efficient solution to the difficult problem of selecting virtual points based in part on recent works in optimal transport .	1	5	6	-2.6353636	2.5990682	1
201-8-28	we provide a theoretical analysis showing the consistency of the approach and establishing some links with other classical metric learning methods .	lastly , we evaluate our approach on several state of the art datasets .	1	5	7	-3.9440536	3.6972356	1
201-8-28	furthermore we propose an efficient solution to the difficult problem of selecting virtual points based in part on recent works in optimal transport .	lastly , we evaluate our approach on several state of the art datasets .	1	6	7	-2.6473942	2.6544316	1
202-5-10	we investigate the robust pca problem of decomposing an observed matrix into the sum of a low-rank and a sparse error matrices via convex programming principal component pursuit ( pcp ) .	in contrast to previous studies that assume the support of the error matrix is generated by uniform bernoulli sampling , we allow non-uniform sampling , i.e. , entries of the low-rank matrix are corrupted by errors with unequal probabilities .	1	0	1	-5.8312798	5.1733503	1
202-5-10	we characterize conditions on error corruption of each individual entry based on the local incoherence of the low-rank matrix , under which correct matrix decomposition by pcp is guaranteed .	we investigate the robust pca problem of decomposing an observed matrix into the sum of a low-rank and a sparse error matrices via convex programming principal component pursuit ( pcp ) .	0	2	0	5.6803083	-5.0064282	0
202-5-10	such a refined analysis of robust pca captures how robust each entry of the low rank matrix combats error corruption .	we investigate the robust pca problem of decomposing an observed matrix into the sum of a low-rank and a sparse error matrices via convex programming principal component pursuit ( pcp ) .	0	3	0	5.618226	-5.0236063	0
202-5-10	in order to deal with non-uniform error corruption , our technical proof introduces a new weighted norm and develops/exploits the concentration properties that such a norm satisfies .	we investigate the robust pca problem of decomposing an observed matrix into the sum of a low-rank and a sparse error matrices via convex programming principal component pursuit ( pcp ) .	0	4	0	5.6509514	-5.0832186	0
202-5-10	we characterize conditions on error corruption of each individual entry based on the local incoherence of the low-rank matrix , under which correct matrix decomposition by pcp is guaranteed .	in contrast to previous studies that assume the support of the error matrix is generated by uniform bernoulli sampling , we allow non-uniform sampling , i.e. , entries of the low-rank matrix are corrupted by errors with unequal probabilities .	0	2	1	3.5434785	-3.2524714	0
202-5-10	in contrast to previous studies that assume the support of the error matrix is generated by uniform bernoulli sampling , we allow non-uniform sampling , i.e. , entries of the low-rank matrix are corrupted by errors with unequal probabilities .	such a refined analysis of robust pca captures how robust each entry of the low rank matrix combats error corruption .	1	1	3	-5.130295	4.736124	1
202-5-10	in order to deal with non-uniform error corruption , our technical proof introduces a new weighted norm and develops/exploits the concentration properties that such a norm satisfies .	in contrast to previous studies that assume the support of the error matrix is generated by uniform bernoulli sampling , we allow non-uniform sampling , i.e. , entries of the low-rank matrix are corrupted by errors with unequal probabilities .	0	4	1	5.1281414	-4.6158457	0
202-5-10	such a refined analysis of robust pca captures how robust each entry of the low rank matrix combats error corruption .	we characterize conditions on error corruption of each individual entry based on the local incoherence of the low-rank matrix , under which correct matrix decomposition by pcp is guaranteed .	0	3	2	-3.8947585	3.66514	1
202-5-10	we characterize conditions on error corruption of each individual entry based on the local incoherence of the low-rank matrix , under which correct matrix decomposition by pcp is guaranteed .	in order to deal with non-uniform error corruption , our technical proof introduces a new weighted norm and develops/exploits the concentration properties that such a norm satisfies .	1	2	4	-5.4309826	4.875984	1
202-5-10	in order to deal with non-uniform error corruption , our technical proof introduces a new weighted norm and develops/exploits the concentration properties that such a norm satisfies .	such a refined analysis of robust pca captures how robust each entry of the low rank matrix combats error corruption .	0	4	3	4.1401753	-3.7432866	0
203-4-6	in this paper we explore the representational power of these models using synthetic grammars designed to exhibit phenomena similar to those found in real transduction problems such as machine translation .	recently , strong results have been demonstrated by deep recurrent neural networks on natural language transduction problems .	0	1	0	5.3040338	-4.702074	0
203-4-6	these experiments lead us to propose new memory-based recurrent networks that implement continuously differentiable analogues of traditional data structures such as stacks , queues , and deques .	recently , strong results have been demonstrated by deep recurrent neural networks on natural language transduction problems .	0	2	0	5.347171	-4.780834	0
203-4-6	we show that these architectures exhibit superior generalisation performance to deep rnns and are often able to learn the underlying generating algorithms in our transduction experiments .	recently , strong results have been demonstrated by deep recurrent neural networks on natural language transduction problems .	0	3	0	5.5645185	-4.9524918	0
203-4-6	these experiments lead us to propose new memory-based recurrent networks that implement continuously differentiable analogues of traditional data structures such as stacks , queues , and deques .	in this paper we explore the representational power of these models using synthetic grammars designed to exhibit phenomena similar to those found in real transduction problems such as machine translation .	0	2	1	5.3583474	-4.725177	0
203-4-6	in this paper we explore the representational power of these models using synthetic grammars designed to exhibit phenomena similar to those found in real transduction problems such as machine translation .	we show that these architectures exhibit superior generalisation performance to deep rnns and are often able to learn the underlying generating algorithms in our transduction experiments .	1	1	3	-5.9983015	5.179369	1
203-4-6	we show that these architectures exhibit superior generalisation performance to deep rnns and are often able to learn the underlying generating algorithms in our transduction experiments .	these experiments lead us to propose new memory-based recurrent networks that implement continuously differentiable analogues of traditional data structures such as stacks , queues , and deques .	0	3	2	3.9390278	-3.5526032	0
204-5-10	deep generative models ( dgms ) are effective on learning multilayered representations of complex data and performing inference of input data by exploring the generative ability .	however , little work has been done on examining or empowering the discriminative ability of dgms on making accurate predictions .	1	0	1	-5.9684906	5.1468344	1
204-5-10	deep generative models ( dgms ) are effective on learning multilayered representations of complex data and performing inference of input data by exploring the generative ability .	this paper presents max-margin deep generative models ( mmdgms ) , which explore the strongly discriminative principle of max-margin learning to improve the discriminative power of dgms , while retaining the generative capability .	1	0	2	-5.898304	5.1926823	1
204-5-10	deep generative models ( dgms ) are effective on learning multilayered representations of complex data and performing inference of input data by exploring the generative ability .	we develop an efficient doubly stochastic subgradient algorithm for the piecewise linear objective .	1	0	3	-6.0131025	5.1835575	1
204-5-10	[CLS] empirical results on mnist and svhn datasets demonstrate that ( 1 ) maxmargin learning can significantly improve the prediction performance of dgms and meanwhile retain the generative ability ; and ( 2 ) mmdgms are competitive to the state - of - the - art fully discriminative networks by	deep generative models ( dgms ) are effective on learning multilayered representations of complex data and performing inference of input data by exploring the generative ability .	0	4	0	5.685153	-5.059061	0
204-5-10	this paper presents max-margin deep generative models ( mmdgms ) , which explore the strongly discriminative principle of max-margin learning to improve the discriminative power of dgms , while retaining the generative capability .	however , little work has been done on examining or empowering the discriminative ability of dgms on making accurate predictions .	0	2	1	3.3642533	-3.1755269	0
204-5-10	we develop an efficient doubly stochastic subgradient algorithm for the piecewise linear objective .	however , little work has been done on examining or empowering the discriminative ability of dgms on making accurate predictions .	0	3	1	5.156352	-4.701762	0
204-5-10	however , little work has been done on examining or empowering the discriminative ability of dgms on making accurate predictions .	[CLS] empirical results on mnist and svhn datasets demonstrate that ( 1 ) maxmargin learning can significantly improve the prediction performance of dgms and meanwhile retain the generative ability ; and ( 2 ) mmdgms are competitive to the state - of - the - art fully discriminative networks by employing deep convolution	1	1	4	-5.83195	5.24306	1
204-5-10	we develop an efficient doubly stochastic subgradient algorithm for the piecewise linear objective .	this paper presents max-margin deep generative models ( mmdgms ) , which explore the strongly discriminative principle of max-margin learning to improve the discriminative power of dgms , while retaining the generative capability .	0	3	2	5.4997644	-4.904642	0
204-5-10	[CLS] empirical results on mnist and svhn datasets demonstrate that ( 1 ) maxmargin learning can significantly improve the prediction performance of dgms and meanwhile retain the generative ability ; and ( 2 ) mmdgms are competitive to	[CLS] this paper presents max - margin deep generative models ( mmdgms ), which explore the strongly discriminative principle of max - margin learning to improve the discriminative power of dgms, while retaining the generative	0	4	2	5.5388145	-4.891305	0
204-5-10	[CLS] empirical results on mnist and svhn datasets demonstrate that ( 1 ) maxmargin learning can significantly improve the prediction performance of dgms and meanwhile retain the generative ability ; and ( 2 ) mmdgms are competitive to the state - of - the - art fully discriminative networks by employing deep convolutional neural networks ( cnns	we develop an efficient doubly stochastic subgradient algorithm for the piecewise linear objective .	0	4	3	4.89778	-4.3704176	0
205-7-21	among the commonly used kernels for nonlinear classification are polynomial kernels , for which low approximation error has thus far necessitated explicit feature maps of large dimensionality , especially for higher-order polynomials .	compact explicit feature maps provide a practical framework to scale kernel methods to large-scale learning , but deriving such maps for many types of kernels remains a challenging open problem .	0	1	0	-2.5709066	2.673036	1
205-7-21	meanwhile , because polynomial kernels are unbounded , they are frequently applied to data that has been normalized to unit 2 norm .	compact explicit feature maps provide a practical framework to scale kernel methods to large-scale learning , but deriving such maps for many types of kernels remains a challenging open problem .	0	2	0	3.7924697	-3.25038	0
205-7-21	the question we address in this work is : if we know a priori that data is normalized , can we devise a more compact map ?	compact explicit feature maps provide a practical framework to scale kernel methods to large-scale learning , but deriving such maps for many types of kernels remains a challenging open problem .	0	3	0	4.938829	-4.3617244	0
205-7-21	we show that a putative affirmative answer to this question based on random fourier features is impossible in this setting , and introduce a new approximation paradigm , spherical random fourier ( srf ) features , which circumvents these issues and delivers a compact approximation to polynomial kernels for data on the unit sphere .	compact explicit feature maps provide a practical framework to scale kernel methods to large-scale learning , but deriving such maps for many types of kernels remains a challenging open problem .	0	4	0	5.461481	-4.8321576	0
205-7-21	compact explicit feature maps provide a practical framework to scale kernel methods to large-scale learning , but deriving such maps for many types of kernels remains a challenging open problem .	compared to prior work , srf features are less rank-deficient , more compact , and achieve better kernel approximation , especially for higher-order polynomials .	1	0	5	-5.8910046	5.2316003	1
205-7-21	the resulting predictions have lower variance and typically yield better classification accuracy .	compact explicit feature maps provide a practical framework to scale kernel methods to large-scale learning , but deriving such maps for many types of kernels remains a challenging open problem .	0	6	0	5.5700636	-4.9354963	0
205-7-21	among the commonly used kernels for nonlinear classification are polynomial kernels , for which low approximation error has thus far necessitated explicit feature maps of large dimensionality , especially for higher-order polynomials .	meanwhile , because polynomial kernels are unbounded , they are frequently applied to data that has been normalized to unit 2 norm .	1	1	2	-5.805999	5.222945	1
205-7-21	among the commonly used kernels for nonlinear classification are polynomial kernels , for which low approximation error has thus far necessitated explicit feature maps of large dimensionality , especially for higher-order polynomials .	the question we address in this work is : if we know a priori that data is normalized , can we devise a more compact map ?	1	1	3	-5.846753	5.2571816	1
205-7-21	[CLS] we show that a putative affirmative answer to this question based on random fourier features is impossible in this setting, and introduce a new approximation paradigm, spherical random fourier ( srf ) features, which circumvents these issues and delivers a compact approximation to polynomial kernels for data on the	among the commonly used kernels for nonlinear classification are polynomial kernels , for which low approximation error has thus far necessitated explicit feature maps of large dimensionality , especially for higher-order polynomials .	0	4	1	5.546859	-4.8657207	0
205-7-21	among the commonly used kernels for nonlinear classification are polynomial kernels , for which low approximation error has thus far necessitated explicit feature maps of large dimensionality , especially for higher-order polynomials .	compared to prior work , srf features are less rank-deficient , more compact , and achieve better kernel approximation , especially for higher-order polynomials .	1	1	5	-5.9034634	5.1439342	1
205-7-21	the resulting predictions have lower variance and typically yield better classification accuracy .	among the commonly used kernels for nonlinear classification are polynomial kernels , for which low approximation error has thus far necessitated explicit feature maps of large dimensionality , especially for higher-order polynomials .	0	6	1	5.4303675	-4.8318033	0
205-7-21	meanwhile , because polynomial kernels are unbounded , they are frequently applied to data that has been normalized to unit 2 norm .	the question we address in this work is : if we know a priori that data is normalized , can we devise a more compact map ?	1	2	3	-5.304503	4.9171176	1
205-7-21	meanwhile , because polynomial kernels are unbounded , they are frequently applied to data that has been normalized to unit 2 norm .	we show that a putative affirmative answer to this question based on random fourier features is impossible in this setting , and introduce a new approximation paradigm , spherical random fourier ( srf ) features , which circumvents these issues and delivers a compact approximation to polynomial kernels for data on the unit sphere .	1	2	4	-5.8103857	5.266376	1
205-7-21	meanwhile , because polynomial kernels are unbounded , they are frequently applied to data that has been normalized to unit 2 norm .	compared to prior work , srf features are less rank-deficient , more compact , and achieve better kernel approximation , especially for higher-order polynomials .	1	2	5	-5.829937	5.2298727	1
205-7-21	meanwhile , because polynomial kernels are unbounded , they are frequently applied to data that has been normalized to unit 2 norm .	the resulting predictions have lower variance and typically yield better classification accuracy .	1	2	6	-5.812418	5.258505	1
205-7-21	we show that a putative affirmative answer to this question based on random fourier features is impossible in this setting , and introduce a new approximation paradigm , spherical random fourier ( srf ) features , which circumvents these issues and delivers a compact approximation to polynomial kernels for data on the unit sphere .	the question we address in this work is : if we know a priori that data is normalized , can we devise a more compact map ?	0	4	3	4.5819225	-4.0146966	0
205-7-21	the question we address in this work is : if we know a priori that data is normalized , can we devise a more compact map ?	compared to prior work , srf features are less rank-deficient , more compact , and achieve better kernel approximation , especially for higher-order polynomials .	1	3	5	-5.929274	5.152224	1
205-7-21	the question we address in this work is : if we know a priori that data is normalized , can we devise a more compact map ?	the resulting predictions have lower variance and typically yield better classification accuracy .	1	3	6	-5.402711	4.974146	1
205-7-21	compared to prior work , srf features are less rank-deficient , more compact , and achieve better kernel approximation , especially for higher-order polynomials .	we show that a putative affirmative answer to this question based on random fourier features is impossible in this setting , and introduce a new approximation paradigm , spherical random fourier ( srf ) features , which circumvents these issues and delivers a compact approximation to polynomial kernels for data on the unit sphere .	0	5	4	5.565965	-5.0121174	0
205-7-21	the resulting predictions have lower variance and typically yield better classification accuracy .	we show that a putative affirmative answer to this question based on random fourier features is impossible in this setting , and introduce a new approximation paradigm , spherical random fourier ( srf ) features , which circumvents these issues and delivers a compact approximation to polynomial kernels for data on the unit sphere .	0	6	4	3.153042	-2.9546256	0
205-7-21	compared to prior work , srf features are less rank-deficient , more compact , and achieve better kernel approximation , especially for higher-order polynomials .	the resulting predictions have lower variance and typically yield better classification accuracy .	1	5	6	-0.1659895	0.42482296	1
206-9-36	we propose rectified factor networks ( rfns ) to efficiently construct very sparse , non-linear , high-dimensional representations of the input .	rfn models identify rare and small events in the input , have a low interference between code units , have a small reconstruction error , and explain the data covariance structure .	1	0	1	-5.8246956	5.2431674	1
206-9-36	rfn learning is a generalized alternating minimization algorithm derived from the posterior regularization method which enforces non-negative and normalized posterior means .	we propose rectified factor networks ( rfns ) to efficiently construct very sparse , non-linear , high-dimensional representations of the input .	0	2	0	5.5773697	-4.9716177	0
206-9-36	we propose rectified factor networks ( rfns ) to efficiently construct very sparse , non-linear , high-dimensional representations of the input .	we proof convergence and correctness of the rfn learning algorithm .	1	0	3	-5.9658732	5.1458445	1
206-9-36	on benchmarks , rfns are compared to other unsupervised methods like autoencoders , rbms , factor analysis , ica , and pca .	we propose rectified factor networks ( rfns ) to efficiently construct very sparse , non-linear , high-dimensional representations of the input .	0	4	0	5.6278462	-4.9867435	0
206-9-36	in contrast to previous sparse coding methods , rfns yield sparser codes , capture the data 's covariance structure more precisely , and have a significantly smaller reconstruction error .	we propose rectified factor networks ( rfns ) to efficiently construct very sparse , non-linear , high-dimensional representations of the input .	0	5	0	5.637006	-4.9551125	0
206-9-36	we propose rectified factor networks ( rfns ) to efficiently construct very sparse , non-linear , high-dimensional representations of the input .	we test rfns as pretraining technique for deep networks on different vision datasets , where rfns were superior to rbms and autoencoders .	1	0	6	-5.9435163	5.170244	1
206-9-36	on gene expression data from two pharmaceutical drug discovery studies , rfns detected small and rare gene modules that revealed highly relevant new biological insights which were so far missed by other unsupervised methods .	we propose rectified factor networks ( rfns ) to efficiently construct very sparse , non-linear , high-dimensional representations of the input .	0	7	0	5.6918306	-5.0049233	0
206-9-36	rfn package for gpu/cpu is available at http : //www.bioinf.jku.at/software/rfn .	we propose rectified factor networks ( rfns ) to efficiently construct very sparse , non-linear , high-dimensional representations of the input .	0	8	0	5.529862	-4.947241	0
206-9-36	rfn models identify rare and small events in the input , have a low interference between code units , have a small reconstruction error , and explain the data covariance structure .	rfn learning is a generalized alternating minimization algorithm derived from the posterior regularization method which enforces non-negative and normalized posterior means .	1	1	2	-1.5353458	1.6535081	1
206-9-36	rfn models identify rare and small events in the input , have a low interference between code units , have a small reconstruction error , and explain the data covariance structure .	we proof convergence and correctness of the rfn learning algorithm .	1	1	3	-5.741727	5.051925	1
206-9-36	on benchmarks , rfns are compared to other unsupervised methods like autoencoders , rbms , factor analysis , ica , and pca .	rfn models identify rare and small events in the input , have a low interference between code units , have a small reconstruction error , and explain the data covariance structure .	0	4	1	5.2072253	-4.6252346	0
206-9-36	in contrast to previous sparse coding methods , rfns yield sparser codes , capture the data 's covariance structure more precisely , and have a significantly smaller reconstruction error .	rfn models identify rare and small events in the input , have a low interference between code units , have a small reconstruction error , and explain the data covariance structure .	0	5	1	2.8635793	-2.684094	0
206-9-36	we test rfns as pretraining technique for deep networks on different vision datasets , where rfns were superior to rbms and autoencoders .	rfn models identify rare and small events in the input , have a low interference between code units , have a small reconstruction error , and explain the data covariance structure .	0	6	1	5.227642	-4.6134367	0
206-9-36	rfn models identify rare and small events in the input , have a low interference between code units , have a small reconstruction error , and explain the data covariance structure .	on gene expression data from two pharmaceutical drug discovery studies , rfns detected small and rare gene modules that revealed highly relevant new biological insights which were so far missed by other unsupervised methods .	1	1	7	-5.1864705	4.757282	1
206-9-36	rfn models identify rare and small events in the input , have a low interference between code units , have a small reconstruction error , and explain the data covariance structure .	rfn package for gpu/cpu is available at http : //www.bioinf.jku.at/software/rfn .	1	1	8	-5.900426	5.003291	1
206-9-36	rfn learning is a generalized alternating minimization algorithm derived from the posterior regularization method which enforces non-negative and normalized posterior means .	we proof convergence and correctness of the rfn learning algorithm .	1	2	3	-5.6392503	4.886733	1
206-9-36	rfn learning is a generalized alternating minimization algorithm derived from the posterior regularization method which enforces non-negative and normalized posterior means .	on benchmarks , rfns are compared to other unsupervised methods like autoencoders , rbms , factor analysis , ica , and pca .	1	2	4	-5.917487	5.1518216	1
206-9-36	in contrast to previous sparse coding methods , rfns yield sparser codes , capture the data 's covariance structure more precisely , and have a significantly smaller reconstruction error .	rfn learning is a generalized alternating minimization algorithm derived from the posterior regularization method which enforces non-negative and normalized posterior means .	0	5	2	1.9325471	-1.8015189	0
206-9-36	we test rfns as pretraining technique for deep networks on different vision datasets , where rfns were superior to rbms and autoencoders .	rfn learning is a generalized alternating minimization algorithm derived from the posterior regularization method which enforces non-negative and normalized posterior means .	0	6	2	5.1335573	-4.5831337	0
206-9-36	rfn learning is a generalized alternating minimization algorithm derived from the posterior regularization method which enforces non-negative and normalized posterior means .	on gene expression data from two pharmaceutical drug discovery studies , rfns detected small and rare gene modules that revealed highly relevant new biological insights which were so far missed by other unsupervised methods .	1	2	7	-5.3661804	4.858551	1
206-9-36	rfn learning is a generalized alternating minimization algorithm derived from the posterior regularization method which enforces non-negative and normalized posterior means .	rfn package for gpu/cpu is available at http : //www.bioinf.jku.at/software/rfn .	1	2	8	-5.802656	4.897252	1
206-9-36	we proof convergence and correctness of the rfn learning algorithm .	on benchmarks , rfns are compared to other unsupervised methods like autoencoders , rbms , factor analysis , ica , and pca .	1	3	4	-4.2040377	3.9304297	1
206-9-36	in contrast to previous sparse coding methods , rfns yield sparser codes , capture the data 's covariance structure more precisely , and have a significantly smaller reconstruction error .	we proof convergence and correctness of the rfn learning algorithm .	0	5	3	-5.662475	5.0129323	1
206-9-36	we proof convergence and correctness of the rfn learning algorithm .	we test rfns as pretraining technique for deep networks on different vision datasets , where rfns were superior to rbms and autoencoders .	1	3	6	-4.9411	4.5629244	1
206-9-36	we proof convergence and correctness of the rfn learning algorithm .	on gene expression data from two pharmaceutical drug discovery studies , rfns detected small and rare gene modules that revealed highly relevant new biological insights which were so far missed by other unsupervised methods .	1	3	7	-2.509304	2.5570645	1
206-9-36	rfn package for gpu/cpu is available at http : //www.bioinf.jku.at/software/rfn .	we proof convergence and correctness of the rfn learning algorithm .	0	8	3	4.5714226	-4.076248	0
206-9-36	in contrast to previous sparse coding methods , rfns yield sparser codes , capture the data 's covariance structure more precisely , and have a significantly smaller reconstruction error .	on benchmarks , rfns are compared to other unsupervised methods like autoencoders , rbms , factor analysis , ica , and pca .	0	5	4	-5.7461486	5.12279	1
206-9-36	on benchmarks , rfns are compared to other unsupervised methods like autoencoders , rbms , factor analysis , ica , and pca .	we test rfns as pretraining technique for deep networks on different vision datasets , where rfns were superior to rbms and autoencoders .	1	4	6	-3.335239	3.2709935	1
206-9-36	on gene expression data from two pharmaceutical drug discovery studies , rfns detected small and rare gene modules that revealed highly relevant new biological insights which were so far missed by other unsupervised methods .	on benchmarks , rfns are compared to other unsupervised methods like autoencoders , rbms , factor analysis , ica , and pca .	0	7	4	-2.792393	2.7154462	1
206-9-36	on benchmarks , rfns are compared to other unsupervised methods like autoencoders , rbms , factor analysis , ica , and pca .	rfn package for gpu/cpu is available at http : //www.bioinf.jku.at/software/rfn .	1	4	8	-5.224264	4.7063184	1
206-9-36	we test rfns as pretraining technique for deep networks on different vision datasets , where rfns were superior to rbms and autoencoders .	in contrast to previous sparse coding methods , rfns yield sparser codes , capture the data 's covariance structure more precisely , and have a significantly smaller reconstruction error .	0	6	5	5.0164967	-4.444606	0
206-9-36	in contrast to previous sparse coding methods , rfns yield sparser codes , capture the data 's covariance structure more precisely , and have a significantly smaller reconstruction error .	on gene expression data from two pharmaceutical drug discovery studies , rfns detected small and rare gene modules that revealed highly relevant new biological insights which were so far missed by other unsupervised methods .	1	5	7	-5.068735	4.661503	1
206-9-36	rfn package for gpu/cpu is available at http : //www.bioinf.jku.at/software/rfn .	in contrast to previous sparse coding methods , rfns yield sparser codes , capture the data 's covariance structure more precisely , and have a significantly smaller reconstruction error .	0	8	5	5.238123	-4.670781	0
206-9-36	on gene expression data from two pharmaceutical drug discovery studies , rfns detected small and rare gene modules that revealed highly relevant new biological insights which were so far missed by other unsupervised methods .	we test rfns as pretraining technique for deep networks on different vision datasets , where rfns were superior to rbms and autoencoders .	0	7	6	-4.5499153	4.1895723	1
206-9-36	we test rfns as pretraining technique for deep networks on different vision datasets , where rfns were superior to rbms and autoencoders .	rfn package for gpu/cpu is available at http : //www.bioinf.jku.at/software/rfn .	1	6	8	-5.0970182	4.5871315	1
206-9-36	rfn package for gpu/cpu is available at http : //www.bioinf.jku.at/software/rfn .	on gene expression data from two pharmaceutical drug discovery studies , rfns detected small and rare gene modules that revealed highly relevant new biological insights which were so far missed by other unsupervised methods .	0	8	7	4.9516287	-4.3796167	0
207-7-21	we present a method for learning bayesian networks from data sets containing thousands of variables without the need for structure constraints .	our approach is made of two parts .	1	0	1	-4.4370637	4.1045637	1
207-7-21	we present a method for learning bayesian networks from data sets containing thousands of variables without the need for structure constraints .	the first is a novel algorithm that effectively explores the space of possible parent sets of a node .	1	0	2	-5.594359	5.0895963	1
207-7-21	it guides the exploration towards the most promising parent sets on the basis of an approximated score function that is computed in constant time .	we present a method for learning bayesian networks from data sets containing thousands of variables without the need for structure constraints .	0	3	0	5.5348587	-4.9024076	0
207-7-21	we present a method for learning bayesian networks from data sets containing thousands of variables without the need for structure constraints .	the second part is an improvement of an existing ordering-based algorithm for structure optimization .	1	0	4	-5.04668	4.6149483	1
207-7-21	the new algorithm provably achieves a higher score compared to its original formulation .	we present a method for learning bayesian networks from data sets containing thousands of variables without the need for structure constraints .	0	5	0	5.472536	-4.8354254	0
207-7-21	we present a method for learning bayesian networks from data sets containing thousands of variables without the need for structure constraints .	our novel approach consistently outperforms the state of the art on very large data sets .	1	0	6	-5.9965477	5.1124573	1
207-7-21	the first is a novel algorithm that effectively explores the space of possible parent sets of a node .	our approach is made of two parts .	0	2	1	4.9685597	-4.3861465	0
207-7-21	it guides the exploration towards the most promising parent sets on the basis of an approximated score function that is computed in constant time .	our approach is made of two parts .	0	3	1	4.6093307	-4.122622	0
207-7-21	the second part is an improvement of an existing ordering-based algorithm for structure optimization .	our approach is made of two parts .	0	4	1	5.289583	-4.6697636	0
207-7-21	the new algorithm provably achieves a higher score compared to its original formulation .	our approach is made of two parts .	0	5	1	5.218296	-4.6136646	0
207-7-21	our novel approach consistently outperforms the state of the art on very large data sets .	our approach is made of two parts .	0	6	1	5.3533	-4.7363663	0
207-7-21	the first is a novel algorithm that effectively explores the space of possible parent sets of a node .	it guides the exploration towards the most promising parent sets on the basis of an approximated score function that is computed in constant time .	1	2	3	-5.1077456	4.7223954	1
207-7-21	the second part is an improvement of an existing ordering-based algorithm for structure optimization .	the first is a novel algorithm that effectively explores the space of possible parent sets of a node .	0	4	2	4.600569	-4.0994453	0
207-7-21	the first is a novel algorithm that effectively explores the space of possible parent sets of a node .	the new algorithm provably achieves a higher score compared to its original formulation .	1	2	5	-5.929686	5.175697	1
207-7-21	the first is a novel algorithm that effectively explores the space of possible parent sets of a node .	our novel approach consistently outperforms the state of the art on very large data sets .	1	2	6	-5.923307	5.086097	1
207-7-21	it guides the exploration towards the most promising parent sets on the basis of an approximated score function that is computed in constant time .	the second part is an improvement of an existing ordering-based algorithm for structure optimization .	1	3	4	-2.003536	2.0876272	1
207-7-21	the new algorithm provably achieves a higher score compared to its original formulation .	it guides the exploration towards the most promising parent sets on the basis of an approximated score function that is computed in constant time .	0	5	3	4.7867327	-4.375439	0
207-7-21	it guides the exploration towards the most promising parent sets on the basis of an approximated score function that is computed in constant time .	our novel approach consistently outperforms the state of the art on very large data sets .	1	3	6	-5.901762	5.027119	1
207-7-21	the new algorithm provably achieves a higher score compared to its original formulation .	the second part is an improvement of an existing ordering-based algorithm for structure optimization .	0	5	4	4.314459	-3.983646	0
207-7-21	our novel approach consistently outperforms the state of the art on very large data sets .	the second part is an improvement of an existing ordering-based algorithm for structure optimization .	0	6	4	4.3858347	-4.049334	0
207-7-21	the new algorithm provably achieves a higher score compared to its original formulation .	our novel approach consistently outperforms the state of the art on very large data sets .	1	5	6	-3.4469504	3.2702456	1
208-7-21	most recent results in matrix completion assume that the matrix under consideration is low-rank or that the columns are in a union of low-rank subspaces .	in real-world settings , however , the linear structure underlying these models is distorted by a ( typically unknown ) nonlinear transformation .	1	0	1	-5.5213156	5.0246305	1
208-7-21	this paper addresses the challenge of matrix completion in the face of such nonlinearities .	most recent results in matrix completion assume that the matrix under consideration is low-rank or that the columns are in a union of low-rank subspaces .	0	2	0	5.463687	-4.7651854	0
208-7-21	given a few observations of a matrix that are obtained by applying a lipschitz , monotonic function to a low rank matrix , our task is to estimate the remaining unobserved entries .	most recent results in matrix completion assume that the matrix under consideration is low-rank or that the columns are in a union of low-rank subspaces .	0	3	0	3.2038054	-2.4829173	0
208-7-21	most recent results in matrix completion assume that the matrix under consideration is low-rank or that the columns are in a union of low-rank subspaces .	we propose a novel matrix completion method that alternates between lowrank matrix estimation and monotonic function estimation to estimate the missing matrix elements .	1	0	4	-5.9322376	5.216397	1
208-7-21	most recent results in matrix completion assume that the matrix under consideration is low-rank or that the columns are in a union of low-rank subspaces .	mean squared error bounds provide insight into how well the matrix can be estimated based on the size , rank of the matrix and properties of the nonlinear transformation .	1	0	5	-6.005106	5.15392	1
208-7-21	empirical results on synthetic and real-world datasets demonstrate the competitiveness of the proposed approach .	most recent results in matrix completion assume that the matrix under consideration is low-rank or that the columns are in a union of low-rank subspaces .	0	6	0	5.6231174	-5.090369	0
208-7-21	this paper addresses the challenge of matrix completion in the face of such nonlinearities .	in real-world settings , however , the linear structure underlying these models is distorted by a ( typically unknown ) nonlinear transformation .	0	2	1	4.447576	-3.982119	0
208-7-21	in real-world settings , however , the linear structure underlying these models is distorted by a ( typically unknown ) nonlinear transformation .	given a few observations of a matrix that are obtained by applying a lipschitz , monotonic function to a low rank matrix , our task is to estimate the remaining unobserved entries .	1	1	3	-2.486233	2.5016465	1
208-7-21	we propose a novel matrix completion method that alternates between lowrank matrix estimation and monotonic function estimation to estimate the missing matrix elements .	in real-world settings , however , the linear structure underlying these models is distorted by a ( typically unknown ) nonlinear transformation .	0	4	1	5.3934293	-4.7502747	0
208-7-21	mean squared error bounds provide insight into how well the matrix can be estimated based on the size , rank of the matrix and properties of the nonlinear transformation .	in real-world settings , however , the linear structure underlying these models is distorted by a ( typically unknown ) nonlinear transformation .	0	5	1	5.54689	-4.889504	0
208-7-21	in real-world settings , however , the linear structure underlying these models is distorted by a ( typically unknown ) nonlinear transformation .	empirical results on synthetic and real-world datasets demonstrate the competitiveness of the proposed approach .	1	1	6	-5.875281	4.963271	1
208-7-21	this paper addresses the challenge of matrix completion in the face of such nonlinearities .	given a few observations of a matrix that are obtained by applying a lipschitz , monotonic function to a low rank matrix , our task is to estimate the remaining unobserved entries .	1	2	3	0.8626832	-0.48914316	0
208-7-21	we propose a novel matrix completion method that alternates between lowrank matrix estimation and monotonic function estimation to estimate the missing matrix elements .	this paper addresses the challenge of matrix completion in the face of such nonlinearities .	0	4	2	4.94345	-4.4399824	0
208-7-21	mean squared error bounds provide insight into how well the matrix can be estimated based on the size , rank of the matrix and properties of the nonlinear transformation .	this paper addresses the challenge of matrix completion in the face of such nonlinearities .	0	5	2	4.6883154	-4.269535	0
208-7-21	this paper addresses the challenge of matrix completion in the face of such nonlinearities .	empirical results on synthetic and real-world datasets demonstrate the competitiveness of the proposed approach .	1	2	6	-5.9064665	5.146154	1
208-7-21	we propose a novel matrix completion method that alternates between lowrank matrix estimation and monotonic function estimation to estimate the missing matrix elements .	given a few observations of a matrix that are obtained by applying a lipschitz , monotonic function to a low rank matrix , our task is to estimate the remaining unobserved entries .	0	4	3	4.181262	-3.7935543	0
208-7-21	given a few observations of a matrix that are obtained by applying a lipschitz , monotonic function to a low rank matrix , our task is to estimate the remaining unobserved entries .	mean squared error bounds provide insight into how well the matrix can be estimated based on the size , rank of the matrix and properties of the nonlinear transformation .	1	3	5	-5.925378	5.2057996	1
208-7-21	empirical results on synthetic and real-world datasets demonstrate the competitiveness of the proposed approach .	given a few observations of a matrix that are obtained by applying a lipschitz , monotonic function to a low rank matrix , our task is to estimate the remaining unobserved entries .	0	6	3	5.549011	-4.9682727	0
208-7-21	we propose a novel matrix completion method that alternates between lowrank matrix estimation and monotonic function estimation to estimate the missing matrix elements .	mean squared error bounds provide insight into how well the matrix can be estimated based on the size , rank of the matrix and properties of the nonlinear transformation .	1	4	5	-4.106923	3.8966827	1
208-7-21	empirical results on synthetic and real-world datasets demonstrate the competitiveness of the proposed approach .	we propose a novel matrix completion method that alternates between lowrank matrix estimation and monotonic function estimation to estimate the missing matrix elements .	0	6	4	5.4933457	-4.846595	0
208-7-21	empirical results on synthetic and real-world datasets demonstrate the competitiveness of the proposed approach .	mean squared error bounds provide insight into how well the matrix can be estimated based on the size , rank of the matrix and properties of the nonlinear transformation .	0	6	5	5.2004843	-4.5875454	0
209-4-6	these questions take the form of image a is to image b as image c is to what .	in this paper , we study the problem of answering visual analogy questions .	0	1	0	5.589687	-5.0154285	0
209-4-6	answering these questions entails discovering the mapping from image a to image b and then extending the mapping to image c and searching for the image d such that the relation from a to b holds for c to d. we pose this problem as learning an embedding that encourages pairs of analogous images with similar transformations to be close together using convolutional neural networks with a quadruple siamese architecture .	in this paper , we study the problem of answering visual analogy questions .	0	2	0	5.6502957	-5.06806	0
209-4-6	we introduce a dataset of visual analogy questions in natural images , and show first results of its kind on solving analogy questions on natural images .	in this paper , we study the problem of answering visual analogy questions .	0	3	0	5.5495963	-4.9107995	0
209-4-6	[CLS] answering these questions entails discovering the mapping from image a to image b and then extending the mapping to image c and searching for the image d such that the relation from a to b holds for c to d. we pose this problem as learning an embedding that encourages pairs of analogous images with similar transformations to be close together using convolutional neural networks with a quadruple	these questions take the form of image a is to image b as image c is to what .	0	2	1	3.7792044	-3.3597798	0
209-4-6	we introduce a dataset of visual analogy questions in natural images , and show first results of its kind on solving analogy questions on natural images .	these questions take the form of image a is to image b as image c is to what .	0	3	1	-5.4944034	5.044793	1
209-4-6	[CLS] answering these questions entails discovering the mapping from image a to image b and then extending the mapping to image c and searching for the image d such that the relation from a to b holds for c to d. we pose this problem as learning an embedding that encourages pairs of analogous images with similar transformations to be close together using convolu	we introduce a dataset of visual analogy questions in natural images , and show first results of its kind on solving analogy questions on natural images .	1	2	3	5.4026575	-4.8383493	0
210-10-45	dirichlet process is a well-known example of nrms .	normalized random measures ( nrms ) provide a broad class of discrete random measures that are often used as priors for bayesian nonparametric models .	0	1	0	5.629264	-5.032565	0
210-10-45	normalized random measures ( nrms ) provide a broad class of discrete random measures that are often used as priors for bayesian nonparametric models .	most of posterior inference methods for nrm mixture models rely on mcmc methods since they are easy to implement and their convergence is well studied .	1	0	2	-5.829131	5.096343	1
210-10-45	normalized random measures ( nrms ) provide a broad class of discrete random measures that are often used as priors for bayesian nonparametric models .	however , mcmc often suffers from slow convergence when the acceptance rate is low .	1	0	3	-5.6657615	4.9723263	1
210-10-45	normalized random measures ( nrms ) provide a broad class of discrete random measures that are often used as priors for bayesian nonparametric models .	tree-based inference is an alternative deterministic posterior inference method , where bayesian hierarchical clustering ( bhc ) or incremental bayesian hierarchical clustering ( ibhc ) have been developed for dp or nrm mixture ( nrmm ) models , respectively .	1	0	4	-5.740451	5.1665874	1
210-10-45	although ibhc is a promising method for posterior inference for nrmm models due to its efficiency and applicability to online inference , its convergence is not guaranteed since it uses heuristics that simply selects the best solution after multiple trials are made .	normalized random measures ( nrms ) provide a broad class of discrete random measures that are often used as priors for bayesian nonparametric models .	0	5	0	5.7178717	-5.052808	0
210-10-45	normalized random measures ( nrms ) provide a broad class of discrete random measures that are often used as priors for bayesian nonparametric models .	in this paper , we present a hybrid inference algorithm for nrmm models , which combines the merits of both mcmc and ibhc .	1	0	6	-5.875376	5.0863605	1
210-10-45	normalized random measures ( nrms ) provide a broad class of discrete random measures that are often used as priors for bayesian nonparametric models .	trees built by ibhc outlines partitions of data , which guides metropolis-hastings procedure to employ appropriate proposals .	1	0	7	-5.985454	5.1261015	1
210-10-45	normalized random measures ( nrms ) provide a broad class of discrete random measures that are often used as priors for bayesian nonparametric models .	inheriting the nature of mcmc , our tree-guided mcmc ( tgmcmc ) is guaranteed to converge , and enjoys the fast convergence thanks to the effective proposals guided by trees .	1	0	8	-5.9744163	5.2038975	1
210-10-45	normalized random measures ( nrms ) provide a broad class of discrete random measures that are often used as priors for bayesian nonparametric models .	experiments on both synthetic and realworld datasets demonstrate the benefit of our method .	1	0	9	-5.9489	5.086246	1
210-10-45	most of posterior inference methods for nrm mixture models rely on mcmc methods since they are easy to implement and their convergence is well studied .	dirichlet process is a well-known example of nrms .	0	2	1	2.5577662	-2.1129208	0
210-10-45	however , mcmc often suffers from slow convergence when the acceptance rate is low .	dirichlet process is a well-known example of nrms .	0	3	1	3.2326756	-2.9887145	0
210-10-45	tree-based inference is an alternative deterministic posterior inference method , where bayesian hierarchical clustering ( bhc ) or incremental bayesian hierarchical clustering ( ibhc ) have been developed for dp or nrm mixture ( nrmm ) models , respectively .	dirichlet process is a well-known example of nrms .	0	4	1	-3.5936766	3.5125673	1
210-10-45	dirichlet process is a well-known example of nrms .	although ibhc is a promising method for posterior inference for nrmm models due to its efficiency and applicability to online inference , its convergence is not guaranteed since it uses heuristics that simply selects the best solution after multiple trials are made .	1	1	5	-4.5916524	4.2990227	1
210-10-45	in this paper , we present a hybrid inference algorithm for nrmm models , which combines the merits of both mcmc and ibhc .	dirichlet process is a well-known example of nrms .	0	6	1	5.2979336	-4.558649	0
210-10-45	trees built by ibhc outlines partitions of data , which guides metropolis-hastings procedure to employ appropriate proposals .	dirichlet process is a well-known example of nrms .	0	7	1	5.101553	-4.4696517	0
210-10-45	dirichlet process is a well-known example of nrms .	inheriting the nature of mcmc , our tree-guided mcmc ( tgmcmc ) is guaranteed to converge , and enjoys the fast convergence thanks to the effective proposals guided by trees .	1	1	8	-5.956114	5.2143316	1
210-10-45	experiments on both synthetic and realworld datasets demonstrate the benefit of our method .	dirichlet process is a well-known example of nrms .	0	9	1	5.589125	-4.9676957	0
210-10-45	however , mcmc often suffers from slow convergence when the acceptance rate is low .	most of posterior inference methods for nrm mixture models rely on mcmc methods since they are easy to implement and their convergence is well studied .	0	3	2	1.6647143	-1.4523573	0
210-10-45	tree-based inference is an alternative deterministic posterior inference method , where bayesian hierarchical clustering ( bhc ) or incremental bayesian hierarchical clustering ( ibhc ) have been developed for dp or nrm mixture ( nrmm ) models , respectively .	most of posterior inference methods for nrm mixture models rely on mcmc methods since they are easy to implement and their convergence is well studied .	0	4	2	-5.6107063	5.0223813	1
210-10-45	most of posterior inference methods for nrm mixture models rely on mcmc methods since they are easy to implement and their convergence is well studied .	although ibhc is a promising method for posterior inference for nrmm models due to its efficiency and applicability to online inference , its convergence is not guaranteed since it uses heuristics that simply selects the best solution after multiple trials are made .	1	2	5	-4.6678085	4.373863	1
210-10-45	in this paper , we present a hybrid inference algorithm for nrmm models , which combines the merits of both mcmc and ibhc .	most of posterior inference methods for nrm mixture models rely on mcmc methods since they are easy to implement and their convergence is well studied .	0	6	2	4.992435	-4.3822846	0
210-10-45	most of posterior inference methods for nrm mixture models rely on mcmc methods since they are easy to implement and their convergence is well studied .	trees built by ibhc outlines partitions of data , which guides metropolis-hastings procedure to employ appropriate proposals .	1	2	7	-5.857338	5.2066593	1
210-10-45	inheriting the nature of mcmc , our tree-guided mcmc ( tgmcmc ) is guaranteed to converge , and enjoys the fast convergence thanks to the effective proposals guided by trees .	most of posterior inference methods for nrm mixture models rely on mcmc methods since they are easy to implement and their convergence is well studied .	0	8	2	4.7771244	-4.317432	0
210-10-45	most of posterior inference methods for nrm mixture models rely on mcmc methods since they are easy to implement and their convergence is well studied .	experiments on both synthetic and realworld datasets demonstrate the benefit of our method .	1	2	9	-5.987561	5.166257	1
210-10-45	tree-based inference is an alternative deterministic posterior inference method , where bayesian hierarchical clustering ( bhc ) or incremental bayesian hierarchical clustering ( ibhc ) have been developed for dp or nrm mixture ( nrmm ) models , respectively .	however , mcmc often suffers from slow convergence when the acceptance rate is low .	0	4	3	-2.3833442	2.4744325	1
210-10-45	however , mcmc often suffers from slow convergence when the acceptance rate is low .	although ibhc is a promising method for posterior inference for nrmm models due to its efficiency and applicability to online inference , its convergence is not guaranteed since it uses heuristics that simply selects the best solution after multiple trials are made .	1	3	5	-2.674298	2.7092927	1
210-10-45	however , mcmc often suffers from slow convergence when the acceptance rate is low .	in this paper , we present a hybrid inference algorithm for nrmm models , which combines the merits of both mcmc and ibhc .	1	3	6	-5.900049	5.095119	1
210-10-45	trees built by ibhc outlines partitions of data , which guides metropolis-hastings procedure to employ appropriate proposals .	however , mcmc often suffers from slow convergence when the acceptance rate is low .	0	7	3	5.009054	-4.4100914	0
210-10-45	however , mcmc often suffers from slow convergence when the acceptance rate is low .	inheriting the nature of mcmc , our tree-guided mcmc ( tgmcmc ) is guaranteed to converge , and enjoys the fast convergence thanks to the effective proposals guided by trees .	1	3	8	-5.84634	5.24457	1
210-10-45	however , mcmc often suffers from slow convergence when the acceptance rate is low .	experiments on both synthetic and realworld datasets demonstrate the benefit of our method .	1	3	9	-5.932101	5.1370225	1
210-10-45	[CLS] although ibhc is a promising method for posterior inference for nrmm models due to its efficiency and applicability to online inference, its convergence is not guaranteed since it uses heuristics that simply selects the best solution after multiple trials are made	[CLS] tree - based inference is an alternative deterministic posterior inference method, where bayesian hierarchical clustering ( bhc ) or incremental bayesian hierarchical clustering ( ibhc ) have been developed for dp or nrm mixture ( nr	0	5	4	4.4445457	-3.857548	0
210-10-45	tree-based inference is an alternative deterministic posterior inference method , where bayesian hierarchical clustering ( bhc ) or incremental bayesian hierarchical clustering ( ibhc ) have been developed for dp or nrm mixture ( nrmm ) models , respectively .	in this paper , we present a hybrid inference algorithm for nrmm models , which combines the merits of both mcmc and ibhc .	1	4	6	-5.9114656	5.002098	1
210-10-45	trees built by ibhc outlines partitions of data , which guides metropolis-hastings procedure to employ appropriate proposals .	tree-based inference is an alternative deterministic posterior inference method , where bayesian hierarchical clustering ( bhc ) or incremental bayesian hierarchical clustering ( ibhc ) have been developed for dp or nrm mixture ( nrmm ) models , respectively .	0	7	4	5.373207	-4.6763687	0
210-10-45	tree-based inference is an alternative deterministic posterior inference method , where bayesian hierarchical clustering ( bhc ) or incremental bayesian hierarchical clustering ( ibhc ) have been developed for dp or nrm mixture ( nrmm ) models , respectively .	inheriting the nature of mcmc , our tree-guided mcmc ( tgmcmc ) is guaranteed to converge , and enjoys the fast convergence thanks to the effective proposals guided by trees .	1	4	8	-5.9811816	5.1662326	1
210-10-45	tree-based inference is an alternative deterministic posterior inference method , where bayesian hierarchical clustering ( bhc ) or incremental bayesian hierarchical clustering ( ibhc ) have been developed for dp or nrm mixture ( nrmm ) models , respectively .	experiments on both synthetic and realworld datasets demonstrate the benefit of our method .	1	4	9	-5.997215	5.143894	1
210-10-45	in this paper , we present a hybrid inference algorithm for nrmm models , which combines the merits of both mcmc and ibhc .	although ibhc is a promising method for posterior inference for nrmm models due to its efficiency and applicability to online inference , its convergence is not guaranteed since it uses heuristics that simply selects the best solution after multiple trials are made .	0	6	5	4.123458	-3.7365623	0
210-10-45	trees built by ibhc outlines partitions of data , which guides metropolis-hastings procedure to employ appropriate proposals .	although ibhc is a promising method for posterior inference for nrmm models due to its efficiency and applicability to online inference , its convergence is not guaranteed since it uses heuristics that simply selects the best solution after multiple trials are made .	0	7	5	3.6103063	-3.3193417	0
210-10-45	inheriting the nature of mcmc , our tree-guided mcmc ( tgmcmc ) is guaranteed to converge , and enjoys the fast convergence thanks to the effective proposals guided by trees .	although ibhc is a promising method for posterior inference for nrmm models due to its efficiency and applicability to online inference , its convergence is not guaranteed since it uses heuristics that simply selects the best solution after multiple trials are made .	0	8	5	3.6942854	-3.3919525	0
210-10-45	although ibhc is a promising method for posterior inference for nrmm models due to its efficiency and applicability to online inference , its convergence is not guaranteed since it uses heuristics that simply selects the best solution after multiple trials are made .	experiments on both synthetic and realworld datasets demonstrate the benefit of our method .	1	5	9	-5.9513874	5.074526	1
210-10-45	in this paper , we present a hybrid inference algorithm for nrmm models , which combines the merits of both mcmc and ibhc .	trees built by ibhc outlines partitions of data , which guides metropolis-hastings procedure to employ appropriate proposals .	1	6	7	-4.753186	4.3759	1
210-10-45	in this paper , we present a hybrid inference algorithm for nrmm models , which combines the merits of both mcmc and ibhc .	inheriting the nature of mcmc , our tree-guided mcmc ( tgmcmc ) is guaranteed to converge , and enjoys the fast convergence thanks to the effective proposals guided by trees .	1	6	8	-4.0193295	3.7403512	1
210-10-45	in this paper , we present a hybrid inference algorithm for nrmm models , which combines the merits of both mcmc and ibhc .	experiments on both synthetic and realworld datasets demonstrate the benefit of our method .	1	6	9	-5.9595246	5.1908426	1
210-10-45	inheriting the nature of mcmc , our tree-guided mcmc ( tgmcmc ) is guaranteed to converge , and enjoys the fast convergence thanks to the effective proposals guided by trees .	trees built by ibhc outlines partitions of data , which guides metropolis-hastings procedure to employ appropriate proposals .	0	8	7	1.9483972	-1.6741701	0
210-10-45	trees built by ibhc outlines partitions of data , which guides metropolis-hastings procedure to employ appropriate proposals .	experiments on both synthetic and realworld datasets demonstrate the benefit of our method .	1	7	9	-5.989857	5.1329603	1
210-10-45	experiments on both synthetic and realworld datasets demonstrate the benefit of our method .	inheriting the nature of mcmc , our tree-guided mcmc ( tgmcmc ) is guaranteed to converge , and enjoys the fast convergence thanks to the effective proposals guided by trees .	0	9	8	5.2388773	-4.6639214	0
211-9-36	equivalently , we are given an input bipartite graph with two types of vertices : items , and associations ( which we refer to as topics ) .	in many applications , the data is of rich structure that can be represented by a hypergraph , where the data items are represented by vertices and the associations among items are represented by hyperedges .	0	1	0	5.1812043	-4.616433	0
211-9-36	we consider the problem of partitioning the set of items into a given number of components such that the maximum number of topics covered by a component is minimized .	in many applications , the data is of rich structure that can be represented by a hypergraph , where the data items are represented by vertices and the associations among items are represented by hyperedges .	0	2	0	-3.1202312	2.9757864	1
211-9-36	this is a clustering problem with various applications , e.g .	in many applications , the data is of rich structure that can be represented by a hypergraph , where the data items are represented by vertices and the associations among items are represented by hyperedges .	0	3	0	4.3652716	-3.7739983	0
211-9-36	partitioning of a set of information objects such as documents , images , and videos , and load balancing in the context of modern computation platforms .	in many applications , the data is of rich structure that can be represented by a hypergraph , where the data items are represented by vertices and the associations among items are represented by hyperedges .	0	4	0	-1.7982816	1.9304287	1
211-9-36	in this paper , we focus on the streaming computation model for this problem , in which items arrive online one at a time and each item must be assigned irrevocably to a component at its arrival time .	in many applications , the data is of rich structure that can be represented by a hypergraph , where the data items are represented by vertices and the associations among items are represented by hyperedges .	0	5	0	5.4880753	-4.858773	0
211-9-36	in many applications , the data is of rich structure that can be represented by a hypergraph , where the data items are represented by vertices and the associations among items are represented by hyperedges .	motivated by scalability requirements , we focus on the class of streaming computation algorithms with memory limited to be at most linear in the number of components .	1	0	6	-5.94635	5.229366	1
211-9-36	in many applications , the data is of rich structure that can be represented by a hypergraph , where the data items are represented by vertices and the associations among items are represented by hyperedges .	we show that a greedy assignment strategy is able to recover a hidden co-clustering of items under a natural set of recovery conditions .	1	0	7	-6.015215	5.1448402	1
211-9-36	in many applications , the data is of rich structure that can be represented by a hypergraph , where the data items are represented by vertices and the associations among items are represented by hyperedges .	we also report results of an extensive empirical evaluation , which demonstrate that this greedy strategy yields superior performance when compared with alternative approaches .	1	0	8	-5.9626226	5.054925	1
211-9-36	we consider the problem of partitioning the set of items into a given number of components such that the maximum number of topics covered by a component is minimized .	equivalently , we are given an input bipartite graph with two types of vertices : items , and associations ( which we refer to as topics ) .	0	2	1	-5.5076637	5.0714207	1
211-9-36	this is a clustering problem with various applications , e.g .	equivalently , we are given an input bipartite graph with two types of vertices : items , and associations ( which we refer to as topics ) .	0	3	1	-2.4562774	2.5259972	1
211-9-36	partitioning of a set of information objects such as documents , images , and videos , and load balancing in the context of modern computation platforms .	equivalently , we are given an input bipartite graph with two types of vertices : items , and associations ( which we refer to as topics ) .	0	4	1	-5.405898	5.013493	1
211-9-36	in this paper , we focus on the streaming computation model for this problem , in which items arrive online one at a time and each item must be assigned irrevocably to a component at its arrival time .	equivalently , we are given an input bipartite graph with two types of vertices : items , and associations ( which we refer to as topics ) .	0	5	1	-5.347723	4.9532337	1
211-9-36	motivated by scalability requirements , we focus on the class of streaming computation algorithms with memory limited to be at most linear in the number of components .	equivalently , we are given an input bipartite graph with two types of vertices : items , and associations ( which we refer to as topics ) .	0	6	1	-3.4855645	3.396141	1
211-9-36	equivalently , we are given an input bipartite graph with two types of vertices : items , and associations ( which we refer to as topics ) .	we show that a greedy assignment strategy is able to recover a hidden co-clustering of items under a natural set of recovery conditions .	1	1	7	-5.313989	4.866448	1
211-9-36	equivalently , we are given an input bipartite graph with two types of vertices : items , and associations ( which we refer to as topics ) .	we also report results of an extensive empirical evaluation , which demonstrate that this greedy strategy yields superior performance when compared with alternative approaches .	1	1	8	-5.964109	5.0683327	1
211-9-36	this is a clustering problem with various applications , e.g .	we consider the problem of partitioning the set of items into a given number of components such that the maximum number of topics covered by a component is minimized .	0	3	2	5.3161726	-4.7878695	0
211-9-36	partitioning of a set of information objects such as documents , images , and videos , and load balancing in the context of modern computation platforms .	we consider the problem of partitioning the set of items into a given number of components such that the maximum number of topics covered by a component is minimized .	0	4	2	-2.1572216	2.448621	1
211-9-36	we consider the problem of partitioning the set of items into a given number of components such that the maximum number of topics covered by a component is minimized .	in this paper , we focus on the streaming computation model for this problem , in which items arrive online one at a time and each item must be assigned irrevocably to a component at its arrival time .	1	2	5	-5.800089	5.2337646	1
211-9-36	we consider the problem of partitioning the set of items into a given number of components such that the maximum number of topics covered by a component is minimized .	motivated by scalability requirements , we focus on the class of streaming computation algorithms with memory limited to be at most linear in the number of components .	1	2	6	-5.7634015	5.177831	1
211-9-36	we consider the problem of partitioning the set of items into a given number of components such that the maximum number of topics covered by a component is minimized .	we show that a greedy assignment strategy is able to recover a hidden co-clustering of items under a natural set of recovery conditions .	1	2	7	-5.964653	5.1669474	1
211-9-36	we consider the problem of partitioning the set of items into a given number of components such that the maximum number of topics covered by a component is minimized .	we also report results of an extensive empirical evaluation , which demonstrate that this greedy strategy yields superior performance when compared with alternative approaches .	1	2	8	-5.9901533	5.1694374	1
211-9-36	partitioning of a set of information objects such as documents , images , and videos , and load balancing in the context of modern computation platforms .	this is a clustering problem with various applications , e.g .	0	4	3	-4.885453	4.536148	1
211-9-36	this is a clustering problem with various applications , e.g .	in this paper , we focus on the streaming computation model for this problem , in which items arrive online one at a time and each item must be assigned irrevocably to a component at its arrival time .	1	3	5	-4.41503	4.2024136	1
211-9-36	motivated by scalability requirements , we focus on the class of streaming computation algorithms with memory limited to be at most linear in the number of components .	this is a clustering problem with various applications , e.g .	0	6	3	2.454205	-2.068391	0
211-9-36	we show that a greedy assignment strategy is able to recover a hidden co-clustering of items under a natural set of recovery conditions .	this is a clustering problem with various applications , e.g .	0	7	3	5.331028	-4.780533	0
211-9-36	this is a clustering problem with various applications , e.g .	we also report results of an extensive empirical evaluation , which demonstrate that this greedy strategy yields superior performance when compared with alternative approaches .	1	3	8	-5.901526	5.012287	1
211-9-36	partitioning of a set of information objects such as documents , images , and videos , and load balancing in the context of modern computation platforms .	in this paper , we focus on the streaming computation model for this problem , in which items arrive online one at a time and each item must be assigned irrevocably to a component at its arrival time .	1	4	5	-5.342247	4.9275823	1
211-9-36	partitioning of a set of information objects such as documents , images , and videos , and load balancing in the context of modern computation platforms .	motivated by scalability requirements , we focus on the class of streaming computation algorithms with memory limited to be at most linear in the number of components .	1	4	6	-4.232103	3.9299424	1
211-9-36	we show that a greedy assignment strategy is able to recover a hidden co-clustering of items under a natural set of recovery conditions .	partitioning of a set of information objects such as documents , images , and videos , and load balancing in the context of modern computation platforms .	0	7	4	5.556718	-4.979063	0
211-9-36	partitioning of a set of information objects such as documents , images , and videos , and load balancing in the context of modern computation platforms .	we also report results of an extensive empirical evaluation , which demonstrate that this greedy strategy yields superior performance when compared with alternative approaches .	1	4	8	-6.0041885	5.2172894	1
211-9-36	motivated by scalability requirements , we focus on the class of streaming computation algorithms with memory limited to be at most linear in the number of components .	in this paper , we focus on the streaming computation model for this problem , in which items arrive online one at a time and each item must be assigned irrevocably to a component at its arrival time .	0	6	5	4.806169	-4.1986527	0
211-9-36	in this paper , we focus on the streaming computation model for this problem , in which items arrive online one at a time and each item must be assigned irrevocably to a component at its arrival time .	we show that a greedy assignment strategy is able to recover a hidden co-clustering of items under a natural set of recovery conditions .	1	5	7	-6.0206723	5.157077	1
211-9-36	in this paper , we focus on the streaming computation model for this problem , in which items arrive online one at a time and each item must be assigned irrevocably to a component at its arrival time .	we also report results of an extensive empirical evaluation , which demonstrate that this greedy strategy yields superior performance when compared with alternative approaches .	1	5	8	-6.000658	5.192531	1
211-9-36	motivated by scalability requirements , we focus on the class of streaming computation algorithms with memory limited to be at most linear in the number of components .	we show that a greedy assignment strategy is able to recover a hidden co-clustering of items under a natural set of recovery conditions .	1	6	7	-5.9334087	5.1968513	1
211-9-36	motivated by scalability requirements , we focus on the class of streaming computation algorithms with memory limited to be at most linear in the number of components .	we also report results of an extensive empirical evaluation , which demonstrate that this greedy strategy yields superior performance when compared with alternative approaches .	1	6	8	-5.9708385	5.077079	1
211-9-36	we also report results of an extensive empirical evaluation , which demonstrate that this greedy strategy yields superior performance when compared with alternative approaches .	we show that a greedy assignment strategy is able to recover a hidden co-clustering of items under a natural set of recovery conditions .	0	8	7	5.2796955	-4.7492137	0
212-8-28	similarly , in revenue management , it is important to predict outcomes of comparisons among those items that have never been compared so far .	in personalized recommendation systems , it is important to predict preferences of a user on items that have not been seen by that user yet .	0	1	0	5.63014	-4.973878	0
212-8-28	in personalized recommendation systems , it is important to predict preferences of a user on items that have not been seen by that user yet .	the multinomial logit model , a popular discrete choice model , captures the structure of the hidden preferences with a low-rank matrix .	1	0	2	-5.624439	5.151479	1
212-8-28	in personalized recommendation systems , it is important to predict preferences of a user on items that have not been seen by that user yet .	in order to predict the preferences , we want to learn the underlying model from noisy observations of the low-rank matrix , collected as revealed preferences in various forms of ordinal data .	1	0	3	-5.867482	5.2367063	1
212-8-28	in personalized recommendation systems , it is important to predict preferences of a user on items that have not been seen by that user yet .	a natural approach to learn such a model is to solve a convex relaxation of nuclear norm minimization .	1	0	4	-5.97462	5.1792746	1
212-8-28	we present the convex relaxation approach in two contexts of interest : collaborative ranking and bundled choice modeling .	in personalized recommendation systems , it is important to predict preferences of a user on items that have not been seen by that user yet .	0	5	0	5.6814785	-5.1051474	0
212-8-28	in both cases , we show that the convex relaxation is minimax optimal .	in personalized recommendation systems , it is important to predict preferences of a user on items that have not been seen by that user yet .	0	6	0	5.6538367	-5.0770674	0
212-8-28	in personalized recommendation systems , it is important to predict preferences of a user on items that have not been seen by that user yet .	we prove an upper bound on the resulting error with finite samples , and provide a matching information-theoretic lower bound .	1	0	7	-5.899418	5.202459	1
212-8-28	the multinomial logit model , a popular discrete choice model , captures the structure of the hidden preferences with a low-rank matrix .	similarly , in revenue management , it is important to predict outcomes of comparisons among those items that have never been compared so far .	0	2	1	-0.068051934	0.4025533	1
212-8-28	similarly , in revenue management , it is important to predict outcomes of comparisons among those items that have never been compared so far .	in order to predict the preferences , we want to learn the underlying model from noisy observations of the low-rank matrix , collected as revealed preferences in various forms of ordinal data .	1	1	3	-2.6561656	2.7014813	1
212-8-28	similarly , in revenue management , it is important to predict outcomes of comparisons among those items that have never been compared so far .	a natural approach to learn such a model is to solve a convex relaxation of nuclear norm minimization .	1	1	4	-5.4974217	5.0471916	1
212-8-28	we present the convex relaxation approach in two contexts of interest : collaborative ranking and bundled choice modeling .	similarly , in revenue management , it is important to predict outcomes of comparisons among those items that have never been compared so far .	0	5	1	4.65752	-4.2079105	0
212-8-28	similarly , in revenue management , it is important to predict outcomes of comparisons among those items that have never been compared so far .	in both cases , we show that the convex relaxation is minimax optimal .	1	1	6	-5.875574	5.2223625	1
212-8-28	we prove an upper bound on the resulting error with finite samples , and provide a matching information-theoretic lower bound .	similarly , in revenue management , it is important to predict outcomes of comparisons among those items that have never been compared so far .	0	7	1	5.203706	-4.624728	0
212-8-28	the multinomial logit model , a popular discrete choice model , captures the structure of the hidden preferences with a low-rank matrix .	in order to predict the preferences , we want to learn the underlying model from noisy observations of the low-rank matrix , collected as revealed preferences in various forms of ordinal data .	1	2	3	-5.773342	5.2218328	1
212-8-28	a natural approach to learn such a model is to solve a convex relaxation of nuclear norm minimization .	the multinomial logit model , a popular discrete choice model , captures the structure of the hidden preferences with a low-rank matrix .	0	4	2	5.3018155	-4.6623693	0
212-8-28	we present the convex relaxation approach in two contexts of interest : collaborative ranking and bundled choice modeling .	the multinomial logit model , a popular discrete choice model , captures the structure of the hidden preferences with a low-rank matrix .	0	5	2	4.6022477	-3.9567182	0
212-8-28	the multinomial logit model , a popular discrete choice model , captures the structure of the hidden preferences with a low-rank matrix .	in both cases , we show that the convex relaxation is minimax optimal .	1	2	6	-6.0261226	5.196758	1
212-8-28	the multinomial logit model , a popular discrete choice model , captures the structure of the hidden preferences with a low-rank matrix .	we prove an upper bound on the resulting error with finite samples , and provide a matching information-theoretic lower bound .	1	2	7	-6.0176	5.207817	1
212-8-28	a natural approach to learn such a model is to solve a convex relaxation of nuclear norm minimization .	in order to predict the preferences , we want to learn the underlying model from noisy observations of the low-rank matrix , collected as revealed preferences in various forms of ordinal data .	0	4	3	4.4475355	-3.9304872	0
212-8-28	in order to predict the preferences , we want to learn the underlying model from noisy observations of the low-rank matrix , collected as revealed preferences in various forms of ordinal data .	we present the convex relaxation approach in two contexts of interest : collaborative ranking and bundled choice modeling .	1	3	5	-4.4202986	4.166233	1
212-8-28	in both cases , we show that the convex relaxation is minimax optimal .	in order to predict the preferences , we want to learn the underlying model from noisy observations of the low-rank matrix , collected as revealed preferences in various forms of ordinal data .	0	6	3	5.179807	-4.573144	0
212-8-28	in order to predict the preferences , we want to learn the underlying model from noisy observations of the low-rank matrix , collected as revealed preferences in various forms of ordinal data .	we prove an upper bound on the resulting error with finite samples , and provide a matching information-theoretic lower bound .	1	3	7	-5.994433	5.179716	1
212-8-28	a natural approach to learn such a model is to solve a convex relaxation of nuclear norm minimization .	we present the convex relaxation approach in two contexts of interest : collaborative ranking and bundled choice modeling .	1	4	5	-5.828864	5.1690326	1
212-8-28	a natural approach to learn such a model is to solve a convex relaxation of nuclear norm minimization .	in both cases , we show that the convex relaxation is minimax optimal .	1	4	6	-6.0011883	5.2285514	1
212-8-28	we prove an upper bound on the resulting error with finite samples , and provide a matching information-theoretic lower bound .	a natural approach to learn such a model is to solve a convex relaxation of nuclear norm minimization .	0	7	4	4.0154743	-3.6590152	0
212-8-28	in both cases , we show that the convex relaxation is minimax optimal .	we present the convex relaxation approach in two contexts of interest : collaborative ranking and bundled choice modeling .	0	6	5	3.986536	-3.6490889	0
212-8-28	we present the convex relaxation approach in two contexts of interest : collaborative ranking and bundled choice modeling .	we prove an upper bound on the resulting error with finite samples , and provide a matching information-theoretic lower bound .	1	5	7	-5.758026	5.170602	1
212-8-28	in both cases , we show that the convex relaxation is minimax optimal .	we prove an upper bound on the resulting error with finite samples , and provide a matching information-theoretic lower bound .	1	6	7	2.3634577	-2.1996832	0
213-8-28	perception is often described as a predictive process based on an optimal inference with respect to a generative model .	we study here the principled construction of a generative model specifically crafted to probe motion perception .	1	0	1	-5.950608	5.154395	1
213-8-28	in that context , we first provide an axiomatic , biologically-driven derivation of the model .	perception is often described as a predictive process based on an optimal inference with respect to a generative model .	0	2	0	5.674129	-4.996174	0
213-8-28	this model synthesizes random dynamic textures which are defined by stationary gaussian distributions obtained by the random aggregation of warped patterns .	perception is often described as a predictive process based on an optimal inference with respect to a generative model .	0	3	0	5.5521317	-4.8776217	0
213-8-28	perception is often described as a predictive process based on an optimal inference with respect to a generative model .	importantly , we show that this model can equivalently be described as a stochastic partial differential equation .	1	0	4	-5.942807	5.0222216	1
213-8-28	perception is often described as a predictive process based on an optimal inference with respect to a generative model .	using this characterization of motion in images , it allows us to recast motion-energy models into a principled bayesian inference framework .	1	0	5	-5.9216022	4.9612365	1
213-8-28	finally , we apply these textures in order to psychophysically probe speed perception in humans .	perception is often described as a predictive process based on an optimal inference with respect to a generative model .	0	6	0	5.6000385	-5.003439	0
213-8-28	in this framework , while the likelihood is derived from the generative model , the prior is estimated from the observed results and accounts for the perceptual bias in a principled fashion .	perception is often described as a predictive process based on an optimal inference with respect to a generative model .	0	7	0	5.72636	-5.0479164	0
213-8-28	we study here the principled construction of a generative model specifically crafted to probe motion perception .	in that context , we first provide an axiomatic , biologically-driven derivation of the model .	1	1	2	-5.9848423	5.1404104	1
213-8-28	this model synthesizes random dynamic textures which are defined by stationary gaussian distributions obtained by the random aggregation of warped patterns .	we study here the principled construction of a generative model specifically crafted to probe motion perception .	0	3	1	5.279298	-4.7292957	0
213-8-28	we study here the principled construction of a generative model specifically crafted to probe motion perception .	importantly , we show that this model can equivalently be described as a stochastic partial differential equation .	1	1	4	-5.9286604	5.1878815	1
213-8-28	we study here the principled construction of a generative model specifically crafted to probe motion perception .	using this characterization of motion in images , it allows us to recast motion-energy models into a principled bayesian inference framework .	1	1	5	-5.340559	4.910997	1
213-8-28	we study here the principled construction of a generative model specifically crafted to probe motion perception .	finally , we apply these textures in order to psychophysically probe speed perception in humans .	1	1	6	-5.9759145	5.203614	1
213-8-28	in this framework , while the likelihood is derived from the generative model , the prior is estimated from the observed results and accounts for the perceptual bias in a principled fashion .	we study here the principled construction of a generative model specifically crafted to probe motion perception .	0	7	1	5.104422	-4.506909	0
213-8-28	this model synthesizes random dynamic textures which are defined by stationary gaussian distributions obtained by the random aggregation of warped patterns .	in that context , we first provide an axiomatic , biologically-driven derivation of the model .	0	3	2	-4.7904882	4.3978806	1
213-8-28	importantly , we show that this model can equivalently be described as a stochastic partial differential equation .	in that context , we first provide an axiomatic , biologically-driven derivation of the model .	0	4	2	2.7236392	-2.6183364	0
213-8-28	using this characterization of motion in images , it allows us to recast motion-energy models into a principled bayesian inference framework .	in that context , we first provide an axiomatic , biologically-driven derivation of the model .	0	5	2	-1.7551844	1.7962062	1
213-8-28	in that context , we first provide an axiomatic , biologically-driven derivation of the model .	finally , we apply these textures in order to psychophysically probe speed perception in humans .	1	2	6	-5.860776	4.978737	1
213-8-28	in this framework , while the likelihood is derived from the generative model , the prior is estimated from the observed results and accounts for the perceptual bias in a principled fashion .	in that context , we first provide an axiomatic , biologically-driven derivation of the model .	0	7	2	-1.8784267	2.0074499	1
213-8-28	this model synthesizes random dynamic textures which are defined by stationary gaussian distributions obtained by the random aggregation of warped patterns .	importantly , we show that this model can equivalently be described as a stochastic partial differential equation .	1	3	4	-5.0813427	4.661154	1
213-8-28	using this characterization of motion in images , it allows us to recast motion-energy models into a principled bayesian inference framework .	this model synthesizes random dynamic textures which are defined by stationary gaussian distributions obtained by the random aggregation of warped patterns .	0	5	3	2.3223348	-2.1664023	0
213-8-28	this model synthesizes random dynamic textures which are defined by stationary gaussian distributions obtained by the random aggregation of warped patterns .	finally , we apply these textures in order to psychophysically probe speed perception in humans .	1	3	6	-5.73577	4.819213	1
213-8-28	this model synthesizes random dynamic textures which are defined by stationary gaussian distributions obtained by the random aggregation of warped patterns .	in this framework , while the likelihood is derived from the generative model , the prior is estimated from the observed results and accounts for the perceptual bias in a principled fashion .	1	3	7	-3.6607351	3.5057561	1
213-8-28	importantly , we show that this model can equivalently be described as a stochastic partial differential equation .	using this characterization of motion in images , it allows us to recast motion-energy models into a principled bayesian inference framework .	1	4	5	-0.0073794685	0.23661594	1
213-8-28	importantly , we show that this model can equivalently be described as a stochastic partial differential equation .	finally , we apply these textures in order to psychophysically probe speed perception in humans .	1	4	6	-5.789773	4.9569664	1
213-8-28	in this framework , while the likelihood is derived from the generative model , the prior is estimated from the observed results and accounts for the perceptual bias in a principled fashion .	importantly , we show that this model can equivalently be described as a stochastic partial differential equation .	0	7	4	-3.30086	3.2949243	1
213-8-28	using this characterization of motion in images , it allows us to recast motion-energy models into a principled bayesian inference framework .	finally , we apply these textures in order to psychophysically probe speed perception in humans .	1	5	6	-5.861657	5.0373936	1
213-8-28	in this framework , while the likelihood is derived from the generative model , the prior is estimated from the observed results and accounts for the perceptual bias in a principled fashion .	using this characterization of motion in images , it allows us to recast motion-energy models into a principled bayesian inference framework .	0	7	5	3.9016075	-3.511791	0
213-8-28	in this framework , while the likelihood is derived from the generative model , the prior is estimated from the observed results and accounts for the perceptual bias in a principled fashion .	finally , we apply these textures in order to psychophysically probe speed perception in humans .	0	7	6	-5.9046702	5.0603504	1
214-5-10	recurrent neural networks have been successful in capturing long-range dependencies in a number of problems but only recently have found their way into generative image models .	modeling the distribution of natural images is challenging , partly because of strong statistical dependencies which can extend over hundreds of pixels .	0	1	0	5.1716795	-4.35834	0
214-5-10	modeling the distribution of natural images is challenging , partly because of strong statistical dependencies which can extend over hundreds of pixels .	we here introduce a recurrent image model based on multidimensional long short-term memory units which are particularly suited for image modeling due to their spatial structure .	1	0	2	-5.8849382	4.943858	1
214-5-10	modeling the distribution of natural images is challenging , partly because of strong statistical dependencies which can extend over hundreds of pixels .	our model scales to images of arbitrary size and its likelihood is computationally tractable .	1	0	3	-6.0097265	5.1306386	1
214-5-10	we find that it outperforms the state of the art in quantitative comparisons on several image datasets and produces promising results when used for texture synthesis and inpainting .	modeling the distribution of natural images is challenging , partly because of strong statistical dependencies which can extend over hundreds of pixels .	0	4	0	5.641961	-5.033991	0
214-5-10	we here introduce a recurrent image model based on multidimensional long short-term memory units which are particularly suited for image modeling due to their spatial structure .	recurrent neural networks have been successful in capturing long-range dependencies in a number of problems but only recently have found their way into generative image models .	0	2	1	5.3855076	-4.752294	0
214-5-10	recurrent neural networks have been successful in capturing long-range dependencies in a number of problems but only recently have found their way into generative image models .	our model scales to images of arbitrary size and its likelihood is computationally tractable .	1	1	3	-5.9815273	5.1343727	1
214-5-10	we find that it outperforms the state of the art in quantitative comparisons on several image datasets and produces promising results when used for texture synthesis and inpainting .	recurrent neural networks have been successful in capturing long-range dependencies in a number of problems but only recently have found their way into generative image models .	0	4	1	5.6360583	-5.0414047	0
214-5-10	our model scales to images of arbitrary size and its likelihood is computationally tractable .	we here introduce a recurrent image model based on multidimensional long short-term memory units which are particularly suited for image modeling due to their spatial structure .	0	3	2	5.4191008	-4.806097	0
214-5-10	we here introduce a recurrent image model based on multidimensional long short-term memory units which are particularly suited for image modeling due to their spatial structure .	we find that it outperforms the state of the art in quantitative comparisons on several image datasets and produces promising results when used for texture synthesis and inpainting .	1	2	4	-5.996649	5.210718	1
214-5-10	our model scales to images of arbitrary size and its likelihood is computationally tractable .	we find that it outperforms the state of the art in quantitative comparisons on several image datasets and produces promising results when used for texture synthesis and inpainting .	1	3	4	-5.581002	5.064334	1
215-2-1	[CLS] the robust principal component analysis ( rpca ) problem seeks to separate lowrank trends from sparse outliers within a data matrix, that is, to approximate a nd matrix d as the sum of a low - rank matrix l and a	[CLS] the scaling of dimension, compression, and signal complexity in our theoretical results is verified empirically through simulations, and we also apply our method to a data set measuring chlorine concentration across a network of sensors to test its performance in practice. [SEP]	1	0	1	-5.9443517	5.102966	1
216-7-21	submodular and supermodular functions have found wide applicability in machine learning , capturing notions such as diversity and regularity , respectively .	these notions have deep consequences for optimization , and the problem of ( approximately ) optimizing submodular functions has received much attention .	1	0	1	-5.1494823	4.779703	1
216-7-21	however , beyond optimization , these notions allow specifying expressive probabilistic models that can be used to quantify predictive uncertainty via marginal inference .	submodular and supermodular functions have found wide applicability in machine learning , capturing notions such as diversity and regularity , respectively .	0	2	0	5.4500184	-4.7842245	0
216-7-21	submodular and supermodular functions have found wide applicability in machine learning , capturing notions such as diversity and regularity , respectively .	prominent , well-studied special cases include ising models and determinantal point processes , but the general class of log-submodular and log-supermodular models is much richer and little studied .	1	0	3	-5.489587	5.075393	1
216-7-21	submodular and supermodular functions have found wide applicability in machine learning , capturing notions such as diversity and regularity , respectively .	in this paper , we investigate the use of markov chain monte carlo sampling to perform approximate inference in general log-submodular and log-supermodular models .	1	0	4	-5.9675894	5.250723	1
216-7-21	submodular and supermodular functions have found wide applicability in machine learning , capturing notions such as diversity and regularity , respectively .	in particular , we consider a simple gibbs sampling procedure , and establish two sufficient conditions , the first guaranteeing polynomial-time , and the second fast ( o ( n log n ) ) mixing .	1	0	5	-6.0103974	5.217407	1
216-7-21	we also evaluate the efficiency of the gibbs sampler on three examples of such models , and compare against a recently proposed variational approach .	submodular and supermodular functions have found wide applicability in machine learning , capturing notions such as diversity and regularity , respectively .	0	6	0	5.621123	-5.0435686	0
216-7-21	these notions have deep consequences for optimization , and the problem of ( approximately ) optimizing submodular functions has received much attention .	however , beyond optimization , these notions allow specifying expressive probabilistic models that can be used to quantify predictive uncertainty via marginal inference .	1	1	2	-5.6660666	5.1510324	1
216-7-21	these notions have deep consequences for optimization , and the problem of ( approximately ) optimizing submodular functions has received much attention .	prominent , well-studied special cases include ising models and determinantal point processes , but the general class of log-submodular and log-supermodular models is much richer and little studied .	1	1	3	-5.6058736	5.116146	1
216-7-21	in this paper , we investigate the use of markov chain monte carlo sampling to perform approximate inference in general log-submodular and log-supermodular models .	these notions have deep consequences for optimization , and the problem of ( approximately ) optimizing submodular functions has received much attention .	0	4	1	5.3668494	-4.8342047	0
216-7-21	these notions have deep consequences for optimization , and the problem of ( approximately ) optimizing submodular functions has received much attention .	in particular , we consider a simple gibbs sampling procedure , and establish two sufficient conditions , the first guaranteeing polynomial-time , and the second fast ( o ( n log n ) ) mixing .	1	1	5	-5.9786105	5.2346992	1
216-7-21	we also evaluate the efficiency of the gibbs sampler on three examples of such models , and compare against a recently proposed variational approach .	these notions have deep consequences for optimization , and the problem of ( approximately ) optimizing submodular functions has received much attention .	0	6	1	5.691164	-5.1042104	0
216-7-21	prominent , well-studied special cases include ising models and determinantal point processes , but the general class of log-submodular and log-supermodular models is much richer and little studied .	however , beyond optimization , these notions allow specifying expressive probabilistic models that can be used to quantify predictive uncertainty via marginal inference .	0	3	2	-1.7807088	1.9646302	1
216-7-21	in this paper , we investigate the use of markov chain monte carlo sampling to perform approximate inference in general log-submodular and log-supermodular models .	however , beyond optimization , these notions allow specifying expressive probabilistic models that can be used to quantify predictive uncertainty via marginal inference .	0	4	2	4.213083	-3.8528	0
216-7-21	in particular , we consider a simple gibbs sampling procedure , and establish two sufficient conditions , the first guaranteeing polynomial-time , and the second fast ( o ( n log n ) ) mixing .	however , beyond optimization , these notions allow specifying expressive probabilistic models that can be used to quantify predictive uncertainty via marginal inference .	0	5	2	4.7078233	-4.2679176	0
216-7-21	we also evaluate the efficiency of the gibbs sampler on three examples of such models , and compare against a recently proposed variational approach .	however , beyond optimization , these notions allow specifying expressive probabilistic models that can be used to quantify predictive uncertainty via marginal inference .	0	6	2	5.6073017	-4.9835405	0
216-7-21	in this paper , we investigate the use of markov chain monte carlo sampling to perform approximate inference in general log-submodular and log-supermodular models .	prominent , well-studied special cases include ising models and determinantal point processes , but the general class of log-submodular and log-supermodular models is much richer and little studied .	0	4	3	4.173613	-3.8176994	0
216-7-21	in particular , we consider a simple gibbs sampling procedure , and establish two sufficient conditions , the first guaranteeing polynomial-time , and the second fast ( o ( n log n ) ) mixing .	prominent , well-studied special cases include ising models and determinantal point processes , but the general class of log-submodular and log-supermodular models is much richer and little studied .	0	5	3	4.9751863	-4.522812	0
216-7-21	we also evaluate the efficiency of the gibbs sampler on three examples of such models , and compare against a recently proposed variational approach .	prominent , well-studied special cases include ising models and determinantal point processes , but the general class of log-submodular and log-supermodular models is much richer and little studied .	0	6	3	5.501173	-4.8545837	0
216-7-21	in this paper , we investigate the use of markov chain monte carlo sampling to perform approximate inference in general log-submodular and log-supermodular models .	in particular , we consider a simple gibbs sampling procedure , and establish two sufficient conditions , the first guaranteeing polynomial-time , and the second fast ( o ( n log n ) ) mixing .	1	4	5	-5.900483	5.218053	1
216-7-21	in this paper , we investigate the use of markov chain monte carlo sampling to perform approximate inference in general log-submodular and log-supermodular models .	we also evaluate the efficiency of the gibbs sampler on three examples of such models , and compare against a recently proposed variational approach .	1	4	6	-5.9803867	5.207346	1
216-7-21	in particular , we consider a simple gibbs sampling procedure , and establish two sufficient conditions , the first guaranteeing polynomial-time , and the second fast ( o ( n log n ) ) mixing .	we also evaluate the efficiency of the gibbs sampler on three examples of such models , and compare against a recently proposed variational approach .	1	5	6	-5.8631287	5.1605363	1
217-7-21	information diffusion in online social networks is affected by the underlying network topology , but it also has the power to change it .	online users are constantly creating new links when exposed to new information sources , and in turn these links are alternating the way information spreads .	1	0	1	-2.581699	2.65311	1
217-7-21	information diffusion in online social networks is affected by the underlying network topology , but it also has the power to change it .	however , these two highly intertwined stochastic processes , information diffusion and network evolution , have been predominantly studied separately , ignoring their co-evolutionary dynamics .	1	0	2	-5.291714	4.892389	1
217-7-21	we propose a temporal point process model , coevolve , for such joint dynamics , allowing the intensity of one process to be modulated by that of the other .	information diffusion in online social networks is affected by the underlying network topology , but it also has the power to change it .	0	3	0	5.758071	-5.16493	0
217-7-21	this model allows us to efficiently simulate interleaved diffusion and network events , and generate traces obeying common diffusion and network patterns observed in real-world networks .	information diffusion in online social networks is affected by the underlying network topology , but it also has the power to change it .	0	4	0	5.7512465	-5.153823	0
217-7-21	information diffusion in online social networks is affected by the underlying network topology , but it also has the power to change it .	furthermore , we also develop a convex optimization framework to learn the parameters of the model from historical diffusion and network evolution traces .	1	0	5	-5.866174	4.951007	1
217-7-21	we experimented with both synthetic data and data gathered from twitter , and show that our model provides a good fit to the data as well as more accurate predictions than alternatives .	information diffusion in online social networks is affected by the underlying network topology , but it also has the power to change it .	0	6	0	5.755904	-5.137635	0
217-7-21	online users are constantly creating new links when exposed to new information sources , and in turn these links are alternating the way information spreads .	however , these two highly intertwined stochastic processes , information diffusion and network evolution , have been predominantly studied separately , ignoring their co-evolutionary dynamics .	1	1	2	-4.6443295	4.3761783	1
217-7-21	we propose a temporal point process model , coevolve , for such joint dynamics , allowing the intensity of one process to be modulated by that of the other .	online users are constantly creating new links when exposed to new information sources , and in turn these links are alternating the way information spreads .	0	3	1	5.5929785	-4.9906063	0
217-7-21	this model allows us to efficiently simulate interleaved diffusion and network events , and generate traces obeying common diffusion and network patterns observed in real-world networks .	online users are constantly creating new links when exposed to new information sources , and in turn these links are alternating the way information spreads .	0	4	1	5.614644	-5.001991	0
217-7-21	online users are constantly creating new links when exposed to new information sources , and in turn these links are alternating the way information spreads .	furthermore , we also develop a convex optimization framework to learn the parameters of the model from historical diffusion and network evolution traces .	1	1	5	-5.901311	4.9924345	1
217-7-21	online users are constantly creating new links when exposed to new information sources , and in turn these links are alternating the way information spreads .	we experimented with both synthetic data and data gathered from twitter , and show that our model provides a good fit to the data as well as more accurate predictions than alternatives .	1	1	6	-5.935759	5.0162096	1
217-7-21	however , these two highly intertwined stochastic processes , information diffusion and network evolution , have been predominantly studied separately , ignoring their co-evolutionary dynamics .	we propose a temporal point process model , coevolve , for such joint dynamics , allowing the intensity of one process to be modulated by that of the other .	1	2	3	-5.901243	5.211774	1
217-7-21	this model allows us to efficiently simulate interleaved diffusion and network events , and generate traces obeying common diffusion and network patterns observed in real-world networks .	however , these two highly intertwined stochastic processes , information diffusion and network evolution , have been predominantly studied separately , ignoring their co-evolutionary dynamics .	0	4	2	5.6498947	-5.0330153	0
217-7-21	furthermore , we also develop a convex optimization framework to learn the parameters of the model from historical diffusion and network evolution traces .	however , these two highly intertwined stochastic processes , information diffusion and network evolution , have been predominantly studied separately , ignoring their co-evolutionary dynamics .	0	5	2	5.600231	-4.9645867	0
217-7-21	we experimented with both synthetic data and data gathered from twitter , and show that our model provides a good fit to the data as well as more accurate predictions than alternatives .	however , these two highly intertwined stochastic processes , information diffusion and network evolution , have been predominantly studied separately , ignoring their co-evolutionary dynamics .	0	6	2	5.669959	-5.006753	0
217-7-21	we propose a temporal point process model , coevolve , for such joint dynamics , allowing the intensity of one process to be modulated by that of the other .	this model allows us to efficiently simulate interleaved diffusion and network events , and generate traces obeying common diffusion and network patterns observed in real-world networks .	1	3	4	-5.335843	4.8701186	1
217-7-21	furthermore , we also develop a convex optimization framework to learn the parameters of the model from historical diffusion and network evolution traces .	we propose a temporal point process model , coevolve , for such joint dynamics , allowing the intensity of one process to be modulated by that of the other .	0	5	3	5.2365313	-4.564173	0
217-7-21	we experimented with both synthetic data and data gathered from twitter , and show that our model provides a good fit to the data as well as more accurate predictions than alternatives .	we propose a temporal point process model , coevolve , for such joint dynamics , allowing the intensity of one process to be modulated by that of the other .	0	6	3	5.5328765	-4.858618	0
217-7-21	furthermore , we also develop a convex optimization framework to learn the parameters of the model from historical diffusion and network evolution traces .	this model allows us to efficiently simulate interleaved diffusion and network events , and generate traces obeying common diffusion and network patterns observed in real-world networks .	0	5	4	4.677458	-4.1775846	0
217-7-21	this model allows us to efficiently simulate interleaved diffusion and network events , and generate traces obeying common diffusion and network patterns observed in real-world networks .	we experimented with both synthetic data and data gathered from twitter , and show that our model provides a good fit to the data as well as more accurate predictions than alternatives .	1	4	6	-5.532135	4.994767	1
217-7-21	we experimented with both synthetic data and data gathered from twitter , and show that our model provides a good fit to the data as well as more accurate predictions than alternatives .	furthermore , we also develop a convex optimization framework to learn the parameters of the model from historical diffusion and network evolution traces .	0	6	5	2.980451	-2.7203302	0
218-6-15	recently there has been substantial interest in spectral methods for learning dynamical systems .	these methods are popular since they often offer a good tradeoff between computational and statistical efficiency .	1	0	1	-5.900079	5.1886015	1
218-6-15	unfortunately , they can be difficult to use and extend in practice : e.g. , they can make it difficult to incorporate prior information such as sparsity or structure .	recently there has been substantial interest in spectral methods for learning dynamical systems .	0	2	0	5.505288	-4.9508753	0
218-6-15	recently there has been substantial interest in spectral methods for learning dynamical systems .	to address this problem , we present a new view of dynamical system learning : we show how to learn dynamical systems by solving a sequence of ordinary supervised learning problems , thereby allowing users to incorporate prior knowledge via standard techniques such as l1 regularization .	1	0	3	-5.998652	5.1722198	1
218-6-15	recently there has been substantial interest in spectral methods for learning dynamical systems .	many existing spectral methods are special cases of this new framework , using linear regression as the supervised learner .	1	0	4	-5.908445	5.2223406	1
218-6-15	recently there has been substantial interest in spectral methods for learning dynamical systems .	we demonstrate the effectiveness of our framework by showing examples where nonlinear regression or lasso let us learn better state representations than plain linear regression does ; the correctness of these instances follows directly from our general analysis .	1	0	5	-6.0017853	5.1597433	1
218-6-15	these methods are popular since they often offer a good tradeoff between computational and statistical efficiency .	unfortunately , they can be difficult to use and extend in practice : e.g. , they can make it difficult to incorporate prior information such as sparsity or structure .	1	1	2	-2.280562	2.3969138	1
218-6-15	to address this problem , we present a new view of dynamical system learning : we show how to learn dynamical systems by solving a sequence of ordinary supervised learning problems , thereby allowing users to incorporate prior knowledge via standard techniques such as l1 regularization .	these methods are popular since they often offer a good tradeoff between computational and statistical efficiency .	0	3	1	3.3890395	-3.0790994	0
218-6-15	many existing spectral methods are special cases of this new framework , using linear regression as the supervised learner .	these methods are popular since they often offer a good tradeoff between computational and statistical efficiency .	0	4	1	3.9113865	-3.4753075	0
218-6-15	we demonstrate the effectiveness of our framework by showing examples where nonlinear regression or lasso let us learn better state representations than plain linear regression does ; the correctness of these instances follows directly from our general analysis .	these methods are popular since they often offer a good tradeoff between computational and statistical efficiency .	0	5	1	5.383788	-4.755477	0
218-6-15	unfortunately , they can be difficult to use and extend in practice : e.g. , they can make it difficult to incorporate prior information such as sparsity or structure .	to address this problem , we present a new view of dynamical system learning : we show how to learn dynamical systems by solving a sequence of ordinary supervised learning problems , thereby allowing users to incorporate prior knowledge via standard techniques such as l1 regularization .	1	2	3	-5.692447	5.201767	1
218-6-15	many existing spectral methods are special cases of this new framework , using linear regression as the supervised learner .	unfortunately , they can be difficult to use and extend in practice : e.g. , they can make it difficult to incorporate prior information such as sparsity or structure .	0	4	2	4.9045534	-4.3193426	0
218-6-15	unfortunately , they can be difficult to use and extend in practice : e.g. , they can make it difficult to incorporate prior information such as sparsity or structure .	we demonstrate the effectiveness of our framework by showing examples where nonlinear regression or lasso let us learn better state representations than plain linear regression does ; the correctness of these instances follows directly from our general analysis .	1	2	5	-5.967072	5.0914	1
218-6-15	to address this problem , we present a new view of dynamical system learning : we show how to learn dynamical systems by solving a sequence of ordinary supervised learning problems , thereby allowing users to incorporate prior knowledge via standard techniques such as l1 regularization .	many existing spectral methods are special cases of this new framework , using linear regression as the supervised learner .	1	3	4	-4.3454857	4.0849013	1
218-6-15	to address this problem , we present a new view of dynamical system learning : we show how to learn dynamical systems by solving a sequence of ordinary supervised learning problems , thereby allowing users to incorporate prior knowledge via standard techniques such as l1 regularization .	we demonstrate the effectiveness of our framework by showing examples where nonlinear regression or lasso let us learn better state representations than plain linear regression does ; the correctness of these instances follows directly from our general analysis .	1	3	5	-6.0319085	5.1934547	1
218-6-15	we demonstrate the effectiveness of our framework by showing examples where nonlinear regression or lasso let us learn better state representations than plain linear regression does ; the correctness of these instances follows directly from our general analysis .	many existing spectral methods are special cases of this new framework , using linear regression as the supervised learner .	0	5	4	5.213908	-4.623473	0
219-7-21	cfr is an iterative algorithm that repeatedly traverses the game tree , updating regrets at each information set .	counterfactual regret minimization ( cfr ) is a leading algorithm for finding a nash equilibrium in large zero-sum imperfect-information games .	0	1	0	5.4761477	-4.8951387	0
219-7-21	counterfactual regret minimization ( cfr ) is a leading algorithm for finding a nash equilibrium in large zero-sum imperfect-information games .	we introduce an improvement to cfr that prunes any path of play in the tree , and its descendants , that has negative regret .	1	0	2	-5.914036	5.114203	1
219-7-21	counterfactual regret minimization ( cfr ) is a leading algorithm for finding a nash equilibrium in large zero-sum imperfect-information games .	it revisits that sequence at the earliest subsequent cfr iteration where the regret could have become positive , had that path been explored on every iteration .	1	0	3	-5.868516	5.166191	1
219-7-21	counterfactual regret minimization ( cfr ) is a leading algorithm for finding a nash equilibrium in large zero-sum imperfect-information games .	the new algorithm maintains cfr 's convergence guarantees while making iterations significantly faster -- even if previously known pruning techniques are used in the comparison .	1	0	4	-5.842	5.1393127	1
219-7-21	this improvement carries over to cfr+ , a recent variant of cfr .	counterfactual regret minimization ( cfr ) is a leading algorithm for finding a nash equilibrium in large zero-sum imperfect-information games .	0	5	0	5.564541	-4.9623194	0
219-7-21	experiments show an order of magnitude speed improvement , and the relative speed improvement increases with the size of the game .	counterfactual regret minimization ( cfr ) is a leading algorithm for finding a nash equilibrium in large zero-sum imperfect-information games .	0	6	0	5.5361133	-4.9850535	0
219-7-21	cfr is an iterative algorithm that repeatedly traverses the game tree , updating regrets at each information set .	we introduce an improvement to cfr that prunes any path of play in the tree , and its descendants , that has negative regret .	1	1	2	-5.6397176	4.7936664	1
219-7-21	cfr is an iterative algorithm that repeatedly traverses the game tree , updating regrets at each information set .	it revisits that sequence at the earliest subsequent cfr iteration where the regret could have become positive , had that path been explored on every iteration .	1	1	3	-5.967819	5.1670017	1
219-7-21	cfr is an iterative algorithm that repeatedly traverses the game tree , updating regrets at each information set .	the new algorithm maintains cfr 's convergence guarantees while making iterations significantly faster -- even if previously known pruning techniques are used in the comparison .	1	1	4	-5.9772096	5.137214	1
219-7-21	cfr is an iterative algorithm that repeatedly traverses the game tree , updating regrets at each information set .	this improvement carries over to cfr+ , a recent variant of cfr .	1	1	5	-5.755497	4.911782	1
219-7-21	experiments show an order of magnitude speed improvement , and the relative speed improvement increases with the size of the game .	cfr is an iterative algorithm that repeatedly traverses the game tree , updating regrets at each information set .	0	6	1	5.4512525	-4.888715	0
219-7-21	it revisits that sequence at the earliest subsequent cfr iteration where the regret could have become positive , had that path been explored on every iteration .	we introduce an improvement to cfr that prunes any path of play in the tree , and its descendants , that has negative regret .	0	3	2	-0.44073877	0.6968548	1
219-7-21	the new algorithm maintains cfr 's convergence guarantees while making iterations significantly faster -- even if previously known pruning techniques are used in the comparison .	we introduce an improvement to cfr that prunes any path of play in the tree , and its descendants , that has negative regret .	0	4	2	3.4187827	-3.1296904	0
219-7-21	this improvement carries over to cfr+ , a recent variant of cfr .	we introduce an improvement to cfr that prunes any path of play in the tree , and its descendants , that has negative regret .	0	5	2	4.1647196	-3.7645707	0
219-7-21	experiments show an order of magnitude speed improvement , and the relative speed improvement increases with the size of the game .	we introduce an improvement to cfr that prunes any path of play in the tree , and its descendants , that has negative regret .	0	6	2	5.2917986	-4.756675	0
219-7-21	it revisits that sequence at the earliest subsequent cfr iteration where the regret could have become positive , had that path been explored on every iteration .	the new algorithm maintains cfr 's convergence guarantees while making iterations significantly faster -- even if previously known pruning techniques are used in the comparison .	1	3	4	-5.0991545	4.6705337	1
219-7-21	this improvement carries over to cfr+ , a recent variant of cfr .	it revisits that sequence at the earliest subsequent cfr iteration where the regret could have become positive , had that path been explored on every iteration .	0	5	3	-1.0492126	1.1795092	1
219-7-21	experiments show an order of magnitude speed improvement , and the relative speed improvement increases with the size of the game .	it revisits that sequence at the earliest subsequent cfr iteration where the regret could have become positive , had that path been explored on every iteration .	0	6	3	5.2537694	-4.742337	0
219-7-21	this improvement carries over to cfr+ , a recent variant of cfr .	the new algorithm maintains cfr 's convergence guarantees while making iterations significantly faster -- even if previously known pruning techniques are used in the comparison .	0	5	4	-4.546294	4.254882	1
219-7-21	experiments show an order of magnitude speed improvement , and the relative speed improvement increases with the size of the game .	the new algorithm maintains cfr 's convergence guarantees while making iterations significantly faster -- even if previously known pruning techniques are used in the comparison .	0	6	4	3.9424632	-3.6809182	0
219-7-21	experiments show an order of magnitude speed improvement , and the relative speed improvement increases with the size of the game .	this improvement carries over to cfr+ , a recent variant of cfr .	0	6	5	3.7289686	-3.4942622	0
220-7-21	two tests are given , both based on an ensemble of distances between analytic functions representing each of the distributions .	we propose a class of nonparametric two-sample tests with a cost linear in the sample size .	0	1	0	5.254108	-4.68779	0
220-7-21	we propose a class of nonparametric two-sample tests with a cost linear in the sample size .	the first test uses smoothed empirical characteristic functions to represent the distributions , the second uses distribution embeddings in a reproducing kernel hilbert space .	1	0	2	-5.7352476	5.221872	1
220-7-21	we propose a class of nonparametric two-sample tests with a cost linear in the sample size .	analyticity implies that differences in the distributions may be detected almost surely at a finite number of randomly chosen locations/frequencies .	1	0	3	4.0118003	-3.714872	0
220-7-21	we propose a class of nonparametric two-sample tests with a cost linear in the sample size .	the new tests are consistent against a larger class of alternatives than the previous linear-time tests based on the ( non-smoothed ) empirical characteristic functions , while being much faster than the current state-of-the-art quadratic-time kernel-based or energy distancebased tests .	1	0	4	-5.951226	5.22036	1
220-7-21	experiments on artificial benchmarks and on challenging real-world testing problems demonstrate that our tests give a better power/time tradeoff than competing approaches , and in some cases , better outright power than even the most expensive quadratic-time tests .	we propose a class of nonparametric two-sample tests with a cost linear in the sample size .	0	5	0	5.658283	-4.98969	0
220-7-21	we propose a class of nonparametric two-sample tests with a cost linear in the sample size .	this performance advantage is retained even in high dimensions , and in cases where the difference in distributions is not observable with low order statistics .	1	0	6	-5.6447515	5.056485	1
220-7-21	two tests are given , both based on an ensemble of distances between analytic functions representing each of the distributions .	the first test uses smoothed empirical characteristic functions to represent the distributions , the second uses distribution embeddings in a reproducing kernel hilbert space .	1	1	2	-5.8092265	5.263793	1
220-7-21	two tests are given , both based on an ensemble of distances between analytic functions representing each of the distributions .	analyticity implies that differences in the distributions may be detected almost surely at a finite number of randomly chosen locations/frequencies .	1	1	3	4.4859056	-4.136483	0
220-7-21	two tests are given , both based on an ensemble of distances between analytic functions representing each of the distributions .	the new tests are consistent against a larger class of alternatives than the previous linear-time tests based on the ( non-smoothed ) empirical characteristic functions , while being much faster than the current state-of-the-art quadratic-time kernel-based or energy distancebased tests .	1	1	4	-5.992801	5.221438	1
220-7-21	two tests are given , both based on an ensemble of distances between analytic functions representing each of the distributions .	experiments on artificial benchmarks and on challenging real-world testing problems demonstrate that our tests give a better power/time tradeoff than competing approaches , and in some cases , better outright power than even the most expensive quadratic-time tests .	1	1	5	-5.839837	5.215524	1
220-7-21	this performance advantage is retained even in high dimensions , and in cases where the difference in distributions is not observable with low order statistics .	two tests are given , both based on an ensemble of distances between analytic functions representing each of the distributions .	0	6	1	3.6979184	-3.430194	0
220-7-21	analyticity implies that differences in the distributions may be detected almost surely at a finite number of randomly chosen locations/frequencies .	the first test uses smoothed empirical characteristic functions to represent the distributions , the second uses distribution embeddings in a reproducing kernel hilbert space .	0	3	2	-4.3813896	4.1032624	1
220-7-21	the new tests are consistent against a larger class of alternatives than the previous linear-time tests based on the ( non-smoothed ) empirical characteristic functions , while being much faster than the current state-of-the-art quadratic-time kernel-based or energy distancebased tests .	the first test uses smoothed empirical characteristic functions to represent the distributions , the second uses distribution embeddings in a reproducing kernel hilbert space .	0	4	2	5.1538277	-4.553506	0
220-7-21	the first test uses smoothed empirical characteristic functions to represent the distributions , the second uses distribution embeddings in a reproducing kernel hilbert space .	experiments on artificial benchmarks and on challenging real-world testing problems demonstrate that our tests give a better power/time tradeoff than competing approaches , and in some cases , better outright power than even the most expensive quadratic-time tests .	1	2	5	-5.419097	4.9593363	1
220-7-21	the first test uses smoothed empirical characteristic functions to represent the distributions , the second uses distribution embeddings in a reproducing kernel hilbert space .	this performance advantage is retained even in high dimensions , and in cases where the difference in distributions is not observable with low order statistics .	1	2	6	-5.1223598	4.733336	1
220-7-21	analyticity implies that differences in the distributions may be detected almost surely at a finite number of randomly chosen locations/frequencies .	the new tests are consistent against a larger class of alternatives than the previous linear-time tests based on the ( non-smoothed ) empirical characteristic functions , while being much faster than the current state-of-the-art quadratic-time kernel-based or energy distancebased tests .	1	3	4	-5.9758177	5.2591963	1
220-7-21	analyticity implies that differences in the distributions may be detected almost surely at a finite number of randomly chosen locations/frequencies .	experiments on artificial benchmarks and on challenging real-world testing problems demonstrate that our tests give a better power/time tradeoff than competing approaches , and in some cases , better outright power than even the most expensive quadratic-time tests .	1	3	5	-6.008234	5.2293205	1
220-7-21	this performance advantage is retained even in high dimensions , and in cases where the difference in distributions is not observable with low order statistics .	analyticity implies that differences in the distributions may be detected almost surely at a finite number of randomly chosen locations/frequencies .	0	6	3	4.9705734	-4.4997444	0
220-7-21	[CLS] the new tests are consistent against a larger class of alternatives than the previous linear - time tests based on the ( non - smoothed ) empirical characteristic functions, while being much faster than the current state - of - the - art quadratic - time kernel	[CLS] experiments on artificial benchmarks and on challenging real - world testing problems demonstrate that our tests give a better power / time tradeoff than competing approaches, and in some cases, better outright power than even the most expensive quadratic - time tests. [SEP]	1	4	5	-2.2561417	2.2442348	1
220-7-21	the new tests are consistent against a larger class of alternatives than the previous linear-time tests based on the ( non-smoothed ) empirical characteristic functions , while being much faster than the current state-of-the-art quadratic-time kernel-based or energy distancebased tests .	this performance advantage is retained even in high dimensions , and in cases where the difference in distributions is not observable with low order statistics .	1	4	6	0.13644104	0.13598353	0
220-7-21	this performance advantage is retained even in high dimensions , and in cases where the difference in distributions is not observable with low order statistics .	experiments on artificial benchmarks and on challenging real-world testing problems demonstrate that our tests give a better power/time tradeoff than competing approaches , and in some cases , better outright power than even the most expensive quadratic-time tests .	0	6	5	-3.5090551	3.351634	1
221-9-36	recent object detection systems rely on two critical steps : ( 1 ) a set of object proposals is predicted as efficiently as possible , and ( 2 ) this set of candidate proposals is then passed to an object classifier .	such approaches have been shown they can be fast , while achieving the state of the art in detection performance .	1	0	1	-3.3175821	3.2071905	1
221-9-36	recent object detection systems rely on two critical steps : ( 1 ) a set of object proposals is predicted as efficiently as possible , and ( 2 ) this set of candidate proposals is then passed to an object classifier .	in this paper , we propose a new way to generate object proposals , introducing an approach based on a discriminative convolutional network .	1	0	2	-5.8289876	5.219267	1
221-9-36	recent object detection systems rely on two critical steps : ( 1 ) a set of object proposals is predicted as efficiently as possible , and ( 2 ) this set of candidate proposals is then passed to an object classifier .	our model is trained jointly with two objectives : given an image patch , the first part of the system outputs a class-agnostic segmentation mask , while the second part of the system outputs the likelihood of the patch being centered on a full object .	1	0	3	-5.9993143	5.215935	1
221-9-36	at test time , the model is efficiently applied on the whole test image and generates a set of segmentation masks , each of them being assigned with a corresponding object likelihood score .	recent object detection systems rely on two critical steps : ( 1 ) a set of object proposals is predicted as efficiently as possible , and ( 2 ) this set of candidate proposals is then passed to an object classifier .	0	4	0	5.6501136	-5.041904	0
221-9-36	we show that our model yields significant improvements over state-of-theart object proposal algorithms .	recent object detection systems rely on two critical steps : ( 1 ) a set of object proposals is predicted as efficiently as possible , and ( 2 ) this set of candidate proposals is then passed to an object classifier .	0	5	0	5.7016764	-5.1214304	0
221-9-36	in particular , compared to previous approaches , our model obtains substantially higher object recall using fewer proposals .	recent object detection systems rely on two critical steps : ( 1 ) a set of object proposals is predicted as efficiently as possible , and ( 2 ) this set of candidate proposals is then passed to an object classifier .	0	6	0	5.634264	-5.0350523	0
221-9-36	recent object detection systems rely on two critical steps : ( 1 ) a set of object proposals is predicted as efficiently as possible , and ( 2 ) this set of candidate proposals is then passed to an object classifier .	we also show that our model is able to generalize to unseen categories it has not seen during training .	1	0	7	-5.8742146	4.94734	1
221-9-36	unlike all previous approaches for generating object masks , we do not rely on edges , superpixels , or any other form of low-level segmentation .	recent object detection systems rely on two critical steps : ( 1 ) a set of object proposals is predicted as efficiently as possible , and ( 2 ) this set of candidate proposals is then passed to an object classifier .	0	8	0	5.6879206	-5.092696	0
221-9-36	such approaches have been shown they can be fast , while achieving the state of the art in detection performance .	in this paper , we propose a new way to generate object proposals , introducing an approach based on a discriminative convolutional network .	1	1	2	-5.6638875	5.1846023	1
221-9-36	such approaches have been shown they can be fast , while achieving the state of the art in detection performance .	our model is trained jointly with two objectives : given an image patch , the first part of the system outputs a class-agnostic segmentation mask , while the second part of the system outputs the likelihood of the patch being centered on a full object .	1	1	3	-5.755921	5.2716417	1
221-9-36	such approaches have been shown they can be fast , while achieving the state of the art in detection performance .	at test time , the model is efficiently applied on the whole test image and generates a set of segmentation masks , each of them being assigned with a corresponding object likelihood score .	1	1	4	-5.4785795	5.065688	1
221-9-36	we show that our model yields significant improvements over state-of-theart object proposal algorithms .	such approaches have been shown they can be fast , while achieving the state of the art in detection performance .	0	5	1	5.5496774	-4.910611	0
221-9-36	such approaches have been shown they can be fast , while achieving the state of the art in detection performance .	in particular , compared to previous approaches , our model obtains substantially higher object recall using fewer proposals .	1	1	6	-5.9495	5.2357936	1
221-9-36	we also show that our model is able to generalize to unseen categories it has not seen during training .	such approaches have been shown they can be fast , while achieving the state of the art in detection performance .	0	7	1	5.4857817	-4.880751	0
221-9-36	unlike all previous approaches for generating object masks , we do not rely on edges , superpixels , or any other form of low-level segmentation .	such approaches have been shown they can be fast , while achieving the state of the art in detection performance .	0	8	1	4.0974455	-3.7253458	0
221-9-36	our model is trained jointly with two objectives : given an image patch , the first part of the system outputs a class-agnostic segmentation mask , while the second part of the system outputs the likelihood of the patch being centered on a full object .	in this paper , we propose a new way to generate object proposals , introducing an approach based on a discriminative convolutional network .	0	3	2	5.496441	-4.8727407	0
221-9-36	in this paper , we propose a new way to generate object proposals , introducing an approach based on a discriminative convolutional network .	at test time , the model is efficiently applied on the whole test image and generates a set of segmentation masks , each of them being assigned with a corresponding object likelihood score .	1	2	4	-5.4895744	4.984872	1
221-9-36	in this paper , we propose a new way to generate object proposals , introducing an approach based on a discriminative convolutional network .	we show that our model yields significant improvements over state-of-theart object proposal algorithms .	1	2	5	-5.9924088	5.1302505	1
221-9-36	in this paper , we propose a new way to generate object proposals , introducing an approach based on a discriminative convolutional network .	in particular , compared to previous approaches , our model obtains substantially higher object recall using fewer proposals .	1	2	6	-5.978875	5.179766	1
221-9-36	in this paper , we propose a new way to generate object proposals , introducing an approach based on a discriminative convolutional network .	we also show that our model is able to generalize to unseen categories it has not seen during training .	1	2	7	-5.9710135	5.1072206	1
221-9-36	in this paper , we propose a new way to generate object proposals , introducing an approach based on a discriminative convolutional network .	unlike all previous approaches for generating object masks , we do not rely on edges , superpixels , or any other form of low-level segmentation .	1	2	8	-5.5027037	4.9864197	1
221-9-36	our model is trained jointly with two objectives : given an image patch , the first part of the system outputs a class-agnostic segmentation mask , while the second part of the system outputs the likelihood of the patch being centered on a full object .	at test time , the model is efficiently applied on the whole test image and generates a set of segmentation masks , each of them being assigned with a corresponding object likelihood score .	1	3	4	-2.9501247	2.8822134	1
221-9-36	our model is trained jointly with two objectives : given an image patch , the first part of the system outputs a class-agnostic segmentation mask , while the second part of the system outputs the likelihood of the patch being centered on a full object .	we show that our model yields significant improvements over state-of-theart object proposal algorithms .	1	3	5	-5.736332	5.059766	1
221-9-36	our model is trained jointly with two objectives : given an image patch , the first part of the system outputs a class-agnostic segmentation mask , while the second part of the system outputs the likelihood of the patch being centered on a full object .	in particular , compared to previous approaches , our model obtains substantially higher object recall using fewer proposals .	1	3	6	-5.695977	5.066176	1
221-9-36	our model is trained jointly with two objectives : given an image patch , the first part of the system outputs a class-agnostic segmentation mask , while the second part of the system outputs the likelihood of the patch being centered on a full object .	we also show that our model is able to generalize to unseen categories it has not seen during training .	1	3	7	-5.972684	5.1515527	1
221-9-36	unlike all previous approaches for generating object masks , we do not rely on edges , superpixels , or any other form of low-level segmentation .	our model is trained jointly with two objectives : given an image patch , the first part of the system outputs a class-agnostic segmentation mask , while the second part of the system outputs the likelihood of the patch being centered on a full object .	0	8	3	2.1530523	-1.6727839	0
221-9-36	at test time , the model is efficiently applied on the whole test image and generates a set of segmentation masks , each of them being assigned with a corresponding object likelihood score .	we show that our model yields significant improvements over state-of-theart object proposal algorithms .	1	4	5	-5.780961	5.0546904	1
221-9-36	in particular , compared to previous approaches , our model obtains substantially higher object recall using fewer proposals .	at test time , the model is efficiently applied on the whole test image and generates a set of segmentation masks , each of them being assigned with a corresponding object likelihood score .	0	6	4	4.4272385	-4.0584316	0
221-9-36	we also show that our model is able to generalize to unseen categories it has not seen during training .	at test time , the model is efficiently applied on the whole test image and generates a set of segmentation masks , each of them being assigned with a corresponding object likelihood score .	0	7	4	4.8585095	-4.3843336	0
221-9-36	at test time , the model is efficiently applied on the whole test image and generates a set of segmentation masks , each of them being assigned with a corresponding object likelihood score .	unlike all previous approaches for generating object masks , we do not rely on edges , superpixels , or any other form of low-level segmentation .	1	4	8	-0.9267774	1.086431	1
221-9-36	in particular , compared to previous approaches , our model obtains substantially higher object recall using fewer proposals .	we show that our model yields significant improvements over state-of-theart object proposal algorithms .	0	6	5	-1.6981277	1.7463607	1
221-9-36	we also show that our model is able to generalize to unseen categories it has not seen during training .	we show that our model yields significant improvements over state-of-theart object proposal algorithms .	0	7	5	2.5743053	-2.5205398	0
221-9-36	we show that our model yields significant improvements over state-of-theart object proposal algorithms .	unlike all previous approaches for generating object masks , we do not rely on edges , superpixels , or any other form of low-level segmentation .	1	5	8	4.7780066	-4.3579044	0
221-9-36	we also show that our model is able to generalize to unseen categories it has not seen during training .	in particular , compared to previous approaches , our model obtains substantially higher object recall using fewer proposals .	0	7	6	2.169947	-2.0740404	0
221-9-36	in particular , compared to previous approaches , our model obtains substantially higher object recall using fewer proposals .	unlike all previous approaches for generating object masks , we do not rely on edges , superpixels , or any other form of low-level segmentation .	1	6	8	4.4332113	-4.0458927	0
221-9-36	we also show that our model is able to generalize to unseen categories it has not seen during training .	unlike all previous approaches for generating object masks , we do not rely on edges , superpixels , or any other form of low-level segmentation .	1	7	8	4.992963	-4.4234853	0
222-7-21	multi-output gaussian processes provide a convenient framework for multi-task problems .	an illustrative and motivating example of a multi-task problem is multi-region electrophysiological time-series data , where experimentalists are interested in both power and phase coherence between channels .	1	0	1	-5.5898094	5.0809965	1
222-7-21	multi-output gaussian processes provide a convenient framework for multi-task problems .	recently , wilson and adams ( 2013 ) proposed the spectral mixture ( sm ) kernel to model the spectral density of a single task in a gaussian process framework .	1	0	2	-5.7145023	5.214248	1
222-7-21	multi-output gaussian processes provide a convenient framework for multi-task problems .	in this paper , we develop a novel covariance kernel for multiple outputs , called the cross-spectral mixture ( csm ) kernel .	1	0	3	-5.94417	5.1977196	1
222-7-21	multi-output gaussian processes provide a convenient framework for multi-task problems .	this new , flexible kernel represents both the power and phase relationship between multiple observation channels .	1	0	4	-5.933254	5.198291	1
222-7-21	multi-output gaussian processes provide a convenient framework for multi-task problems .	we demonstrate the expressive capabilities of the csm kernel through implementation of a bayesian hidden markov model , where the emission distribution is a multi-output gaussian process with a csm covariance kernel .	1	0	5	-5.9756417	5.128827	1
222-7-21	results are presented for measured multi-region electrophysiological data .	multi-output gaussian processes provide a convenient framework for multi-task problems .	0	6	0	5.7202625	-5.1366625	0
222-7-21	recently , wilson and adams ( 2013 ) proposed the spectral mixture ( sm ) kernel to model the spectral density of a single task in a gaussian process framework .	an illustrative and motivating example of a multi-task problem is multi-region electrophysiological time-series data , where experimentalists are interested in both power and phase coherence between channels .	0	2	1	1.1935657	-0.8495716	0
222-7-21	an illustrative and motivating example of a multi-task problem is multi-region electrophysiological time-series data , where experimentalists are interested in both power and phase coherence between channels .	in this paper , we develop a novel covariance kernel for multiple outputs , called the cross-spectral mixture ( csm ) kernel .	1	1	3	-5.2782707	4.9676456	1
222-7-21	an illustrative and motivating example of a multi-task problem is multi-region electrophysiological time-series data , where experimentalists are interested in both power and phase coherence between channels .	this new , flexible kernel represents both the power and phase relationship between multiple observation channels .	1	1	4	-5.0650167	4.748076	1
222-7-21	we demonstrate the expressive capabilities of the csm kernel through implementation of a bayesian hidden markov model , where the emission distribution is a multi-output gaussian process with a csm covariance kernel .	an illustrative and motivating example of a multi-task problem is multi-region electrophysiological time-series data , where experimentalists are interested in both power and phase coherence between channels .	0	5	1	5.459403	-4.8453493	0
222-7-21	results are presented for measured multi-region electrophysiological data .	an illustrative and motivating example of a multi-task problem is multi-region electrophysiological time-series data , where experimentalists are interested in both power and phase coherence between channels .	0	6	1	5.695745	-5.0314736	0
222-7-21	in this paper , we develop a novel covariance kernel for multiple outputs , called the cross-spectral mixture ( csm ) kernel .	recently , wilson and adams ( 2013 ) proposed the spectral mixture ( sm ) kernel to model the spectral density of a single task in a gaussian process framework .	0	3	2	4.9378805	-4.310541	0
222-7-21	recently , wilson and adams ( 2013 ) proposed the spectral mixture ( sm ) kernel to model the spectral density of a single task in a gaussian process framework .	this new , flexible kernel represents both the power and phase relationship between multiple observation channels .	1	2	4	-5.9756136	5.235275	1
222-7-21	recently , wilson and adams ( 2013 ) proposed the spectral mixture ( sm ) kernel to model the spectral density of a single task in a gaussian process framework .	we demonstrate the expressive capabilities of the csm kernel through implementation of a bayesian hidden markov model , where the emission distribution is a multi-output gaussian process with a csm covariance kernel .	1	2	5	-5.9952126	5.1437106	1
222-7-21	recently , wilson and adams ( 2013 ) proposed the spectral mixture ( sm ) kernel to model the spectral density of a single task in a gaussian process framework .	results are presented for measured multi-region electrophysiological data .	1	2	6	-5.9617124	5.1243014	1
222-7-21	this new , flexible kernel represents both the power and phase relationship between multiple observation channels .	in this paper , we develop a novel covariance kernel for multiple outputs , called the cross-spectral mixture ( csm ) kernel .	0	4	3	5.198991	-4.62324	0
222-7-21	in this paper , we develop a novel covariance kernel for multiple outputs , called the cross-spectral mixture ( csm ) kernel .	we demonstrate the expressive capabilities of the csm kernel through implementation of a bayesian hidden markov model , where the emission distribution is a multi-output gaussian process with a csm covariance kernel .	1	3	5	-5.9483147	5.155375	1
222-7-21	in this paper , we develop a novel covariance kernel for multiple outputs , called the cross-spectral mixture ( csm ) kernel .	results are presented for measured multi-region electrophysiological data .	1	3	6	-5.9842777	5.1675305	1
222-7-21	we demonstrate the expressive capabilities of the csm kernel through implementation of a bayesian hidden markov model , where the emission distribution is a multi-output gaussian process with a csm covariance kernel .	this new , flexible kernel represents both the power and phase relationship between multiple observation channels .	0	5	4	4.883857	-4.324774	0
222-7-21	this new , flexible kernel represents both the power and phase relationship between multiple observation channels .	results are presented for measured multi-region electrophysiological data .	1	4	6	-5.96077	5.122855	1
222-7-21	we demonstrate the expressive capabilities of the csm kernel through implementation of a bayesian hidden markov model , where the emission distribution is a multi-output gaussian process with a csm covariance kernel .	results are presented for measured multi-region electrophysiological data .	1	5	6	-4.810631	4.401325	1
223-6-15	each party wants to compute a function , which could differ from party to party , and there could be a central observer interested in computing a separate function .	we study the problem of interactive function computation by multiple parties , each possessing a bit , in a differential privacy setting ( i.e. , there remains an uncertainty in any party 's bit even when given the transcript of interactions and all the other parties ' bits ) .	0	1	0	5.3057265	-4.728709	0
223-6-15	performance at each party is measured via the accuracy of the function to be computed .	we study the problem of interactive function computation by multiple parties , each possessing a bit , in a differential privacy setting ( i.e. , there remains an uncertainty in any party 's bit even when given the transcript of interactions and all the other parties ' bits ) .	0	2	0	5.5977373	-5.0231485	0
223-6-15	we study the problem of interactive function computation by multiple parties , each possessing a bit , in a differential privacy setting ( i.e. , there remains an uncertainty in any party 's bit even when given the transcript of interactions and all the other parties ' bits ) .	we allow for an arbitrary cost metric to measure the distortion between the true and the computed function values .	1	0	3	-5.83086	5.1991568	1
223-6-15	we study the problem of interactive function computation by multiple parties , each possessing a bit , in a differential privacy setting ( i.e. , there remains an uncertainty in any party 's bit even when given the transcript of interactions and all the other parties ' bits ) .	our main result is the optimality of a simple non-interactive protocol : each party randomizes its bit ( sufficiently ) and shares the privatized version with the other parties .	1	0	4	-5.908005	5.1992135	1
223-6-15	[CLS] we study the problem of interactive function computation by multiple parties, each possessing a bit, in a differential privacy setting ( i. e., there remains an uncertainty in any party's bit even when given the transcript of interactions and all the other	[CLS] this optimality result is very general : it holds for all types of functions, heterogeneous privacy conditions on the parties, all types of cost metrics, and both average and worst - case ( over the inputs ) measures of accuracy.	1	0	5	-5.9285374	5.2403593	1
223-6-15	performance at each party is measured via the accuracy of the function to be computed .	each party wants to compute a function , which could differ from party to party , and there could be a central observer interested in computing a separate function .	0	2	1	5.569732	-4.883815	0
223-6-15	we allow for an arbitrary cost metric to measure the distortion between the true and the computed function values .	each party wants to compute a function , which could differ from party to party , and there could be a central observer interested in computing a separate function .	0	3	1	5.244576	-4.639004	0
223-6-15	each party wants to compute a function , which could differ from party to party , and there could be a central observer interested in computing a separate function .	our main result is the optimality of a simple non-interactive protocol : each party randomizes its bit ( sufficiently ) and shares the privatized version with the other parties .	1	1	4	-5.7970896	5.194715	1
223-6-15	this optimality result is very general : it holds for all types of functions , heterogeneous privacy conditions on the parties , all types of cost metrics , and both average and worst-case ( over the inputs ) measures of accuracy .	each party wants to compute a function , which could differ from party to party , and there could be a central observer interested in computing a separate function .	0	5	1	5.5514374	-4.91302	0
223-6-15	performance at each party is measured via the accuracy of the function to be computed .	we allow for an arbitrary cost metric to measure the distortion between the true and the computed function values .	1	2	3	3.9246242	-3.636998	0
223-6-15	performance at each party is measured via the accuracy of the function to be computed .	our main result is the optimality of a simple non-interactive protocol : each party randomizes its bit ( sufficiently ) and shares the privatized version with the other parties .	1	2	4	2.2684703	-2.0757647	0
223-6-15	this optimality result is very general : it holds for all types of functions , heterogeneous privacy conditions on the parties , all types of cost metrics , and both average and worst-case ( over the inputs ) measures of accuracy .	performance at each party is measured via the accuracy of the function to be computed .	0	5	2	3.444271	-3.2452006	0
223-6-15	our main result is the optimality of a simple non-interactive protocol : each party randomizes its bit ( sufficiently ) and shares the privatized version with the other parties .	we allow for an arbitrary cost metric to measure the distortion between the true and the computed function values .	0	4	3	3.6240578	-3.3837934	0
223-6-15	this optimality result is very general : it holds for all types of functions , heterogeneous privacy conditions on the parties , all types of cost metrics , and both average and worst-case ( over the inputs ) measures of accuracy .	we allow for an arbitrary cost metric to measure the distortion between the true and the computed function values .	0	5	3	5.2420235	-4.6817255	0
223-6-15	our main result is the optimality of a simple non-interactive protocol : each party randomizes its bit ( sufficiently ) and shares the privatized version with the other parties .	this optimality result is very general : it holds for all types of functions , heterogeneous privacy conditions on the parties , all types of cost metrics , and both average and worst-case ( over the inputs ) measures of accuracy .	1	4	5	-5.573221	5.060162	1
224-4-6	in this work we introduce a new learnable module , the spatial transformer , which explicitly allows the spatial manipulation of data within the network .	convolutional neural networks define an exceptionally powerful class of models , but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner .	0	1	0	5.557523	-4.860983	0
224-4-6	convolutional neural networks define an exceptionally powerful class of models , but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner .	this differentiable module can be inserted into existing convolutional architectures , giving neural networks the ability to actively spatially transform feature maps , conditional on the feature map itself , without any extra training supervision or modification to the optimisation process .	1	0	2	-5.826295	5.242201	1
224-4-6	we show that the use of spatial transformers results in models which learn invariance to translation , scale , rotation and more generic warping , resulting in state-of-the-art performance on several benchmarks , and for a number of classes of transformations .	convolutional neural networks define an exceptionally powerful class of models , but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner .	0	3	0	5.6265025	-4.972968	0
224-4-6	in this work we introduce a new learnable module , the spatial transformer , which explicitly allows the spatial manipulation of data within the network .	this differentiable module can be inserted into existing convolutional architectures , giving neural networks the ability to actively spatially transform feature maps , conditional on the feature map itself , without any extra training supervision or modification to the optimisation process .	1	1	2	-5.846033	5.1418524	1
224-4-6	we show that the use of spatial transformers results in models which learn invariance to translation , scale , rotation and more generic warping , resulting in state-of-the-art performance on several benchmarks , and for a number of classes of transformations .	in this work we introduce a new learnable module , the spatial transformer , which explicitly allows the spatial manipulation of data within the network .	0	3	1	5.358783	-4.734196	0
224-4-6	[CLS] this differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation	[CLS] we show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state - of - the - art performance on several benchmarks, and for a number of classes	1	2	3	-2.8949819	2.8437388	1
225-6-15	we introduce the laplace hazard matrix and show that its spectral radius fully characterizes the dynamics of the contagion both in terms of influence and of explosion time .	the paper studies transition phenomena in information cascades observed along a diffusion process over some graph .	0	1	0	5.2276773	-4.5994596	0
225-6-15	using this concept , we prove tight non-asymptotic bounds for the influence of a set of nodes , and we also provide an in-depth analysis of the critical time after which the contagion becomes super-critical .	the paper studies transition phenomena in information cascades observed along a diffusion process over some graph .	0	2	0	5.584924	-4.969528	0
225-6-15	our contributions include formal definitions and tight lower bounds of critical explosion time .	the paper studies transition phenomena in information cascades observed along a diffusion process over some graph .	0	3	0	5.512212	-4.9040136	0
225-6-15	we illustrate the relevance of our theoretical results through several examples of information cascades used in epidemiology and viral marketing models .	the paper studies transition phenomena in information cascades observed along a diffusion process over some graph .	0	4	0	5.4861975	-4.875332	0
225-6-15	finally , we provide a series of numerical experiments for various types of networks which confirm the tightness of the theoretical bounds .	the paper studies transition phenomena in information cascades observed along a diffusion process over some graph .	0	5	0	5.5442677	-4.9334784	0
225-6-15	we introduce the laplace hazard matrix and show that its spectral radius fully characterizes the dynamics of the contagion both in terms of influence and of explosion time .	using this concept , we prove tight non-asymptotic bounds for the influence of a set of nodes , and we also provide an in-depth analysis of the critical time after which the contagion becomes super-critical .	1	1	2	-4.877383	4.503305	1
225-6-15	we introduce the laplace hazard matrix and show that its spectral radius fully characterizes the dynamics of the contagion both in terms of influence and of explosion time .	our contributions include formal definitions and tight lower bounds of critical explosion time .	1	1	3	-3.2695901	3.1373987	1
225-6-15	we illustrate the relevance of our theoretical results through several examples of information cascades used in epidemiology and viral marketing models .	we introduce the laplace hazard matrix and show that its spectral radius fully characterizes the dynamics of the contagion both in terms of influence and of explosion time .	0	4	1	5.3988824	-4.7525864	0
225-6-15	finally , we provide a series of numerical experiments for various types of networks which confirm the tightness of the theoretical bounds .	we introduce the laplace hazard matrix and show that its spectral radius fully characterizes the dynamics of the contagion both in terms of influence and of explosion time .	0	5	1	5.317807	-4.7266498	0
225-6-15	using this concept , we prove tight non-asymptotic bounds for the influence of a set of nodes , and we also provide an in-depth analysis of the critical time after which the contagion becomes super-critical .	our contributions include formal definitions and tight lower bounds of critical explosion time .	1	2	3	3.8959827	-3.5361354	0
225-6-15	using this concept , we prove tight non-asymptotic bounds for the influence of a set of nodes , and we also provide an in-depth analysis of the critical time after which the contagion becomes super-critical .	we illustrate the relevance of our theoretical results through several examples of information cascades used in epidemiology and viral marketing models .	1	2	4	-5.814389	5.0879116	1
225-6-15	using this concept , we prove tight non-asymptotic bounds for the influence of a set of nodes , and we also provide an in-depth analysis of the critical time after which the contagion becomes super-critical .	finally , we provide a series of numerical experiments for various types of networks which confirm the tightness of the theoretical bounds .	1	2	5	-5.7280135	5.0302014	1
225-6-15	we illustrate the relevance of our theoretical results through several examples of information cascades used in epidemiology and viral marketing models .	our contributions include formal definitions and tight lower bounds of critical explosion time .	0	4	3	5.0334673	-4.4344206	0
225-6-15	our contributions include formal definitions and tight lower bounds of critical explosion time .	finally , we provide a series of numerical experiments for various types of networks which confirm the tightness of the theoretical bounds .	1	3	5	-5.9279613	5.028628	1
225-6-15	finally , we provide a series of numerical experiments for various types of networks which confirm the tightness of the theoretical bounds .	we illustrate the relevance of our theoretical results through several examples of information cascades used in epidemiology and viral marketing models .	0	5	4	-3.540769	3.3422332	1
226-4-6	the theoretical analysis motivates us to introduce a new multi-class classification machine based on p -norm regularization , where the parameter p controls the complexity of the corresponding bounds .	[CLS] this paper studies the generalization performance of multi - class classification algorithms, for which we obtain - - for the first time - - a data - dependent generalization error bound with a logarithmic dependence on the class size, substantially improving the state - of - the - art linear dependence in the existing	0	1	0	3.9109836	-3.5279036	0
226-4-6	we derive an efficient optimization algorithm based on fenchel duality theory .	this paper studies the generalization performance of multi-class classification algorithms , for which we obtain -- for the first time -- a data-dependent generalization error bound with a logarithmic dependence on the class size , substantially improving the state-of-the-art linear dependence in the existing data-dependent generalization analysis .	0	2	0	5.4886456	-4.864684	0
226-4-6	benchmarks on several real-world datasets show that the proposed algorithm can achieve significant accuracy gains over the state of the art .	this paper studies the generalization performance of multi-class classification algorithms , for which we obtain -- for the first time -- a data-dependent generalization error bound with a logarithmic dependence on the class size , substantially improving the state-of-the-art linear dependence in the existing data-dependent generalization analysis .	0	3	0	5.549634	-4.9522886	0
226-4-6	the theoretical analysis motivates us to introduce a new multi-class classification machine based on p -norm regularization , where the parameter p controls the complexity of the corresponding bounds .	we derive an efficient optimization algorithm based on fenchel duality theory .	1	1	2	-2.843924	2.8419394	1
226-4-6	benchmarks on several real-world datasets show that the proposed algorithm can achieve significant accuracy gains over the state of the art .	the theoretical analysis motivates us to introduce a new multi-class classification machine based on p -norm regularization , where the parameter p controls the complexity of the corresponding bounds .	0	3	1	4.5365853	-4.067097	0
226-4-6	we derive an efficient optimization algorithm based on fenchel duality theory .	benchmarks on several real-world datasets show that the proposed algorithm can achieve significant accuracy gains over the state of the art .	1	2	3	-5.8618283	5.048281	1
227-7-21	latent factor models have been widely used to analyze simultaneous recordings of spike trains from large , heterogeneous neural populations .	these models assume the signal of interest in the population is a low-dimensional latent intensity that evolves over time , which is observed in high dimension via noisy point-process observations .	1	0	1	-5.8781176	5.1895723	1
227-7-21	latent factor models have been widely used to analyze simultaneous recordings of spike trains from large , heterogeneous neural populations .	these techniques have been well used to capture neural correlations across a population and to provide a smooth , denoised , and concise representation of high-dimensional spiking data .	1	0	2	-5.9416676	5.19034	1
227-7-21	latent factor models have been widely used to analyze simultaneous recordings of spike trains from large , heterogeneous neural populations .	one limitation of many current models is that the observation model is assumed to be poisson , which lacks the flexibility to capture under- and over-dispersion that is common in recorded neural data , thereby introducing bias into estimates of covariance .	1	0	3	-5.8390226	5.2482934	1
227-7-21	here we develop the generalized count linear dynamical system , which relaxes the poisson assumption by using a more general exponential family for count data .	latent factor models have been widely used to analyze simultaneous recordings of spike trains from large , heterogeneous neural populations .	0	4	0	5.6611238	-4.9993777	0
227-7-21	in addition to containing poisson , bernoulli , negative binomial , and other common count distributions as special cases , we show that this model can be tractably learned by extending recent advances in variational inference techniques .	latent factor models have been widely used to analyze simultaneous recordings of spike trains from large , heterogeneous neural populations .	0	5	0	5.6599407	-5.0174017	0
227-7-21	we apply our model to data from primate motor cortex and demonstrate performance improvements over state-of-the-art methods , both in capturing the variance structure of the data and in held-out prediction .	latent factor models have been widely used to analyze simultaneous recordings of spike trains from large , heterogeneous neural populations .	0	6	0	5.690156	-5.0972376	0
227-7-21	these techniques have been well used to capture neural correlations across a population and to provide a smooth , denoised , and concise representation of high-dimensional spiking data .	these models assume the signal of interest in the population is a low-dimensional latent intensity that evolves over time , which is observed in high dimension via noisy point-process observations .	0	2	1	-3.0373244	3.0322952	1
227-7-21	one limitation of many current models is that the observation model is assumed to be poisson , which lacks the flexibility to capture under- and over-dispersion that is common in recorded neural data , thereby introducing bias into estimates of covariance .	these models assume the signal of interest in the population is a low-dimensional latent intensity that evolves over time , which is observed in high dimension via noisy point-process observations .	0	3	1	-1.9367507	2.1331477	1
227-7-21	here we develop the generalized count linear dynamical system , which relaxes the poisson assumption by using a more general exponential family for count data .	these models assume the signal of interest in the population is a low-dimensional latent intensity that evolves over time , which is observed in high dimension via noisy point-process observations .	0	4	1	4.4227343	-3.9237595	0
227-7-21	in addition to containing poisson , bernoulli , negative binomial , and other common count distributions as special cases , we show that this model can be tractably learned by extending recent advances in variational inference techniques .	these models assume the signal of interest in the population is a low-dimensional latent intensity that evolves over time , which is observed in high dimension via noisy point-process observations .	0	5	1	5.0851765	-4.428028	0
227-7-21	these models assume the signal of interest in the population is a low-dimensional latent intensity that evolves over time , which is observed in high dimension via noisy point-process observations .	we apply our model to data from primate motor cortex and demonstrate performance improvements over state-of-the-art methods , both in capturing the variance structure of the data and in held-out prediction .	1	1	6	-5.8797045	4.9764647	1
227-7-21	one limitation of many current models is that the observation model is assumed to be poisson , which lacks the flexibility to capture under- and over-dispersion that is common in recorded neural data , thereby introducing bias into estimates of covariance .	these techniques have been well used to capture neural correlations across a population and to provide a smooth , denoised , and concise representation of high-dimensional spiking data .	0	3	2	-1.9848499	2.2244883	1
227-7-21	these techniques have been well used to capture neural correlations across a population and to provide a smooth , denoised , and concise representation of high-dimensional spiking data .	here we develop the generalized count linear dynamical system , which relaxes the poisson assumption by using a more general exponential family for count data .	1	2	4	-5.8667145	5.255235	1
227-7-21	these techniques have been well used to capture neural correlations across a population and to provide a smooth , denoised , and concise representation of high-dimensional spiking data .	in addition to containing poisson , bernoulli , negative binomial , and other common count distributions as special cases , we show that this model can be tractably learned by extending recent advances in variational inference techniques .	1	2	5	-5.9569426	5.2702236	1
227-7-21	we apply our model to data from primate motor cortex and demonstrate performance improvements over state-of-the-art methods , both in capturing the variance structure of the data and in held-out prediction .	these techniques have been well used to capture neural correlations across a population and to provide a smooth , denoised , and concise representation of high-dimensional spiking data .	0	6	2	5.5782924	-4.985343	0
227-7-21	here we develop the generalized count linear dynamical system , which relaxes the poisson assumption by using a more general exponential family for count data .	one limitation of many current models is that the observation model is assumed to be poisson , which lacks the flexibility to capture under- and over-dispersion that is common in recorded neural data , thereby introducing bias into estimates of covariance .	0	4	3	5.258703	-4.612007	0
227-7-21	in addition to containing poisson , bernoulli , negative binomial , and other common count distributions as special cases , we show that this model can be tractably learned by extending recent advances in variational inference techniques .	one limitation of many current models is that the observation model is assumed to be poisson , which lacks the flexibility to capture under- and over-dispersion that is common in recorded neural data , thereby introducing bias into estimates of covariance .	0	5	3	5.486111	-4.831889	0
227-7-21	we apply our model to data from primate motor cortex and demonstrate performance improvements over state-of-the-art methods , both in capturing the variance structure of the data and in held-out prediction .	one limitation of many current models is that the observation model is assumed to be poisson , which lacks the flexibility to capture under- and over-dispersion that is common in recorded neural data , thereby introducing bias into estimates of covariance .	0	6	3	5.6294866	-5.0323677	0
227-7-21	here we develop the generalized count linear dynamical system , which relaxes the poisson assumption by using a more general exponential family for count data .	in addition to containing poisson , bernoulli , negative binomial , and other common count distributions as special cases , we show that this model can be tractably learned by extending recent advances in variational inference techniques .	1	4	5	-5.9360623	5.1960163	1
227-7-21	we apply our model to data from primate motor cortex and demonstrate performance improvements over state-of-the-art methods , both in capturing the variance structure of the data and in held-out prediction .	here we develop the generalized count linear dynamical system , which relaxes the poisson assumption by using a more general exponential family for count data .	0	6	4	5.6581707	-5.0145903	0
227-7-21	in addition to containing poisson , bernoulli , negative binomial , and other common count distributions as special cases , we show that this model can be tractably learned by extending recent advances in variational inference techniques .	we apply our model to data from primate motor cortex and demonstrate performance improvements over state-of-the-art methods , both in capturing the variance structure of the data and in held-out prediction .	1	5	6	-5.152991	4.6961775	1
228-8-28	in this paper we develop a loss function for multi-label learning , based on the wasserstein distance .	learning to predict multi-label outputs is challenging , but in many problems there is a natural metric on the outputs that can be used to improve predictions .	0	1	0	5.2394915	-4.6254406	0
228-8-28	learning to predict multi-label outputs is challenging , but in many problems there is a natural metric on the outputs that can be used to improve predictions .	the wasserstein distance provides a natural notion of dissimilarity for probability measures .	1	0	2	-5.6867647	5.1972404	1
228-8-28	although optimizing with respect to the exact wasserstein distance is costly , recent work has described a regularized approximation that is efficiently computed .	learning to predict multi-label outputs is challenging , but in many problems there is a natural metric on the outputs that can be used to improve predictions .	0	3	0	5.330664	-4.6182747	0
228-8-28	learning to predict multi-label outputs is challenging , but in many problems there is a natural metric on the outputs that can be used to improve predictions .	we describe an efficient learning algorithm based on this regularization , as well as a novel extension of the wasserstein distance from probability measures to unnormalized measures .	1	0	4	-5.9521217	5.1509705	1
228-8-28	learning to predict multi-label outputs is challenging , but in many problems there is a natural metric on the outputs that can be used to improve predictions .	we also describe a statistical learning bound for the loss .	1	0	5	-5.9937563	5.1713037	1
228-8-28	the wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space .	learning to predict multi-label outputs is challenging , but in many problems there is a natural metric on the outputs that can be used to improve predictions .	0	6	0	5.667121	-5.1298656	0
228-8-28	learning to predict multi-label outputs is challenging , but in many problems there is a natural metric on the outputs that can be used to improve predictions .	we demonstrate this property on a real-data tag prediction problem , using the yahoo flickr creative commons dataset , outperforming a baseline that does n't use the metric .	1	0	7	-5.9233932	5.1523104	1
228-8-28	in this paper we develop a loss function for multi-label learning , based on the wasserstein distance .	the wasserstein distance provides a natural notion of dissimilarity for probability measures .	1	1	2	-3.8654194	3.6543124	1
228-8-28	in this paper we develop a loss function for multi-label learning , based on the wasserstein distance .	although optimizing with respect to the exact wasserstein distance is costly , recent work has described a regularized approximation that is efficiently computed .	1	1	3	2.990343	-2.7333496	0
228-8-28	in this paper we develop a loss function for multi-label learning , based on the wasserstein distance .	we describe an efficient learning algorithm based on this regularization , as well as a novel extension of the wasserstein distance from probability measures to unnormalized measures .	1	1	4	-5.8638577	5.2764063	1
228-8-28	in this paper we develop a loss function for multi-label learning , based on the wasserstein distance .	we also describe a statistical learning bound for the loss .	1	1	5	-5.9946656	5.188674	1
228-8-28	in this paper we develop a loss function for multi-label learning , based on the wasserstein distance .	the wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space .	1	1	6	-4.5441465	4.208918	1
228-8-28	in this paper we develop a loss function for multi-label learning , based on the wasserstein distance .	we demonstrate this property on a real-data tag prediction problem , using the yahoo flickr creative commons dataset , outperforming a baseline that does n't use the metric .	1	1	7	-5.9693613	5.2005706	1
228-8-28	the wasserstein distance provides a natural notion of dissimilarity for probability measures .	although optimizing with respect to the exact wasserstein distance is costly , recent work has described a regularized approximation that is efficiently computed .	1	2	3	-0.9486576	1.1292416	1
228-8-28	we describe an efficient learning algorithm based on this regularization , as well as a novel extension of the wasserstein distance from probability measures to unnormalized measures .	the wasserstein distance provides a natural notion of dissimilarity for probability measures .	0	4	2	1.721253	-1.5062841	0
228-8-28	we also describe a statistical learning bound for the loss .	the wasserstein distance provides a natural notion of dissimilarity for probability measures .	0	5	2	4.9172873	-4.3819666	0
228-8-28	the wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space .	the wasserstein distance provides a natural notion of dissimilarity for probability measures .	0	6	2	2.7227478	-2.4261465	0
228-8-28	we demonstrate this property on a real-data tag prediction problem , using the yahoo flickr creative commons dataset , outperforming a baseline that does n't use the metric .	the wasserstein distance provides a natural notion of dissimilarity for probability measures .	0	7	2	5.3702936	-4.8102045	0
228-8-28	although optimizing with respect to the exact wasserstein distance is costly , recent work has described a regularized approximation that is efficiently computed .	we describe an efficient learning algorithm based on this regularization , as well as a novel extension of the wasserstein distance from probability measures to unnormalized measures .	1	3	4	-5.9811296	5.205325	1
228-8-28	we also describe a statistical learning bound for the loss .	although optimizing with respect to the exact wasserstein distance is costly , recent work has described a regularized approximation that is efficiently computed .	0	5	3	5.308078	-4.6634874	0
228-8-28	although optimizing with respect to the exact wasserstein distance is costly , recent work has described a regularized approximation that is efficiently computed .	the wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space .	1	3	6	2.1153297	-1.8636551	0
228-8-28	although optimizing with respect to the exact wasserstein distance is costly , recent work has described a regularized approximation that is efficiently computed .	we demonstrate this property on a real-data tag prediction problem , using the yahoo flickr creative commons dataset , outperforming a baseline that does n't use the metric .	1	3	7	-5.984341	5.190162	1
228-8-28	we also describe a statistical learning bound for the loss .	we describe an efficient learning algorithm based on this regularization , as well as a novel extension of the wasserstein distance from probability measures to unnormalized measures .	0	5	4	3.140247	-2.9743497	0
228-8-28	the wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space .	we describe an efficient learning algorithm based on this regularization , as well as a novel extension of the wasserstein distance from probability measures to unnormalized measures .	0	6	4	-5.590678	5.063955	1
228-8-28	we describe an efficient learning algorithm based on this regularization , as well as a novel extension of the wasserstein distance from probability measures to unnormalized measures .	we demonstrate this property on a real-data tag prediction problem , using the yahoo flickr creative commons dataset , outperforming a baseline that does n't use the metric .	1	4	7	-5.229621	4.752983	1
228-8-28	we also describe a statistical learning bound for the loss .	the wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space .	1	5	6	5.0853786	-4.5253935	0
228-8-28	we also describe a statistical learning bound for the loss .	we demonstrate this property on a real-data tag prediction problem , using the yahoo flickr creative commons dataset , outperforming a baseline that does n't use the metric .	1	5	7	-3.9301405	3.7273839	1
228-8-28	we demonstrate this property on a real-data tag prediction problem , using the yahoo flickr creative commons dataset , outperforming a baseline that does n't use the metric .	the wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space .	0	7	6	5.441354	-4.867787	0
229-5-10	we consider the problem of sparse signal recovery from m linear measurements quantized to b bits .	b-bit marginal regression is proposed as recovery algorithm .	1	0	1	-5.838008	5.10213	1
229-5-10	we study the question of choosing b in the setting of a given budget of bits b = m * b and derive a single easy-to-compute expression characterizing the trade-off between m and b .	we consider the problem of sparse signal recovery from m linear measurements quantized to b bits .	0	2	0	5.518027	-4.9343853	0
229-5-10	the choice b = 1 turns out to be optimal for estimating the unit vector corresponding to the signal for any level of additive gaussian noise before quantization as well as for adversarial noise .	we consider the problem of sparse signal recovery from m linear measurements quantized to b bits .	0	3	0	5.5531883	-5.012605	0
229-5-10	we consider the problem of sparse signal recovery from m linear measurements quantized to b bits .	for b 2 , we show that lloyd-max quantization constitutes an optimal quantization scheme and that the norm of the signal can be estimated consistently by maximum likelihood by extending .	1	0	4	-5.8376575	5.1466136	1
229-5-10	b-bit marginal regression is proposed as recovery algorithm .	we study the question of choosing b in the setting of a given budget of bits b = m * b and derive a single easy-to-compute expression characterizing the trade-off between m and b .	1	1	2	5.161765	-4.626726	0
229-5-10	the choice b = 1 turns out to be optimal for estimating the unit vector corresponding to the signal for any level of additive gaussian noise before quantization as well as for adversarial noise .	b-bit marginal regression is proposed as recovery algorithm .	0	3	1	1.2215732	-0.9990705	0
229-5-10	for b 2 , we show that lloyd-max quantization constitutes an optimal quantization scheme and that the norm of the signal can be estimated consistently by maximum likelihood by extending .	b-bit marginal regression is proposed as recovery algorithm .	0	4	1	-1.8279476	2.0202303	1
229-5-10	we study the question of choosing b in the setting of a given budget of bits b = m * b and derive a single easy-to-compute expression characterizing the trade-off between m and b .	the choice b = 1 turns out to be optimal for estimating the unit vector corresponding to the signal for any level of additive gaussian noise before quantization as well as for adversarial noise .	1	2	3	-5.9690523	5.184296	1
229-5-10	we study the question of choosing b in the setting of a given budget of bits b = m * b and derive a single easy-to-compute expression characterizing the trade-off between m and b .	for b 2 , we show that lloyd-max quantization constitutes an optimal quantization scheme and that the norm of the signal can be estimated consistently by maximum likelihood by extending .	1	2	4	-5.9538007	5.2126207	1
229-5-10	for b 2 , we show that lloyd-max quantization constitutes an optimal quantization scheme and that the norm of the signal can be estimated consistently by maximum likelihood by extending .	the choice b = 1 turns out to be optimal for estimating the unit vector corresponding to the signal for any level of additive gaussian noise before quantization as well as for adversarial noise .	0	4	3	-3.2493653	3.1168876	1
230-4-6	we introduce natural neural networks , a novel family of algorithms that speed up convergence by adapting their internal representation during training to improve conditioning of the fisher matrix .	in particular , we show a specific example that employs a simple and efficient reparametrization of the neural network weights by implicitly whitening the representation obtained at each layer , while preserving the feed-forward computation of the network .	1	0	1	-5.978606	5.225106	1
230-4-6	such networks can be trained efficiently via the proposed projected natural gradient descent algorithm ( prong ) , which amortizes the cost of these reparametrizations over many parameter updates and is closely related to the mirror descent online learning algorithm .	we introduce natural neural networks , a novel family of algorithms that speed up convergence by adapting their internal representation during training to improve conditioning of the fisher matrix .	0	2	0	5.5725503	-4.88796	0
230-4-6	we highlight the benefits of our method on both unsupervised and supervised learning tasks , and showcase its scalability by training on the large-scale imagenet challenge dataset .	we introduce natural neural networks , a novel family of algorithms that speed up convergence by adapting their internal representation during training to improve conditioning of the fisher matrix .	0	3	0	5.4832735	-4.829544	0
230-4-6	such networks can be trained efficiently via the proposed projected natural gradient descent algorithm ( prong ) , which amortizes the cost of these reparametrizations over many parameter updates and is closely related to the mirror descent online learning algorithm .	in particular , we show a specific example that employs a simple and efficient reparametrization of the neural network weights by implicitly whitening the representation obtained at each layer , while preserving the feed-forward computation of the network .	0	2	1	2.5890996	-2.429546	0
230-4-6	we highlight the benefits of our method on both unsupervised and supervised learning tasks , and showcase its scalability by training on the large-scale imagenet challenge dataset .	in particular , we show a specific example that employs a simple and efficient reparametrization of the neural network weights by implicitly whitening the representation obtained at each layer , while preserving the feed-forward computation of the network .	0	3	1	4.702857	-4.191206	0
230-4-6	we highlight the benefits of our method on both unsupervised and supervised learning tasks , and showcase its scalability by training on the large-scale imagenet challenge dataset .	such networks can be trained efficiently via the proposed projected natural gradient descent algorithm ( prong ) , which amortizes the cost of these reparametrizations over many parameter updates and is closely related to the mirror descent online learning algorithm .	0	3	2	5.192292	-4.5642996	0
231-6-15	the algorithm starts with the view that the stochasticity of the pseudo-samples generated by the simulator can be controlled externally by a vector of random numbers u , in such a way that the outcome , knowing u , is deterministic .	we describe an embarrassingly parallel , anytime monte carlo method for likelihood-free models .	0	1	0	3.9395514	-3.5601897	0
231-6-15	we describe an embarrassingly parallel , anytime monte carlo method for likelihood-free models .	for each instantiation of u we run an optimization procedure to minimize the distance between summary statistics of the simulator and the data .	1	0	2	-5.5223303	5.0450463	1
231-6-15	after reweighing these samples using the prior and the jacobian ( accounting for the change of volume in transforming from the space of summary statistics to the space of parameters ) we show that this weighted ensemble represents a monte carlo estimate of the posterior distribution .	we describe an embarrassingly parallel , anytime monte carlo method for likelihood-free models .	0	3	0	4.392748	-3.9556627	0
231-6-15	the procedure can be run embarrassingly parallel ( each node handling one sample ) and anytime ( by allocating resources to the worst performing sample ) .	we describe an embarrassingly parallel , anytime monte carlo method for likelihood-free models .	0	4	0	5.2254257	-4.670912	0
231-6-15	the procedure is validated on six experiments .	we describe an embarrassingly parallel , anytime monte carlo method for likelihood-free models .	0	5	0	5.5341783	-4.936899	0
231-6-15	for each instantiation of u we run an optimization procedure to minimize the distance between summary statistics of the simulator and the data .	the algorithm starts with the view that the stochasticity of the pseudo-samples generated by the simulator can be controlled externally by a vector of random numbers u , in such a way that the outcome , knowing u , is deterministic .	0	2	1	-2.1115808	2.1712127	1
231-6-15	[CLS] the algorithm starts with the view that the stochasticity of the pseudo - samples generated by the simulator can be controlled externally by a vector of random numbers u, in such a way that the outcome, knowing u, is deterministic.	[CLS] after reweighing these samples using the prior and the jacobian ( accounting for the change of volume in transforming from the space of summary statistics to the space of parameters ) we show that this weighted ensemble represents a monte carlo estimate of the posterior distribution	1	1	3	-4.365523	4.097641	1
231-6-15	the algorithm starts with the view that the stochasticity of the pseudo-samples generated by the simulator can be controlled externally by a vector of random numbers u , in such a way that the outcome , knowing u , is deterministic .	the procedure can be run embarrassingly parallel ( each node handling one sample ) and anytime ( by allocating resources to the worst performing sample ) .	1	1	4	-5.622408	5.0759645	1
231-6-15	the procedure is validated on six experiments .	the algorithm starts with the view that the stochasticity of the pseudo-samples generated by the simulator can be controlled externally by a vector of random numbers u , in such a way that the outcome , knowing u , is deterministic .	0	5	1	5.361849	-4.768708	0
231-6-15	for each instantiation of u we run an optimization procedure to minimize the distance between summary statistics of the simulator and the data .	after reweighing these samples using the prior and the jacobian ( accounting for the change of volume in transforming from the space of summary statistics to the space of parameters ) we show that this weighted ensemble represents a monte carlo estimate of the posterior distribution .	1	2	3	-3.1346478	3.1219943	1
231-6-15	the procedure can be run embarrassingly parallel ( each node handling one sample ) and anytime ( by allocating resources to the worst performing sample ) .	for each instantiation of u we run an optimization procedure to minimize the distance between summary statistics of the simulator and the data .	0	4	2	4.8983407	-4.3895884	0
231-6-15	for each instantiation of u we run an optimization procedure to minimize the distance between summary statistics of the simulator and the data .	the procedure is validated on six experiments .	1	2	5	-5.9993415	5.169533	1
231-6-15	after reweighing these samples using the prior and the jacobian ( accounting for the change of volume in transforming from the space of summary statistics to the space of parameters ) we show that this weighted ensemble represents a monte carlo estimate of the posterior distribution .	the procedure can be run embarrassingly parallel ( each node handling one sample ) and anytime ( by allocating resources to the worst performing sample ) .	1	3	4	-2.324292	2.4062657	1
231-6-15	after reweighing these samples using the prior and the jacobian ( accounting for the change of volume in transforming from the space of summary statistics to the space of parameters ) we show that this weighted ensemble represents a monte carlo estimate of the posterior distribution .	the procedure is validated on six experiments .	1	3	5	-5.7644377	5.064362	1
231-6-15	the procedure can be run embarrassingly parallel ( each node handling one sample ) and anytime ( by allocating resources to the worst performing sample ) .	the procedure is validated on six experiments .	1	4	5	-5.0069027	4.6576967	1
232-7-21	the primal-dual hybrid gradient ( pdhg ) method is a powerful alternative that often has simpler sub-steps than admm , thus producing lower complexity solvers .	the alternating direction method of multipliers ( admm ) is an important tool for solving complex optimization problems , but it involves minimization sub-steps that are often difficult to solve efficiently .	0	1	0	5.5731587	-4.954529	0
232-7-21	the alternating direction method of multipliers ( admm ) is an important tool for solving complex optimization problems , but it involves minimization sub-steps that are often difficult to solve efficiently .	despite the flexibility of this method , pdhg is often impractical because it requires the careful choice of multiple stepsize parameters .	1	0	2	-5.9291754	5.221114	1
232-7-21	there is often no intuitive way to choose these parameters to maximize efficiency , or even achieve convergence .	the alternating direction method of multipliers ( admm ) is an important tool for solving complex optimization problems , but it involves minimization sub-steps that are often difficult to solve efficiently .	0	3	0	5.396483	-4.7825212	0
232-7-21	we propose self-adaptive stepsize rules that automatically tune pdhg parameters for optimal convergence .	the alternating direction method of multipliers ( admm ) is an important tool for solving complex optimization problems , but it involves minimization sub-steps that are often difficult to solve efficiently .	0	4	0	5.6489625	-5.008431	0
232-7-21	the alternating direction method of multipliers ( admm ) is an important tool for solving complex optimization problems , but it involves minimization sub-steps that are often difficult to solve efficiently .	we rigorously analyze our methods , and identify convergence rates .	1	0	5	-5.966742	5.134163	1
232-7-21	numerical experiments show that adaptive pdhg has strong advantages over non-adaptive methods in terms of both efficiency and simplicity for the user .	the alternating direction method of multipliers ( admm ) is an important tool for solving complex optimization problems , but it involves minimization sub-steps that are often difficult to solve efficiently .	0	6	0	5.6721373	-5.08178	0
232-7-21	despite the flexibility of this method , pdhg is often impractical because it requires the careful choice of multiple stepsize parameters .	the primal-dual hybrid gradient ( pdhg ) method is a powerful alternative that often has simpler sub-steps than admm , thus producing lower complexity solvers .	0	2	1	5.379615	-4.7709126	0
232-7-21	there is often no intuitive way to choose these parameters to maximize efficiency , or even achieve convergence .	the primal-dual hybrid gradient ( pdhg ) method is a powerful alternative that often has simpler sub-steps than admm , thus producing lower complexity solvers .	0	3	1	-4.937229	4.6895313	1
232-7-21	we propose self-adaptive stepsize rules that automatically tune pdhg parameters for optimal convergence .	the primal-dual hybrid gradient ( pdhg ) method is a powerful alternative that often has simpler sub-steps than admm , thus producing lower complexity solvers .	0	4	1	5.435995	-4.760064	0
232-7-21	the primal-dual hybrid gradient ( pdhg ) method is a powerful alternative that often has simpler sub-steps than admm , thus producing lower complexity solvers .	we rigorously analyze our methods , and identify convergence rates .	1	1	5	-5.3246956	4.895879	1
232-7-21	numerical experiments show that adaptive pdhg has strong advantages over non-adaptive methods in terms of both efficiency and simplicity for the user .	the primal-dual hybrid gradient ( pdhg ) method is a powerful alternative that often has simpler sub-steps than admm , thus producing lower complexity solvers .	0	6	1	5.649381	-5.05811	0
232-7-21	there is often no intuitive way to choose these parameters to maximize efficiency , or even achieve convergence .	despite the flexibility of this method , pdhg is often impractical because it requires the careful choice of multiple stepsize parameters .	0	3	2	2.5762086	-2.3866751	0
232-7-21	despite the flexibility of this method , pdhg is often impractical because it requires the careful choice of multiple stepsize parameters .	we propose self-adaptive stepsize rules that automatically tune pdhg parameters for optimal convergence .	1	2	4	-5.955632	5.1735106	1
232-7-21	we rigorously analyze our methods , and identify convergence rates .	despite the flexibility of this method , pdhg is often impractical because it requires the careful choice of multiple stepsize parameters .	0	5	2	5.432846	-4.8204775	0
232-7-21	despite the flexibility of this method , pdhg is often impractical because it requires the careful choice of multiple stepsize parameters .	numerical experiments show that adaptive pdhg has strong advantages over non-adaptive methods in terms of both efficiency and simplicity for the user .	1	2	6	-5.942337	5.174382	1
232-7-21	there is often no intuitive way to choose these parameters to maximize efficiency , or even achieve convergence .	we propose self-adaptive stepsize rules that automatically tune pdhg parameters for optimal convergence .	1	3	4	-5.9099646	5.2300987	1
232-7-21	we rigorously analyze our methods , and identify convergence rates .	there is often no intuitive way to choose these parameters to maximize efficiency , or even achieve convergence .	0	5	3	5.575985	-4.8995676	0
232-7-21	numerical experiments show that adaptive pdhg has strong advantages over non-adaptive methods in terms of both efficiency and simplicity for the user .	there is often no intuitive way to choose these parameters to maximize efficiency , or even achieve convergence .	0	6	3	5.5965967	-4.991279	0
232-7-21	we propose self-adaptive stepsize rules that automatically tune pdhg parameters for optimal convergence .	we rigorously analyze our methods , and identify convergence rates .	1	4	5	-5.666408	5.068097	1
232-7-21	we propose self-adaptive stepsize rules that automatically tune pdhg parameters for optimal convergence .	numerical experiments show that adaptive pdhg has strong advantages over non-adaptive methods in terms of both efficiency and simplicity for the user .	1	4	6	-5.894723	5.1051993	1
232-7-21	we rigorously analyze our methods , and identify convergence rates .	numerical experiments show that adaptive pdhg has strong advantages over non-adaptive methods in terms of both efficiency and simplicity for the user .	1	5	6	-3.2053845	3.1298208	1
233-4-6	we provide the first analysis of instances where variational inference algorithms converge to the global optimum , in the setting of topic models .	variational inference is an efficient , popular heuristic used in the context of latent variable models .	0	1	0	5.5528135	-4.953993	0
233-4-6	our initializations are natural , one of them being used in lda-c , the most popular implementation of variational inference .	variational inference is an efficient , popular heuristic used in the context of latent variable models .	0	2	0	5.4186225	-4.812642	0
233-4-6	variational inference is an efficient , popular heuristic used in the context of latent variable models .	in addition to providing intuition into why this heuristic might work in practice , the multiplicative , rather than additive nature of the variational inference updates forces us to use non-standard proof arguments , which we believe might be of general theoretical interest .	1	0	3	-5.957843	5.123433	1
233-4-6	we provide the first analysis of instances where variational inference algorithms converge to the global optimum , in the setting of topic models .	our initializations are natural , one of them being used in lda-c , the most popular implementation of variational inference .	1	1	2	-2.7757516	2.7906983	1
233-4-6	in addition to providing intuition into why this heuristic might work in practice , the multiplicative , rather than additive nature of the variational inference updates forces us to use non-standard proof arguments , which we believe might be of general theoretical interest .	we provide the first analysis of instances where variational inference algorithms converge to the global optimum , in the setting of topic models .	0	3	1	3.042266	-2.8069043	0
233-4-6	in addition to providing intuition into why this heuristic might work in practice , the multiplicative , rather than additive nature of the variational inference updates forces us to use non-standard proof arguments , which we believe might be of general theoretical interest .	our initializations are natural , one of them being used in lda-c , the most popular implementation of variational inference .	0	3	2	2.6445441	-2.516491	0
234-6-15	often , additional information about the variables is known , and it is reasonable to assume that incorporating this information will lead to better predictions .	low rank matrix completion plays a fundamental role in collaborative filtering applications , the key idea being that the variables lie in a smaller subspace than the ambient space .	0	1	0	5.545179	-4.989561	0
234-6-15	we tackle the problem of matrix completion when pairwise relationships among variables are known , via a graph .	low rank matrix completion plays a fundamental role in collaborative filtering applications , the key idea being that the variables lie in a smaller subspace than the ambient space .	0	2	0	-1.8831091	2.326962	1
234-6-15	low rank matrix completion plays a fundamental role in collaborative filtering applications , the key idea being that the variables lie in a smaller subspace than the ambient space .	we formulate and derive a highly efficient , conjugate gradient based alternating minimization scheme that solves optimizations with over 55 million observations up to 2 orders of magnitude faster than state-of-the-art ( stochastic ) gradient-descent based methods .	1	0	3	-5.966447	5.192622	1
234-6-15	low rank matrix completion plays a fundamental role in collaborative filtering applications , the key idea being that the variables lie in a smaller subspace than the ambient space .	on the theoretical front , we show that such methods generalize weighted nuclear norm formulations , and derive statistical consistency guarantees .	1	0	4	-6.0061903	5.1482215	1
234-6-15	we validate our results on both real and synthetic datasets .	low rank matrix completion plays a fundamental role in collaborative filtering applications , the key idea being that the variables lie in a smaller subspace than the ambient space .	0	5	0	5.63284	-5.0487747	0
234-6-15	often , additional information about the variables is known , and it is reasonable to assume that incorporating this information will lead to better predictions .	we tackle the problem of matrix completion when pairwise relationships among variables are known , via a graph .	1	1	2	5.5630283	-5.055672	0
234-6-15	we formulate and derive a highly efficient , conjugate gradient based alternating minimization scheme that solves optimizations with over 55 million observations up to 2 orders of magnitude faster than state-of-the-art ( stochastic ) gradient-descent based methods .	often , additional information about the variables is known , and it is reasonable to assume that incorporating this information will lead to better predictions .	0	3	1	5.548002	-4.8936625	0
234-6-15	on the theoretical front , we show that such methods generalize weighted nuclear norm formulations , and derive statistical consistency guarantees .	often , additional information about the variables is known , and it is reasonable to assume that incorporating this information will lead to better predictions .	0	4	1	5.1865506	-4.567933	0
234-6-15	we validate our results on both real and synthetic datasets .	often , additional information about the variables is known , and it is reasonable to assume that incorporating this information will lead to better predictions .	0	5	1	5.5861564	-4.9864407	0
234-6-15	we formulate and derive a highly efficient , conjugate gradient based alternating minimization scheme that solves optimizations with over 55 million observations up to 2 orders of magnitude faster than state-of-the-art ( stochastic ) gradient-descent based methods .	we tackle the problem of matrix completion when pairwise relationships among variables are known , via a graph .	0	3	2	5.676524	-5.058908	0
234-6-15	we tackle the problem of matrix completion when pairwise relationships among variables are known , via a graph .	on the theoretical front , we show that such methods generalize weighted nuclear norm formulations , and derive statistical consistency guarantees .	1	2	4	-5.9757814	5.1510925	1
234-6-15	we validate our results on both real and synthetic datasets .	we tackle the problem of matrix completion when pairwise relationships among variables are known , via a graph .	0	5	2	5.630852	-5.0750127	0
234-6-15	on the theoretical front , we show that such methods generalize weighted nuclear norm formulations , and derive statistical consistency guarantees .	we formulate and derive a highly efficient , conjugate gradient based alternating minimization scheme that solves optimizations with over 55 million observations up to 2 orders of magnitude faster than state-of-the-art ( stochastic ) gradient-descent based methods .	0	4	3	-2.127201	2.2730029	1
234-6-15	we formulate and derive a highly efficient , conjugate gradient based alternating minimization scheme that solves optimizations with over 55 million observations up to 2 orders of magnitude faster than state-of-the-art ( stochastic ) gradient-descent based methods .	we validate our results on both real and synthetic datasets .	1	3	5	-5.7839756	5.1829743	1
234-6-15	we validate our results on both real and synthetic datasets .	on the theoretical front , we show that such methods generalize weighted nuclear norm formulations , and derive statistical consistency guarantees .	0	5	4	5.266754	-4.6397343	0
235-5-10	in the stochastic setting under semi-bandit feedback , we derive a problem-specific regret lower bound , and discuss its scaling with the dimension of the decision space .	this paper investigates stochastic and adversarial combinatorial multi-armed bandit problems .	0	1	0	5.645335	-5.0594215	0
235-5-10	this paper investigates stochastic and adversarial combinatorial multi-armed bandit problems .	we propose escb , an algorithm that efficiently exploits the structure of the problem and provide a finite-time analysis of its regret .	1	0	2	-5.869681	5.1588407	1
235-5-10	this paper investigates stochastic and adversarial combinatorial multi-armed bandit problems .	escb has better performance guarantees than existing algorithms , and significantly outperforms these algorithms in practice .	1	0	3	-5.9376917	5.1848435	1
235-5-10	in the adversarial setting under bandit feedback , we propose c omb exp , an algorithm with the same regret scaling as state-of-the-art algorithms , but with lower computational complexity for some combinatorial problems .	this paper investigates stochastic and adversarial combinatorial multi-armed bandit problems .	0	4	0	5.6336465	-5.0062456	0
235-5-10	in the stochastic setting under semi-bandit feedback , we derive a problem-specific regret lower bound , and discuss its scaling with the dimension of the decision space .	we propose escb , an algorithm that efficiently exploits the structure of the problem and provide a finite-time analysis of its regret .	1	1	2	-4.364295	4.02079	1
235-5-10	escb has better performance guarantees than existing algorithms , and significantly outperforms these algorithms in practice .	in the stochastic setting under semi-bandit feedback , we derive a problem-specific regret lower bound , and discuss its scaling with the dimension of the decision space .	0	3	1	4.1137686	-3.7668424	0
235-5-10	in the stochastic setting under semi-bandit feedback , we derive a problem-specific regret lower bound , and discuss its scaling with the dimension of the decision space .	in the adversarial setting under bandit feedback , we propose c omb exp , an algorithm with the same regret scaling as state-of-the-art algorithms , but with lower computational complexity for some combinatorial problems .	1	1	4	-3.7314105	3.5921698	1
235-5-10	we propose escb , an algorithm that efficiently exploits the structure of the problem and provide a finite-time analysis of its regret .	escb has better performance guarantees than existing algorithms , and significantly outperforms these algorithms in practice .	1	2	3	-5.835141	5.140443	1
235-5-10	we propose escb , an algorithm that efficiently exploits the structure of the problem and provide a finite-time analysis of its regret .	in the adversarial setting under bandit feedback , we propose c omb exp , an algorithm with the same regret scaling as state-of-the-art algorithms , but with lower computational complexity for some combinatorial problems .	1	2	4	-1.8376963	1.9657772	1
235-5-10	escb has better performance guarantees than existing algorithms , and significantly outperforms these algorithms in practice .	in the adversarial setting under bandit feedback , we propose c omb exp , an algorithm with the same regret scaling as state-of-the-art algorithms , but with lower computational complexity for some combinatorial problems .	1	3	4	4.038187	-3.6677012	0
236-5-10	the mutual information is a core statistical quantity that has applications in all areas of machine learning , whether this is in training of density models over multiple data modalities , in maximising the efficiency of noisy transmission channels , or when learning behaviour policies for exploration by artificial agents .	most learning algorithms that involve optimisation of the mutual information rely on the blahut-arimoto algorithm -- an enumerative algorithm with exponential complexity that is not suitable for modern machine learning applications .	1	0	1	-3.775388	3.515915	1
236-5-10	the mutual information is a core statistical quantity that has applications in all areas of machine learning , whether this is in training of density models over multiple data modalities , in maximising the efficiency of noisy transmission channels , or when learning behaviour policies for exploration by artificial agents .	this paper provides a new approach for scalable optimisation of the mutual information by merging techniques from variational inference and deep learning .	1	0	2	-5.1313734	4.703215	1
236-5-10	the mutual information is a core statistical quantity that has applications in all areas of machine learning , whether this is in training of density models over multiple data modalities , in maximising the efficiency of noisy transmission channels , or when learning behaviour policies for exploration by artificial agents .	we develop our approach by focusing on the problem of intrinsically-motivated learning , where the mutual information forms the definition of a well-known internal drive known as empowerment .	1	0	3	3.1822317	-2.9391317	0
236-5-10	[CLS] the mutual information is a core statistical quantity that has applications in all areas of machine learning, whether this is in training of density models over multiple data modalities, in maximising the efficiency of noisy transmission channels, or when learning behaviour policies for	[CLS] using a variational lower bound on the mutual information, combined with convolutional networks for handling visual input streams, we develop a stochastic optimisation algorithm that allows for scalable information maximisation and empowerment - based reasoning directly	1	0	4	-4.395998	4.0018873	1
236-5-10	most learning algorithms that involve optimisation of the mutual information rely on the blahut-arimoto algorithm -- an enumerative algorithm with exponential complexity that is not suitable for modern machine learning applications .	this paper provides a new approach for scalable optimisation of the mutual information by merging techniques from variational inference and deep learning .	1	1	2	-5.14911	4.7866325	1
236-5-10	most learning algorithms that involve optimisation of the mutual information rely on the blahut-arimoto algorithm -- an enumerative algorithm with exponential complexity that is not suitable for modern machine learning applications .	we develop our approach by focusing on the problem of intrinsically-motivated learning , where the mutual information forms the definition of a well-known internal drive known as empowerment .	1	1	3	-5.7335835	5.261199	1
236-5-10	most learning algorithms that involve optimisation of the mutual information rely on the blahut-arimoto algorithm -- an enumerative algorithm with exponential complexity that is not suitable for modern machine learning applications .	using a variational lower bound on the mutual information , combined with convolutional networks for handling visual input streams , we develop a stochastic optimisation algorithm that allows for scalable information maximisation and empowerment-based reasoning directly from pixels to actions .	1	1	4	-5.799244	5.302717	1
236-5-10	this paper provides a new approach for scalable optimisation of the mutual information by merging techniques from variational inference and deep learning .	we develop our approach by focusing on the problem of intrinsically-motivated learning , where the mutual information forms the definition of a well-known internal drive known as empowerment .	1	2	3	-5.8026667	5.2450533	1
236-5-10	this paper provides a new approach for scalable optimisation of the mutual information by merging techniques from variational inference and deep learning .	using a variational lower bound on the mutual information , combined with convolutional networks for handling visual input streams , we develop a stochastic optimisation algorithm that allows for scalable information maximisation and empowerment-based reasoning directly from pixels to actions .	1	2	4	-5.739354	5.202161	1
236-5-10	we develop our approach by focusing on the problem of intrinsically-motivated learning , where the mutual information forms the definition of a well-known internal drive known as empowerment .	using a variational lower bound on the mutual information , combined with convolutional networks for handling visual input streams , we develop a stochastic optimisation algorithm that allows for scalable information maximisation and empowerment-based reasoning directly from pixels to actions .	1	3	4	-5.266217	4.8484855	1
237-7-21	our framework is inspired by state-of-the-art smoothing techniques used in natural language processing ( nlp ) .	in this paper , we propose a general smoothing framework for graph kernels by taking structural similarity into account , and apply it to derive smoothed variants of popular graph kernels .	0	1	0	5.3837004	-4.761471	0
237-7-21	however , unlike nlp applications that primarily deal with strings , we show how one can apply smoothing to a richer class of inter-dependent sub-structures that naturally arise in graphs .	in this paper , we propose a general smoothing framework for graph kernels by taking structural similarity into account , and apply it to derive smoothed variants of popular graph kernels .	0	2	0	3.6880462	-3.3500404	0
237-7-21	moreover , we discuss extensions of the pitman-yor process that can be adapted to smooth structured objects , thereby leading to novel graph kernels .	in this paper , we propose a general smoothing framework for graph kernels by taking structural similarity into account , and apply it to derive smoothed variants of popular graph kernels .	0	3	0	5.0171833	-4.4113417	0
237-7-21	our kernels are able to tackle the diagonal dominance problem while respecting the structural similarity between features .	in this paper , we propose a general smoothing framework for graph kernels by taking structural similarity into account , and apply it to derive smoothed variants of popular graph kernels .	0	4	0	5.1632924	-4.597887	0
237-7-21	in this paper , we propose a general smoothing framework for graph kernels by taking structural similarity into account , and apply it to derive smoothed variants of popular graph kernels .	experimental evaluation shows that not only our kernels achieve statistically significant improvements over the unsmoothed variants , but also outperform several other graph kernels in the literature .	1	0	5	-5.96864	5.1144657	1
237-7-21	in this paper , we propose a general smoothing framework for graph kernels by taking structural similarity into account , and apply it to derive smoothed variants of popular graph kernels .	our kernels are competitive in terms of runtime , and offer a viable option for practitioners .	1	0	6	-5.7004356	5.047307	1
237-7-21	however , unlike nlp applications that primarily deal with strings , we show how one can apply smoothing to a richer class of inter-dependent sub-structures that naturally arise in graphs .	our framework is inspired by state-of-the-art smoothing techniques used in natural language processing ( nlp ) .	0	2	1	5.0186887	-4.431964	0
237-7-21	our framework is inspired by state-of-the-art smoothing techniques used in natural language processing ( nlp ) .	moreover , we discuss extensions of the pitman-yor process that can be adapted to smooth structured objects , thereby leading to novel graph kernels .	1	1	3	-5.243459	4.8372955	1
237-7-21	our kernels are able to tackle the diagonal dominance problem while respecting the structural similarity between features .	our framework is inspired by state-of-the-art smoothing techniques used in natural language processing ( nlp ) .	0	4	1	1.5826967	-1.3773217	0
237-7-21	experimental evaluation shows that not only our kernels achieve statistically significant improvements over the unsmoothed variants , but also outperform several other graph kernels in the literature .	our framework is inspired by state-of-the-art smoothing techniques used in natural language processing ( nlp ) .	0	5	1	5.128958	-4.6034427	0
237-7-21	our kernels are competitive in terms of runtime , and offer a viable option for practitioners .	our framework is inspired by state-of-the-art smoothing techniques used in natural language processing ( nlp ) .	0	6	1	1.3376619	-1.134967	0
237-7-21	however , unlike nlp applications that primarily deal with strings , we show how one can apply smoothing to a richer class of inter-dependent sub-structures that naturally arise in graphs .	moreover , we discuss extensions of the pitman-yor process that can be adapted to smooth structured objects , thereby leading to novel graph kernels .	1	2	3	-4.975129	4.579555	1
237-7-21	our kernels are able to tackle the diagonal dominance problem while respecting the structural similarity between features .	however , unlike nlp applications that primarily deal with strings , we show how one can apply smoothing to a richer class of inter-dependent sub-structures that naturally arise in graphs .	0	4	2	-1.9309345	2.014629	1
237-7-21	however , unlike nlp applications that primarily deal with strings , we show how one can apply smoothing to a richer class of inter-dependent sub-structures that naturally arise in graphs .	experimental evaluation shows that not only our kernels achieve statistically significant improvements over the unsmoothed variants , but also outperform several other graph kernels in the literature .	1	2	5	-5.8006744	5.141055	1
237-7-21	our kernels are competitive in terms of runtime , and offer a viable option for practitioners .	however , unlike nlp applications that primarily deal with strings , we show how one can apply smoothing to a richer class of inter-dependent sub-structures that naturally arise in graphs .	0	6	2	0.6484955	-0.4628183	0
237-7-21	our kernels are able to tackle the diagonal dominance problem while respecting the structural similarity between features .	moreover , we discuss extensions of the pitman-yor process that can be adapted to smooth structured objects , thereby leading to novel graph kernels .	0	4	3	-1.1004685	1.3091432	1
237-7-21	moreover , we discuss extensions of the pitman-yor process that can be adapted to smooth structured objects , thereby leading to novel graph kernels .	experimental evaluation shows that not only our kernels achieve statistically significant improvements over the unsmoothed variants , but also outperform several other graph kernels in the literature .	1	3	5	-5.1712456	4.652565	1
237-7-21	our kernels are competitive in terms of runtime , and offer a viable option for practitioners .	moreover , we discuss extensions of the pitman-yor process that can be adapted to smooth structured objects , thereby leading to novel graph kernels .	0	6	3	2.4554772	-2.2767344	0
237-7-21	our kernels are able to tackle the diagonal dominance problem while respecting the structural similarity between features .	experimental evaluation shows that not only our kernels achieve statistically significant improvements over the unsmoothed variants , but also outperform several other graph kernels in the literature .	1	4	5	-5.6880913	5.074999	1
237-7-21	our kernels are competitive in terms of runtime , and offer a viable option for practitioners .	our kernels are able to tackle the diagonal dominance problem while respecting the structural similarity between features .	0	6	4	3.2175155	-3.030139	0
237-7-21	experimental evaluation shows that not only our kernels achieve statistically significant improvements over the unsmoothed variants , but also outperform several other graph kernels in the literature .	our kernels are competitive in terms of runtime , and offer a viable option for practitioners .	1	5	6	3.2954402	-3.1522095	0
238-9-36	estimating distributions over large alphabets is a fundamental machine-learning tenet .	yet no method is known to estimate all distributions well .	1	0	1	-5.9002533	5.179541	1
238-9-36	estimating distributions over large alphabets is a fundamental machine-learning tenet .	for example , add-constant estimators are nearly min-max optimal but often perform poorly in practice , and practical estimators such as absolute discounting , jelinek-mercer , and good-turing are not known to be near optimal for essentially any distribution .	1	0	2	-5.8673306	5.1800528	1
238-9-36	estimating distributions over large alphabets is a fundamental machine-learning tenet .	we describe the first universally near-optimal probability estimators .	1	0	3	-5.7120733	5.1705823	1
238-9-36	for every discrete distribution , they are provably nearly the best in the following two competitive ways .	estimating distributions over large alphabets is a fundamental machine-learning tenet .	0	4	0	5.525075	-4.8580894	0
238-9-36	estimating distributions over large alphabets is a fundamental machine-learning tenet .	first they estimate every distribution nearly as well as the best estimator designed with prior knowledge of the distribution up to a permutation .	1	0	5	-5.85081	5.2155867	1
238-9-36	estimating distributions over large alphabets is a fundamental machine-learning tenet .	second , they estimate every distribution nearly as well as the best estimator designed with prior knowledge of the exact distribution , but as all natural estimators , restricted to assign the same probability to all symbols appearing the same number of times .	1	0	6	-5.8203354	5.2116303	1
238-9-36	specifically , for distributions over k symbols and n samples , we show that for both comparisons , a simple variant of good-turing estimator is always within kl divergence of ( 3 + on ( 1 ) ) /n1/3 from the best estimator , and that a more involved estimator is within on ( min ( k/n , 1/ n ) ) .	estimating distributions over large alphabets is a fundamental machine-learning tenet .	0	7	0	5.6582804	-5.012433	0
238-9-36	conversely , we show that any esti n ( min ( k/n , 1/n2/3 ) ) over the best mator must have a kl divergence at least n ( min ( k/n , 1/n ) ) for the secestimator for the first comparison , and at least ond .	estimating distributions over large alphabets is a fundamental machine-learning tenet .	0	8	0	5.645733	-5.0189075	0
238-9-36	for example , add-constant estimators are nearly min-max optimal but often perform poorly in practice , and practical estimators such as absolute discounting , jelinek-mercer , and good-turing are not known to be near optimal for essentially any distribution .	yet no method is known to estimate all distributions well .	0	2	1	-1.0428839	1.3232392	1
238-9-36	we describe the first universally near-optimal probability estimators .	yet no method is known to estimate all distributions well .	0	3	1	0.15649194	0.2705773	1
238-9-36	for every discrete distribution , they are provably nearly the best in the following two competitive ways .	yet no method is known to estimate all distributions well .	0	4	1	3.364193	-3.0375535	0
238-9-36	yet no method is known to estimate all distributions well .	first they estimate every distribution nearly as well as the best estimator designed with prior knowledge of the distribution up to a permutation .	1	1	5	-5.8240986	5.2232666	1
238-9-36	yet no method is known to estimate all distributions well .	second , they estimate every distribution nearly as well as the best estimator designed with prior knowledge of the exact distribution , but as all natural estimators , restricted to assign the same probability to all symbols appearing the same number of times .	1	1	6	-5.786045	5.2147684	1
238-9-36	specifically , for distributions over k symbols and n samples , we show that for both comparisons , a simple variant of good-turing estimator is always within kl divergence of ( 3 + on ( 1 ) ) /n1/3 from the best estimator , and that a more involved estimator is within on ( min ( k/n , 1/ n ) ) .	yet no method is known to estimate all distributions well .	0	7	1	4.89742	-4.311479	0
238-9-36	yet no method is known to estimate all distributions well .	conversely , we show that any esti n ( min ( k/n , 1/n2/3 ) ) over the best mator must have a kl divergence at least n ( min ( k/n , 1/n ) ) for the secestimator for the first comparison , and at least ond .	1	1	8	-5.760305	5.215707	1
238-9-36	we describe the first universally near-optimal probability estimators .	for example , add-constant estimators are nearly min-max optimal but often perform poorly in practice , and practical estimators such as absolute discounting , jelinek-mercer , and good-turing are not known to be near optimal for essentially any distribution .	0	3	2	0.8121429	-0.49351406	0
238-9-36	for example , add-constant estimators are nearly min-max optimal but often perform poorly in practice , and practical estimators such as absolute discounting , jelinek-mercer , and good-turing are not known to be near optimal for essentially any distribution .	for every discrete distribution , they are provably nearly the best in the following two competitive ways .	1	2	4	-3.3191178	3.2971187	1
238-9-36	for example , add-constant estimators are nearly min-max optimal but often perform poorly in practice , and practical estimators such as absolute discounting , jelinek-mercer , and good-turing are not known to be near optimal for essentially any distribution .	first they estimate every distribution nearly as well as the best estimator designed with prior knowledge of the distribution up to a permutation .	1	2	5	-5.035581	4.7860975	1
238-9-36	[CLS] for example, add - constant estimators are nearly min - max optimal but often perform poorly in practice, and practical estimators such as absolute discounting, jelinek - mercer, and good - turing are not known to be near	[CLS] second, they estimate every distribution nearly as well as the best estimator designed with prior knowledge of the exact distribution, but as all natural estimators, restricted to assign the same probability to all symbols appearing the same number of times. [SEP]	1	2	6	-4.198747	4.0021605	1
238-9-36	[CLS] for example, add - constant estimators are nearly min - max optimal but often perform poorly in practice, and practical estimators such as absolute discounting, jelinek - mercer, and good - turing are not known to be near	[CLS] specifically, for distributions over k symbols and n samples, we show that for both comparisons, a simple variant of good - turing estimator is always within kl divergence of ( 3 + on ( 1 ) ) / n1 / 3	1	2	7	-3.8302019	3.650424	1
238-9-36	[CLS] for example, add - constant estimators are nearly min - max optimal but often perform poorly in practice, and practical estimators such as absolute discounting, jelinek - mercer, and good - turing are not known to be near	[CLS] conversely, we show that any esti n ( min ( k / n, 1 / n2 / 3 ) ) over the best mator must have a kl divergence at least n ( min ( k / n, 1 / n	1	2	8	-1.9086716	2.0063639	1
238-9-36	we describe the first universally near-optimal probability estimators .	for every discrete distribution , they are provably nearly the best in the following two competitive ways .	1	3	4	-4.81507	4.493266	1
238-9-36	we describe the first universally near-optimal probability estimators .	first they estimate every distribution nearly as well as the best estimator designed with prior knowledge of the distribution up to a permutation .	1	3	5	-5.6801844	5.175835	1
238-9-36	second , they estimate every distribution nearly as well as the best estimator designed with prior knowledge of the exact distribution , but as all natural estimators , restricted to assign the same probability to all symbols appearing the same number of times .	we describe the first universally near-optimal probability estimators .	0	6	3	5.334147	-4.7669764	0
238-9-36	we describe the first universally near-optimal probability estimators .	specifically , for distributions over k symbols and n samples , we show that for both comparisons , a simple variant of good-turing estimator is always within kl divergence of ( 3 + on ( 1 ) ) /n1/3 from the best estimator , and that a more involved estimator is within on ( min ( k/n , 1/ n ) ) .	1	3	7	-5.8927565	5.2477303	1
238-9-36	we describe the first universally near-optimal probability estimators .	conversely , we show that any esti n ( min ( k/n , 1/n2/3 ) ) over the best mator must have a kl divergence at least n ( min ( k/n , 1/n ) ) for the secestimator for the first comparison , and at least ond .	1	3	8	-5.892645	5.226856	1
238-9-36	first they estimate every distribution nearly as well as the best estimator designed with prior knowledge of the distribution up to a permutation .	for every discrete distribution , they are provably nearly the best in the following two competitive ways .	0	5	4	2.456761	-2.2011626	0
238-9-36	for every discrete distribution , they are provably nearly the best in the following two competitive ways .	second , they estimate every distribution nearly as well as the best estimator designed with prior knowledge of the exact distribution , but as all natural estimators , restricted to assign the same probability to all symbols appearing the same number of times .	1	4	6	-5.610156	5.118567	1
238-9-36	[CLS] specifically, for distributions over k symbols and n samples, we show that for both comparisons, a simple variant of good - turing estimator is always within kl divergence of ( 3 + on ( 1 ) ) / n1 / 3 from the best estimator, and that a more involved estimator is within on ( min ( k / n, 1 / n	for every discrete distribution , they are provably nearly the best in the following two competitive ways .	0	7	4	0.80897593	-0.48163348	0
238-9-36	conversely , we show that any esti n ( min ( k/n , 1/n2/3 ) ) over the best mator must have a kl divergence at least n ( min ( k/n , 1/n ) ) for the secestimator for the first comparison , and at least ond .	for every discrete distribution , they are provably nearly the best in the following two competitive ways .	0	8	4	3.2106242	-2.9455814	0
238-9-36	second , they estimate every distribution nearly as well as the best estimator designed with prior knowledge of the exact distribution , but as all natural estimators , restricted to assign the same probability to all symbols appearing the same number of times .	first they estimate every distribution nearly as well as the best estimator designed with prior knowledge of the distribution up to a permutation .	0	6	5	5.0252857	-4.4082375	0
238-9-36	first they estimate every distribution nearly as well as the best estimator designed with prior knowledge of the distribution up to a permutation .	[CLS] specifically, for distributions over k symbols and n samples, we show that for both comparisons, a simple variant of good - turing estimator is always within kl divergence of ( 3 + on ( 1 ) ) / n1 / 3 from the best estimator, and that a more involved estimator is within on ( min	1	5	7	-4.1867805	3.9243326	1
238-9-36	conversely , we show that any esti n ( min ( k/n , 1/n2/3 ) ) over the best mator must have a kl divergence at least n ( min ( k/n , 1/n ) ) for the secestimator for the first comparison , and at least ond .	first they estimate every distribution nearly as well as the best estimator designed with prior knowledge of the distribution up to a permutation .	0	8	5	4.072405	-3.728647	0
238-9-36	[CLS] second, they estimate every distribution nearly as well as the best estimator designed with prior knowledge of the exact distribution, but as all natural estimators, restricted to assign the same probability to all symbols appearing the same number of times. [SEP]	[CLS] specifically, for distributions over k symbols and n samples, we show that for both comparisons, a simple variant of good - turing estimator is always within kl divergence of ( 3 + on ( 1 ) ) / n1 / 3	1	6	7	2.1473048	-1.8461838	0
238-9-36	[CLS] second, they estimate every distribution nearly as well as the best estimator designed with prior knowledge of the exact distribution, but as all natural estimators, restricted to assign the same probability to all symbols appearing the same number of times. [SEP]	[CLS] conversely, we show that any esti n ( min ( k / n, 1 / n2 / 3 ) ) over the best mator must have a kl divergence at least n ( min ( k / n, 1 / n	1	6	8	-1.5736537	1.8044534	1
238-9-36	[CLS] conversely, we show that any esti n ( min ( k / n, 1 / n2 / 3 ) ) over the best mator must have a kl divergence at least n ( min ( k / n, 1 / n	[CLS] specifically, for distributions over k symbols and n samples, we show that for both comparisons, a simple variant of good - turing estimator is always within kl divergence of ( 3 + on ( 1 ) ) / n1 / 3	0	8	7	2.1592202	-1.9523506	0
239-8-28	our goal is to learn decision rules that adaptively select sensors for each example as necessary to make a confident prediction .	we study the problem of reducing test-time acquisition costs in classification systems .	0	1	0	5.4435806	-4.8714576	0
239-8-28	we study the problem of reducing test-time acquisition costs in classification systems .	we model our system as a directed acyclic graph ( dag ) where internal nodes correspond to sensor subsets and decision functions at each node choose whether to acquire a new sensor or classify using the available measurements .	1	0	2	-5.817152	5.1948285	1
239-8-28	we study the problem of reducing test-time acquisition costs in classification systems .	this problem can be posed as an empirical risk minimization over training data .	1	0	3	-5.917391	5.188134	1
239-8-28	rather than jointly optimizing such a highly coupled and non-convex problem over all decision nodes , we propose an efficient algorithm motivated by dynamic programming .	we study the problem of reducing test-time acquisition costs in classification systems .	0	4	0	5.531315	-4.9665775	0
239-8-28	we learn node policies in the dag by reducing the global objective to a series of cost sensitive learning problems .	we study the problem of reducing test-time acquisition costs in classification systems .	0	5	0	5.5928745	-5.0027933	0
239-8-28	we study the problem of reducing test-time acquisition costs in classification systems .	our approach is computationally efficient and has proven guarantees of convergence to the optimal system for a fixed architecture .	1	0	6	-5.831397	5.191593	1
239-8-28	we study the problem of reducing test-time acquisition costs in classification systems .	in addition , we present an extension to map other budgeted learning problems with large number of sensors to our dag architecture and demonstrate empirical performance exceeding state-of-the-art algorithms for data composed of both few and many sensors .	1	0	7	-5.9020624	5.1991897	1
239-8-28	our goal is to learn decision rules that adaptively select sensors for each example as necessary to make a confident prediction .	we model our system as a directed acyclic graph ( dag ) where internal nodes correspond to sensor subsets and decision functions at each node choose whether to acquire a new sensor or classify using the available measurements .	1	1	2	1.4368781	-0.9400893	0
239-8-28	this problem can be posed as an empirical risk minimization over training data .	our goal is to learn decision rules that adaptively select sensors for each example as necessary to make a confident prediction .	0	3	1	2.736195	-2.5844958	0
239-8-28	our goal is to learn decision rules that adaptively select sensors for each example as necessary to make a confident prediction .	rather than jointly optimizing such a highly coupled and non-convex problem over all decision nodes , we propose an efficient algorithm motivated by dynamic programming .	1	1	4	-4.501095	4.2520313	1
239-8-28	we learn node policies in the dag by reducing the global objective to a series of cost sensitive learning problems .	our goal is to learn decision rules that adaptively select sensors for each example as necessary to make a confident prediction .	0	5	1	3.8415327	-3.54589	0
239-8-28	our approach is computationally efficient and has proven guarantees of convergence to the optimal system for a fixed architecture .	our goal is to learn decision rules that adaptively select sensors for each example as necessary to make a confident prediction .	0	6	1	3.3898282	-3.1760664	0
239-8-28	in addition , we present an extension to map other budgeted learning problems with large number of sensors to our dag architecture and demonstrate empirical performance exceeding state-of-the-art algorithms for data composed of both few and many sensors .	our goal is to learn decision rules that adaptively select sensors for each example as necessary to make a confident prediction .	0	7	1	5.2800674	-4.6663	0
239-8-28	this problem can be posed as an empirical risk minimization over training data .	we model our system as a directed acyclic graph ( dag ) where internal nodes correspond to sensor subsets and decision functions at each node choose whether to acquire a new sensor or classify using the available measurements .	0	3	2	0.505753	-0.26011893	0
239-8-28	rather than jointly optimizing such a highly coupled and non-convex problem over all decision nodes , we propose an efficient algorithm motivated by dynamic programming .	we model our system as a directed acyclic graph ( dag ) where internal nodes correspond to sensor subsets and decision functions at each node choose whether to acquire a new sensor or classify using the available measurements .	0	4	2	3.6965194	-3.3597403	0
239-8-28	we learn node policies in the dag by reducing the global objective to a series of cost sensitive learning problems .	we model our system as a directed acyclic graph ( dag ) where internal nodes correspond to sensor subsets and decision functions at each node choose whether to acquire a new sensor or classify using the available measurements .	0	5	2	5.515118	-4.772997	0
239-8-28	we model our system as a directed acyclic graph ( dag ) where internal nodes correspond to sensor subsets and decision functions at each node choose whether to acquire a new sensor or classify using the available measurements .	our approach is computationally efficient and has proven guarantees of convergence to the optimal system for a fixed architecture .	1	2	6	-5.1881533	4.728083	1
239-8-28	in addition , we present an extension to map other budgeted learning problems with large number of sensors to our dag architecture and demonstrate empirical performance exceeding state-of-the-art algorithms for data composed of both few and many sensors .	we model our system as a directed acyclic graph ( dag ) where internal nodes correspond to sensor subsets and decision functions at each node choose whether to acquire a new sensor or classify using the available measurements .	0	7	2	5.5021544	-4.8285832	0
239-8-28	rather than jointly optimizing such a highly coupled and non-convex problem over all decision nodes , we propose an efficient algorithm motivated by dynamic programming .	this problem can be posed as an empirical risk minimization over training data .	0	4	3	2.9279752	-2.7171211	0
239-8-28	this problem can be posed as an empirical risk minimization over training data .	we learn node policies in the dag by reducing the global objective to a series of cost sensitive learning problems .	1	3	5	2.4547968	-2.2928605	0
239-8-28	this problem can be posed as an empirical risk minimization over training data .	our approach is computationally efficient and has proven guarantees of convergence to the optimal system for a fixed architecture .	1	3	6	-4.086871	3.8556318	1
239-8-28	in addition , we present an extension to map other budgeted learning problems with large number of sensors to our dag architecture and demonstrate empirical performance exceeding state-of-the-art algorithms for data composed of both few and many sensors .	this problem can be posed as an empirical risk minimization over training data .	0	7	3	4.9364285	-4.407977	0
239-8-28	rather than jointly optimizing such a highly coupled and non-convex problem over all decision nodes , we propose an efficient algorithm motivated by dynamic programming .	we learn node policies in the dag by reducing the global objective to a series of cost sensitive learning problems .	1	4	5	-1.9071512	2.1050217	1
239-8-28	our approach is computationally efficient and has proven guarantees of convergence to the optimal system for a fixed architecture .	rather than jointly optimizing such a highly coupled and non-convex problem over all decision nodes , we propose an efficient algorithm motivated by dynamic programming .	0	6	4	3.375827	-3.1268604	0
239-8-28	in addition , we present an extension to map other budgeted learning problems with large number of sensors to our dag architecture and demonstrate empirical performance exceeding state-of-the-art algorithms for data composed of both few and many sensors .	rather than jointly optimizing such a highly coupled and non-convex problem over all decision nodes , we propose an efficient algorithm motivated by dynamic programming .	0	7	4	4.9653177	-4.3980517	0
239-8-28	our approach is computationally efficient and has proven guarantees of convergence to the optimal system for a fixed architecture .	we learn node policies in the dag by reducing the global objective to a series of cost sensitive learning problems .	0	6	5	1.3083755	-1.0820978	0
239-8-28	in addition , we present an extension to map other budgeted learning problems with large number of sensors to our dag architecture and demonstrate empirical performance exceeding state-of-the-art algorithms for data composed of both few and many sensors .	we learn node policies in the dag by reducing the global objective to a series of cost sensitive learning problems .	0	7	5	5.0275574	-4.514085	0
239-8-28	in addition , we present an extension to map other budgeted learning problems with large number of sensors to our dag architecture and demonstrate empirical performance exceeding state-of-the-art algorithms for data composed of both few and many sensors .	our approach is computationally efficient and has proven guarantees of convergence to the optimal system for a fixed architecture .	0	7	6	5.0857997	-4.4939775	0
240-3-3	this paper concerns the introduction of a new markov chain monte carlo scheme for posterior sampling in bayesian nonparametric mixture models with priors that belong to the general poisson-kingman class .	we present a novel compact way of representing the infinite dimensional component of the model such that while explicitly representing this infinite component it has less memory and storage requirements than previous mcmc schemes .	1	0	1	-6.002936	5.1860046	1
240-3-3	this paper concerns the introduction of a new markov chain monte carlo scheme for posterior sampling in bayesian nonparametric mixture models with priors that belong to the general poisson-kingman class .	we describe comparative simulation results demonstrating the efficacy of the proposed mcmc algorithm against existing marginal and conditional mcmc samplers .	1	0	2	-5.9574385	5.1566906	1
240-3-3	we present a novel compact way of representing the infinite dimensional component of the model such that while explicitly representing this infinite component it has less memory and storage requirements than previous mcmc schemes .	we describe comparative simulation results demonstrating the efficacy of the proposed mcmc algorithm against existing marginal and conditional mcmc samplers .	1	1	2	-5.9314957	5.1808615	1
241-9-36	let f : { -1 , 1 } n r be an n-variate polynomial consisting of 2n monomials , in which only s 2n coefficients are non-zero .	the goal is to learn the polynomial by querying the values of f .	1	0	1	-5.7191024	5.145847	1
241-9-36	we introduce an active learning framework that is associated with a low query cost and computational runtime .	let f : { -1 , 1 } n r be an n-variate polynomial consisting of 2n monomials , in which only s 2n coefficients are non-zero .	0	2	0	-0.45161232	0.81681573	1
241-9-36	let f : { -1 , 1 } n r be an n-variate polynomial consisting of 2n monomials , in which only s 2n coefficients are non-zero .	the significant savings are enabled by leveraging sampling strategies based on modern coding theory , specifically , the design and analysis of sparse-graph codes , such as low-density-parity-check ( ldpc ) codes , which represent the state-of-the-art of modern packet communications .	1	0	3	-5.98601	5.122672	1
241-9-36	let f : { -1 , 1 } n r be an n-variate polynomial consisting of 2n monomials , in which only s 2n coefficients are non-zero .	more significantly , we show how this design perspective leads to exciting , and to the best of our knowledge , largely unexplored intellectual connections between learning and coding .	1	0	4	-5.9615164	5.0602083	1
241-9-36	the key is to relax the worst-case assumption with an ensemble-average setting , where the polynomial is assumed to be drawn uniformly at random from the ensemble of all polynomials ( of a given size n and sparsity s ) .	let f : { -1 , 1 } n r be an n-variate polynomial consisting of 2n monomials , in which only s 2n coefficients are non-zero .	0	5	0	5.548167	-4.922355	0
241-9-36	[CLS] our framework succeeds with high probability with respect to the polynomial ensemble with sparsity up to s = o ( 2n ) for any ( 0, 1 ), where f is exactly learned using o ( ns ) queries in time o ( ns log s ), even if the queries	let f : { -1 , 1 } n r be an n-variate polynomial consisting of 2n monomials , in which only s 2n coefficients are non-zero .	0	6	0	5.676985	-5.050776	0
241-9-36	let f : { -1 , 1 } n r be an n-variate polynomial consisting of 2n monomials , in which only s 2n coefficients are non-zero .	we further apply the proposed framework to graph sketching , which is the problem of inferring sparse graphs by querying graph cuts .	1	0	7	-5.9012136	4.962181	1
241-9-36	let f : { -1 , 1 } n r be an n-variate polynomial consisting of 2n monomials , in which only s 2n coefficients are non-zero .	[CLS] by writing the cut function as a polynomial and exploiting the graph structure, we propose a sketching algorithm to learn the an arbitrary n - node unknown graph using only few cut queries, which scales almost linearly in the number of edges and sub - linearly in the graph size n.	1	0	8	-5.992993	5.1731677	1
241-9-36	the goal is to learn the polynomial by querying the values of f .	we introduce an active learning framework that is associated with a low query cost and computational runtime .	1	1	2	1.9173322	-1.6336168	0
241-9-36	the goal is to learn the polynomial by querying the values of f .	the significant savings are enabled by leveraging sampling strategies based on modern coding theory , specifically , the design and analysis of sparse-graph codes , such as low-density-parity-check ( ldpc ) codes , which represent the state-of-the-art of modern packet communications .	1	1	3	-5.9720683	5.088693	1
241-9-36	more significantly , we show how this design perspective leads to exciting , and to the best of our knowledge , largely unexplored intellectual connections between learning and coding .	the goal is to learn the polynomial by querying the values of f .	0	4	1	4.8003216	-4.3089294	0
241-9-36	the key is to relax the worst-case assumption with an ensemble-average setting , where the polynomial is assumed to be drawn uniformly at random from the ensemble of all polynomials ( of a given size n and sparsity s ) .	the goal is to learn the polynomial by querying the values of f .	0	5	1	2.4729228	-2.3217387	0
241-9-36	the goal is to learn the polynomial by querying the values of f .	our framework succeeds with high probability with respect to the polynomial ensemble with sparsity up to s = o ( 2n ) for any ( 0 , 1 ) , where f is exactly learned using o ( ns ) queries in time o ( ns log s ) , even if the queries are perturbed by gaussian noise .	1	1	6	-5.9597173	5.159608	1
241-9-36	the goal is to learn the polynomial by querying the values of f .	we further apply the proposed framework to graph sketching , which is the problem of inferring sparse graphs by querying graph cuts .	1	1	7	-5.760995	4.8469687	1
241-9-36	the goal is to learn the polynomial by querying the values of f .	by writing the cut function as a polynomial and exploiting the graph structure , we propose a sketching algorithm to learn the an arbitrary n-node unknown graph using only few cut queries , which scales almost linearly in the number of edges and sub-linearly in the graph size n. experiments on real datasets show significant reductions in the runtime and query complexity compared with competitive schemes .	1	1	8	-5.065096	4.707472	1
241-9-36	the significant savings are enabled by leveraging sampling strategies based on modern coding theory , specifically , the design and analysis of sparse-graph codes , such as low-density-parity-check ( ldpc ) codes , which represent the state-of-the-art of modern packet communications .	we introduce an active learning framework that is associated with a low query cost and computational runtime .	0	3	2	5.503856	-4.845544	0
241-9-36	more significantly , we show how this design perspective leads to exciting , and to the best of our knowledge , largely unexplored intellectual connections between learning and coding .	we introduce an active learning framework that is associated with a low query cost and computational runtime .	0	4	2	5.2198524	-4.5720425	0
241-9-36	the key is to relax the worst-case assumption with an ensemble-average setting , where the polynomial is assumed to be drawn uniformly at random from the ensemble of all polynomials ( of a given size n and sparsity s ) .	we introduce an active learning framework that is associated with a low query cost and computational runtime .	0	5	2	3.990547	-3.5577085	0
241-9-36	we introduce an active learning framework that is associated with a low query cost and computational runtime .	our framework succeeds with high probability with respect to the polynomial ensemble with sparsity up to s = o ( 2n ) for any ( 0 , 1 ) , where f is exactly learned using o ( ns ) queries in time o ( ns log s ) , even if the queries are perturbed by gaussian noise .	1	2	6	-5.854012	5.252743	1
241-9-36	we further apply the proposed framework to graph sketching , which is the problem of inferring sparse graphs by querying graph cuts .	we introduce an active learning framework that is associated with a low query cost and computational runtime .	0	7	2	5.4614506	-4.8580847	0
241-9-36	we introduce an active learning framework that is associated with a low query cost and computational runtime .	[CLS] by writing the cut function as a polynomial and exploiting the graph structure, we propose a sketching algorithm to learn the an arbitrary n - node unknown graph using only few cut queries, which scales almost linearly in the number of edges and sub - linearly in the graph size n. experiments on real datasets show significant reductions in the runtime and query complexity compared with competitive	1	2	8	-5.841921	5.2365522	1
241-9-36	more significantly , we show how this design perspective leads to exciting , and to the best of our knowledge , largely unexplored intellectual connections between learning and coding .	the significant savings are enabled by leveraging sampling strategies based on modern coding theory , specifically , the design and analysis of sparse-graph codes , such as low-density-parity-check ( ldpc ) codes , which represent the state-of-the-art of modern packet communications .	0	4	3	1.0601223	-0.7560864	0
241-9-36	the key is to relax the worst-case assumption with an ensemble-average setting , where the polynomial is assumed to be drawn uniformly at random from the ensemble of all polynomials ( of a given size n and sparsity s ) .	[CLS] the significant savings are enabled by leveraging sampling strategies based on modern coding theory, specifically, the design and analysis of sparse - graph codes, such as low - density - parity - check ( ldpc ) codes, which represent the state - of	0	5	3	-5.907073	5.157635	1
241-9-36	[CLS] the significant savings are enabled by leveraging sampling strategies based on modern coding theory, specifically, the design and analysis of sparse - graph codes, such as low - density - parity - check ( ldpc ) codes, which represent the state -	[CLS] our framework succeeds with high probability with respect to the polynomial ensemble with sparsity up to s = o ( 2n ) for any ( 0, 1 ), where f is exactly learned using o ( ns ) queries in time o (	1	3	6	3.8044186	-3.511921	0
241-9-36	the significant savings are enabled by leveraging sampling strategies based on modern coding theory , specifically , the design and analysis of sparse-graph codes , such as low-density-parity-check ( ldpc ) codes , which represent the state-of-the-art of modern packet communications .	we further apply the proposed framework to graph sketching , which is the problem of inferring sparse graphs by querying graph cuts .	1	3	7	3.024994	-2.80778	0
241-9-36	[CLS] the significant savings are enabled by leveraging sampling strategies based on modern coding theory, specifically, the design and analysis of sparse - graph codes, such as low - density - parity - check ( ldpc ) codes, which represent the state -	[CLS] by writing the cut function as a polynomial and exploiting the graph structure, we propose a sketching algorithm to learn the an arbitrary n - node unknown graph using only few cut queries, which scales almost linearly in the number of edges and	1	3	8	4.5957336	-3.9995012	0
241-9-36	more significantly , we show how this design perspective leads to exciting , and to the best of our knowledge , largely unexplored intellectual connections between learning and coding .	the key is to relax the worst-case assumption with an ensemble-average setting , where the polynomial is assumed to be drawn uniformly at random from the ensemble of all polynomials ( of a given size n and sparsity s ) .	1	4	5	4.29648	-3.9686627	0
241-9-36	[CLS] our framework succeeds with high probability with respect to the polynomial ensemble with sparsity up to s = o ( 2n ) for any ( 0, 1 ), where f is exactly learned using o ( ns ) queries in time o ( ns log s ), even if the queries are perturbed by	more significantly , we show how this design perspective leads to exciting , and to the best of our knowledge , largely unexplored intellectual connections between learning and coding .	0	6	4	-3.1747322	3.0873046	1
241-9-36	we further apply the proposed framework to graph sketching , which is the problem of inferring sparse graphs by querying graph cuts .	more significantly , we show how this design perspective leads to exciting , and to the best of our knowledge , largely unexplored intellectual connections between learning and coding .	0	7	4	-1.6438665	1.7720472	1
241-9-36	more significantly , we show how this design perspective leads to exciting , and to the best of our knowledge , largely unexplored intellectual connections between learning and coding .	[CLS] by writing the cut function as a polynomial and exploiting the graph structure, we propose a sketching algorithm to learn the an arbitrary n - node unknown graph using only few cut queries, which scales almost linearly in the number of edges and sub - linearly in the graph size n. experiments on real dataset	1	4	8	4.102876	-3.667369	0
241-9-36	[CLS] our framework succeeds with high probability with respect to the polynomial ensemble with sparsity up to s = o ( 2n ) for any ( 0, 1 ), where f is exactly learned using o ( ns ) queries in time o ( ns	the key is to relax the worst-case assumption with an ensemble-average setting , where the polynomial is assumed to be drawn uniformly at random from the ensemble of all polynomials ( of a given size n and sparsity s ) .	0	6	5	4.667254	-4.2115035	0
241-9-36	we further apply the proposed framework to graph sketching , which is the problem of inferring sparse graphs by querying graph cuts .	the key is to relax the worst-case assumption with an ensemble-average setting , where the polynomial is assumed to be drawn uniformly at random from the ensemble of all polynomials ( of a given size n and sparsity s ) .	0	7	5	4.931282	-4.435546	0
241-9-36	the key is to relax the worst-case assumption with an ensemble-average setting , where the polynomial is assumed to be drawn uniformly at random from the ensemble of all polynomials ( of a given size n and sparsity s ) .	[CLS] by writing the cut function as a polynomial and exploiting the graph structure, we propose a sketching algorithm to learn the an arbitrary n - node unknown graph using only few cut queries, which scales almost linearly in the number of edges and sub	1	5	8	-1.219095	1.3847086	1
241-9-36	our framework succeeds with high probability with respect to the polynomial ensemble with sparsity up to s = o ( 2n ) for any ( 0 , 1 ) , where f is exactly learned using o ( ns ) queries in time o ( ns log s ) , even if the queries are perturbed by gaussian noise .	we further apply the proposed framework to graph sketching , which is the problem of inferring sparse graphs by querying graph cuts .	1	6	7	-2.9986382	2.920074	1
241-9-36	[CLS] our framework succeeds with high probability with respect to the polynomial ensemble with sparsity up to s = o ( 2n ) for any ( 0, 1 ), where f is exactly learned using o ( ns ) queries in time o (	[CLS] by writing the cut function as a polynomial and exploiting the graph structure, we propose a sketching algorithm to learn the an arbitrary n - node unknown graph using only few cut queries, which scales almost linearly in the number of edges and	1	6	8	4.3920937	-3.8439107	0
241-9-36	[CLS] by writing the cut function as a polynomial and exploiting the graph structure, we propose a sketching algorithm to learn the an arbitrary n - node unknown graph using only few cut queries, which scales almost linearly in the number of edges and sub - linearly in the graph size n. experiments on real datasets show significant reductions in the	we further apply the proposed framework to graph sketching , which is the problem of inferring sparse graphs by querying graph cuts .	0	8	7	-3.1879613	3.0283082	1
242-5-10	for a large family of penalized empirical risk minimization problems , our methods exploit data dependent local smoothness of the loss functions near the optimum , while maintaining convergence guarantees .	we propose a family of non-uniform sampling strategies to provably speed up a class of stochastic optimization algorithms with linear convergence including stochastic variance reduced gradient ( svrg ) and stochastic dual coordinate ascent ( sdca ) .	0	1	0	5.377249	-4.732577	0
242-5-10	our bounds are the first to quantify the advantage gained from local smoothness which are significant for some problems significantly better .	we propose a family of non-uniform sampling strategies to provably speed up a class of stochastic optimization algorithms with linear convergence including stochastic variance reduced gradient ( svrg ) and stochastic dual coordinate ascent ( sdca ) .	0	2	0	5.5588408	-4.901263	0
242-5-10	empirically , we provide thorough numerical results to back up our theory .	we propose a family of non-uniform sampling strategies to provably speed up a class of stochastic optimization algorithms with linear convergence including stochastic variance reduced gradient ( svrg ) and stochastic dual coordinate ascent ( sdca ) .	0	3	0	5.4930487	-4.8235664	0
242-5-10	we propose a family of non-uniform sampling strategies to provably speed up a class of stochastic optimization algorithms with linear convergence including stochastic variance reduced gradient ( svrg ) and stochastic dual coordinate ascent ( sdca ) .	additionally we present algorithms exploiting local smoothness in more aggressive ways , which perform even better in practice .	1	0	4	-6.0033383	5.2002225	1
242-5-10	for a large family of penalized empirical risk minimization problems , our methods exploit data dependent local smoothness of the loss functions near the optimum , while maintaining convergence guarantees .	our bounds are the first to quantify the advantage gained from local smoothness which are significant for some problems significantly better .	1	1	2	-5.702647	5.0994387	1
242-5-10	for a large family of penalized empirical risk minimization problems , our methods exploit data dependent local smoothness of the loss functions near the optimum , while maintaining convergence guarantees .	empirically , we provide thorough numerical results to back up our theory .	1	1	3	-5.9661217	5.090056	1
242-5-10	for a large family of penalized empirical risk minimization problems , our methods exploit data dependent local smoothness of the loss functions near the optimum , while maintaining convergence guarantees .	additionally we present algorithms exploiting local smoothness in more aggressive ways , which perform even better in practice .	1	1	4	-5.82583	5.116588	1
242-5-10	empirically , we provide thorough numerical results to back up our theory .	our bounds are the first to quantify the advantage gained from local smoothness which are significant for some problems significantly better .	0	3	2	3.3126512	-3.1234398	0
242-5-10	our bounds are the first to quantify the advantage gained from local smoothness which are significant for some problems significantly better .	additionally we present algorithms exploiting local smoothness in more aggressive ways , which perform even better in practice .	1	2	4	-4.872386	4.535476	1
242-5-10	empirically , we provide thorough numerical results to back up our theory .	additionally we present algorithms exploiting local smoothness in more aggressive ways , which perform even better in practice .	1	3	4	-1.9335707	1.9945161	1
243-5-10	this proposal is shown to relate to a variety of classic research contributions in scale-space theory , interest point detection , bilateral filtering , and to existing models of visual saliency .	in this paper we present a definition for visual saliency grounded in information theory .	0	1	0	5.482774	-4.8355155	0
243-5-10	in this paper we present a definition for visual saliency grounded in information theory .	based on the proposed definition of visual saliency , we demonstrate results competitive with the state-of-the art for both prediction of human fixations , and segmentation of salient objects .	1	0	2	-5.9685163	5.1925774	1
243-5-10	we also characterize different properties of this model including robustness to image transformations , and extension to a wide range of other data types with 3d mesh models serving as an example .	in this paper we present a definition for visual saliency grounded in information theory .	0	3	0	5.6005774	-4.9503517	0
243-5-10	in this paper we present a definition for visual saliency grounded in information theory .	finally , we relate this proposal more generally to the role of saliency computation in visual information processing and draw connections to putative mechanisms for saliency computation in human vision .	1	0	4	-5.9544067	5.049512	1
243-5-10	based on the proposed definition of visual saliency , we demonstrate results competitive with the state-of-the art for both prediction of human fixations , and segmentation of salient objects .	this proposal is shown to relate to a variety of classic research contributions in scale-space theory , interest point detection , bilateral filtering , and to existing models of visual saliency .	0	2	1	2.1066003	-1.8983653	0
243-5-10	this proposal is shown to relate to a variety of classic research contributions in scale-space theory , interest point detection , bilateral filtering , and to existing models of visual saliency .	we also characterize different properties of this model including robustness to image transformations , and extension to a wide range of other data types with 3d mesh models serving as an example .	1	1	3	-0.31990135	0.5761473	1
243-5-10	this proposal is shown to relate to a variety of classic research contributions in scale-space theory , interest point detection , bilateral filtering , and to existing models of visual saliency .	finally , we relate this proposal more generally to the role of saliency computation in visual information processing and draw connections to putative mechanisms for saliency computation in human vision .	1	1	4	-5.1079693	4.613553	1
243-5-10	based on the proposed definition of visual saliency , we demonstrate results competitive with the state-of-the art for both prediction of human fixations , and segmentation of salient objects .	we also characterize different properties of this model including robustness to image transformations , and extension to a wide range of other data types with 3d mesh models serving as an example .	1	2	3	2.0887122	-1.8053479	0
243-5-10	finally , we relate this proposal more generally to the role of saliency computation in visual information processing and draw connections to putative mechanisms for saliency computation in human vision .	based on the proposed definition of visual saliency , we demonstrate results competitive with the state-of-the art for both prediction of human fixations , and segmentation of salient objects .	0	4	2	2.785973	-2.6491241	0
243-5-10	finally , we relate this proposal more generally to the role of saliency computation in visual information processing and draw connections to putative mechanisms for saliency computation in human vision .	we also characterize different properties of this model including robustness to image transformations , and extension to a wide range of other data types with 3d mesh models serving as an example .	0	4	3	3.392293	-3.1952095	0
244-5-10	we provide a new analysis framework for the adversarial multi-armed bandit problem .	using the notion of convex smoothing , we define a novel family of algorithms with minimax optimal regret guarantees .	1	0	1	-5.8916593	5.241564	1
244-5-10	first , we show that regularizationp via the tsallis entropy , which includes exp3 as a special case , matches the o ( n t ) minimax regret with a smaller constant factor .	we provide a new analysis framework for the adversarial multi-armed bandit problem .	0	2	0	5.5061836	-4.900633	0
244-5-10	second , we show that a p wide class of perturbation methods achieve a near-optimal regret as low as o ( n t log n ) , as long as the perturbation distribution has a bounded hazard function .	we provide a new analysis framework for the adversarial multi-armed bandit problem .	0	3	0	5.2875886	-4.7419744	0
244-5-10	we provide a new analysis framework for the adversarial multi-armed bandit problem .	for example , the gumbel , weibull , frechet , pareto , and gamma distributions all satisfy this key property and lead to near-optimal algorithms .	1	0	4	-5.5181327	5.023202	1
244-5-10	using the notion of convex smoothing , we define a novel family of algorithms with minimax optimal regret guarantees .	first , we show that regularizationp via the tsallis entropy , which includes exp3 as a special case , matches the o ( n t ) minimax regret with a smaller constant factor .	1	1	2	-1.6013536	1.6798593	1
244-5-10	using the notion of convex smoothing , we define a novel family of algorithms with minimax optimal regret guarantees .	second , we show that a p wide class of perturbation methods achieve a near-optimal regret as low as o ( n t log n ) , as long as the perturbation distribution has a bounded hazard function .	1	1	3	-0.23086715	0.50521207	1
244-5-10	using the notion of convex smoothing , we define a novel family of algorithms with minimax optimal regret guarantees .	for example , the gumbel , weibull , frechet , pareto , and gamma distributions all satisfy this key property and lead to near-optimal algorithms .	1	1	4	1.8978484	-1.7891798	0
244-5-10	first , we show that regularizationp via the tsallis entropy , which includes exp3 as a special case , matches the o ( n t ) minimax regret with a smaller constant factor .	second , we show that a p wide class of perturbation methods achieve a near-optimal regret as low as o ( n t log n ) , as long as the perturbation distribution has a bounded hazard function .	1	2	3	-5.797639	5.202261	1
244-5-10	for example , the gumbel , weibull , frechet , pareto , and gamma distributions all satisfy this key property and lead to near-optimal algorithms .	first , we show that regularizationp via the tsallis entropy , which includes exp3 as a special case , matches the o ( n t ) minimax regret with a smaller constant factor .	0	4	2	-2.5989811	2.6204195	1
244-5-10	second , we show that a p wide class of perturbation methods achieve a near-optimal regret as low as o ( n t log n ) , as long as the perturbation distribution has a bounded hazard function .	for example , the gumbel , weibull , frechet , pareto , and gamma distributions all satisfy this key property and lead to near-optimal algorithms .	1	3	4	0.27124113	0.106929906	0
245-6-15	existing results only consider sub-gaussian designs and noise , and both the sample complexity and non-asymptotic estimation error have been shown to depend on the gaussian width of suitable sets .	we consider the problem of high-dimensional structured estimation with normregularized estimators , such as lasso , when the design matrix and noise are drawn from sub-exponential distributions .	0	1	0	5.6695995	-5.112278	0
245-6-15	we consider the problem of high-dimensional structured estimation with normregularized estimators , such as lasso , when the design matrix and noise are drawn from sub-exponential distributions .	in contrast , for the sub-exponential setting , we show that the sample complexity and the estimation error will depend on the exponential width of the corresponding sets , and the analysis holds for any norm .	1	0	2	-5.8500304	5.1677995	1
245-6-15	further , using generic chaining , we show that the exponential width for any set will be at most log p times the gaussian width of the set , yielding gaussian width based results even for the sub-exponential case .	we consider the problem of high-dimensional structured estimation with normregularized estimators , such as lasso , when the design matrix and noise are drawn from sub-exponential distributions .	0	3	0	5.6869636	-5.100026	0
245-6-15	we consider the problem of high-dimensional structured estimation with normregularized estimators , such as lasso , when the design matrix and noise are drawn from sub-exponential distributions .	further , for certain popular estimators , viz lasso and group lasso , using a vcdimension based analysis , we show that the sample complexity will in fact be the same order as gaussian designs .	1	0	4	-5.8973627	5.1907644	1
245-6-15	our general analysis and results are the first in the sub-exponential setting , and are readily applicable to special sub-exponential families such as log-concave and extreme-value distributions .	we consider the problem of high-dimensional structured estimation with normregularized estimators , such as lasso , when the design matrix and noise are drawn from sub-exponential distributions .	0	5	0	5.6575828	-5.09217	0
245-6-15	in contrast , for the sub-exponential setting , we show that the sample complexity and the estimation error will depend on the exponential width of the corresponding sets , and the analysis holds for any norm .	existing results only consider sub-gaussian designs and noise , and both the sample complexity and non-asymptotic estimation error have been shown to depend on the gaussian width of suitable sets .	0	2	1	5.1659155	-4.5291014	0
245-6-15	existing results only consider sub-gaussian designs and noise , and both the sample complexity and non-asymptotic estimation error have been shown to depend on the gaussian width of suitable sets .	further , using generic chaining , we show that the exponential width for any set will be at most log p times the gaussian width of the set , yielding gaussian width based results even for the sub-exponential case .	1	1	3	-5.956444	5.2201715	1
245-6-15	existing results only consider sub-gaussian designs and noise , and both the sample complexity and non-asymptotic estimation error have been shown to depend on the gaussian width of suitable sets .	further , for certain popular estimators , viz lasso and group lasso , using a vcdimension based analysis , we show that the sample complexity will in fact be the same order as gaussian designs .	1	1	4	-5.9396796	5.2501116	1
245-6-15	our general analysis and results are the first in the sub-exponential setting , and are readily applicable to special sub-exponential families such as log-concave and extreme-value distributions .	existing results only consider sub-gaussian designs and noise , and both the sample complexity and non-asymptotic estimation error have been shown to depend on the gaussian width of suitable sets .	0	5	1	5.2214537	-4.661279	0
245-6-15	further , using generic chaining , we show that the exponential width for any set will be at most log p times the gaussian width of the set , yielding gaussian width based results even for the sub-exponential case .	in contrast , for the sub-exponential setting , we show that the sample complexity and the estimation error will depend on the exponential width of the corresponding sets , and the analysis holds for any norm .	0	3	2	3.699428	-3.4039164	0
245-6-15	further , for certain popular estimators , viz lasso and group lasso , using a vcdimension based analysis , we show that the sample complexity will in fact be the same order as gaussian designs .	in contrast , for the sub-exponential setting , we show that the sample complexity and the estimation error will depend on the exponential width of the corresponding sets , and the analysis holds for any norm .	0	4	2	2.6253357	-2.4454153	0
245-6-15	in contrast , for the sub-exponential setting , we show that the sample complexity and the estimation error will depend on the exponential width of the corresponding sets , and the analysis holds for any norm .	our general analysis and results are the first in the sub-exponential setting , and are readily applicable to special sub-exponential families such as log-concave and extreme-value distributions .	1	2	5	-4.632574	4.285842	1
245-6-15	further , using generic chaining , we show that the exponential width for any set will be at most log p times the gaussian width of the set , yielding gaussian width based results even for the sub-exponential case .	further , for certain popular estimators , viz lasso and group lasso , using a vcdimension based analysis , we show that the sample complexity will in fact be the same order as gaussian designs .	1	3	4	1.425845	-0.97396547	0
245-6-15	further , using generic chaining , we show that the exponential width for any set will be at most log p times the gaussian width of the set , yielding gaussian width based results even for the sub-exponential case .	our general analysis and results are the first in the sub-exponential setting , and are readily applicable to special sub-exponential families such as log-concave and extreme-value distributions .	1	3	5	-3.7669418	3.5815783	1
245-6-15	further , for certain popular estimators , viz lasso and group lasso , using a vcdimension based analysis , we show that the sample complexity will in fact be the same order as gaussian designs .	our general analysis and results are the first in the sub-exponential setting , and are readily applicable to special sub-exponential families such as log-concave and extreme-value distributions .	1	4	5	-3.527939	3.4192195	1
246-14-91	recent literature suggests that embedding a graph on an unit sphere leads to better generalization for graph transduction .	however , the choice of optimal embedding and an efficient algorithm to compute the same remains open .	1	0	1	-5.9795475	5.089306	1
246-14-91	in this paper , we show that orthonormal representations , a class of unit-sphere graph embeddings are pac learnable .	recent literature suggests that embedding a graph on an unit sphere leads to better generalization for graph transduction .	0	2	0	5.0902224	-4.506612	0
246-14-91	existing pac-based analysis do not apply as the vc dimension of the function class is infinite .	recent literature suggests that embedding a graph on an unit sphere leads to better generalization for graph transduction .	0	3	0	4.175644	-3.786115	0
246-14-91	we propose an alternative pac-based bound , which do not depend on the vc dimension of the underlying function class , but is related to the famous lovasz function .	recent literature suggests that embedding a graph on an unit sphere leads to better generalization for graph transduction .	0	4	0	5.315673	-4.679286	0
246-14-91	the main contribution of the paper is spore , a spectral regularized orthonormal embedding for graph transduction , derived from the pac bound .	recent literature suggests that embedding a graph on an unit sphere leads to better generalization for graph transduction .	0	5	0	4.9022284	-4.3512716	0
246-14-91	recent literature suggests that embedding a graph on an unit sphere leads to better generalization for graph transduction .	spore is posed as a non-smooth convex function over an elliptope .	1	0	6	-4.3477283	4.1596785	1
246-14-91	recent literature suggests that embedding a graph on an unit sphere leads to better generalization for graph transduction .	these problems are usually solved as semi-definite programs ( sdps ) with time complexity o ( n6 ) .	1	0	7	-0.48265904	0.8600032	1
246-14-91	recent literature suggests that embedding a graph on an unit sphere leads to better generalization for graph transduction .	we present , infeasible inexact proximal ( iip ) : an inexact proximal method which performs subgradient procedure on an approximate projection , not necessarily feasible .	1	0	8	-5.922655	5.179907	1
246-14-91	recent literature suggests that embedding a graph on an unit sphere leads to better generalization for graph transduction .	iip is more scalable than sdp , has an o ( 1t ) convergence , and is generally applicable whenever a suitable approximate projection is available .	1	0	9	-5.9217367	5.206233	1
246-14-91	we use iip to compute spore where the approximate projection step is computed by fista , an accelerated gradient descent procedure .	recent literature suggests that embedding a graph on an unit sphere leads to better generalization for graph transduction .	0	10	0	5.437643	-4.783757	0
246-14-91	recent literature suggests that embedding a graph on an unit sphere leads to better generalization for graph transduction .	we show that the method has a convergence rate of o ( 1t ) .	1	0	11	-6.0095205	5.155564	1
246-14-91	recent literature suggests that embedding a graph on an unit sphere leads to better generalization for graph transduction .	the proposed algorithm easily scales to 1000 's of vertices , while the standard sdp computation does not scale beyond few hundred vertices .	1	0	12	-6.005766	5.193236	1
246-14-91	furthermore , the analysis presented here easily extends to the multiple graph setting .	recent literature suggests that embedding a graph on an unit sphere leads to better generalization for graph transduction .	0	13	0	5.5274854	-4.939787	0
246-14-91	however , the choice of optimal embedding and an efficient algorithm to compute the same remains open .	in this paper , we show that orthonormal representations , a class of unit-sphere graph embeddings are pac learnable .	1	1	2	2.1145651	-1.7011198	0
246-14-91	existing pac-based analysis do not apply as the vc dimension of the function class is infinite .	however , the choice of optimal embedding and an efficient algorithm to compute the same remains open .	0	3	1	-5.790906	5.1646204	1
246-14-91	however , the choice of optimal embedding and an efficient algorithm to compute the same remains open .	we propose an alternative pac-based bound , which do not depend on the vc dimension of the underlying function class , but is related to the famous lovasz function .	1	1	4	-2.828145	2.8183155	1
246-14-91	however , the choice of optimal embedding and an efficient algorithm to compute the same remains open .	the main contribution of the paper is spore , a spectral regularized orthonormal embedding for graph transduction , derived from the pac bound .	1	1	5	-3.2589047	3.121241	1
246-14-91	spore is posed as a non-smooth convex function over an elliptope .	however , the choice of optimal embedding and an efficient algorithm to compute the same remains open .	0	6	1	-5.0780544	4.600416	1
246-14-91	these problems are usually solved as semi-definite programs ( sdps ) with time complexity o ( n6 ) .	however , the choice of optimal embedding and an efficient algorithm to compute the same remains open .	0	7	1	-5.8518553	5.15738	1
246-14-91	we present , infeasible inexact proximal ( iip ) : an inexact proximal method which performs subgradient procedure on an approximate projection , not necessarily feasible .	however , the choice of optimal embedding and an efficient algorithm to compute the same remains open .	0	8	1	-1.5722383	1.6650094	1
246-14-91	however , the choice of optimal embedding and an efficient algorithm to compute the same remains open .	iip is more scalable than sdp , has an o ( 1t ) convergence , and is generally applicable whenever a suitable approximate projection is available .	1	1	9	-3.623858	3.439227	1
246-14-91	however , the choice of optimal embedding and an efficient algorithm to compute the same remains open .	we use iip to compute spore where the approximate projection step is computed by fista , an accelerated gradient descent procedure .	1	1	10	-4.023227	3.8682573	1
246-14-91	however , the choice of optimal embedding and an efficient algorithm to compute the same remains open .	we show that the method has a convergence rate of o ( 1t ) .	1	1	11	-4.247147	4.0212474	1
246-14-91	however , the choice of optimal embedding and an efficient algorithm to compute the same remains open .	the proposed algorithm easily scales to 1000 's of vertices , while the standard sdp computation does not scale beyond few hundred vertices .	1	1	12	-5.447318	4.885098	1
246-14-91	however , the choice of optimal embedding and an efficient algorithm to compute the same remains open .	furthermore , the analysis presented here easily extends to the multiple graph setting .	1	1	13	-5.7622848	5.005084	1
246-14-91	in this paper , we show that orthonormal representations , a class of unit-sphere graph embeddings are pac learnable .	existing pac-based analysis do not apply as the vc dimension of the function class is infinite .	1	2	3	-0.8569071	1.0705968	1
246-14-91	we propose an alternative pac-based bound , which do not depend on the vc dimension of the underlying function class , but is related to the famous lovasz function .	in this paper , we show that orthonormal representations , a class of unit-sphere graph embeddings are pac learnable .	0	4	2	5.1119394	-4.4722915	0
246-14-91	in this paper , we show that orthonormal representations , a class of unit-sphere graph embeddings are pac learnable .	the main contribution of the paper is spore , a spectral regularized orthonormal embedding for graph transduction , derived from the pac bound .	1	2	5	-5.9998956	5.1496696	1
246-14-91	spore is posed as a non-smooth convex function over an elliptope .	in this paper , we show that orthonormal representations , a class of unit-sphere graph embeddings are pac learnable .	0	6	2	3.499921	-3.143637	0
246-14-91	in this paper , we show that orthonormal representations , a class of unit-sphere graph embeddings are pac learnable .	these problems are usually solved as semi-definite programs ( sdps ) with time complexity o ( n6 ) .	1	2	7	2.9254303	-2.4602833	0
246-14-91	we present , infeasible inexact proximal ( iip ) : an inexact proximal method which performs subgradient procedure on an approximate projection , not necessarily feasible .	in this paper , we show that orthonormal representations , a class of unit-sphere graph embeddings are pac learnable .	0	8	2	4.521929	-4.027257	0
246-14-91	iip is more scalable than sdp , has an o ( 1t ) convergence , and is generally applicable whenever a suitable approximate projection is available .	in this paper , we show that orthonormal representations , a class of unit-sphere graph embeddings are pac learnable .	0	9	2	5.0446434	-4.4907236	0
246-14-91	we use iip to compute spore where the approximate projection step is computed by fista , an accelerated gradient descent procedure .	in this paper , we show that orthonormal representations , a class of unit-sphere graph embeddings are pac learnable .	0	10	2	5.3960648	-4.6980534	0
246-14-91	in this paper , we show that orthonormal representations , a class of unit-sphere graph embeddings are pac learnable .	we show that the method has a convergence rate of o ( 1t ) .	1	2	11	-6.0001416	5.235984	1
246-14-91	the proposed algorithm easily scales to 1000 's of vertices , while the standard sdp computation does not scale beyond few hundred vertices .	in this paper , we show that orthonormal representations , a class of unit-sphere graph embeddings are pac learnable .	0	12	2	5.469139	-4.861541	0
246-14-91	in this paper , we show that orthonormal representations , a class of unit-sphere graph embeddings are pac learnable .	furthermore , the analysis presented here easily extends to the multiple graph setting .	1	2	13	-5.9929204	5.1760592	1
246-14-91	existing pac-based analysis do not apply as the vc dimension of the function class is infinite .	we propose an alternative pac-based bound , which do not depend on the vc dimension of the underlying function class , but is related to the famous lovasz function .	1	3	4	-5.7739544	5.251835	1
246-14-91	the main contribution of the paper is spore , a spectral regularized orthonormal embedding for graph transduction , derived from the pac bound .	existing pac-based analysis do not apply as the vc dimension of the function class is infinite .	0	5	3	4.621969	-4.09997	0
246-14-91	existing pac-based analysis do not apply as the vc dimension of the function class is infinite .	spore is posed as a non-smooth convex function over an elliptope .	1	3	6	2.5680346	-2.3660388	0
246-14-91	existing pac-based analysis do not apply as the vc dimension of the function class is infinite .	these problems are usually solved as semi-definite programs ( sdps ) with time complexity o ( n6 ) .	1	3	7	3.9339924	-3.496026	0
246-14-91	we present , infeasible inexact proximal ( iip ) : an inexact proximal method which performs subgradient procedure on an approximate projection , not necessarily feasible .	existing pac-based analysis do not apply as the vc dimension of the function class is infinite .	0	8	3	3.34115	-3.0424333	0
246-14-91	iip is more scalable than sdp , has an o ( 1t ) convergence , and is generally applicable whenever a suitable approximate projection is available .	existing pac-based analysis do not apply as the vc dimension of the function class is infinite .	0	9	3	4.793851	-4.2943563	0
246-14-91	existing pac-based analysis do not apply as the vc dimension of the function class is infinite .	we use iip to compute spore where the approximate projection step is computed by fista , an accelerated gradient descent procedure .	1	3	10	-5.9500237	5.1047087	1
246-14-91	existing pac-based analysis do not apply as the vc dimension of the function class is infinite .	we show that the method has a convergence rate of o ( 1t ) .	1	3	11	-5.9296303	5.079341	1
246-14-91	the proposed algorithm easily scales to 1000 's of vertices , while the standard sdp computation does not scale beyond few hundred vertices .	existing pac-based analysis do not apply as the vc dimension of the function class is infinite .	0	12	3	5.368394	-4.7824316	0
246-14-91	existing pac-based analysis do not apply as the vc dimension of the function class is infinite .	furthermore , the analysis presented here easily extends to the multiple graph setting .	1	3	13	-5.943286	5.044259	1
246-14-91	the main contribution of the paper is spore , a spectral regularized orthonormal embedding for graph transduction , derived from the pac bound .	we propose an alternative pac-based bound , which do not depend on the vc dimension of the underlying function class , but is related to the famous lovasz function .	0	5	4	3.4779027	-3.2153344	0
246-14-91	spore is posed as a non-smooth convex function over an elliptope .	we propose an alternative pac-based bound , which do not depend on the vc dimension of the underlying function class , but is related to the famous lovasz function .	0	6	4	-4.7815313	4.416067	1
246-14-91	these problems are usually solved as semi-definite programs ( sdps ) with time complexity o ( n6 ) .	we propose an alternative pac-based bound , which do not depend on the vc dimension of the underlying function class , but is related to the famous lovasz function .	0	7	4	-5.9480934	5.230792	1
246-14-91	we present , infeasible inexact proximal ( iip ) : an inexact proximal method which performs subgradient procedure on an approximate projection , not necessarily feasible .	we propose an alternative pac-based bound , which do not depend on the vc dimension of the underlying function class , but is related to the famous lovasz function .	0	8	4	-4.54866	4.296074	1
246-14-91	we propose an alternative pac-based bound , which do not depend on the vc dimension of the underlying function class , but is related to the famous lovasz function .	iip is more scalable than sdp , has an o ( 1t ) convergence , and is generally applicable whenever a suitable approximate projection is available .	1	4	9	-5.1656713	4.7331824	1
246-14-91	we use iip to compute spore where the approximate projection step is computed by fista , an accelerated gradient descent procedure .	we propose an alternative pac-based bound , which do not depend on the vc dimension of the underlying function class , but is related to the famous lovasz function .	0	10	4	4.2011385	-3.7448754	0
246-14-91	we propose an alternative pac-based bound , which do not depend on the vc dimension of the underlying function class , but is related to the famous lovasz function .	we show that the method has a convergence rate of o ( 1t ) .	1	4	11	-5.3570585	4.879321	1
246-14-91	we propose an alternative pac-based bound , which do not depend on the vc dimension of the underlying function class , but is related to the famous lovasz function .	the proposed algorithm easily scales to 1000 's of vertices , while the standard sdp computation does not scale beyond few hundred vertices .	1	4	12	-5.8271294	5.1425095	1
246-14-91	furthermore , the analysis presented here easily extends to the multiple graph setting .	we propose an alternative pac-based bound , which do not depend on the vc dimension of the underlying function class , but is related to the famous lovasz function .	0	13	4	4.88994	-4.3490267	0
246-14-91	spore is posed as a non-smooth convex function over an elliptope .	the main contribution of the paper is spore , a spectral regularized orthonormal embedding for graph transduction , derived from the pac bound .	0	6	5	-3.419633	3.3095717	1
246-14-91	these problems are usually solved as semi-definite programs ( sdps ) with time complexity o ( n6 ) .	the main contribution of the paper is spore , a spectral regularized orthonormal embedding for graph transduction , derived from the pac bound .	0	7	5	-6.0173116	5.1033754	1
246-14-91	we present , infeasible inexact proximal ( iip ) : an inexact proximal method which performs subgradient procedure on an approximate projection , not necessarily feasible .	the main contribution of the paper is spore , a spectral regularized orthonormal embedding for graph transduction , derived from the pac bound .	0	8	5	-5.884278	5.1209426	1
246-14-91	iip is more scalable than sdp , has an o ( 1t ) convergence , and is generally applicable whenever a suitable approximate projection is available .	the main contribution of the paper is spore , a spectral regularized orthonormal embedding for graph transduction , derived from the pac bound .	0	9	5	2.2634656	-2.1412377	0
246-14-91	the main contribution of the paper is spore , a spectral regularized orthonormal embedding for graph transduction , derived from the pac bound .	we use iip to compute spore where the approximate projection step is computed by fista , an accelerated gradient descent procedure .	1	5	10	-5.788187	5.1447654	1
246-14-91	the main contribution of the paper is spore , a spectral regularized orthonormal embedding for graph transduction , derived from the pac bound .	we show that the method has a convergence rate of o ( 1t ) .	1	5	11	-3.6641564	3.5002902	1
246-14-91	the proposed algorithm easily scales to 1000 's of vertices , while the standard sdp computation does not scale beyond few hundred vertices .	the main contribution of the paper is spore , a spectral regularized orthonormal embedding for graph transduction , derived from the pac bound .	0	12	5	3.2453153	-3.1035228	0
246-14-91	the main contribution of the paper is spore , a spectral regularized orthonormal embedding for graph transduction , derived from the pac bound .	furthermore , the analysis presented here easily extends to the multiple graph setting .	1	5	13	-5.4753175	4.9371085	1
246-14-91	these problems are usually solved as semi-definite programs ( sdps ) with time complexity o ( n6 ) .	spore is posed as a non-smooth convex function over an elliptope .	0	7	6	-3.3053737	3.1902778	1
246-14-91	spore is posed as a non-smooth convex function over an elliptope .	we present , infeasible inexact proximal ( iip ) : an inexact proximal method which performs subgradient procedure on an approximate projection , not necessarily feasible .	1	6	8	2.8541431	-2.4963665	0
246-14-91	spore is posed as a non-smooth convex function over an elliptope .	iip is more scalable than sdp , has an o ( 1t ) convergence , and is generally applicable whenever a suitable approximate projection is available .	1	6	9	-5.8740473	5.1887536	1
246-14-91	we use iip to compute spore where the approximate projection step is computed by fista , an accelerated gradient descent procedure .	spore is posed as a non-smooth convex function over an elliptope .	0	10	6	5.0063047	-4.3793626	0
246-14-91	we show that the method has a convergence rate of o ( 1t ) .	spore is posed as a non-smooth convex function over an elliptope .	0	11	6	5.1978436	-4.62595	0
246-14-91	the proposed algorithm easily scales to 1000 's of vertices , while the standard sdp computation does not scale beyond few hundred vertices .	spore is posed as a non-smooth convex function over an elliptope .	0	12	6	5.329483	-4.786499	0
246-14-91	spore is posed as a non-smooth convex function over an elliptope .	furthermore , the analysis presented here easily extends to the multiple graph setting .	1	6	13	-5.941979	5.0407934	1
246-14-91	these problems are usually solved as semi-definite programs ( sdps ) with time complexity o ( n6 ) .	we present , infeasible inexact proximal ( iip ) : an inexact proximal method which performs subgradient procedure on an approximate projection , not necessarily feasible .	1	7	8	-4.4494333	4.2392144	1
246-14-91	iip is more scalable than sdp , has an o ( 1t ) convergence , and is generally applicable whenever a suitable approximate projection is available .	these problems are usually solved as semi-definite programs ( sdps ) with time complexity o ( n6 ) .	0	9	7	5.616995	-5.008245	0
246-14-91	these problems are usually solved as semi-definite programs ( sdps ) with time complexity o ( n6 ) .	we use iip to compute spore where the approximate projection step is computed by fista , an accelerated gradient descent procedure .	1	7	10	-6.0331903	5.1853933	1
246-14-91	these problems are usually solved as semi-definite programs ( sdps ) with time complexity o ( n6 ) .	we show that the method has a convergence rate of o ( 1t ) .	1	7	11	-6.021637	5.129959	1
246-14-91	these problems are usually solved as semi-definite programs ( sdps ) with time complexity o ( n6 ) .	the proposed algorithm easily scales to 1000 's of vertices , while the standard sdp computation does not scale beyond few hundred vertices .	1	7	12	-5.9767427	5.1443133	1
246-14-91	these problems are usually solved as semi-definite programs ( sdps ) with time complexity o ( n6 ) .	furthermore , the analysis presented here easily extends to the multiple graph setting .	1	7	13	-6.014908	5.1257567	1
246-14-91	we present , infeasible inexact proximal ( iip ) : an inexact proximal method which performs subgradient procedure on an approximate projection , not necessarily feasible .	iip is more scalable than sdp , has an o ( 1t ) convergence , and is generally applicable whenever a suitable approximate projection is available .	1	8	9	-5.897729	5.201523	1
246-14-91	we use iip to compute spore where the approximate projection step is computed by fista , an accelerated gradient descent procedure .	we present , infeasible inexact proximal ( iip ) : an inexact proximal method which performs subgradient procedure on an approximate projection , not necessarily feasible .	0	10	8	5.599011	-4.9405813	0
246-14-91	we show that the method has a convergence rate of o ( 1t ) .	we present , infeasible inexact proximal ( iip ) : an inexact proximal method which performs subgradient procedure on an approximate projection , not necessarily feasible .	0	11	8	5.489831	-4.8582263	0
246-14-91	the proposed algorithm easily scales to 1000 's of vertices , while the standard sdp computation does not scale beyond few hundred vertices .	we present , infeasible inexact proximal ( iip ) : an inexact proximal method which performs subgradient procedure on an approximate projection , not necessarily feasible .	0	12	8	5.489973	-4.909353	0
246-14-91	furthermore , the analysis presented here easily extends to the multiple graph setting .	we present , infeasible inexact proximal ( iip ) : an inexact proximal method which performs subgradient procedure on an approximate projection , not necessarily feasible .	0	13	8	5.2732234	-4.653536	0
246-14-91	we use iip to compute spore where the approximate projection step is computed by fista , an accelerated gradient descent procedure .	iip is more scalable than sdp , has an o ( 1t ) convergence , and is generally applicable whenever a suitable approximate projection is available .	0	10	9	-1.4367663	1.594522	1
246-14-91	iip is more scalable than sdp , has an o ( 1t ) convergence , and is generally applicable whenever a suitable approximate projection is available .	we show that the method has a convergence rate of o ( 1t ) .	1	9	11	2.853326	-2.6909564	0
246-14-91	the proposed algorithm easily scales to 1000 's of vertices , while the standard sdp computation does not scale beyond few hundred vertices .	iip is more scalable than sdp , has an o ( 1t ) convergence , and is generally applicable whenever a suitable approximate projection is available .	0	12	9	2.1644907	-2.0927134	0
246-14-91	furthermore , the analysis presented here easily extends to the multiple graph setting .	iip is more scalable than sdp , has an o ( 1t ) convergence , and is generally applicable whenever a suitable approximate projection is available .	0	13	9	3.4561934	-3.2948985	0
246-14-91	we use iip to compute spore where the approximate projection step is computed by fista , an accelerated gradient descent procedure .	we show that the method has a convergence rate of o ( 1t ) .	1	10	11	-1.9881603	2.1479177	1
246-14-91	the proposed algorithm easily scales to 1000 's of vertices , while the standard sdp computation does not scale beyond few hundred vertices .	we use iip to compute spore where the approximate projection step is computed by fista , an accelerated gradient descent procedure .	0	12	10	3.7171714	-3.4719987	0
246-14-91	we use iip to compute spore where the approximate projection step is computed by fista , an accelerated gradient descent procedure .	furthermore , the analysis presented here easily extends to the multiple graph setting .	1	10	13	-5.102689	4.626485	1
246-14-91	the proposed algorithm easily scales to 1000 's of vertices , while the standard sdp computation does not scale beyond few hundred vertices .	we show that the method has a convergence rate of o ( 1t ) .	0	12	11	3.04511	-2.878789	0
246-14-91	we show that the method has a convergence rate of o ( 1t ) .	furthermore , the analysis presented here easily extends to the multiple graph setting .	1	11	13	-5.246259	4.7872343	1
246-14-91	furthermore , the analysis presented here easily extends to the multiple graph setting .	the proposed algorithm easily scales to 1000 's of vertices , while the standard sdp computation does not scale beyond few hundred vertices .	0	13	12	2.8085635	-2.7125793	0
247-4-6	these networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape .	we introduce a convolutional neural network that operates directly on graphs .	0	1	0	4.772894	-4.2243924	0
247-4-6	we introduce a convolutional neural network that operates directly on graphs .	the architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints .	1	0	2	-5.3461437	4.894096	1
247-4-6	we introduce a convolutional neural network that operates directly on graphs .	we show that these data-driven features are more interpretable , and have better predictive performance on a variety of tasks .	1	0	3	-5.8476973	5.125984	1
247-4-6	these networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape .	the architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints .	1	1	2	-1.6691027	1.8525329	1
247-4-6	we show that these data-driven features are more interpretable , and have better predictive performance on a variety of tasks .	these networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape .	0	3	1	5.06227	-4.5510306	0
247-4-6	the architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints .	we show that these data-driven features are more interpretable , and have better predictive performance on a variety of tasks .	1	2	3	-5.277549	4.8410845	1
248-8-28	we investigate two novel mixed robust/average-case submodular data partitioning problems that we collectively call submodular partitioning .	[CLS] these problems generalize purely robust instances of the problem, namely max - min submodular fair allocation ( sfa ) and min - max submodular load balancing ( slb ), and also average - case instances, that is the submodular welfare problem ( swp ) and submodular multiway partition	1	0	1	-5.9542603	5.1968913	1
248-8-28	we investigate two novel mixed robust/average-case submodular data partitioning problems that we collectively call submodular partitioning .	while the robust versions have been studied in the theory community , existing work has focused on tight approximation guarantees , and the resultant algorithms are not generally scalable to large real-world applications .	1	0	2	4.188467	-3.848499	0
248-8-28	we investigate two novel mixed robust/average-case submodular data partitioning problems that we collectively call submodular partitioning .	this is in contrast to the average case , where most of the algorithms are scalable .	1	0	3	-5.7847023	5.1481676	1
248-8-28	we investigate two novel mixed robust/average-case submodular data partitioning problems that we collectively call submodular partitioning .	in the present paper , we bridge this gap , by proposing several new algorithms ( including greedy , majorization-minimization , minorization-maximization , and relaxation algorithms ) that not only scale to large datasets but that also achieve theoretical approximation guarantees comparable to the state-of-the-art .	1	0	4	-3.7580278	3.7051616	1
248-8-28	we moreover provide new scalable algorithms that apply to additive combinations of the robust and average-case objectives .	we investigate two novel mixed robust/average-case submodular data partitioning problems that we collectively call submodular partitioning .	0	5	0	5.4756217	-4.848916	0
248-8-28	we investigate two novel mixed robust/average-case submodular data partitioning problems that we collectively call submodular partitioning .	we show that these problems have many applications in machine learning ( ml ) , including data partitioning and load balancing for distributed ml , data clustering , and image segmentation .	1	0	6	-3.8322449	3.707415	1
248-8-28	we investigate two novel mixed robust/average-case submodular data partitioning problems that we collectively call submodular partitioning .	we empirically demonstrate the efficacy of our algorithms on real-world problems involving data partitioning for distributed optimization ( of convex and deep neural network objectives ) , and also purely unsupervised image segmentation .	1	0	7	-6.013447	5.1617956	1
248-8-28	[CLS] these problems generalize purely robust instances of the problem, namely max - min submodular fair allocation ( sfa ) and min - max submodular load balancing ( slb ), and also average - case instances, that is the submodular welfare problem ( swp )	while the robust versions have been studied in the theory community , existing work has focused on tight approximation guarantees , and the resultant algorithms are not generally scalable to large real-world applications .	1	1	2	3.9858139	-3.6212869	0
248-8-28	this is in contrast to the average case , where most of the algorithms are scalable .	these problems generalize purely robust instances of the problem , namely max-min submodular fair allocation ( sfa ) and min-max submodular load balancing ( slb ) , and also average-case instances , that is the submodular welfare problem ( swp ) and submodular multiway partition ( smp ) .	0	3	1	4.0150805	-3.5957708	0
248-8-28	[CLS] these problems generalize purely robust instances of the problem, namely max - min submodular fair allocation ( sfa ) and min - max submodular load balancing ( slb ), and also average - case instances, that is	[CLS] in the present paper, we bridge this gap, by proposing several new algorithms ( including greedy, majorization - minimization, minorization - maximization, and relaxation algorithms ) that not only scale to large datasets but that also achieve	1	1	4	0.9368764	-0.6461786	0
248-8-28	we moreover provide new scalable algorithms that apply to additive combinations of the robust and average-case objectives .	these problems generalize purely robust instances of the problem , namely max-min submodular fair allocation ( sfa ) and min-max submodular load balancing ( slb ) , and also average-case instances , that is the submodular welfare problem ( swp ) and submodular multiway partition ( smp ) .	0	5	1	5.3383627	-4.7340994	0
248-8-28	we show that these problems have many applications in machine learning ( ml ) , including data partitioning and load balancing for distributed ml , data clustering , and image segmentation .	[CLS] these problems generalize purely robust instances of the problem, namely max - min submodular fair allocation ( sfa ) and min - max submodular load balancing ( slb ), and also average - case instances, that is the submodular welfare problem ( swp ) and sub	0	6	1	-5.885885	5.2609396	1
248-8-28	we empirically demonstrate the efficacy of our algorithms on real-world problems involving data partitioning for distributed optimization ( of convex and deep neural network objectives ) , and also purely unsupervised image segmentation .	[CLS] these problems generalize purely robust instances of the problem, namely max - min submodular fair allocation ( sfa ) and min - max submodular load balancing ( slb ), and also average - case instances, that is the submodular	0	7	1	5.4084797	-4.7951612	0
248-8-28	while the robust versions have been studied in the theory community , existing work has focused on tight approximation guarantees , and the resultant algorithms are not generally scalable to large real-world applications .	this is in contrast to the average case , where most of the algorithms are scalable .	1	2	3	-5.7505703	5.163892	1
248-8-28	[CLS] in the present paper, we bridge this gap, by proposing several new algorithms ( including greedy, majorization - minimization, minorization - maximization, and relaxation algorithms ) that not only scale to large datasets but that also achieve theoretical approximation guarantees comparable to the state - of - the	while the robust versions have been studied in the theory community , existing work has focused on tight approximation guarantees , and the resultant algorithms are not generally scalable to large real-world applications .	0	4	2	5.2931848	-4.658766	0
248-8-28	while the robust versions have been studied in the theory community , existing work has focused on tight approximation guarantees , and the resultant algorithms are not generally scalable to large real-world applications .	we moreover provide new scalable algorithms that apply to additive combinations of the robust and average-case objectives .	1	2	5	-5.965186	5.155816	1
248-8-28	we show that these problems have many applications in machine learning ( ml ) , including data partitioning and load balancing for distributed ml , data clustering , and image segmentation .	while the robust versions have been studied in the theory community , existing work has focused on tight approximation guarantees , and the resultant algorithms are not generally scalable to large real-world applications .	0	6	2	-4.4900446	4.2344995	1
248-8-28	we empirically demonstrate the efficacy of our algorithms on real-world problems involving data partitioning for distributed optimization ( of convex and deep neural network objectives ) , and also purely unsupervised image segmentation .	while the robust versions have been studied in the theory community , existing work has focused on tight approximation guarantees , and the resultant algorithms are not generally scalable to large real-world applications .	0	7	2	5.61993	-5.0157766	0
248-8-28	this is in contrast to the average case , where most of the algorithms are scalable .	in the present paper , we bridge this gap , by proposing several new algorithms ( including greedy , majorization-minimization , minorization-maximization , and relaxation algorithms ) that not only scale to large datasets but that also achieve theoretical approximation guarantees comparable to the state-of-the-art .	1	3	4	1.5955698	-1.4030247	0
248-8-28	we moreover provide new scalable algorithms that apply to additive combinations of the robust and average-case objectives .	this is in contrast to the average case , where most of the algorithms are scalable .	0	5	3	4.3147764	-3.9814963	0
248-8-28	this is in contrast to the average case , where most of the algorithms are scalable .	we show that these problems have many applications in machine learning ( ml ) , including data partitioning and load balancing for distributed ml , data clustering , and image segmentation .	1	3	6	5.1289277	-4.481834	0
248-8-28	we empirically demonstrate the efficacy of our algorithms on real-world problems involving data partitioning for distributed optimization ( of convex and deep neural network objectives ) , and also purely unsupervised image segmentation .	this is in contrast to the average case , where most of the algorithms are scalable .	0	7	3	4.9146056	-4.3959937	0
248-8-28	in the present paper , we bridge this gap , by proposing several new algorithms ( including greedy , majorization-minimization , minorization-maximization , and relaxation algorithms ) that not only scale to large datasets but that also achieve theoretical approximation guarantees comparable to the state-of-the-art .	we moreover provide new scalable algorithms that apply to additive combinations of the robust and average-case objectives .	1	4	5	-5.8235435	5.1844926	1
248-8-28	we show that these problems have many applications in machine learning ( ml ) , including data partitioning and load balancing for distributed ml , data clustering , and image segmentation .	[CLS] in the present paper, we bridge this gap, by proposing several new algorithms ( including greedy, majorization - minimization, minorization - maximization, and relaxation algorithms ) that not only scale to large datasets but that also achieve theoretical approximation guarantees comparable to the state - of - the - art	0	6	4	-5.9013324	5.145257	1
248-8-28	[CLS] in the present paper, we bridge this gap, by proposing several new algorithms ( including greedy, majorization - minimization, minorization - maximization, and relaxation algorithms ) that not only scale to large datasets but that also achieve theoretical approximation guarantees comparable to	we empirically demonstrate the efficacy of our algorithms on real-world problems involving data partitioning for distributed optimization ( of convex and deep neural network objectives ) , and also purely unsupervised image segmentation .	1	4	7	-5.9499884	5.2274194	1
248-8-28	we show that these problems have many applications in machine learning ( ml ) , including data partitioning and load balancing for distributed ml , data clustering , and image segmentation .	we moreover provide new scalable algorithms that apply to additive combinations of the robust and average-case objectives .	0	6	5	-5.9897223	5.217531	1
248-8-28	we empirically demonstrate the efficacy of our algorithms on real-world problems involving data partitioning for distributed optimization ( of convex and deep neural network objectives ) , and also purely unsupervised image segmentation .	we moreover provide new scalable algorithms that apply to additive combinations of the robust and average-case objectives .	0	7	5	4.0387335	-3.6849377	0
248-8-28	we empirically demonstrate the efficacy of our algorithms on real-world problems involving data partitioning for distributed optimization ( of convex and deep neural network objectives ) , and also purely unsupervised image segmentation .	we show that these problems have many applications in machine learning ( ml ) , including data partitioning and load balancing for distributed ml , data clustering , and image segmentation .	0	7	6	5.4862967	-4.8423595	0
249-8-28	tractable learning aims to learn probabilistic models where inference is guaranteed to be efficient .	however , the particular class of queries that is tractable depends on the model and underlying representation .	1	0	1	-5.7884693	5.2154403	1
249-8-28	tractable learning aims to learn probabilistic models where inference is guaranteed to be efficient .	usually this class is mpe or conditional probabilities pr ( x|y ) for joint assignments x , y .	1	0	2	-5.6344285	5.152507	1
249-8-28	tractable learning aims to learn probabilistic models where inference is guaranteed to be efficient .	we propose a tractable learner that guarantees efficient inference for a broader class of queries .	1	0	3	-5.8473887	5.234482	1
249-8-28	it simultaneously learns a markov network and its tractable circuit representation , in order to guarantee and measure tractability .	tractable learning aims to learn probabilistic models where inference is guaranteed to be efficient .	0	4	0	5.5225005	-4.9277325	0
249-8-28	tractable learning aims to learn probabilistic models where inference is guaranteed to be efficient .	our approach differs from earlier work by using sentential decision diagrams ( sdd ) as the tractable language instead of arithmetic circuits ( ac ) .	1	0	5	-5.869035	5.207368	1
249-8-28	sdds have desirable properties , which more general representations such as acs lack , that enable basic primitives for boolean circuit compilation .	tractable learning aims to learn probabilistic models where inference is guaranteed to be efficient .	0	6	0	5.674574	-5.05794	0
249-8-28	tractable learning aims to learn probabilistic models where inference is guaranteed to be efficient .	this allows us to support a broader class of complex probability queries , including counting , threshold , and parity , in polytime .	1	0	7	-5.8937807	5.1807947	1
249-8-28	however , the particular class of queries that is tractable depends on the model and underlying representation .	usually this class is mpe or conditional probabilities pr ( x|y ) for joint assignments x , y .	1	1	2	3.613947	-3.256007	0
249-8-28	we propose a tractable learner that guarantees efficient inference for a broader class of queries .	however , the particular class of queries that is tractable depends on the model and underlying representation .	0	3	1	4.9086657	-4.3980665	0
249-8-28	however , the particular class of queries that is tractable depends on the model and underlying representation .	it simultaneously learns a markov network and its tractable circuit representation , in order to guarantee and measure tractability .	1	1	4	-5.5933294	5.081863	1
249-8-28	our approach differs from earlier work by using sentential decision diagrams ( sdd ) as the tractable language instead of arithmetic circuits ( ac ) .	however , the particular class of queries that is tractable depends on the model and underlying representation .	0	5	1	4.722811	-4.222649	0
249-8-28	however , the particular class of queries that is tractable depends on the model and underlying representation .	sdds have desirable properties , which more general representations such as acs lack , that enable basic primitives for boolean circuit compilation .	1	1	6	-2.8759344	2.9079804	1
249-8-28	however , the particular class of queries that is tractable depends on the model and underlying representation .	this allows us to support a broader class of complex probability queries , including counting , threshold , and parity , in polytime .	1	1	7	-5.9532814	5.098101	1
249-8-28	we propose a tractable learner that guarantees efficient inference for a broader class of queries .	usually this class is mpe or conditional probabilities pr ( x|y ) for joint assignments x , y .	0	3	2	4.910949	-4.3895917	0
249-8-28	it simultaneously learns a markov network and its tractable circuit representation , in order to guarantee and measure tractability .	usually this class is mpe or conditional probabilities pr ( x|y ) for joint assignments x , y .	0	4	2	5.3760977	-4.7566223	0
249-8-28	usually this class is mpe or conditional probabilities pr ( x|y ) for joint assignments x , y .	our approach differs from earlier work by using sentential decision diagrams ( sdd ) as the tractable language instead of arithmetic circuits ( ac ) .	1	2	5	-5.933053	5.002404	1
249-8-28	usually this class is mpe or conditional probabilities pr ( x|y ) for joint assignments x , y .	sdds have desirable properties , which more general representations such as acs lack , that enable basic primitives for boolean circuit compilation .	1	2	6	-4.957385	4.552951	1
249-8-28	this allows us to support a broader class of complex probability queries , including counting , threshold , and parity , in polytime .	usually this class is mpe or conditional probabilities pr ( x|y ) for joint assignments x , y .	0	7	2	5.39023	-4.721838	0
249-8-28	we propose a tractable learner that guarantees efficient inference for a broader class of queries .	it simultaneously learns a markov network and its tractable circuit representation , in order to guarantee and measure tractability .	1	3	4	-2.8183403	2.7281108	1
249-8-28	our approach differs from earlier work by using sentential decision diagrams ( sdd ) as the tractable language instead of arithmetic circuits ( ac ) .	we propose a tractable learner that guarantees efficient inference for a broader class of queries .	0	5	3	3.8799944	-3.4699118	0
249-8-28	sdds have desirable properties , which more general representations such as acs lack , that enable basic primitives for boolean circuit compilation .	we propose a tractable learner that guarantees efficient inference for a broader class of queries .	0	6	3	-3.9372938	3.8096561	1
249-8-28	this allows us to support a broader class of complex probability queries , including counting , threshold , and parity , in polytime .	we propose a tractable learner that guarantees efficient inference for a broader class of queries .	0	7	3	4.028575	-3.6197357	0
249-8-28	it simultaneously learns a markov network and its tractable circuit representation , in order to guarantee and measure tractability .	our approach differs from earlier work by using sentential decision diagrams ( sdd ) as the tractable language instead of arithmetic circuits ( ac ) .	1	4	5	1.9270179	-1.8190839	0
249-8-28	it simultaneously learns a markov network and its tractable circuit representation , in order to guarantee and measure tractability .	sdds have desirable properties , which more general representations such as acs lack , that enable basic primitives for boolean circuit compilation .	1	4	6	2.9930575	-2.808567	0
249-8-28	this allows us to support a broader class of complex probability queries , including counting , threshold , and parity , in polytime .	it simultaneously learns a markov network and its tractable circuit representation , in order to guarantee and measure tractability .	0	7	4	3.3053975	-3.1003268	0
249-8-28	sdds have desirable properties , which more general representations such as acs lack , that enable basic primitives for boolean circuit compilation .	our approach differs from earlier work by using sentential decision diagrams ( sdd ) as the tractable language instead of arithmetic circuits ( ac ) .	0	6	5	4.873497	-4.3353267	0
249-8-28	our approach differs from earlier work by using sentential decision diagrams ( sdd ) as the tractable language instead of arithmetic circuits ( ac ) .	this allows us to support a broader class of complex probability queries , including counting , threshold , and parity , in polytime .	1	5	7	-5.418188	4.924942	1
249-8-28	sdds have desirable properties , which more general representations such as acs lack , that enable basic primitives for boolean circuit compilation .	this allows us to support a broader class of complex probability queries , including counting , threshold , and parity , in polytime .	1	6	7	-5.8791556	5.167775	1
250-3-3	we first show that the convergence rate of these methods can be preserved under a decreasing sequence of errors in the control variate , and use this to derive variants of svrg that use growing-batch strategies to reduce the number of gradient calculations required in the early iterations .	we present and analyze several strategies for improving the performance of stochastic variance-reduced gradient ( svrg ) methods .	0	1	0	5.6975484	-5.094137	0
250-3-3	we present and analyze several strategies for improving the performance of stochastic variance-reduced gradient ( svrg ) methods .	we further ( i ) show how to exploit support vectors to reduce the number of gradient computations in the later iterations , ( ii ) prove that the commonly-used regularized svrg iteration is justified and improves the convergence rate , ( iii ) consider alternate mini-batch selection strategies , and ( iv ) consider the generalization error of the method .	1	0	2	-5.774193	5.1792	1
250-3-3	[CLS] we further ( i ) show how to exploit support vectors to reduce the number of gradient computations in the later iterations, ( ii ) prove that the commonly - used regularized svrg iteration is justified and improves the convergence rate, ( iii	[CLS] we first show that the convergence rate of these methods can be preserved under a decreasing sequence of errors in the control variate, and use this to derive variants of svrg that use growing - batch strategies to reduce the number of gradient calculations required in	0	2	1	5.0901966	-4.478942	0
251-5-10	we present the mind the gap model ( mgm ) , an approach for interpretable feature extraction and selection .	by placing interpretability criteria directly into the model , we allow for the model to both optimize parameters related to interpretability and to directly report a global set of distinguishable dimensions to assist with further data exploration and hypothesis generation .	1	0	1	-5.906031	5.2590365	1
251-5-10	mgm extracts distinguishing features on real-world datasets of animal features , recipes ingredients , and disease co-occurrence .	we present the mind the gap model ( mgm ) , an approach for interpretable feature extraction and selection .	0	2	0	5.677023	-5.0636415	0
251-5-10	we present the mind the gap model ( mgm ) , an approach for interpretable feature extraction and selection .	it also maintains or improves performance when compared to related approaches .	1	0	3	-5.815699	5.2069135	1
251-5-10	we present the mind the gap model ( mgm ) , an approach for interpretable feature extraction and selection .	we perform a user study with domain experts to show the mgm 's ability to help with dataset exploration .	1	0	4	-5.888194	5.225136	1
251-5-10	mgm extracts distinguishing features on real-world datasets of animal features , recipes ingredients , and disease co-occurrence .	by placing interpretability criteria directly into the model , we allow for the model to both optimize parameters related to interpretability and to directly report a global set of distinguishable dimensions to assist with further data exploration and hypothesis generation .	0	2	1	-2.2407203	2.3960848	1
251-5-10	it also maintains or improves performance when compared to related approaches .	by placing interpretability criteria directly into the model , we allow for the model to both optimize parameters related to interpretability and to directly report a global set of distinguishable dimensions to assist with further data exploration and hypothesis generation .	0	3	1	3.698366	-3.463688	0
251-5-10	we perform a user study with domain experts to show the mgm 's ability to help with dataset exploration .	by placing interpretability criteria directly into the model , we allow for the model to both optimize parameters related to interpretability and to directly report a global set of distinguishable dimensions to assist with further data exploration and hypothesis generation .	0	4	1	-0.633659	0.9402858	1
251-5-10	mgm extracts distinguishing features on real-world datasets of animal features , recipes ingredients , and disease co-occurrence .	it also maintains or improves performance when compared to related approaches .	1	2	3	-5.125813	4.6705704	1
251-5-10	we perform a user study with domain experts to show the mgm 's ability to help with dataset exploration .	mgm extracts distinguishing features on real-world datasets of animal features , recipes ingredients , and disease co-occurrence .	0	4	2	4.1205826	-3.754178	0
251-5-10	it also maintains or improves performance when compared to related approaches .	we perform a user study with domain experts to show the mgm 's ability to help with dataset exploration .	1	3	4	2.4001522	-2.2403405	0
252-10-45	because such analysis begins with dimensionality reduction , modeling early sensory processing requires biologically plausible online dimensionality reduction algorithms .	to make sense of the world our brains must analyze high-dimensional datasets streamed by our sensory organs .	0	1	0	5.058611	-4.3367047	0
252-10-45	to make sense of the world our brains must analyze high-dimensional datasets streamed by our sensory organs .	recently , we derived such an algorithm , termed similarity matching , from a multidimensional scaling ( mds ) objective function .	1	0	2	-5.801672	5.193452	1
252-10-45	however , in the existing algorithm , the number of output dimensions is set a priori by the number of output neurons and can not be changed .	to make sense of the world our brains must analyze high-dimensional datasets streamed by our sensory organs .	0	3	0	5.2758665	-4.6272554	0
252-10-45	because the number of informative dimensions in sensory inputs is variable there is a need for adaptive dimensionality reduction .	to make sense of the world our brains must analyze high-dimensional datasets streamed by our sensory organs .	0	4	0	4.765007	-4.1666403	0
252-10-45	here , we derive biologically plausible dimensionality reduction algorithms which adapt the number of output dimensions to the eigenspectrum of the input covariance matrix .	to make sense of the world our brains must analyze high-dimensional datasets streamed by our sensory organs .	0	5	0	5.482088	-4.7798023	0
252-10-45	to make sense of the world our brains must analyze high-dimensional datasets streamed by our sensory organs .	we formulate three objective functions which , in the offline setting , are optimized by the projections of the input dataset onto its principal subspace scaled by the eigenvalues of the output covariance matrix .	1	0	6	-5.8525395	5.1741056	1
252-10-45	to make sense of the world our brains must analyze high-dimensional datasets streamed by our sensory organs .	in turn , the output eigenvalues are computed as i ) soft-thresholded , ii ) hard-thresholded , iii ) equalized thresholded eigenvalues of the input covariance matrix .	1	0	7	-5.8631177	5.1762013	1
252-10-45	to make sense of the world our brains must analyze high-dimensional datasets streamed by our sensory organs .	in the online setting , we derive the three corresponding adaptive algorithms and map them onto the dynamics of neuronal activity in networks with biologically plausible local learning rules .	1	0	8	-5.90504	5.061604	1
252-10-45	to make sense of the world our brains must analyze high-dimensional datasets streamed by our sensory organs .	remarkably , in the last two networks , neurons are divided into two classes which we identify with principal neurons and interneurons in biological circuits .	1	0	9	-5.960746	5.167378	1
252-10-45	because such analysis begins with dimensionality reduction , modeling early sensory processing requires biologically plausible online dimensionality reduction algorithms .	recently , we derived such an algorithm , termed similarity matching , from a multidimensional scaling ( mds ) objective function .	1	1	2	-5.800129	5.2760777	1
252-10-45	however , in the existing algorithm , the number of output dimensions is set a priori by the number of output neurons and can not be changed .	because such analysis begins with dimensionality reduction , modeling early sensory processing requires biologically plausible online dimensionality reduction algorithms .	0	3	1	5.32679	-4.6641874	0
252-10-45	because such analysis begins with dimensionality reduction , modeling early sensory processing requires biologically plausible online dimensionality reduction algorithms .	because the number of informative dimensions in sensory inputs is variable there is a need for adaptive dimensionality reduction .	1	1	4	-4.0244923	3.7499063	1
252-10-45	here , we derive biologically plausible dimensionality reduction algorithms which adapt the number of output dimensions to the eigenspectrum of the input covariance matrix .	because such analysis begins with dimensionality reduction , modeling early sensory processing requires biologically plausible online dimensionality reduction algorithms .	0	5	1	5.5150566	-4.897809	0
252-10-45	we formulate three objective functions which , in the offline setting , are optimized by the projections of the input dataset onto its principal subspace scaled by the eigenvalues of the output covariance matrix .	because such analysis begins with dimensionality reduction , modeling early sensory processing requires biologically plausible online dimensionality reduction algorithms .	0	6	1	5.0378027	-4.544187	0
252-10-45	in turn , the output eigenvalues are computed as i ) soft-thresholded , ii ) hard-thresholded , iii ) equalized thresholded eigenvalues of the input covariance matrix .	because such analysis begins with dimensionality reduction , modeling early sensory processing requires biologically plausible online dimensionality reduction algorithms .	0	7	1	5.5370755	-4.968194	0
252-10-45	because such analysis begins with dimensionality reduction , modeling early sensory processing requires biologically plausible online dimensionality reduction algorithms .	in the online setting , we derive the three corresponding adaptive algorithms and map them onto the dynamics of neuronal activity in networks with biologically plausible local learning rules .	1	1	8	-5.963731	5.198965	1
252-10-45	because such analysis begins with dimensionality reduction , modeling early sensory processing requires biologically plausible online dimensionality reduction algorithms .	remarkably , in the last two networks , neurons are divided into two classes which we identify with principal neurons and interneurons in biological circuits .	1	1	9	-5.9708886	5.2159977	1
252-10-45	however , in the existing algorithm , the number of output dimensions is set a priori by the number of output neurons and can not be changed .	recently , we derived such an algorithm , termed similarity matching , from a multidimensional scaling ( mds ) objective function .	0	3	2	0.39478198	-0.06822692	0
252-10-45	recently , we derived such an algorithm , termed similarity matching , from a multidimensional scaling ( mds ) objective function .	because the number of informative dimensions in sensory inputs is variable there is a need for adaptive dimensionality reduction .	1	2	4	4.468714	-4.0216584	0
252-10-45	here , we derive biologically plausible dimensionality reduction algorithms which adapt the number of output dimensions to the eigenspectrum of the input covariance matrix .	recently , we derived such an algorithm , termed similarity matching , from a multidimensional scaling ( mds ) objective function .	0	5	2	3.804666	-3.4748092	0
252-10-45	recently , we derived such an algorithm , termed similarity matching , from a multidimensional scaling ( mds ) objective function .	we formulate three objective functions which , in the offline setting , are optimized by the projections of the input dataset onto its principal subspace scaled by the eigenvalues of the output covariance matrix .	1	2	6	-5.4340596	5.0466175	1
252-10-45	in turn , the output eigenvalues are computed as i ) soft-thresholded , ii ) hard-thresholded , iii ) equalized thresholded eigenvalues of the input covariance matrix .	recently , we derived such an algorithm , termed similarity matching , from a multidimensional scaling ( mds ) objective function .	0	7	2	4.698124	-4.185939	0
252-10-45	in the online setting , we derive the three corresponding adaptive algorithms and map them onto the dynamics of neuronal activity in networks with biologically plausible local learning rules .	recently , we derived such an algorithm , termed similarity matching , from a multidimensional scaling ( mds ) objective function .	0	8	2	5.112336	-4.4670486	0
252-10-45	recently , we derived such an algorithm , termed similarity matching , from a multidimensional scaling ( mds ) objective function .	remarkably , in the last two networks , neurons are divided into two classes which we identify with principal neurons and interneurons in biological circuits .	1	2	9	-5.8733234	5.2342234	1
252-10-45	however , in the existing algorithm , the number of output dimensions is set a priori by the number of output neurons and can not be changed .	because the number of informative dimensions in sensory inputs is variable there is a need for adaptive dimensionality reduction .	1	3	4	4.9904423	-4.3705177	0
252-10-45	however , in the existing algorithm , the number of output dimensions is set a priori by the number of output neurons and can not be changed .	here , we derive biologically plausible dimensionality reduction algorithms which adapt the number of output dimensions to the eigenspectrum of the input covariance matrix .	1	3	5	-5.4907036	5.119861	1
252-10-45	we formulate three objective functions which , in the offline setting , are optimized by the projections of the input dataset onto its principal subspace scaled by the eigenvalues of the output covariance matrix .	however , in the existing algorithm , the number of output dimensions is set a priori by the number of output neurons and can not be changed .	0	6	3	1.8032615	-1.6544003	0
252-10-45	in turn , the output eigenvalues are computed as i ) soft-thresholded , ii ) hard-thresholded , iii ) equalized thresholded eigenvalues of the input covariance matrix .	however , in the existing algorithm , the number of output dimensions is set a priori by the number of output neurons and can not be changed .	0	7	3	3.662949	-3.3931088	0
252-10-45	however , in the existing algorithm , the number of output dimensions is set a priori by the number of output neurons and can not be changed .	in the online setting , we derive the three corresponding adaptive algorithms and map them onto the dynamics of neuronal activity in networks with biologically plausible local learning rules .	1	3	8	-5.596695	5.12188	1
252-10-45	remarkably , in the last two networks , neurons are divided into two classes which we identify with principal neurons and interneurons in biological circuits .	however , in the existing algorithm , the number of output dimensions is set a priori by the number of output neurons and can not be changed .	0	9	3	3.677723	-3.3497663	0
252-10-45	because the number of informative dimensions in sensory inputs is variable there is a need for adaptive dimensionality reduction .	here , we derive biologically plausible dimensionality reduction algorithms which adapt the number of output dimensions to the eigenspectrum of the input covariance matrix .	1	4	5	-5.8803873	5.2555346	1
252-10-45	we formulate three objective functions which , in the offline setting , are optimized by the projections of the input dataset onto its principal subspace scaled by the eigenvalues of the output covariance matrix .	because the number of informative dimensions in sensory inputs is variable there is a need for adaptive dimensionality reduction .	0	6	4	4.8647947	-4.3946404	0
252-10-45	in turn , the output eigenvalues are computed as i ) soft-thresholded , ii ) hard-thresholded , iii ) equalized thresholded eigenvalues of the input covariance matrix .	because the number of informative dimensions in sensory inputs is variable there is a need for adaptive dimensionality reduction .	0	7	4	5.1264133	-4.604642	0
252-10-45	because the number of informative dimensions in sensory inputs is variable there is a need for adaptive dimensionality reduction .	in the online setting , we derive the three corresponding adaptive algorithms and map them onto the dynamics of neuronal activity in networks with biologically plausible local learning rules .	1	4	8	-5.943618	5.219417	1
252-10-45	remarkably , in the last two networks , neurons are divided into two classes which we identify with principal neurons and interneurons in biological circuits .	because the number of informative dimensions in sensory inputs is variable there is a need for adaptive dimensionality reduction .	0	9	4	5.15477	-4.5323143	0
252-10-45	here , we derive biologically plausible dimensionality reduction algorithms which adapt the number of output dimensions to the eigenspectrum of the input covariance matrix .	we formulate three objective functions which , in the offline setting , are optimized by the projections of the input dataset onto its principal subspace scaled by the eigenvalues of the output covariance matrix .	1	5	6	-2.6262107	2.620232	1
252-10-45	here , we derive biologically plausible dimensionality reduction algorithms which adapt the number of output dimensions to the eigenspectrum of the input covariance matrix .	in turn , the output eigenvalues are computed as i ) soft-thresholded , ii ) hard-thresholded , iii ) equalized thresholded eigenvalues of the input covariance matrix .	1	5	7	-5.4519773	4.9992514	1
252-10-45	here , we derive biologically plausible dimensionality reduction algorithms which adapt the number of output dimensions to the eigenspectrum of the input covariance matrix .	in the online setting , we derive the three corresponding adaptive algorithms and map them onto the dynamics of neuronal activity in networks with biologically plausible local learning rules .	1	5	8	-5.72771	5.197937	1
252-10-45	remarkably , in the last two networks , neurons are divided into two classes which we identify with principal neurons and interneurons in biological circuits .	here , we derive biologically plausible dimensionality reduction algorithms which adapt the number of output dimensions to the eigenspectrum of the input covariance matrix .	0	9	5	3.5045996	-3.2173495	0
252-10-45	in turn , the output eigenvalues are computed as i ) soft-thresholded , ii ) hard-thresholded , iii ) equalized thresholded eigenvalues of the input covariance matrix .	we formulate three objective functions which , in the offline setting , are optimized by the projections of the input dataset onto its principal subspace scaled by the eigenvalues of the output covariance matrix .	0	7	6	4.8110347	-4.3390055	0
252-10-45	we formulate three objective functions which , in the offline setting , are optimized by the projections of the input dataset onto its principal subspace scaled by the eigenvalues of the output covariance matrix .	in the online setting , we derive the three corresponding adaptive algorithms and map them onto the dynamics of neuronal activity in networks with biologically plausible local learning rules .	1	6	8	-5.51138	4.993577	1
252-10-45	remarkably , in the last two networks , neurons are divided into two classes which we identify with principal neurons and interneurons in biological circuits .	we formulate three objective functions which , in the offline setting , are optimized by the projections of the input dataset onto its principal subspace scaled by the eigenvalues of the output covariance matrix .	0	9	6	3.8365664	-3.5363774	0
252-10-45	in turn , the output eigenvalues are computed as i ) soft-thresholded , ii ) hard-thresholded , iii ) equalized thresholded eigenvalues of the input covariance matrix .	in the online setting , we derive the three corresponding adaptive algorithms and map them onto the dynamics of neuronal activity in networks with biologically plausible local learning rules .	1	7	8	-1.4648132	1.6632576	1
252-10-45	in turn , the output eigenvalues are computed as i ) soft-thresholded , ii ) hard-thresholded , iii ) equalized thresholded eigenvalues of the input covariance matrix .	remarkably , in the last two networks , neurons are divided into two classes which we identify with principal neurons and interneurons in biological circuits .	1	7	9	-2.1591835	2.3306084	1
252-10-45	remarkably , in the last two networks , neurons are divided into two classes which we identify with principal neurons and interneurons in biological circuits .	in the online setting , we derive the three corresponding adaptive algorithms and map them onto the dynamics of neuronal activity in networks with biologically plausible local learning rules .	0	9	8	3.9013953	-3.5394373	0
253-7-21	while finite-time convergence properties of the sgld with a 1st-order euler integrator have recently been studied , corresponding theory for general sg-mcmcs has not been explored .	[CLS] recent advances in bayesian learning with large - scale data have witnessed emergence of stochastic gradient mcmc algorithms ( sg - mcmc ), such as stochastic gradient langevin dynamics ( sgld ), stochastic gradient hamiltonian mcmc ( sghmc ), and	0	1	0	4.6538644	-4.077549	0
253-7-21	[CLS] recent advances in bayesian learning with large - scale data have witnessed emergence of stochastic gradient mcmc algorithms ( sg - mcmc ), such as stochastic gradient langevin dynamics ( sgld ), stochastic gradient hamiltonian mcmc ( sghmc ), and	in this paper we consider general sg-mcmcs with high-order integrators , and develop theory to analyze finite-time convergence properties and their asymptotic invariant measures .	1	0	2	-5.969694	5.1049767	1
253-7-21	our theoretical results show faster convergence rates and more accurate invariant measures for sg-mcmcs with higher-order integrators .	recent advances in bayesian learning with large-scale data have witnessed emergence of stochastic gradient mcmc algorithms ( sg-mcmc ) , such as stochastic gradient langevin dynamics ( sgld ) , stochastic gradient hamiltonian mcmc ( sghmc ) , and the stochastic gradient thermostat .	0	3	0	5.6288195	-4.9980354	0
253-7-21	[CLS] for example, with the proposed efficient 2nd - order symmetric splitting integrator, the mean square error ( mse ) of the posterior average for the sghmc achieves an optimal convergence rate of l - 4 / 5 at l iterations	[CLS] recent advances in bayesian learning with large - scale data have witnessed emergence of stochastic gradient mcmc algorithms ( sg - mcmc ), such as stochastic gradient langevin dynamics ( sgld ), stochastic gradient hamiltonian	0	4	0	4.939763	-4.344941	0
253-7-21	[CLS] recent advances in bayesian learning with large - scale data have witnessed emergence of stochastic gradient mcmc algorithms ( sg - mcmc ), such as stochastic gradient langevin dynamics ( sgld ), stochastic gradient hamiltonian mcmc ( sghmc ), and the	furthermore , convergence results of decreasing-step-size sg-mcmcs are also developed , with the same convergence rates as their fixed-step-size counterparts for a specific decreasing sequence .	1	0	5	-5.954978	5.162054	1
253-7-21	experiments on both synthetic and real datasets verify our theory , and show advantages of the proposed method in two large-scale real applications .	recent advances in bayesian learning with large-scale data have witnessed emergence of stochastic gradient mcmc algorithms ( sg-mcmc ) , such as stochastic gradient langevin dynamics ( sgld ) , stochastic gradient hamiltonian mcmc ( sghmc ) , and the stochastic gradient thermostat .	0	6	0	5.6647744	-5.050564	0
253-7-21	while finite-time convergence properties of the sgld with a 1st-order euler integrator have recently been studied , corresponding theory for general sg-mcmcs has not been explored .	in this paper we consider general sg-mcmcs with high-order integrators , and develop theory to analyze finite-time convergence properties and their asymptotic invariant measures .	1	1	2	-5.3946633	5.020771	1
253-7-21	our theoretical results show faster convergence rates and more accurate invariant measures for sg-mcmcs with higher-order integrators .	while finite-time convergence properties of the sgld with a 1st-order euler integrator have recently been studied , corresponding theory for general sg-mcmcs has not been explored .	0	3	1	5.485216	-4.85852	0
253-7-21	while finite-time convergence properties of the sgld with a 1st-order euler integrator have recently been studied , corresponding theory for general sg-mcmcs has not been explored .	[CLS] for example, with the proposed efficient 2nd - order symmetric splitting integrator, the mean square error ( mse ) of the posterior average for the sghmc achieves an optimal convergence rate of l - 4 / 5 at l iterations, compared to l - 2 / 3 for	1	1	4	-5.9016805	5.2439938	1
253-7-21	furthermore , convergence results of decreasing-step-size sg-mcmcs are also developed , with the same convergence rates as their fixed-step-size counterparts for a specific decreasing sequence .	while finite-time convergence properties of the sgld with a 1st-order euler integrator have recently been studied , corresponding theory for general sg-mcmcs has not been explored .	0	5	1	5.0790825	-4.495983	0
253-7-21	experiments on both synthetic and real datasets verify our theory , and show advantages of the proposed method in two large-scale real applications .	while finite-time convergence properties of the sgld with a 1st-order euler integrator have recently been studied , corresponding theory for general sg-mcmcs has not been explored .	0	6	1	5.5607705	-4.956522	0
253-7-21	our theoretical results show faster convergence rates and more accurate invariant measures for sg-mcmcs with higher-order integrators .	in this paper we consider general sg-mcmcs with high-order integrators , and develop theory to analyze finite-time convergence properties and their asymptotic invariant measures .	0	3	2	5.534947	-4.9432364	0
253-7-21	[CLS] for example, with the proposed efficient 2nd - order symmetric splitting integrator, the mean square error ( mse ) of the posterior average for the sghmc achieves an optimal convergence rate of l - 4 / 5 at l iterations, compared to l - 2 / 3 for	in this paper we consider general sg-mcmcs with high-order integrators , and develop theory to analyze finite-time convergence properties and their asymptotic invariant measures .	0	4	2	4.655701	-4.139181	0
253-7-21	in this paper we consider general sg-mcmcs with high-order integrators , and develop theory to analyze finite-time convergence properties and their asymptotic invariant measures .	furthermore , convergence results of decreasing-step-size sg-mcmcs are also developed , with the same convergence rates as their fixed-step-size counterparts for a specific decreasing sequence .	1	2	5	-5.700595	5.0983224	1
253-7-21	experiments on both synthetic and real datasets verify our theory , and show advantages of the proposed method in two large-scale real applications .	in this paper we consider general sg-mcmcs with high-order integrators , and develop theory to analyze finite-time convergence properties and their asymptotic invariant measures .	0	6	2	5.617239	-5.0091853	0
253-7-21	our theoretical results show faster convergence rates and more accurate invariant measures for sg-mcmcs with higher-order integrators .	[CLS] for example, with the proposed efficient 2nd - order symmetric splitting integrator, the mean square error ( mse ) of the posterior average for the sghmc achieves an optimal convergence rate of l - 4 / 5 at l iterations, compared to l - 2 / 3 for the sghmc and sgld with 1st - order eu	1	3	4	2.0021124	-1.7563032	0
253-7-21	our theoretical results show faster convergence rates and more accurate invariant measures for sg-mcmcs with higher-order integrators .	furthermore , convergence results of decreasing-step-size sg-mcmcs are also developed , with the same convergence rates as their fixed-step-size counterparts for a specific decreasing sequence .	1	3	5	3.7586277	-3.507358	0
253-7-21	experiments on both synthetic and real datasets verify our theory , and show advantages of the proposed method in two large-scale real applications .	our theoretical results show faster convergence rates and more accurate invariant measures for sg-mcmcs with higher-order integrators .	0	6	3	2.1017337	-1.8262417	0
253-7-21	[CLS] for example, with the proposed efficient 2nd - order symmetric splitting integrator, the mean square error ( mse ) of the posterior average for the sghmc achieves an optimal convergence rate of l - 4 / 5 at l iterations, compared to l - 2 / 3 for the	furthermore , convergence results of decreasing-step-size sg-mcmcs are also developed , with the same convergence rates as their fixed-step-size counterparts for a specific decreasing sequence .	1	4	5	2.317382	-2.080418	0
253-7-21	experiments on both synthetic and real datasets verify our theory , and show advantages of the proposed method in two large-scale real applications .	[CLS] for example, with the proposed efficient 2nd - order symmetric splitting integrator, the mean square error ( mse ) of the posterior average for the sghmc achieves an optimal convergence rate of l - 4 / 5 at l iterations, compared to l - 2 / 3 for the sghmc and sgld with 1st - order	0	6	4	3.1652544	-2.9901235	0
253-7-21	experiments on both synthetic and real datasets verify our theory , and show advantages of the proposed method in two large-scale real applications .	furthermore , convergence results of decreasing-step-size sg-mcmcs are also developed , with the same convergence rates as their fixed-step-size counterparts for a specific decreasing sequence .	0	6	5	4.2382393	-3.9134278	0
254-8-28	current approaches are mainly focused on learning the structure under restrictive parametric assumptions , which limits the applicability of these methods .	learning the structure of a probabilistic graphical models is a well studied problem in the machine learning community due to its importance in many applications .	0	1	0	5.688612	-5.114279	0
254-8-28	in this paper , we study the problem of estimating the structure of a probabilistic graphical model without assuming a particular parametric model .	learning the structure of a probabilistic graphical models is a well studied problem in the machine learning community due to its importance in many applications .	0	2	0	5.677769	-5.0267763	0
254-8-28	learning the structure of a probabilistic graphical models is a well studied problem in the machine learning community due to its importance in many applications .	we consider probabilities that are members of an infinite dimensional exponential family , which is parametrized by a reproducing kernel hilbert space ( rkhs ) h and its kernel k. one difficulty in learning nonparametric densities is the evaluation of the normalizing constant .	1	0	3	-5.6417885	5.1446967	1
254-8-28	in order to avoid this issue , our procedure minimizes the penalized score matching objective .	learning the structure of a probabilistic graphical models is a well studied problem in the machine learning community due to its importance in many applications .	0	4	0	5.703072	-5.1223593	0
254-8-28	we show how to efficiently minimize the proposed objective using existing group lasso solvers .	learning the structure of a probabilistic graphical models is a well studied problem in the machine learning community due to its importance in many applications .	0	5	0	5.6331663	-5.051532	0
254-8-28	learning the structure of a probabilistic graphical models is a well studied problem in the machine learning community due to its importance in many applications .	furthermore , we prove that our procedure recovers the graph structure with high-probability under mild conditions .	1	0	6	-5.924527	5.145856	1
254-8-28	simulation studies illustrate ability of our procedure to recover the true graph structure without the knowledge of the data generating process .	learning the structure of a probabilistic graphical models is a well studied problem in the machine learning community due to its importance in many applications .	0	7	0	5.596246	-5.0444813	0
254-8-28	current approaches are mainly focused on learning the structure under restrictive parametric assumptions , which limits the applicability of these methods .	in this paper , we study the problem of estimating the structure of a probabilistic graphical model without assuming a particular parametric model .	1	1	2	-4.5993366	4.2971907	1
254-8-28	current approaches are mainly focused on learning the structure under restrictive parametric assumptions , which limits the applicability of these methods .	we consider probabilities that are members of an infinite dimensional exponential family , which is parametrized by a reproducing kernel hilbert space ( rkhs ) h and its kernel k. one difficulty in learning nonparametric densities is the evaluation of the normalizing constant .	1	1	3	2.9677043	-2.3546033	0
254-8-28	current approaches are mainly focused on learning the structure under restrictive parametric assumptions , which limits the applicability of these methods .	in order to avoid this issue , our procedure minimizes the penalized score matching objective .	1	1	4	-6.0034842	5.11334	1
254-8-28	current approaches are mainly focused on learning the structure under restrictive parametric assumptions , which limits the applicability of these methods .	we show how to efficiently minimize the proposed objective using existing group lasso solvers .	1	1	5	-5.952524	5.064375	1
254-8-28	current approaches are mainly focused on learning the structure under restrictive parametric assumptions , which limits the applicability of these methods .	furthermore , we prove that our procedure recovers the graph structure with high-probability under mild conditions .	1	1	6	-5.9695444	5.125285	1
254-8-28	simulation studies illustrate ability of our procedure to recover the true graph structure without the knowledge of the data generating process .	current approaches are mainly focused on learning the structure under restrictive parametric assumptions , which limits the applicability of these methods .	0	7	1	5.62696	-5.0249777	0
254-8-28	in this paper , we study the problem of estimating the structure of a probabilistic graphical model without assuming a particular parametric model .	we consider probabilities that are members of an infinite dimensional exponential family , which is parametrized by a reproducing kernel hilbert space ( rkhs ) h and its kernel k. one difficulty in learning nonparametric densities is the evaluation of the normalizing constant .	1	2	3	3.281138	-2.8032641	0
254-8-28	in this paper , we study the problem of estimating the structure of a probabilistic graphical model without assuming a particular parametric model .	in order to avoid this issue , our procedure minimizes the penalized score matching objective .	1	2	4	-5.9440327	5.230223	1
254-8-28	we show how to efficiently minimize the proposed objective using existing group lasso solvers .	in this paper , we study the problem of estimating the structure of a probabilistic graphical model without assuming a particular parametric model .	0	5	2	5.598944	-4.9985085	0
254-8-28	in this paper , we study the problem of estimating the structure of a probabilistic graphical model without assuming a particular parametric model .	furthermore , we prove that our procedure recovers the graph structure with high-probability under mild conditions .	1	2	6	-5.9836307	5.148542	1
254-8-28	simulation studies illustrate ability of our procedure to recover the true graph structure without the knowledge of the data generating process .	in this paper , we study the problem of estimating the structure of a probabilistic graphical model without assuming a particular parametric model .	0	7	2	5.613838	-5.057864	0
254-8-28	we consider probabilities that are members of an infinite dimensional exponential family , which is parametrized by a reproducing kernel hilbert space ( rkhs ) h and its kernel k. one difficulty in learning nonparametric densities is the evaluation of the normalizing constant .	in order to avoid this issue , our procedure minimizes the penalized score matching objective .	1	3	4	-6.006714	5.1875706	1
254-8-28	we consider probabilities that are members of an infinite dimensional exponential family , which is parametrized by a reproducing kernel hilbert space ( rkhs ) h and its kernel k. one difficulty in learning nonparametric densities is the evaluation of the normalizing constant .	we show how to efficiently minimize the proposed objective using existing group lasso solvers .	1	3	5	-5.9820614	5.1737347	1
254-8-28	we consider probabilities that are members of an infinite dimensional exponential family , which is parametrized by a reproducing kernel hilbert space ( rkhs ) h and its kernel k. one difficulty in learning nonparametric densities is the evaluation of the normalizing constant .	furthermore , we prove that our procedure recovers the graph structure with high-probability under mild conditions .	1	3	6	-5.979247	5.083386	1
254-8-28	simulation studies illustrate ability of our procedure to recover the true graph structure without the knowledge of the data generating process .	we consider probabilities that are members of an infinite dimensional exponential family , which is parametrized by a reproducing kernel hilbert space ( rkhs ) h and its kernel k. one difficulty in learning nonparametric densities is the evaluation of the normalizing constant .	0	7	3	5.5847025	-5.016366	0
254-8-28	in order to avoid this issue , our procedure minimizes the penalized score matching objective .	we show how to efficiently minimize the proposed objective using existing group lasso solvers .	1	4	5	-5.79992	5.114743	1
254-8-28	in order to avoid this issue , our procedure minimizes the penalized score matching objective .	furthermore , we prove that our procedure recovers the graph structure with high-probability under mild conditions .	1	4	6	-5.564297	5.0108995	1
254-8-28	simulation studies illustrate ability of our procedure to recover the true graph structure without the knowledge of the data generating process .	in order to avoid this issue , our procedure minimizes the penalized score matching objective .	0	7	4	5.0824575	-4.4981723	0
254-8-28	we show how to efficiently minimize the proposed objective using existing group lasso solvers .	furthermore , we prove that our procedure recovers the graph structure with high-probability under mild conditions .	1	5	6	-3.953696	3.7848673	1
254-8-28	simulation studies illustrate ability of our procedure to recover the true graph structure without the knowledge of the data generating process .	we show how to efficiently minimize the proposed objective using existing group lasso solvers .	0	7	5	4.3587303	-3.945683	0
254-8-28	simulation studies illustrate ability of our procedure to recover the true graph structure without the knowledge of the data generating process .	furthermore , we prove that our procedure recovers the graph structure with high-probability under mild conditions .	0	7	6	3.573801	-3.3480616	0
255-14-91	the answer can be a sentence , a phrase or a single word .	in this paper , we present the mqa model , which is able to answer questions about the content of an image .	0	1	0	2.0607028	-1.7965984	0
255-14-91	in this paper , we present the mqa model , which is able to answer questions about the content of an image .	our model contains four components : a long short-term memory ( lstm ) to extract the question representation , a convolutional neural network ( cnn ) to extract the visual representation , an lstm for storing the linguistic context in an answer , and a fusing component to combine the information from the first three components and generate the answer .	1	0	2	-5.737749	5.2265673	1
255-14-91	we construct a freestyle multilingual image question answering ( fm-iqa ) dataset to train and evaluate our mqa model .	in this paper , we present the mqa model , which is able to answer questions about the content of an image .	0	3	0	5.54939	-4.8684425	0
255-14-91	it contains over 150,000 images and 310,000 freestyle chinese question-answer pairs and their english translations .	in this paper , we present the mqa model , which is able to answer questions about the content of an image .	0	4	0	3.5209043	-3.2086596	0
255-14-91	in this paper , we present the mqa model , which is able to answer questions about the content of an image .	the quality of the generated answers of our mqa model on this dataset is evaluated by human judges through a turing test .	1	0	5	-5.919261	5.210476	1
255-14-91	specifically , we mix the answers provided by humans and our model .	in this paper , we present the mqa model , which is able to answer questions about the content of an image .	0	6	0	5.047806	-4.4095473	0
255-14-91	the human judges need to distinguish our model from the human .	in this paper , we present the mqa model , which is able to answer questions about the content of an image .	0	7	0	-4.9270444	4.6660357	1
255-14-91	in this paper , we present the mqa model , which is able to answer questions about the content of an image .	they will also provide a score ( i.e .	1	0	8	-4.8915586	4.543421	1
255-14-91	in this paper , we present the mqa model , which is able to answer questions about the content of an image .	0 , 1 , 2 , the larger the better ) indicating the quality of the answer .	1	0	9	-5.486533	4.9445496	1
255-14-91	in this paper , we present the mqa model , which is able to answer questions about the content of an image .	we propose strategies to monitor the quality of this evaluation process .	1	0	10	-3.7432508	3.5598004	1
255-14-91	the experiments show that in 64.7 % of cases , the human judges can not distinguish our model from humans .	in this paper , we present the mqa model , which is able to answer questions about the content of an image .	0	11	0	5.534135	-4.8840523	0
255-14-91	the average score is 1.454 ( 1.918 for human ) .	in this paper , we present the mqa model , which is able to answer questions about the content of an image .	0	12	0	4.380434	-4.0066166	0
255-14-91	the details of this work , including the fm-iqa dataset , can be found on the project page : http : //idl.baidu.com/fm-iqa.html .	in this paper , we present the mqa model , which is able to answer questions about the content of an image .	0	13	0	5.4454308	-4.7780385	0
255-14-91	our model contains four components : a long short-term memory ( lstm ) to extract the question representation , a convolutional neural network ( cnn ) to extract the visual representation , an lstm for storing the linguistic context in an answer , and a fusing component to combine the information from the first three components and generate the answer .	the answer can be a sentence , a phrase or a single word .	0	2	1	2.9929714	-2.6897826	0
255-14-91	we construct a freestyle multilingual image question answering ( fm-iqa ) dataset to train and evaluate our mqa model .	the answer can be a sentence , a phrase or a single word .	0	3	1	2.5792785	-2.3951278	0
255-14-91	the answer can be a sentence , a phrase or a single word .	it contains over 150,000 images and 310,000 freestyle chinese question-answer pairs and their english translations .	1	1	4	0.4790443	-0.16159603	0
255-14-91	the answer can be a sentence , a phrase or a single word .	the quality of the generated answers of our mqa model on this dataset is evaluated by human judges through a turing test .	1	1	5	-5.355727	4.8875127	1
255-14-91	specifically , we mix the answers provided by humans and our model .	the answer can be a sentence , a phrase or a single word .	0	6	1	-2.5545878	2.4956613	1
255-14-91	the answer can be a sentence , a phrase or a single word .	the human judges need to distinguish our model from the human .	1	1	7	2.93618	-2.6078467	0
255-14-91	they will also provide a score ( i.e .	the answer can be a sentence , a phrase or a single word .	0	8	1	3.5622246	-3.3773339	0
255-14-91	the answer can be a sentence , a phrase or a single word .	0 , 1 , 2 , the larger the better ) indicating the quality of the answer .	1	1	9	-5.0701895	4.6412926	1
255-14-91	the answer can be a sentence , a phrase or a single word .	we propose strategies to monitor the quality of this evaluation process .	1	1	10	-1.7259183	1.9218607	1
255-14-91	the experiments show that in 64.7 % of cases , the human judges can not distinguish our model from humans .	the answer can be a sentence , a phrase or a single word .	0	11	1	5.256977	-4.7259007	0
255-14-91	the average score is 1.454 ( 1.918 for human ) .	the answer can be a sentence , a phrase or a single word .	0	12	1	5.116329	-4.691223	0
255-14-91	the details of this work , including the fm-iqa dataset , can be found on the project page : http : //idl.baidu.com/fm-iqa.html .	the answer can be a sentence , a phrase or a single word .	0	13	1	5.3858023	-4.7890706	0
255-14-91	[CLS] our model contains four components : a long short - term memory ( lstm ) to extract the question representation, a convolutional neural network ( cnn ) to extract the visual representation, an lstm for storing the linguistic context in an answer, and a fusing component to combine the information from the first three components and generate the	we construct a freestyle multilingual image question answering ( fm-iqa ) dataset to train and evaluate our mqa model .	1	2	3	-3.5188632	3.3231204	1
255-14-91	it contains over 150,000 images and 310,000 freestyle chinese question-answer pairs and their english translations .	our model contains four components : a long short-term memory ( lstm ) to extract the question representation , a convolutional neural network ( cnn ) to extract the visual representation , an lstm for storing the linguistic context in an answer , and a fusing component to combine the information from the first three components and generate the answer .	0	4	2	1.5612864	-1.1078578	0
255-14-91	the quality of the generated answers of our mqa model on this dataset is evaluated by human judges through a turing test .	our model contains four components : a long short-term memory ( lstm ) to extract the question representation , a convolutional neural network ( cnn ) to extract the visual representation , an lstm for storing the linguistic context in an answer , and a fusing component to combine the information from the first three components and generate the answer .	0	5	2	5.181942	-4.573357	0
255-14-91	specifically , we mix the answers provided by humans and our model .	our model contains four components : a long short-term memory ( lstm ) to extract the question representation , a convolutional neural network ( cnn ) to extract the visual representation , an lstm for storing the linguistic context in an answer , and a fusing component to combine the information from the first three components and generate the answer .	0	6	2	-0.97777677	1.1141417	1
255-14-91	our model contains four components : a long short-term memory ( lstm ) to extract the question representation , a convolutional neural network ( cnn ) to extract the visual representation , an lstm for storing the linguistic context in an answer , and a fusing component to combine the information from the first three components and generate the answer .	the human judges need to distinguish our model from the human .	1	2	7	-1.2494369	1.5589268	1
255-14-91	our model contains four components : a long short-term memory ( lstm ) to extract the question representation , a convolutional neural network ( cnn ) to extract the visual representation , an lstm for storing the linguistic context in an answer , and a fusing component to combine the information from the first three components and generate the answer .	they will also provide a score ( i.e .	1	2	8	-3.523701	3.4015815	1
255-14-91	0 , 1 , 2 , the larger the better ) indicating the quality of the answer .	our model contains four components : a long short-term memory ( lstm ) to extract the question representation , a convolutional neural network ( cnn ) to extract the visual representation , an lstm for storing the linguistic context in an answer , and a fusing component to combine the information from the first three components and generate the answer .	0	9	2	4.2660217	-3.8688576	0
255-14-91	we propose strategies to monitor the quality of this evaluation process .	our model contains four components : a long short-term memory ( lstm ) to extract the question representation , a convolutional neural network ( cnn ) to extract the visual representation , an lstm for storing the linguistic context in an answer , and a fusing component to combine the information from the first three components and generate the answer .	0	10	2	-4.6176786	4.2902236	1
255-14-91	the experiments show that in 64.7 % of cases , the human judges can not distinguish our model from humans .	our model contains four components : a long short-term memory ( lstm ) to extract the question representation , a convolutional neural network ( cnn ) to extract the visual representation , an lstm for storing the linguistic context in an answer , and a fusing component to combine the information from the first three components and generate the answer .	0	11	2	5.377608	-4.7280984	0
255-14-91	the average score is 1.454 ( 1.918 for human ) .	our model contains four components : a long short-term memory ( lstm ) to extract the question representation , a convolutional neural network ( cnn ) to extract the visual representation , an lstm for storing the linguistic context in an answer , and a fusing component to combine the information from the first three components and generate the answer .	0	12	2	4.632889	-4.222231	0
255-14-91	[CLS] our model contains four components : a long short - term memory ( lstm ) to extract the question representation, a convolutional neural network ( cnn ) to extract the visual representation, an lstm for storing the linguistic context in an answer, and a	the details of this work , including the fm-iqa dataset , can be found on the project page : http : //idl.baidu.com/fm-iqa.html .	1	2	13	-5.9623966	5.103265	1
255-14-91	we construct a freestyle multilingual image question answering ( fm-iqa ) dataset to train and evaluate our mqa model .	it contains over 150,000 images and 310,000 freestyle chinese question-answer pairs and their english translations .	1	3	4	0.48045787	-0.09132582	0
255-14-91	we construct a freestyle multilingual image question answering ( fm-iqa ) dataset to train and evaluate our mqa model .	the quality of the generated answers of our mqa model on this dataset is evaluated by human judges through a turing test .	1	3	5	-5.307505	4.826713	1
255-14-91	specifically , we mix the answers provided by humans and our model .	we construct a freestyle multilingual image question answering ( fm-iqa ) dataset to train and evaluate our mqa model .	0	6	3	-3.2307324	3.205357	1
255-14-91	the human judges need to distinguish our model from the human .	we construct a freestyle multilingual image question answering ( fm-iqa ) dataset to train and evaluate our mqa model .	0	7	3	-5.8743677	5.1009073	1
255-14-91	they will also provide a score ( i.e .	we construct a freestyle multilingual image question answering ( fm-iqa ) dataset to train and evaluate our mqa model .	0	8	3	2.7490478	-2.4868631	0
255-14-91	we construct a freestyle multilingual image question answering ( fm-iqa ) dataset to train and evaluate our mqa model .	0 , 1 , 2 , the larger the better ) indicating the quality of the answer .	1	3	9	-2.917929	2.7599177	1
255-14-91	we construct a freestyle multilingual image question answering ( fm-iqa ) dataset to train and evaluate our mqa model .	we propose strategies to monitor the quality of this evaluation process .	1	3	10	4.287212	-3.842223	0
255-14-91	the experiments show that in 64.7 % of cases , the human judges can not distinguish our model from humans .	we construct a freestyle multilingual image question answering ( fm-iqa ) dataset to train and evaluate our mqa model .	0	11	3	4.7566524	-4.208437	0
255-14-91	we construct a freestyle multilingual image question answering ( fm-iqa ) dataset to train and evaluate our mqa model .	the average score is 1.454 ( 1.918 for human ) .	1	3	12	-2.631064	2.5597148	1
255-14-91	we construct a freestyle multilingual image question answering ( fm-iqa ) dataset to train and evaluate our mqa model .	the details of this work , including the fm-iqa dataset , can be found on the project page : http : //idl.baidu.com/fm-iqa.html .	1	3	13	-5.910397	4.9911213	1
255-14-91	it contains over 150,000 images and 310,000 freestyle chinese question-answer pairs and their english translations .	the quality of the generated answers of our mqa model on this dataset is evaluated by human judges through a turing test .	1	4	5	-4.978677	4.5379715	1
255-14-91	specifically , we mix the answers provided by humans and our model .	it contains over 150,000 images and 310,000 freestyle chinese question-answer pairs and their english translations .	0	6	4	-0.028471412	0.2874431	1
255-14-91	the human judges need to distinguish our model from the human .	it contains over 150,000 images and 310,000 freestyle chinese question-answer pairs and their english translations .	0	7	4	-3.9306855	3.783885	1
255-14-91	they will also provide a score ( i.e .	it contains over 150,000 images and 310,000 freestyle chinese question-answer pairs and their english translations .	0	8	4	2.8566017	-2.598743	0
255-14-91	0 , 1 , 2 , the larger the better ) indicating the quality of the answer .	it contains over 150,000 images and 310,000 freestyle chinese question-answer pairs and their english translations .	0	9	4	3.3398752	-3.100749	0
255-14-91	we propose strategies to monitor the quality of this evaluation process .	it contains over 150,000 images and 310,000 freestyle chinese question-answer pairs and their english translations .	0	10	4	-1.1950355	1.453763	1
255-14-91	it contains over 150,000 images and 310,000 freestyle chinese question-answer pairs and their english translations .	the experiments show that in 64.7 % of cases , the human judges can not distinguish our model from humans .	1	4	11	-5.4612713	4.9536443	1
255-14-91	it contains over 150,000 images and 310,000 freestyle chinese question-answer pairs and their english translations .	the average score is 1.454 ( 1.918 for human ) .	1	4	12	-4.134046	3.8001177	1
255-14-91	the details of this work , including the fm-iqa dataset , can be found on the project page : http : //idl.baidu.com/fm-iqa.html .	it contains over 150,000 images and 310,000 freestyle chinese question-answer pairs and their english translations .	0	13	4	5.0118184	-4.505501	0
255-14-91	the quality of the generated answers of our mqa model on this dataset is evaluated by human judges through a turing test .	specifically , we mix the answers provided by humans and our model .	1	5	6	4.507058	-4.026692	0
255-14-91	the quality of the generated answers of our mqa model on this dataset is evaluated by human judges through a turing test .	the human judges need to distinguish our model from the human .	1	5	7	5.0308375	-4.423324	0
255-14-91	they will also provide a score ( i.e .	the quality of the generated answers of our mqa model on this dataset is evaluated by human judges through a turing test .	0	8	5	-3.0864573	3.0740743	1
255-14-91	0 , 1 , 2 , the larger the better ) indicating the quality of the answer .	the quality of the generated answers of our mqa model on this dataset is evaluated by human judges through a turing test .	0	9	5	-2.0840452	2.2056832	1
255-14-91	the quality of the generated answers of our mqa model on this dataset is evaluated by human judges through a turing test .	we propose strategies to monitor the quality of this evaluation process .	1	5	10	4.982752	-4.442977	0
255-14-91	the experiments show that in 64.7 % of cases , the human judges can not distinguish our model from humans .	the quality of the generated answers of our mqa model on this dataset is evaluated by human judges through a turing test .	0	11	5	4.9103336	-4.404816	0
255-14-91	the quality of the generated answers of our mqa model on this dataset is evaluated by human judges through a turing test .	the average score is 1.454 ( 1.918 for human ) .	1	5	12	-1.9342964	1.9635819	1
255-14-91	the details of this work , including the fm-iqa dataset , can be found on the project page : http : //idl.baidu.com/fm-iqa.html .	the quality of the generated answers of our mqa model on this dataset is evaluated by human judges through a turing test .	0	13	5	4.6580915	-4.2227125	0
255-14-91	the human judges need to distinguish our model from the human .	specifically , we mix the answers provided by humans and our model .	0	7	6	-5.180529	4.84348	1
255-14-91	specifically , we mix the answers provided by humans and our model .	they will also provide a score ( i.e .	1	6	8	-2.4809432	2.5352106	1
255-14-91	0 , 1 , 2 , the larger the better ) indicating the quality of the answer .	specifically , we mix the answers provided by humans and our model .	0	9	6	2.7503238	-2.5742054	0
255-14-91	we propose strategies to monitor the quality of this evaluation process .	specifically , we mix the answers provided by humans and our model .	0	10	6	-4.791943	4.498349	1
255-14-91	specifically , we mix the answers provided by humans and our model .	the experiments show that in 64.7 % of cases , the human judges can not distinguish our model from humans .	1	6	11	-5.7210083	5.037752	1
255-14-91	the average score is 1.454 ( 1.918 for human ) .	specifically , we mix the answers provided by humans and our model .	0	12	6	3.8095517	-3.5321486	0
255-14-91	specifically , we mix the answers provided by humans and our model .	the details of this work , including the fm-iqa dataset , can be found on the project page : http : //idl.baidu.com/fm-iqa.html .	1	6	13	-5.788394	4.8450584	1
255-14-91	the human judges need to distinguish our model from the human .	they will also provide a score ( i.e .	1	7	8	-5.617915	5.0319304	1
255-14-91	0 , 1 , 2 , the larger the better ) indicating the quality of the answer .	the human judges need to distinguish our model from the human .	0	9	7	2.8799944	-2.7292697	0
255-14-91	the human judges need to distinguish our model from the human .	we propose strategies to monitor the quality of this evaluation process .	1	7	10	-5.3176994	4.885066	1
255-14-91	the experiments show that in 64.7 % of cases , the human judges can not distinguish our model from humans .	the human judges need to distinguish our model from the human .	0	11	7	5.344944	-4.6868935	0
255-14-91	the human judges need to distinguish our model from the human .	the average score is 1.454 ( 1.918 for human ) .	1	7	12	-5.4234457	4.934102	1
255-14-91	the details of this work , including the fm-iqa dataset , can be found on the project page : http : //idl.baidu.com/fm-iqa.html .	the human judges need to distinguish our model from the human .	0	13	7	5.3680296	-4.719048	0
255-14-91	they will also provide a score ( i.e .	0 , 1 , 2 , the larger the better ) indicating the quality of the answer .	1	8	9	-2.8754492	2.7307436	1
255-14-91	they will also provide a score ( i.e .	we propose strategies to monitor the quality of this evaluation process .	1	8	10	3.1715226	-2.9219785	0
255-14-91	they will also provide a score ( i.e .	the experiments show that in 64.7 % of cases , the human judges can not distinguish our model from humans .	1	8	11	-3.4455338	3.4119945	1
255-14-91	the average score is 1.454 ( 1.918 for human ) .	they will also provide a score ( i.e .	0	12	8	3.0587478	-2.9262757	0
255-14-91	the details of this work , including the fm-iqa dataset , can be found on the project page : http : //idl.baidu.com/fm-iqa.html .	they will also provide a score ( i.e .	0	13	8	4.4477024	-4.010721	0
255-14-91	we propose strategies to monitor the quality of this evaluation process .	0 , 1 , 2 , the larger the better ) indicating the quality of the answer .	0	10	9	-4.794774	4.436865	1
255-14-91	the experiments show that in 64.7 % of cases , the human judges can not distinguish our model from humans .	0 , 1 , 2 , the larger the better ) indicating the quality of the answer .	0	11	9	3.2955542	-3.1454241	0
255-14-91	0 , 1 , 2 , the larger the better ) indicating the quality of the answer .	the average score is 1.454 ( 1.918 for human ) .	1	9	12	-0.48445368	0.6742586	1
255-14-91	the details of this work , including the fm-iqa dataset , can be found on the project page : http : //idl.baidu.com/fm-iqa.html .	0 , 1 , 2 , the larger the better ) indicating the quality of the answer .	0	13	9	5.025884	-4.595891	0
255-14-91	we propose strategies to monitor the quality of this evaluation process .	the experiments show that in 64.7 % of cases , the human judges can not distinguish our model from humans .	1	10	11	-5.911174	5.1698394	1
255-14-91	we propose strategies to monitor the quality of this evaluation process .	the average score is 1.454 ( 1.918 for human ) .	1	10	12	-4.7713985	4.44412	1
255-14-91	we propose strategies to monitor the quality of this evaluation process .	the details of this work , including the fm-iqa dataset , can be found on the project page : http : //idl.baidu.com/fm-iqa.html .	1	10	13	-5.8458567	4.880147	1
255-14-91	the experiments show that in 64.7 % of cases , the human judges can not distinguish our model from humans .	the average score is 1.454 ( 1.918 for human ) .	1	11	12	2.5327647	-2.4106627	0
255-14-91	the experiments show that in 64.7 % of cases , the human judges can not distinguish our model from humans .	the details of this work , including the fm-iqa dataset , can be found on the project page : http : //idl.baidu.com/fm-iqa.html .	1	11	13	-5.1597285	4.619932	1
255-14-91	the details of this work , including the fm-iqa dataset , can be found on the project page : http : //idl.baidu.com/fm-iqa.html .	the average score is 1.454 ( 1.918 for human ) .	0	13	12	4.2910433	-3.9710042	0
256-7-21	stochastic gradient descent ( sgd ) is a workhorse in machine learning , yet its slow convergence can be a computational bottleneck .	variance reduction techniques such as sag , svrg and saga have been proposed to overcome this weakness , achieving linear convergence .	1	0	1	-5.806302	5.221738	1
256-7-21	stochastic gradient descent ( sgd ) is a workhorse in machine learning , yet its slow convergence can be a computational bottleneck .	however , these methods are either based on computations of full gradients at pivot points , or on keeping per data point corrections in memory .	1	0	2	-5.6593447	5.2132196	1
256-7-21	therefore speed-ups relative to sgd may need a minimal number of epochs in order to materialize .	stochastic gradient descent ( sgd ) is a workhorse in machine learning , yet its slow convergence can be a computational bottleneck .	0	3	0	5.686878	-5.0825834	0
256-7-21	this paper investigates algorithms that can exploit neighborhood structure in the training data to share and re-use information about past stochastic gradients across data points , which offers advantages in the transient optimization phase .	stochastic gradient descent ( sgd ) is a workhorse in machine learning , yet its slow convergence can be a computational bottleneck .	0	4	0	5.614321	-4.923768	0
256-7-21	as a side-product we provide a unified convergence analysis for a family of variance reduction algorithms , which we call memorization algorithms .	stochastic gradient descent ( sgd ) is a workhorse in machine learning , yet its slow convergence can be a computational bottleneck .	0	5	0	5.6549063	-5.050682	0
256-7-21	stochastic gradient descent ( sgd ) is a workhorse in machine learning , yet its slow convergence can be a computational bottleneck .	we provide experimental results supporting our theory .	1	0	6	-5.9284344	5.0390253	1
256-7-21	variance reduction techniques such as sag , svrg and saga have been proposed to overcome this weakness , achieving linear convergence .	however , these methods are either based on computations of full gradients at pivot points , or on keeping per data point corrections in memory .	1	1	2	-2.4492812	2.5218816	1
256-7-21	therefore speed-ups relative to sgd may need a minimal number of epochs in order to materialize .	variance reduction techniques such as sag , svrg and saga have been proposed to overcome this weakness , achieving linear convergence .	0	3	1	-1.8669639	2.1299841	1
256-7-21	variance reduction techniques such as sag , svrg and saga have been proposed to overcome this weakness , achieving linear convergence .	this paper investigates algorithms that can exploit neighborhood structure in the training data to share and re-use information about past stochastic gradients across data points , which offers advantages in the transient optimization phase .	1	1	4	-0.08102931	0.4242482	1
256-7-21	as a side-product we provide a unified convergence analysis for a family of variance reduction algorithms , which we call memorization algorithms .	variance reduction techniques such as sag , svrg and saga have been proposed to overcome this weakness , achieving linear convergence .	0	5	1	5.3879404	-4.7535477	0
256-7-21	variance reduction techniques such as sag , svrg and saga have been proposed to overcome this weakness , achieving linear convergence .	we provide experimental results supporting our theory .	1	1	6	-5.9698286	5.0717783	1
256-7-21	however , these methods are either based on computations of full gradients at pivot points , or on keeping per data point corrections in memory .	therefore speed-ups relative to sgd may need a minimal number of epochs in order to materialize .	1	2	3	-2.093454	2.2583253	1
256-7-21	however , these methods are either based on computations of full gradients at pivot points , or on keeping per data point corrections in memory .	this paper investigates algorithms that can exploit neighborhood structure in the training data to share and re-use information about past stochastic gradients across data points , which offers advantages in the transient optimization phase .	1	2	4	-2.3991299	2.4742627	1
256-7-21	as a side-product we provide a unified convergence analysis for a family of variance reduction algorithms , which we call memorization algorithms .	however , these methods are either based on computations of full gradients at pivot points , or on keeping per data point corrections in memory .	0	5	2	5.42236	-4.7639823	0
256-7-21	we provide experimental results supporting our theory .	however , these methods are either based on computations of full gradients at pivot points , or on keeping per data point corrections in memory .	0	6	2	5.4805455	-4.8660045	0
256-7-21	therefore speed-ups relative to sgd may need a minimal number of epochs in order to materialize .	this paper investigates algorithms that can exploit neighborhood structure in the training data to share and re-use information about past stochastic gradients across data points , which offers advantages in the transient optimization phase .	1	3	4	-1.8666124	2.0642924	1
256-7-21	therefore speed-ups relative to sgd may need a minimal number of epochs in order to materialize .	as a side-product we provide a unified convergence analysis for a family of variance reduction algorithms , which we call memorization algorithms .	1	3	5	-5.855996	4.9605865	1
256-7-21	we provide experimental results supporting our theory .	therefore speed-ups relative to sgd may need a minimal number of epochs in order to materialize .	0	6	3	5.385144	-4.815277	0
256-7-21	as a side-product we provide a unified convergence analysis for a family of variance reduction algorithms , which we call memorization algorithms .	this paper investigates algorithms that can exploit neighborhood structure in the training data to share and re-use information about past stochastic gradients across data points , which offers advantages in the transient optimization phase .	0	5	4	5.4870515	-4.8465753	0
256-7-21	we provide experimental results supporting our theory .	this paper investigates algorithms that can exploit neighborhood structure in the training data to share and re-use information about past stochastic gradients across data points , which offers advantages in the transient optimization phase .	0	6	4	5.5103784	-4.9046516	0
256-7-21	we provide experimental results supporting our theory .	as a side-product we provide a unified convergence analysis for a family of variance reduction algorithms , which we call memorization algorithms .	0	6	5	3.9262595	-3.6620493	0
257-6-15	we find iterative control laws analytically without a priori policy parameterization based on probabilistic representation of the learned dynamics model .	we present a data-driven optimal control framework that is derived using the path integral ( pi ) control approach .	0	1	0	5.5195518	-4.881169	0
257-6-15	the proposed algorithm operates in a forward-backward manner which differentiate it from other pi-related methods that perform forward sampling to find optimal controls .	we present a data-driven optimal control framework that is derived using the path integral ( pi ) control approach .	0	2	0	5.544177	-4.89923	0
257-6-15	our method uses significantly less samples to find analytic control laws compared to other approaches within the pi control family that rely on extensive sampling from given dynamics models or trials on physical systems in a model-free fashion .	we present a data-driven optimal control framework that is derived using the path integral ( pi ) control approach .	0	3	0	5.6059394	-4.9418826	0
257-6-15	we present a data-driven optimal control framework that is derived using the path integral ( pi ) control approach .	in addition , the learned controllers can be generalized to new tasks without re-sampling based on the compositionality theory for the linearly-solvable optimal control framework .	1	0	4	-5.6845684	5.1254745	1
257-6-15	we provide experimental results on three different tasks and comparisons with state-of-the-art model-based methods to demonstrate the efficiency and generalizability of the proposed framework .	we present a data-driven optimal control framework that is derived using the path integral ( pi ) control approach .	0	5	0	5.5808754	-4.8754253	0
257-6-15	the proposed algorithm operates in a forward-backward manner which differentiate it from other pi-related methods that perform forward sampling to find optimal controls .	we find iterative control laws analytically without a priori policy parameterization based on probabilistic representation of the learned dynamics model .	0	2	1	2.6273735	-2.3868737	0
257-6-15	we find iterative control laws analytically without a priori policy parameterization based on probabilistic representation of the learned dynamics model .	our method uses significantly less samples to find analytic control laws compared to other approaches within the pi control family that rely on extensive sampling from given dynamics models or trials on physical systems in a model-free fashion .	1	1	3	-3.6400754	3.4330087	1
257-6-15	in addition , the learned controllers can be generalized to new tasks without re-sampling based on the compositionality theory for the linearly-solvable optimal control framework .	we find iterative control laws analytically without a priori policy parameterization based on probabilistic representation of the learned dynamics model .	0	4	1	2.8385992	-2.626998	0
257-6-15	we provide experimental results on three different tasks and comparisons with state-of-the-art model-based methods to demonstrate the efficiency and generalizability of the proposed framework .	we find iterative control laws analytically without a priori policy parameterization based on probabilistic representation of the learned dynamics model .	0	5	1	4.9818554	-4.38424	0
257-6-15	the proposed algorithm operates in a forward-backward manner which differentiate it from other pi-related methods that perform forward sampling to find optimal controls .	our method uses significantly less samples to find analytic control laws compared to other approaches within the pi control family that rely on extensive sampling from given dynamics models or trials on physical systems in a model-free fashion .	1	2	3	-2.0030713	2.0930655	1
257-6-15	the proposed algorithm operates in a forward-backward manner which differentiate it from other pi-related methods that perform forward sampling to find optimal controls .	in addition , the learned controllers can be generalized to new tasks without re-sampling based on the compositionality theory for the linearly-solvable optimal control framework .	1	2	4	0.10678734	0.102987334	0
257-6-15	the proposed algorithm operates in a forward-backward manner which differentiate it from other pi-related methods that perform forward sampling to find optimal controls .	we provide experimental results on three different tasks and comparisons with state-of-the-art model-based methods to demonstrate the efficiency and generalizability of the proposed framework .	1	2	5	-5.6944885	5.104319	1
257-6-15	our method uses significantly less samples to find analytic control laws compared to other approaches within the pi control family that rely on extensive sampling from given dynamics models or trials on physical systems in a model-free fashion .	in addition , the learned controllers can be generalized to new tasks without re-sampling based on the compositionality theory for the linearly-solvable optimal control framework .	1	3	4	-0.7318814	0.9182035	1
257-6-15	our method uses significantly less samples to find analytic control laws compared to other approaches within the pi control family that rely on extensive sampling from given dynamics models or trials on physical systems in a model-free fashion .	we provide experimental results on three different tasks and comparisons with state-of-the-art model-based methods to demonstrate the efficiency and generalizability of the proposed framework .	1	3	5	-5.249378	4.7996016	1
257-6-15	in addition , the learned controllers can be generalized to new tasks without re-sampling based on the compositionality theory for the linearly-solvable optimal control framework .	we provide experimental results on three different tasks and comparisons with state-of-the-art model-based methods to demonstrate the efficiency and generalizability of the proposed framework .	1	4	5	-5.479824	4.9044466	1
258-8-28	ep approximates the full intractable posterior distribution through a set of local approximations that are iteratively refined for each datapoint .	expectation propagation ( ep ) is a deterministic approximation algorithm that is often used to perform approximate bayesian parameter learning .	0	1	0	5.5771	-4.9209986	0
258-8-28	expectation propagation ( ep ) is a deterministic approximation algorithm that is often used to perform approximate bayesian parameter learning .	ep can offer analytic and computational advantages over other approximations , such as variational inference ( vi ) , and is the method of choice for a number of models .	1	0	2	-5.8091917	5.1700172	1
258-8-28	expectation propagation ( ep ) is a deterministic approximation algorithm that is often used to perform approximate bayesian parameter learning .	the local nature of ep appears to make it an ideal candidate for performing bayesian learning on large models in large-scale dataset settings .	1	0	3	-5.9347434	5.099039	1
258-8-28	however , ep has a crucial limitation in this context : the number of approximating factors needs to increase with the number of datapoints , n , which often entails a prohibitively large memory overhead .	expectation propagation ( ep ) is a deterministic approximation algorithm that is often used to perform approximate bayesian parameter learning .	0	4	0	5.52851	-4.939101	0
258-8-28	expectation propagation ( ep ) is a deterministic approximation algorithm that is often used to perform approximate bayesian parameter learning .	this paper presents an extension to ep , called stochastic expectation propagation ( sep ) , that maintains a global posterior approximation ( like vi ) but updates it in a local way ( like ep ) .	1	0	5	-5.8741508	5.093615	1
258-8-28	experiments on a number of canonical learning problems using synthetic and real-world datasets indicate that sep performs almost as well as full ep , but reduces the memory consumption by a factor of n .	expectation propagation ( ep ) is a deterministic approximation algorithm that is often used to perform approximate bayesian parameter learning .	0	6	0	5.552431	-4.9797745	0
258-8-28	expectation propagation ( ep ) is a deterministic approximation algorithm that is often used to perform approximate bayesian parameter learning .	sep is therefore ideally suited to performing approximate bayesian learning in the large model , large dataset setting .	1	0	7	-6.002948	5.1866765	1
258-8-28	ep can offer analytic and computational advantages over other approximations , such as variational inference ( vi ) , and is the method of choice for a number of models .	ep approximates the full intractable posterior distribution through a set of local approximations that are iteratively refined for each datapoint .	0	2	1	2.5652065	-2.3401997	0
258-8-28	the local nature of ep appears to make it an ideal candidate for performing bayesian learning on large models in large-scale dataset settings .	ep approximates the full intractable posterior distribution through a set of local approximations that are iteratively refined for each datapoint .	0	3	1	5.027064	-4.52754	0
258-8-28	ep approximates the full intractable posterior distribution through a set of local approximations that are iteratively refined for each datapoint .	however , ep has a crucial limitation in this context : the number of approximating factors needs to increase with the number of datapoints , n , which often entails a prohibitively large memory overhead .	1	1	4	5.0650344	-4.5145836	0
258-8-28	ep approximates the full intractable posterior distribution through a set of local approximations that are iteratively refined for each datapoint .	this paper presents an extension to ep , called stochastic expectation propagation ( sep ) , that maintains a global posterior approximation ( like vi ) but updates it in a local way ( like ep ) .	1	1	5	4.831353	-4.265973	0
258-8-28	experiments on a number of canonical learning problems using synthetic and real-world datasets indicate that sep performs almost as well as full ep , but reduces the memory consumption by a factor of n .	ep approximates the full intractable posterior distribution through a set of local approximations that are iteratively refined for each datapoint .	0	6	1	5.305747	-4.7804384	0
258-8-28	ep approximates the full intractable posterior distribution through a set of local approximations that are iteratively refined for each datapoint .	sep is therefore ideally suited to performing approximate bayesian learning in the large model , large dataset setting .	1	1	7	-4.9635687	4.5464845	1
258-8-28	the local nature of ep appears to make it an ideal candidate for performing bayesian learning on large models in large-scale dataset settings .	ep can offer analytic and computational advantages over other approximations , such as variational inference ( vi ) , and is the method of choice for a number of models .	0	3	2	4.353718	-3.9636793	0
258-8-28	however , ep has a crucial limitation in this context : the number of approximating factors needs to increase with the number of datapoints , n , which often entails a prohibitively large memory overhead .	ep can offer analytic and computational advantages over other approximations , such as variational inference ( vi ) , and is the method of choice for a number of models .	0	4	2	-2.5889115	2.6419334	1
258-8-28	ep can offer analytic and computational advantages over other approximations , such as variational inference ( vi ) , and is the method of choice for a number of models .	this paper presents an extension to ep , called stochastic expectation propagation ( sep ) , that maintains a global posterior approximation ( like vi ) but updates it in a local way ( like ep ) .	1	2	5	-5.593053	5.100135	1
258-8-28	ep can offer analytic and computational advantages over other approximations , such as variational inference ( vi ) , and is the method of choice for a number of models .	experiments on a number of canonical learning problems using synthetic and real-world datasets indicate that sep performs almost as well as full ep , but reduces the memory consumption by a factor of n .	1	2	6	-5.811658	5.18813	1
258-8-28	ep can offer analytic and computational advantages over other approximations , such as variational inference ( vi ) , and is the method of choice for a number of models .	sep is therefore ideally suited to performing approximate bayesian learning in the large model , large dataset setting .	1	2	7	-5.0075073	4.653944	1
258-8-28	however , ep has a crucial limitation in this context : the number of approximating factors needs to increase with the number of datapoints , n , which often entails a prohibitively large memory overhead .	the local nature of ep appears to make it an ideal candidate for performing bayesian learning on large models in large-scale dataset settings .	0	4	3	-5.880178	5.000643	1
258-8-28	this paper presents an extension to ep , called stochastic expectation propagation ( sep ) , that maintains a global posterior approximation ( like vi ) but updates it in a local way ( like ep ) .	the local nature of ep appears to make it an ideal candidate for performing bayesian learning on large models in large-scale dataset settings .	0	5	3	-5.8038363	4.999869	1
258-8-28	the local nature of ep appears to make it an ideal candidate for performing bayesian learning on large models in large-scale dataset settings .	experiments on a number of canonical learning problems using synthetic and real-world datasets indicate that sep performs almost as well as full ep , but reduces the memory consumption by a factor of n .	1	3	6	2.0254533	-1.8761731	0
258-8-28	the local nature of ep appears to make it an ideal candidate for performing bayesian learning on large models in large-scale dataset settings .	sep is therefore ideally suited to performing approximate bayesian learning in the large model , large dataset setting .	1	3	7	2.4431465	-2.3279815	0
258-8-28	however , ep has a crucial limitation in this context : the number of approximating factors needs to increase with the number of datapoints , n , which often entails a prohibitively large memory overhead .	this paper presents an extension to ep , called stochastic expectation propagation ( sep ) , that maintains a global posterior approximation ( like vi ) but updates it in a local way ( like ep ) .	1	4	5	-5.518271	5.1202602	1
258-8-28	experiments on a number of canonical learning problems using synthetic and real-world datasets indicate that sep performs almost as well as full ep , but reduces the memory consumption by a factor of n .	however , ep has a crucial limitation in this context : the number of approximating factors needs to increase with the number of datapoints , n , which often entails a prohibitively large memory overhead .	0	6	4	5.6577764	-4.99685	0
258-8-28	sep is therefore ideally suited to performing approximate bayesian learning in the large model , large dataset setting .	however , ep has a crucial limitation in this context : the number of approximating factors needs to increase with the number of datapoints , n , which often entails a prohibitively large memory overhead .	0	7	4	5.3064976	-4.67533	0
258-8-28	this paper presents an extension to ep , called stochastic expectation propagation ( sep ) , that maintains a global posterior approximation ( like vi ) but updates it in a local way ( like ep ) .	experiments on a number of canonical learning problems using synthetic and real-world datasets indicate that sep performs almost as well as full ep , but reduces the memory consumption by a factor of n .	1	5	6	-5.9789042	5.1553674	1
258-8-28	this paper presents an extension to ep , called stochastic expectation propagation ( sep ) , that maintains a global posterior approximation ( like vi ) but updates it in a local way ( like ep ) .	sep is therefore ideally suited to performing approximate bayesian learning in the large model , large dataset setting .	1	5	7	-6.0001044	5.153402	1
258-8-28	experiments on a number of canonical learning problems using synthetic and real-world datasets indicate that sep performs almost as well as full ep , but reduces the memory consumption by a factor of n .	sep is therefore ideally suited to performing approximate bayesian learning in the large model , large dataset setting .	1	6	7	3.514464	-3.2968414	0
259-6-15	as a result , for discrete graphical models , the map problem is often approximated using linear programming relaxations .	computing the map assignment in graphical models is generally intractable .	0	1	0	5.622595	-4.9855723	0
259-6-15	much research has focused on characterizing when these lp relaxations are tight , and while they are relatively well-understood in the discrete case , only a few results are known for their continuous analog .	computing the map assignment in graphical models is generally intractable .	0	2	0	5.6925273	-5.0531116	0
259-6-15	computing the map assignment in graphical models is generally intractable .	in this work , we use graph covers to provide necessary and sufficient conditions for continuous map relaxations to be tight .	1	0	3	-5.944199	5.1313763	1
259-6-15	computing the map assignment in graphical models is generally intractable .	we use this characterization to give simple proofs that the relaxation is tight for log-concave decomposable and logsupermodular decomposable models .	1	0	4	-5.9083853	5.12066	1
259-6-15	we conclude by exploring the relationship between these two seemingly distinct classes of functions and providing specific conditions under which the map relaxation can and can not be tight .	computing the map assignment in graphical models is generally intractable .	0	5	0	5.681105	-5.0467343	0
259-6-15	as a result , for discrete graphical models , the map problem is often approximated using linear programming relaxations .	much research has focused on characterizing when these lp relaxations are tight , and while they are relatively well-understood in the discrete case , only a few results are known for their continuous analog .	1	1	2	2.4090228	-1.9365232	0
259-6-15	as a result , for discrete graphical models , the map problem is often approximated using linear programming relaxations .	in this work , we use graph covers to provide necessary and sufficient conditions for continuous map relaxations to be tight .	1	1	3	-5.345614	4.9041705	1
259-6-15	we use this characterization to give simple proofs that the relaxation is tight for log-concave decomposable and logsupermodular decomposable models .	as a result , for discrete graphical models , the map problem is often approximated using linear programming relaxations .	0	4	1	5.2408514	-4.6561375	0
259-6-15	we conclude by exploring the relationship between these two seemingly distinct classes of functions and providing specific conditions under which the map relaxation can and can not be tight .	as a result , for discrete graphical models , the map problem is often approximated using linear programming relaxations .	0	5	1	4.7199316	-4.285385	0
259-6-15	in this work , we use graph covers to provide necessary and sufficient conditions for continuous map relaxations to be tight .	much research has focused on characterizing when these lp relaxations are tight , and while they are relatively well-understood in the discrete case , only a few results are known for their continuous analog .	0	3	2	5.066449	-4.4155703	0
259-6-15	much research has focused on characterizing when these lp relaxations are tight , and while they are relatively well-understood in the discrete case , only a few results are known for their continuous analog .	we use this characterization to give simple proofs that the relaxation is tight for log-concave decomposable and logsupermodular decomposable models .	1	2	4	-5.952924	5.23736	1
259-6-15	much research has focused on characterizing when these lp relaxations are tight , and while they are relatively well-understood in the discrete case , only a few results are known for their continuous analog .	we conclude by exploring the relationship between these two seemingly distinct classes of functions and providing specific conditions under which the map relaxation can and can not be tight .	1	2	5	-5.9476824	5.1753416	1
259-6-15	in this work , we use graph covers to provide necessary and sufficient conditions for continuous map relaxations to be tight .	we use this characterization to give simple proofs that the relaxation is tight for log-concave decomposable and logsupermodular decomposable models .	1	3	4	-5.9510922	5.1660743	1
259-6-15	we conclude by exploring the relationship between these two seemingly distinct classes of functions and providing specific conditions under which the map relaxation can and can not be tight .	in this work , we use graph covers to provide necessary and sufficient conditions for continuous map relaxations to be tight .	0	5	3	5.3622456	-4.7221155	0
259-6-15	we use this characterization to give simple proofs that the relaxation is tight for log-concave decomposable and logsupermodular decomposable models .	we conclude by exploring the relationship between these two seemingly distinct classes of functions and providing specific conditions under which the map relaxation can and can not be tight .	1	4	5	-4.468508	4.2277346	1
260-7-21	nonlinear component analysis such as kernel principle component analysis ( kpca ) and kernel canonical correlation analysis ( kcca ) are widely used in machine learning , statistics and data analysis , but they can not scale up to big datasets .	recent attempts have employed random feature approximations to convert the problem to the primal form for linear computational complexity .	1	0	1	-5.878367	5.2368755	1
260-7-21	however , to obtain high quality solutions , the number of random features should be the same order of magnitude as the number of data points , making such approach not directly applicable to the regime with millions of data points .	nonlinear component analysis such as kernel principle component analysis ( kpca ) and kernel canonical correlation analysis ( kcca ) are widely used in machine learning , statistics and data analysis , but they can not scale up to big datasets .	0	2	0	5.6649103	-4.986099	0
260-7-21	nonlinear component analysis such as kernel principle component analysis ( kpca ) and kernel canonical correlation analysis ( kcca ) are widely used in machine learning , statistics and data analysis , but they can not scale up to big datasets .	[CLS] we propose a simple, computationally efficient, and memory friendly algorithm based on the ` ` doubly stochastic gradients'' to scale up a range of kernel nonlinear component analysis, such as kernel pca, cca and svd	1	0	3	-5.8969107	5.1821017	1
260-7-21	nonlinear component analysis such as kernel principle component analysis ( kpca ) and kernel canonical correlation analysis ( kcca ) are widely used in machine learning , statistics and data analysis , but they can not scale up to big datasets .	despite the non-convex nature of these problems , our method enjoys theoretical guarantees that it con verges at the rate o ( 1/t ) to the global optimum , even for the top k eigen subspace .	1	0	4	-6.000724	5.2018256	1
260-7-21	nonlinear component analysis such as kernel principle component analysis ( kpca ) and kernel canonical correlation analysis ( kcca ) are widely used in machine learning , statistics and data analysis , but they can not scale up to big datasets .	unlike many alternatives , our algorithm does not require explicit orthogonalization , which is infeasible on big datasets .	1	0	5	-5.9562063	5.1831336	1
260-7-21	nonlinear component analysis such as kernel principle component analysis ( kpca ) and kernel canonical correlation analysis ( kcca ) are widely used in machine learning , statistics and data analysis , but they can not scale up to big datasets .	we demonstrate the effectiveness and scalability of our algorithm on large scale synthetic and real world datasets .	1	0	6	-5.950915	5.134186	1
260-7-21	recent attempts have employed random feature approximations to convert the problem to the primal form for linear computational complexity .	however , to obtain high quality solutions , the number of random features should be the same order of magnitude as the number of data points , making such approach not directly applicable to the regime with millions of data points .	1	1	2	-5.0972686	4.7851944	1
260-7-21	recent attempts have employed random feature approximations to convert the problem to the primal form for linear computational complexity .	we propose a simple , computationally efficient , and memory friendly algorithm based on the `` doubly stochastic gradients '' to scale up a range of kernel nonlinear component analysis , such as kernel pca , cca and svd .	1	1	3	-5.038495	4.7847943	1
260-7-21	despite the non-convex nature of these problems , our method enjoys theoretical guarantees that it con verges at the rate o ( 1/t ) to the global optimum , even for the top k eigen subspace .	recent attempts have employed random feature approximations to convert the problem to the primal form for linear computational complexity .	0	4	1	1.3106838	-0.95653546	0
260-7-21	recent attempts have employed random feature approximations to convert the problem to the primal form for linear computational complexity .	unlike many alternatives , our algorithm does not require explicit orthogonalization , which is infeasible on big datasets .	1	1	5	-5.9706836	5.2170677	1
260-7-21	recent attempts have employed random feature approximations to convert the problem to the primal form for linear computational complexity .	we demonstrate the effectiveness and scalability of our algorithm on large scale synthetic and real world datasets .	1	1	6	-5.974494	5.14001	1
260-7-21	however , to obtain high quality solutions , the number of random features should be the same order of magnitude as the number of data points , making such approach not directly applicable to the regime with millions of data points .	we propose a simple , computationally efficient , and memory friendly algorithm based on the `` doubly stochastic gradients '' to scale up a range of kernel nonlinear component analysis , such as kernel pca , cca and svd .	1	2	3	-5.023864	4.7954507	1
260-7-21	despite the non-convex nature of these problems , our method enjoys theoretical guarantees that it con verges at the rate o ( 1/t ) to the global optimum , even for the top k eigen subspace .	however , to obtain high quality solutions , the number of random features should be the same order of magnitude as the number of data points , making such approach not directly applicable to the regime with millions of data points .	0	4	2	-2.4352887	2.509245	1
260-7-21	however , to obtain high quality solutions , the number of random features should be the same order of magnitude as the number of data points , making such approach not directly applicable to the regime with millions of data points .	unlike many alternatives , our algorithm does not require explicit orthogonalization , which is infeasible on big datasets .	1	2	5	-5.717225	5.1380806	1
260-7-21	however , to obtain high quality solutions , the number of random features should be the same order of magnitude as the number of data points , making such approach not directly applicable to the regime with millions of data points .	we demonstrate the effectiveness and scalability of our algorithm on large scale synthetic and real world datasets .	1	2	6	-5.946338	5.063112	1
260-7-21	we propose a simple , computationally efficient , and memory friendly algorithm based on the `` doubly stochastic gradients '' to scale up a range of kernel nonlinear component analysis , such as kernel pca , cca and svd .	despite the non-convex nature of these problems , our method enjoys theoretical guarantees that it con verges at the rate o ( 1/t ) to the global optimum , even for the top k eigen subspace .	1	3	4	-3.3084729	3.1909134	1
260-7-21	unlike many alternatives , our algorithm does not require explicit orthogonalization , which is infeasible on big datasets .	we propose a simple , computationally efficient , and memory friendly algorithm based on the `` doubly stochastic gradients '' to scale up a range of kernel nonlinear component analysis , such as kernel pca , cca and svd .	0	5	3	5.464702	-4.7496514	0
260-7-21	we demonstrate the effectiveness and scalability of our algorithm on large scale synthetic and real world datasets .	we propose a simple , computationally efficient , and memory friendly algorithm based on the `` doubly stochastic gradients '' to scale up a range of kernel nonlinear component analysis , such as kernel pca , cca and svd .	0	6	3	5.453836	-4.773048	0
260-7-21	unlike many alternatives , our algorithm does not require explicit orthogonalization , which is infeasible on big datasets .	despite the non-convex nature of these problems , our method enjoys theoretical guarantees that it con verges at the rate o ( 1/t ) to the global optimum , even for the top k eigen subspace .	0	5	4	4.323991	-3.8650734	0
260-7-21	despite the non-convex nature of these problems , our method enjoys theoretical guarantees that it con verges at the rate o ( 1/t ) to the global optimum , even for the top k eigen subspace .	we demonstrate the effectiveness and scalability of our algorithm on large scale synthetic and real world datasets .	1	4	6	-5.936287	5.054368	1
260-7-21	we demonstrate the effectiveness and scalability of our algorithm on large scale synthetic and real world datasets .	unlike many alternatives , our algorithm does not require explicit orthogonalization , which is infeasible on big datasets .	0	6	5	3.9809246	-3.680466	0
261-13-78	overfitting is the bane of data analysts , even when data are plentiful .	formal approaches to understanding this problem focus on statistical inference and generalization of individual analysis procedures .	1	0	1	-5.1061373	4.754207	1
261-13-78	overfitting is the bane of data analysts , even when data are plentiful .	yet the practice of data analysis is an inherently interactive and adaptive process : new analyses and hypotheses are proposed after seeing the results of previous ones , parameters are tuned on the basis of obtained results , and datasets are shared and reused .	1	0	2	-5.6097345	5.1513786	1
261-13-78	overfitting is the bane of data analysts , even when data are plentiful .	an investigation of this gap has recently been initiated by the authors in , where we focused on the problem of estimating expectations of adaptively chosen functions .	1	0	3	-5.966723	5.18301	1
261-13-78	overfitting is the bane of data analysts , even when data are plentiful .	in this paper , we give a simple and practical method for reusing a holdout ( or testing ) set to validate the accuracy of hypotheses produced by a learning algorithm operating on a training set .	1	0	4	-5.965025	5.1524835	1
261-13-78	reusing a holdout set adaptively multiple times can easily lead to overfitting to the holdout set itself .	overfitting is the bane of data analysts , even when data are plentiful .	0	5	0	5.548885	-4.9792767	0
261-13-78	we give an algorithm that enables the validation of a large number of adaptively chosen hypotheses , while provably avoiding overfitting .	overfitting is the bane of data analysts , even when data are plentiful .	0	6	0	5.704675	-5.0832195	0
261-13-78	we illustrate the advantages of our algorithm over the standard use of the holdout set via a simple synthetic experiment .	overfitting is the bane of data analysts , even when data are plentiful .	0	7	0	5.636854	-5.0510125	0
261-13-78	we also formalize and address the general problem of data reuse in adaptive data analysis .	overfitting is the bane of data analysts , even when data are plentiful .	0	8	0	5.605518	-4.9998417	0
261-13-78	we show how the differential-privacy based approach given in is applicable much more broadly to adaptive data analysis .	overfitting is the bane of data analysts , even when data are plentiful .	0	9	0	5.694598	-5.0813737	0
261-13-78	we then show that a simple approach based on description length can also be used to give guarantees of statistical validity in adaptive settings .	overfitting is the bane of data analysts , even when data are plentiful .	0	10	0	5.7165093	-5.0781174	0
261-13-78	overfitting is the bane of data analysts , even when data are plentiful .	finally , we demonstrate that these incomparable approaches can be unified via the notion of approximate max-information that we introduce .	1	0	11	-5.9676948	5.106117	1
261-13-78	overfitting is the bane of data analysts , even when data are plentiful .	this , in particular , allows the preservation of statistical validity guarantees even when an analyst adaptively composes algorithms which have guarantees based on either of the two approaches .	1	0	12	-5.911906	5.1237	1
261-13-78	formal approaches to understanding this problem focus on statistical inference and generalization of individual analysis procedures .	yet the practice of data analysis is an inherently interactive and adaptive process : new analyses and hypotheses are proposed after seeing the results of previous ones , parameters are tuned on the basis of obtained results , and datasets are shared and reused .	1	1	2	-3.9625807	3.8782911	1
261-13-78	formal approaches to understanding this problem focus on statistical inference and generalization of individual analysis procedures .	an investigation of this gap has recently been initiated by the authors in , where we focused on the problem of estimating expectations of adaptively chosen functions .	1	1	3	-3.968661	3.8636024	1
261-13-78	formal approaches to understanding this problem focus on statistical inference and generalization of individual analysis procedures .	in this paper , we give a simple and practical method for reusing a holdout ( or testing ) set to validate the accuracy of hypotheses produced by a learning algorithm operating on a training set .	1	1	4	-5.9535255	5.202629	1
261-13-78	formal approaches to understanding this problem focus on statistical inference and generalization of individual analysis procedures .	reusing a holdout set adaptively multiple times can easily lead to overfitting to the holdout set itself .	1	1	5	-4.2983475	4.0817013	1
261-13-78	we give an algorithm that enables the validation of a large number of adaptively chosen hypotheses , while provably avoiding overfitting .	formal approaches to understanding this problem focus on statistical inference and generalization of individual analysis procedures .	0	6	1	5.630456	-4.9985995	0
261-13-78	we illustrate the advantages of our algorithm over the standard use of the holdout set via a simple synthetic experiment .	formal approaches to understanding this problem focus on statistical inference and generalization of individual analysis procedures .	0	7	1	5.622176	-5.0398164	0
261-13-78	we also formalize and address the general problem of data reuse in adaptive data analysis .	formal approaches to understanding this problem focus on statistical inference and generalization of individual analysis procedures .	0	8	1	5.234787	-4.709082	0
261-13-78	formal approaches to understanding this problem focus on statistical inference and generalization of individual analysis procedures .	we show how the differential-privacy based approach given in is applicable much more broadly to adaptive data analysis .	1	1	9	-5.9970894	5.067013	1
261-13-78	formal approaches to understanding this problem focus on statistical inference and generalization of individual analysis procedures .	we then show that a simple approach based on description length can also be used to give guarantees of statistical validity in adaptive settings .	1	1	10	-5.9913745	5.179346	1
261-13-78	finally , we demonstrate that these incomparable approaches can be unified via the notion of approximate max-information that we introduce .	formal approaches to understanding this problem focus on statistical inference and generalization of individual analysis procedures .	0	11	1	5.6185184	-5.0093718	0
261-13-78	this , in particular , allows the preservation of statistical validity guarantees even when an analyst adaptively composes algorithms which have guarantees based on either of the two approaches .	formal approaches to understanding this problem focus on statistical inference and generalization of individual analysis procedures .	0	12	1	5.575752	-4.9493847	0
261-13-78	an investigation of this gap has recently been initiated by the authors in , where we focused on the problem of estimating expectations of adaptively chosen functions .	yet the practice of data analysis is an inherently interactive and adaptive process : new analyses and hypotheses are proposed after seeing the results of previous ones , parameters are tuned on the basis of obtained results , and datasets are shared and reused .	0	3	2	4.056098	-3.6387582	0
261-13-78	in this paper , we give a simple and practical method for reusing a holdout ( or testing ) set to validate the accuracy of hypotheses produced by a learning algorithm operating on a training set .	yet the practice of data analysis is an inherently interactive and adaptive process : new analyses and hypotheses are proposed after seeing the results of previous ones , parameters are tuned on the basis of obtained results , and datasets are shared and reused .	0	4	2	5.205261	-4.532512	0
261-13-78	yet the practice of data analysis is an inherently interactive and adaptive process : new analyses and hypotheses are proposed after seeing the results of previous ones , parameters are tuned on the basis of obtained results , and datasets are shared and reused .	reusing a holdout set adaptively multiple times can easily lead to overfitting to the holdout set itself .	1	2	5	-3.1139123	3.1084437	1
261-13-78	we give an algorithm that enables the validation of a large number of adaptively chosen hypotheses , while provably avoiding overfitting .	yet the practice of data analysis is an inherently interactive and adaptive process : new analyses and hypotheses are proposed after seeing the results of previous ones , parameters are tuned on the basis of obtained results , and datasets are shared and reused .	0	6	2	5.2589016	-4.6131115	0
261-13-78	we illustrate the advantages of our algorithm over the standard use of the holdout set via a simple synthetic experiment .	yet the practice of data analysis is an inherently interactive and adaptive process : new analyses and hypotheses are proposed after seeing the results of previous ones , parameters are tuned on the basis of obtained results , and datasets are shared and reused .	0	7	2	5.5248656	-4.9262457	0
261-13-78	yet the practice of data analysis is an inherently interactive and adaptive process : new analyses and hypotheses are proposed after seeing the results of previous ones , parameters are tuned on the basis of obtained results , and datasets are shared and reused .	we also formalize and address the general problem of data reuse in adaptive data analysis .	1	2	8	-5.8973274	5.066469	1
261-13-78	yet the practice of data analysis is an inherently interactive and adaptive process : new analyses and hypotheses are proposed after seeing the results of previous ones , parameters are tuned on the basis of obtained results , and datasets are shared and reused .	we show how the differential-privacy based approach given in is applicable much more broadly to adaptive data analysis .	1	2	9	-5.956856	5.0241375	1
261-13-78	yet the practice of data analysis is an inherently interactive and adaptive process : new analyses and hypotheses are proposed after seeing the results of previous ones , parameters are tuned on the basis of obtained results , and datasets are shared and reused .	we then show that a simple approach based on description length can also be used to give guarantees of statistical validity in adaptive settings .	1	2	10	-5.9540863	5.0266685	1
261-13-78	finally , we demonstrate that these incomparable approaches can be unified via the notion of approximate max-information that we introduce .	yet the practice of data analysis is an inherently interactive and adaptive process : new analyses and hypotheses are proposed after seeing the results of previous ones , parameters are tuned on the basis of obtained results , and datasets are shared and reused .	0	11	2	5.4229083	-4.771819	0
261-13-78	this , in particular , allows the preservation of statistical validity guarantees even when an analyst adaptively composes algorithms which have guarantees based on either of the two approaches .	yet the practice of data analysis is an inherently interactive and adaptive process : new analyses and hypotheses are proposed after seeing the results of previous ones , parameters are tuned on the basis of obtained results , and datasets are shared and reused .	0	12	2	4.6786127	-4.1307383	0
261-13-78	in this paper , we give a simple and practical method for reusing a holdout ( or testing ) set to validate the accuracy of hypotheses produced by a learning algorithm operating on a training set .	an investigation of this gap has recently been initiated by the authors in , where we focused on the problem of estimating expectations of adaptively chosen functions .	0	4	3	4.4820566	-3.9534588	0
261-13-78	reusing a holdout set adaptively multiple times can easily lead to overfitting to the holdout set itself .	an investigation of this gap has recently been initiated by the authors in , where we focused on the problem of estimating expectations of adaptively chosen functions .	0	5	3	-3.1989875	3.1564136	1
261-13-78	we give an algorithm that enables the validation of a large number of adaptively chosen hypotheses , while provably avoiding overfitting .	an investigation of this gap has recently been initiated by the authors in , where we focused on the problem of estimating expectations of adaptively chosen functions .	0	6	3	5.0630665	-4.5046844	0
261-13-78	an investigation of this gap has recently been initiated by the authors in , where we focused on the problem of estimating expectations of adaptively chosen functions .	we illustrate the advantages of our algorithm over the standard use of the holdout set via a simple synthetic experiment .	1	3	7	-5.990157	5.1878824	1
261-13-78	an investigation of this gap has recently been initiated by the authors in , where we focused on the problem of estimating expectations of adaptively chosen functions .	we also formalize and address the general problem of data reuse in adaptive data analysis .	1	3	8	-5.3647575	4.7293777	1
261-13-78	an investigation of this gap has recently been initiated by the authors in , where we focused on the problem of estimating expectations of adaptively chosen functions .	we show how the differential-privacy based approach given in is applicable much more broadly to adaptive data analysis .	1	3	9	-5.9474473	5.1598554	1
261-13-78	an investigation of this gap has recently been initiated by the authors in , where we focused on the problem of estimating expectations of adaptively chosen functions .	we then show that a simple approach based on description length can also be used to give guarantees of statistical validity in adaptive settings .	1	3	10	-5.9791527	5.1743293	1
261-13-78	finally , we demonstrate that these incomparable approaches can be unified via the notion of approximate max-information that we introduce .	an investigation of this gap has recently been initiated by the authors in , where we focused on the problem of estimating expectations of adaptively chosen functions .	0	11	3	5.3147154	-4.710265	0
261-13-78	this , in particular , allows the preservation of statistical validity guarantees even when an analyst adaptively composes algorithms which have guarantees based on either of the two approaches .	an investigation of this gap has recently been initiated by the authors in , where we focused on the problem of estimating expectations of adaptively chosen functions .	0	12	3	2.2972734	-2.1875448	0
261-13-78	reusing a holdout set adaptively multiple times can easily lead to overfitting to the holdout set itself .	in this paper , we give a simple and practical method for reusing a holdout ( or testing ) set to validate the accuracy of hypotheses produced by a learning algorithm operating on a training set .	0	5	4	-5.606625	5.121541	1
261-13-78	in this paper , we give a simple and practical method for reusing a holdout ( or testing ) set to validate the accuracy of hypotheses produced by a learning algorithm operating on a training set .	we give an algorithm that enables the validation of a large number of adaptively chosen hypotheses , while provably avoiding overfitting .	1	4	6	-5.81443	5.1588774	1
261-13-78	in this paper , we give a simple and practical method for reusing a holdout ( or testing ) set to validate the accuracy of hypotheses produced by a learning algorithm operating on a training set .	we illustrate the advantages of our algorithm over the standard use of the holdout set via a simple synthetic experiment .	1	4	7	-5.923543	5.183595	1
261-13-78	in this paper , we give a simple and practical method for reusing a holdout ( or testing ) set to validate the accuracy of hypotheses produced by a learning algorithm operating on a training set .	we also formalize and address the general problem of data reuse in adaptive data analysis .	1	4	8	-4.6266947	4.1623826	1
261-13-78	we show how the differential-privacy based approach given in is applicable much more broadly to adaptive data analysis .	in this paper , we give a simple and practical method for reusing a holdout ( or testing ) set to validate the accuracy of hypotheses produced by a learning algorithm operating on a training set .	0	9	4	5.3802934	-4.756228	0
261-13-78	in this paper , we give a simple and practical method for reusing a holdout ( or testing ) set to validate the accuracy of hypotheses produced by a learning algorithm operating on a training set .	we then show that a simple approach based on description length can also be used to give guarantees of statistical validity in adaptive settings .	1	4	10	-5.988515	5.1829324	1
261-13-78	in this paper , we give a simple and practical method for reusing a holdout ( or testing ) set to validate the accuracy of hypotheses produced by a learning algorithm operating on a training set .	finally , we demonstrate that these incomparable approaches can be unified via the notion of approximate max-information that we introduce .	1	4	11	-6.0005164	5.1930804	1
261-13-78	this , in particular , allows the preservation of statistical validity guarantees even when an analyst adaptively composes algorithms which have guarantees based on either of the two approaches .	in this paper , we give a simple and practical method for reusing a holdout ( or testing ) set to validate the accuracy of hypotheses produced by a learning algorithm operating on a training set .	0	12	4	3.3400874	-3.0792341	0
261-13-78	we give an algorithm that enables the validation of a large number of adaptively chosen hypotheses , while provably avoiding overfitting .	reusing a holdout set adaptively multiple times can easily lead to overfitting to the holdout set itself .	0	6	5	4.9465394	-4.3630276	0
261-13-78	we illustrate the advantages of our algorithm over the standard use of the holdout set via a simple synthetic experiment .	reusing a holdout set adaptively multiple times can easily lead to overfitting to the holdout set itself .	0	7	5	5.569188	-4.943745	0
261-13-78	reusing a holdout set adaptively multiple times can easily lead to overfitting to the holdout set itself .	we also formalize and address the general problem of data reuse in adaptive data analysis .	1	5	8	-5.4620285	4.9265842	1
261-13-78	reusing a holdout set adaptively multiple times can easily lead to overfitting to the holdout set itself .	we show how the differential-privacy based approach given in is applicable much more broadly to adaptive data analysis .	1	5	9	-5.9493895	5.1471376	1
261-13-78	reusing a holdout set adaptively multiple times can easily lead to overfitting to the holdout set itself .	we then show that a simple approach based on description length can also be used to give guarantees of statistical validity in adaptive settings .	1	5	10	-5.933779	5.1410832	1
261-13-78	finally , we demonstrate that these incomparable approaches can be unified via the notion of approximate max-information that we introduce .	reusing a holdout set adaptively multiple times can easily lead to overfitting to the holdout set itself .	0	11	5	5.2916183	-4.672045	0
261-13-78	reusing a holdout set adaptively multiple times can easily lead to overfitting to the holdout set itself .	this , in particular , allows the preservation of statistical validity guarantees even when an analyst adaptively composes algorithms which have guarantees based on either of the two approaches .	1	5	12	-4.812393	4.5221586	1
261-13-78	we illustrate the advantages of our algorithm over the standard use of the holdout set via a simple synthetic experiment .	we give an algorithm that enables the validation of a large number of adaptively chosen hypotheses , while provably avoiding overfitting .	0	7	6	5.2327557	-4.5801687	0
261-13-78	we give an algorithm that enables the validation of a large number of adaptively chosen hypotheses , while provably avoiding overfitting .	we also formalize and address the general problem of data reuse in adaptive data analysis .	1	6	8	-2.012792	2.1606917	1
261-13-78	we give an algorithm that enables the validation of a large number of adaptively chosen hypotheses , while provably avoiding overfitting .	we show how the differential-privacy based approach given in is applicable much more broadly to adaptive data analysis .	1	6	9	-3.0655665	3.0530593	1
261-13-78	we then show that a simple approach based on description length can also be used to give guarantees of statistical validity in adaptive settings .	we give an algorithm that enables the validation of a large number of adaptively chosen hypotheses , while provably avoiding overfitting .	0	10	6	3.1698544	-2.9238212	0
261-13-78	we give an algorithm that enables the validation of a large number of adaptively chosen hypotheses , while provably avoiding overfitting .	finally , we demonstrate that these incomparable approaches can be unified via the notion of approximate max-information that we introduce .	1	6	11	-5.664455	5.0511074	1
261-13-78	we give an algorithm that enables the validation of a large number of adaptively chosen hypotheses , while provably avoiding overfitting .	this , in particular , allows the preservation of statistical validity guarantees even when an analyst adaptively composes algorithms which have guarantees based on either of the two approaches .	1	6	12	-1.751493	1.8775544	1
261-13-78	we illustrate the advantages of our algorithm over the standard use of the holdout set via a simple synthetic experiment .	we also formalize and address the general problem of data reuse in adaptive data analysis .	1	7	8	3.844943	-3.572745	0
261-13-78	we show how the differential-privacy based approach given in is applicable much more broadly to adaptive data analysis .	we illustrate the advantages of our algorithm over the standard use of the holdout set via a simple synthetic experiment .	0	9	7	-5.2045937	4.7338915	1
261-13-78	we illustrate the advantages of our algorithm over the standard use of the holdout set via a simple synthetic experiment .	we then show that a simple approach based on description length can also be used to give guarantees of statistical validity in adaptive settings .	1	7	10	4.498667	-4.032593	0
261-13-78	finally , we demonstrate that these incomparable approaches can be unified via the notion of approximate max-information that we introduce .	we illustrate the advantages of our algorithm over the standard use of the holdout set via a simple synthetic experiment .	0	11	7	-2.6673236	2.7303534	1
261-13-78	we illustrate the advantages of our algorithm over the standard use of the holdout set via a simple synthetic experiment .	this , in particular , allows the preservation of statistical validity guarantees even when an analyst adaptively composes algorithms which have guarantees based on either of the two approaches .	1	7	12	5.1105556	-4.556903	0
261-13-78	we also formalize and address the general problem of data reuse in adaptive data analysis .	we show how the differential-privacy based approach given in is applicable much more broadly to adaptive data analysis .	1	8	9	2.6507564	-2.5003855	0
261-13-78	we then show that a simple approach based on description length can also be used to give guarantees of statistical validity in adaptive settings .	we also formalize and address the general problem of data reuse in adaptive data analysis .	0	10	8	-2.2928853	2.4337683	1
261-13-78	finally , we demonstrate that these incomparable approaches can be unified via the notion of approximate max-information that we introduce .	we also formalize and address the general problem of data reuse in adaptive data analysis .	0	11	8	1.8977855	-1.6641517	0
261-13-78	we also formalize and address the general problem of data reuse in adaptive data analysis .	this , in particular , allows the preservation of statistical validity guarantees even when an analyst adaptively composes algorithms which have guarantees based on either of the two approaches .	1	8	12	2.0771828	-1.9427809	0
261-13-78	we then show that a simple approach based on description length can also be used to give guarantees of statistical validity in adaptive settings .	we show how the differential-privacy based approach given in is applicable much more broadly to adaptive data analysis .	0	10	9	-2.7247417	2.8094783	1
261-13-78	we show how the differential-privacy based approach given in is applicable much more broadly to adaptive data analysis .	finally , we demonstrate that these incomparable approaches can be unified via the notion of approximate max-information that we introduce .	1	9	11	-4.0512185	3.8117108	1
261-13-78	we show how the differential-privacy based approach given in is applicable much more broadly to adaptive data analysis .	this , in particular , allows the preservation of statistical validity guarantees even when an analyst adaptively composes algorithms which have guarantees based on either of the two approaches .	1	9	12	3.2870567	-3.064956	0
261-13-78	we then show that a simple approach based on description length can also be used to give guarantees of statistical validity in adaptive settings .	finally , we demonstrate that these incomparable approaches can be unified via the notion of approximate max-information that we introduce .	1	10	11	-5.011686	4.6228843	1
261-13-78	we then show that a simple approach based on description length can also be used to give guarantees of statistical validity in adaptive settings .	this , in particular , allows the preservation of statistical validity guarantees even when an analyst adaptively composes algorithms which have guarantees based on either of the two approaches .	1	10	12	2.4990864	-2.3863685	0
261-13-78	this , in particular , allows the preservation of statistical validity guarantees even when an analyst adaptively composes algorithms which have guarantees based on either of the two approaches .	finally , we demonstrate that these incomparable approaches can be unified via the notion of approximate max-information that we introduce .	0	12	11	-5.475582	5.021451	1
262-6-15	in this paper , we add to a growing body of research aimed at understanding the precise manner in which the price process induced by a msr incorporates private information from agents who deviate from the assumption of risk-neutrality .	a market scoring rule ( msr ) - a popular tool for designing algorithmic prediction markets - is an incentive-compatible mechanism for the aggregation of probabilistic beliefs from myopic risk-neutral agents .	0	1	0	5.6238008	-4.975755	0
262-6-15	we first establish that , for a myopic trading agent with a risk-averse utility function , a msr satisfying mild regularity conditions elicits the agent 's risk-neutral probability conditional on the latest market state rather than her true subjective probability .	a market scoring rule ( msr ) - a popular tool for designing algorithmic prediction markets - is an incentive-compatible mechanism for the aggregation of probabilistic beliefs from myopic risk-neutral agents .	0	2	0	5.7032566	-5.0752163	0
262-6-15	hence , we show that a msr under these conditions effectively behaves like a more traditional method of belief aggregation , namely an opinion pool , for agents ' true probabilities .	a market scoring rule ( msr ) - a popular tool for designing algorithmic prediction markets - is an incentive-compatible mechanism for the aggregation of probabilistic beliefs from myopic risk-neutral agents .	0	3	0	5.6201153	-4.986529	0
262-6-15	in particular , the logarithmic market scoring rule acts as a logarithmic pool for constant absolute risk aversion utility agents , and as a linear pool for an atypical budgetconstrained agent utility with decreasing absolute risk aversion .	a market scoring rule ( msr ) - a popular tool for designing algorithmic prediction markets - is an incentive-compatible mechanism for the aggregation of probabilistic beliefs from myopic risk-neutral agents .	0	4	0	5.58936	-4.9746532	0
262-6-15	a market scoring rule ( msr ) - a popular tool for designing algorithmic prediction markets - is an incentive-compatible mechanism for the aggregation of probabilistic beliefs from myopic risk-neutral agents .	we also point out the interpretation of a market maker under these conditions as a bayesian learner even when agent beliefs are static .	1	0	5	-6.0091114	5.179947	1
262-6-15	we first establish that , for a myopic trading agent with a risk-averse utility function , a msr satisfying mild regularity conditions elicits the agent 's risk-neutral probability conditional on the latest market state rather than her true subjective probability .	in this paper , we add to a growing body of research aimed at understanding the precise manner in which the price process induced by a msr incorporates private information from agents who deviate from the assumption of risk-neutrality .	0	2	1	2.998671	-2.719797	0
262-6-15	in this paper , we add to a growing body of research aimed at understanding the precise manner in which the price process induced by a msr incorporates private information from agents who deviate from the assumption of risk-neutrality .	hence , we show that a msr under these conditions effectively behaves like a more traditional method of belief aggregation , namely an opinion pool , for agents ' true probabilities .	1	1	3	-5.1226916	4.7715583	1
262-6-15	in particular , the logarithmic market scoring rule acts as a logarithmic pool for constant absolute risk aversion utility agents , and as a linear pool for an atypical budgetconstrained agent utility with decreasing absolute risk aversion .	in this paper , we add to a growing body of research aimed at understanding the precise manner in which the price process induced by a msr incorporates private information from agents who deviate from the assumption of risk-neutrality .	0	4	1	1.7147267	-1.5375302	0
262-6-15	we also point out the interpretation of a market maker under these conditions as a bayesian learner even when agent beliefs are static .	in this paper , we add to a growing body of research aimed at understanding the precise manner in which the price process induced by a msr incorporates private information from agents who deviate from the assumption of risk-neutrality .	0	5	1	5.32755	-4.6932516	0
262-6-15	we first establish that , for a myopic trading agent with a risk-averse utility function , a msr satisfying mild regularity conditions elicits the agent 's risk-neutral probability conditional on the latest market state rather than her true subjective probability .	hence , we show that a msr under these conditions effectively behaves like a more traditional method of belief aggregation , namely an opinion pool , for agents ' true probabilities .	1	2	3	-5.786236	5.183617	1
262-6-15	[CLS] in particular, the logarithmic market scoring rule acts as a logarithmic pool for constant absolute risk aversion utility agents, and as a linear pool for an atypical budgetconstrained agent utility with decreasing absolute risk	[CLS] we first establish that, for a myopic trading agent with a risk - averse utility function, a msr satisfying mild regularity conditions elicits the agent's risk - neutral probability conditional on the latest market state rather than her	0	4	2	2.4034748	-2.0780325	0
262-6-15	we first establish that , for a myopic trading agent with a risk-averse utility function , a msr satisfying mild regularity conditions elicits the agent 's risk-neutral probability conditional on the latest market state rather than her true subjective probability .	we also point out the interpretation of a market maker under these conditions as a bayesian learner even when agent beliefs are static .	1	2	5	-5.8987684	5.1626635	1
262-6-15	hence , we show that a msr under these conditions effectively behaves like a more traditional method of belief aggregation , namely an opinion pool , for agents ' true probabilities .	in particular , the logarithmic market scoring rule acts as a logarithmic pool for constant absolute risk aversion utility agents , and as a linear pool for an atypical budgetconstrained agent utility with decreasing absolute risk aversion .	1	3	4	2.5059474	-2.331699	0
262-6-15	we also point out the interpretation of a market maker under these conditions as a bayesian learner even when agent beliefs are static .	hence , we show that a msr under these conditions effectively behaves like a more traditional method of belief aggregation , namely an opinion pool , for agents ' true probabilities .	0	5	3	2.5721116	-2.4241781	0
262-6-15	in particular , the logarithmic market scoring rule acts as a logarithmic pool for constant absolute risk aversion utility agents , and as a linear pool for an atypical budgetconstrained agent utility with decreasing absolute risk aversion .	we also point out the interpretation of a market maker under these conditions as a bayesian learner even when agent beliefs are static .	1	4	5	-3.6463218	3.5753937	1
263-7-21	however , the complexity offered by state-of-the-art algorithms ( i.e .	over the past decades , linear programming ( lp ) has been widely used in different areas and considered as one of the mature technologies in numerical optimization .	0	1	0	5.3413982	-4.840864	0
263-7-21	interior-point method and primal , dual simplex methods ) is still unsatisfactory for problems in machine learning with huge number of variables and constraints .	over the past decades , linear programming ( lp ) has been widely used in different areas and considered as one of the mature technologies in numerical optimization .	0	2	0	4.364661	-3.7697768	0
263-7-21	[CLS] in this paper, we investigate a general lp algorithm based on the combination of augmented lagrangian and coordinate descent ( al - cd ), giving an iteration complexity of o ( ( log ( 1 / ) ) 2 ) with o ( nnz ( a ) ) cost per iteration, where nnz ( a ) is the number of	over the past decades , linear programming ( lp ) has been widely used in different areas and considered as one of the mature technologies in numerical optimization .	0	3	0	5.713506	-5.0983677	0
263-7-21	over the past decades , linear programming ( lp ) has been widely used in different areas and considered as one of the mature technologies in numerical optimization .	the algorithm thus yields a tractable alternative to standard lp methods for large-scale problems of sparse solutions and nnz ( a ) mn .	1	0	4	-5.8775473	5.1352677	1
263-7-21	we conduct experiments on large-scale lp instances from 1 -regularized multi-class svm , sparse inverse covariance estimation , and nonnegative matrix factorization , where the proposed approach finds solutions of 10-3 precision orders of magnitude faster than state-of-the-art implementations of interior-point and simplex methods .	over the past decades , linear programming ( lp ) has been widely used in different areas and considered as one of the mature technologies in numerical optimization .	0	5	0	5.7265844	-5.0633554	0
263-7-21	over the past decades , linear programming ( lp ) has been widely used in different areas and considered as one of the mature technologies in numerical optimization .	1	1	0	6	-5.9011517	5.149069	1
263-7-21	interior-point method and primal , dual simplex methods ) is still unsatisfactory for problems in machine learning with huge number of variables and constraints .	however , the complexity offered by state-of-the-art algorithms ( i.e .	0	2	1	-3.202612	3.1194422	1
263-7-21	however , the complexity offered by state-of-the-art algorithms ( i.e .	[CLS] in this paper, we investigate a general lp algorithm based on the combination of augmented lagrangian and coordinate descent ( al - cd ), giving an iteration complexity of o ( ( log ( 1 / ) ) 2 ) with o ( nnz ( a ) ) cost per iteration, where nnz ( a ) is the number of non - zeros in the m x n constraint	1	1	3	-5.8989325	5.2321825	1
263-7-21	however , the complexity offered by state-of-the-art algorithms ( i.e .	the algorithm thus yields a tractable alternative to standard lp methods for large-scale problems of sparse solutions and nnz ( a ) mn .	1	1	4	-5.9257545	5.0533447	1
263-7-21	however , the complexity offered by state-of-the-art algorithms ( i.e .	we conduct experiments on large-scale lp instances from 1 -regularized multi-class svm , sparse inverse covariance estimation , and nonnegative matrix factorization , where the proposed approach finds solutions of 10-3 precision orders of magnitude faster than state-of-the-art implementations of interior-point and simplex methods .	1	1	5	-5.9237337	5.0160966	1
263-7-21	1	however , the complexity offered by state-of-the-art algorithms ( i.e .	0	6	1	5.4843116	-4.7409353	0
263-7-21	interior-point method and primal , dual simplex methods ) is still unsatisfactory for problems in machine learning with huge number of variables and constraints .	[CLS] in this paper, we investigate a general lp algorithm based on the combination of augmented lagrangian and coordinate descent ( al - cd ), giving an iteration complexity of o ( ( log ( 1 / ) ) 2 ) with o ( nnz ( a ) ) cost per iteration, where nnz ( a ) is the	1	2	3	-5.8607225	5.2329783	1
263-7-21	interior-point method and primal , dual simplex methods ) is still unsatisfactory for problems in machine learning with huge number of variables and constraints .	the algorithm thus yields a tractable alternative to standard lp methods for large-scale problems of sparse solutions and nnz ( a ) mn .	1	2	4	-5.9453554	5.2364845	1
263-7-21	interior-point method and primal , dual simplex methods ) is still unsatisfactory for problems in machine learning with huge number of variables and constraints .	[CLS] we conduct experiments on large - scale lp instances from 1 - regularized multi - class svm, sparse inverse covariance estimation, and nonnegative matrix factorization, where the proposed approach finds solutions of 10 - 3 precision orders of magnitude faster than state - of - the - art implementations of interior - point and simplex	1	2	5	-5.8982997	5.145465	1
263-7-21	1	interior-point method and primal , dual simplex methods ) is still unsatisfactory for problems in machine learning with huge number of variables and constraints .	0	6	2	5.477806	-4.999305	0
263-7-21	[CLS] in this paper, we investigate a general lp algorithm based on the combination of augmented lagrangian and coordinate descent ( al - cd ), giving an iteration complexity of o ( ( log ( 1 / ) ) 2 ) with o ( nnz ( a ) ) cost per iteration, where nnz ( a ) is the number of non	the algorithm thus yields a tractable alternative to standard lp methods for large-scale problems of sparse solutions and nnz ( a ) mn .	1	3	4	-5.9149866	5.156467	1
263-7-21	[CLS] we conduct experiments on large - scale lp instances from 1 - regularized multi - class svm, sparse inverse covariance estimation, and nonnegative matrix factorization, where the proposed approach finds solutions of 10 - 3 precision orders of magnitude	[CLS] in this paper, we investigate a general lp algorithm based on the combination of augmented lagrangian and coordinate descent ( al - cd ), giving an iteration complexity of o ( ( log ( 1 / ) ) 2 ) with o ( n	0	5	3	5.2962523	-4.656093	0
263-7-21	[CLS] in this paper, we investigate a general lp algorithm based on the combination of augmented lagrangian and coordinate descent ( al - cd ), giving an iteration complexity of o ( ( log ( 1 / ) ) 2 ) with o ( nnz ( a ) ) cost per iteration, where nnz ( a ) is the number of non - zeros in the m x n constraint matrix a, and in practice, one can further reduce cost per iteration to the order of	1	1	3	6	-2.9824955	2.8327456	1
263-7-21	the algorithm thus yields a tractable alternative to standard lp methods for large-scale problems of sparse solutions and nnz ( a ) mn .	we conduct experiments on large-scale lp instances from 1 -regularized multi-class svm , sparse inverse covariance estimation , and nonnegative matrix factorization , where the proposed approach finds solutions of 10-3 precision orders of magnitude faster than state-of-the-art implementations of interior-point and simplex methods .	1	4	5	-1.7237844	1.8710189	1
263-7-21	the algorithm thus yields a tractable alternative to standard lp methods for large-scale problems of sparse solutions and nnz ( a ) mn .	1	1	4	6	-2.3729234	2.4239554	1
263-7-21	1	we conduct experiments on large-scale lp instances from 1 -regularized multi-class svm , sparse inverse covariance estimation , and nonnegative matrix factorization , where the proposed approach finds solutions of 10-3 precision orders of magnitude faster than state-of-the-art implementations of interior-point and simplex methods .	0	6	5	1.5063938	-1.0312916	0
264-7-21	however , training becomes more difficult as depth increases , and training of very deep networks remains an open problem .	theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success .	0	1	0	3.5793843	-3.1039758	0
264-7-21	here we introduce a new architecture designed to overcome this .	theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success .	0	2	0	5.3519297	-4.8013806	0
264-7-21	our so-called highway networks allow unimpeded information flow across many layers on information highways .	theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success .	0	3	0	4.5820427	-4.130124	0
264-7-21	theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success .	they are inspired by long short-term memory recurrent networks and use adaptive gating units to regulate the information flow .	1	0	4	-4.9647436	4.6608114	1
264-7-21	even with hundreds of layers , highway networks can be trained directly through simple gradient descent .	theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success .	0	5	0	5.168269	-4.5912814	0
264-7-21	theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success .	this enables the study of extremely deep and efficient architectures .	1	0	6	-5.935755	5.003435	1
264-7-21	however , training becomes more difficult as depth increases , and training of very deep networks remains an open problem .	here we introduce a new architecture designed to overcome this .	1	1	2	-5.953145	5.1063557	1
264-7-21	our so-called highway networks allow unimpeded information flow across many layers on information highways .	however , training becomes more difficult as depth increases , and training of very deep networks remains an open problem .	0	3	1	3.4365761	-3.2165728	0
264-7-21	they are inspired by long short-term memory recurrent networks and use adaptive gating units to regulate the information flow .	however , training becomes more difficult as depth increases , and training of very deep networks remains an open problem .	0	4	1	4.7201047	-4.2668285	0
264-7-21	however , training becomes more difficult as depth increases , and training of very deep networks remains an open problem .	even with hundreds of layers , highway networks can be trained directly through simple gradient descent .	1	1	5	-5.2159123	4.865532	1
264-7-21	however , training becomes more difficult as depth increases , and training of very deep networks remains an open problem .	this enables the study of extremely deep and efficient architectures .	1	1	6	-5.926608	4.995061	1
264-7-21	here we introduce a new architecture designed to overcome this .	our so-called highway networks allow unimpeded information flow across many layers on information highways .	1	2	3	1.7771499	-1.571164	0
264-7-21	they are inspired by long short-term memory recurrent networks and use adaptive gating units to regulate the information flow .	here we introduce a new architecture designed to overcome this .	0	4	2	3.5911167	-3.0788188	0
264-7-21	here we introduce a new architecture designed to overcome this .	even with hundreds of layers , highway networks can be trained directly through simple gradient descent .	1	2	5	-2.9223852	2.9345098	1
264-7-21	here we introduce a new architecture designed to overcome this .	this enables the study of extremely deep and efficient architectures .	1	2	6	-4.569817	4.291484	1
264-7-21	our so-called highway networks allow unimpeded information flow across many layers on information highways .	they are inspired by long short-term memory recurrent networks and use adaptive gating units to regulate the information flow .	1	3	4	-4.070709	3.8741713	1
264-7-21	even with hundreds of layers , highway networks can be trained directly through simple gradient descent .	our so-called highway networks allow unimpeded information flow across many layers on information highways .	0	5	3	3.349218	-3.1277125	0
264-7-21	our so-called highway networks allow unimpeded information flow across many layers on information highways .	this enables the study of extremely deep and efficient architectures .	1	3	6	-5.828916	5.199218	1
264-7-21	they are inspired by long short-term memory recurrent networks and use adaptive gating units to regulate the information flow .	even with hundreds of layers , highway networks can be trained directly through simple gradient descent .	1	4	5	-4.2452493	3.9838746	1
264-7-21	they are inspired by long short-term memory recurrent networks and use adaptive gating units to regulate the information flow .	this enables the study of extremely deep and efficient architectures .	1	4	6	-5.0951424	4.6730347	1
264-7-21	even with hundreds of layers , highway networks can be trained directly through simple gradient descent .	this enables the study of extremely deep and efficient architectures .	1	5	6	-4.3171253	4.0732107	1
265-6-15	although our method can work with arbitrary models , we focus on actively learning the appropriate structure for gaussian process ( gp ) models with arbitrary observation likelihoods .	we introduce a novel information-theoretic approach for active model selection and demonstrate its effectiveness in a real-world application .	0	1	0	0.7079375	-0.33833337	0
265-6-15	we introduce a novel information-theoretic approach for active model selection and demonstrate its effectiveness in a real-world application .	we then apply this framework to rapid screening for noise-induced hearing loss ( nihl ) , a widespread and preventible disability , if diagnosed early .	1	0	2	-0.2282129	0.46747106	1
265-6-15	we construct a gp model for pure-tone audiometric responses of patients with nihl .	we introduce a novel information-theoretic approach for active model selection and demonstrate its effectiveness in a real-world application .	0	3	0	-5.6955223	5.138158	1
265-6-15	using this and a previously published model for healthy responses , the proposed method is shown to be capable of diagnosing the presence or absence of nihl with drastically fewer samples than existing approaches .	we introduce a novel information-theoretic approach for active model selection and demonstrate its effectiveness in a real-world application .	0	4	0	3.9425287	-3.6009083	0
265-6-15	we introduce a novel information-theoretic approach for active model selection and demonstrate its effectiveness in a real-world application .	further , the method is extremely fast and enables the diagnosis to be performed in real time .	1	0	5	-3.3925045	3.3148842	1
265-6-15	we then apply this framework to rapid screening for noise-induced hearing loss ( nihl ) , a widespread and preventible disability , if diagnosed early .	although our method can work with arbitrary models , we focus on actively learning the appropriate structure for gaussian process ( gp ) models with arbitrary observation likelihoods .	0	2	1	5.046341	-4.405464	0
265-6-15	we construct a gp model for pure-tone audiometric responses of patients with nihl .	although our method can work with arbitrary models , we focus on actively learning the appropriate structure for gaussian process ( gp ) models with arbitrary observation likelihoods .	0	3	1	1.9430194	-1.368835	0
265-6-15	using this and a previously published model for healthy responses , the proposed method is shown to be capable of diagnosing the presence or absence of nihl with drastically fewer samples than existing approaches .	although our method can work with arbitrary models , we focus on actively learning the appropriate structure for gaussian process ( gp ) models with arbitrary observation likelihoods .	0	4	1	5.03236	-4.4004745	0
265-6-15	further , the method is extremely fast and enables the diagnosis to be performed in real time .	although our method can work with arbitrary models , we focus on actively learning the appropriate structure for gaussian process ( gp ) models with arbitrary observation likelihoods .	0	5	1	4.9890575	-4.374137	0
265-6-15	we construct a gp model for pure-tone audiometric responses of patients with nihl .	we then apply this framework to rapid screening for noise-induced hearing loss ( nihl ) , a widespread and preventible disability , if diagnosed early .	0	3	2	4.6732864	-4.094772	0
265-6-15	using this and a previously published model for healthy responses , the proposed method is shown to be capable of diagnosing the presence or absence of nihl with drastically fewer samples than existing approaches .	we then apply this framework to rapid screening for noise-induced hearing loss ( nihl ) , a widespread and preventible disability , if diagnosed early .	0	4	2	5.3856587	-4.6646476	0
265-6-15	we then apply this framework to rapid screening for noise-induced hearing loss ( nihl ) , a widespread and preventible disability , if diagnosed early .	further , the method is extremely fast and enables the diagnosis to be performed in real time .	1	2	5	-3.577197	3.4451487	1
265-6-15	using this and a previously published model for healthy responses , the proposed method is shown to be capable of diagnosing the presence or absence of nihl with drastically fewer samples than existing approaches .	we construct a gp model for pure-tone audiometric responses of patients with nihl .	0	4	3	5.0130334	-4.3938613	0
265-6-15	further , the method is extremely fast and enables the diagnosis to be performed in real time .	we construct a gp model for pure-tone audiometric responses of patients with nihl .	0	5	3	4.683494	-4.114462	0
265-6-15	further , the method is extremely fast and enables the diagnosis to be performed in real time .	using this and a previously published model for healthy responses , the proposed method is shown to be capable of diagnosing the presence or absence of nihl with drastically fewer samples than existing approaches .	0	5	4	-4.0507307	3.8230171	1
266-5-10	infinite hidden markov models ( ihmm 's ) are an attractive , nonparametric generalization of the classical hidden markov model which can automatically infer the number of hidden states in the system .	however , due to the infinite-dimensional nature of the transition dynamics , performing inference in the ihmm is difficult .	1	0	1	-5.9096107	5.1212664	1
266-5-10	infinite hidden markov models ( ihmm 's ) are an attractive , nonparametric generalization of the classical hidden markov model which can automatically infer the number of hidden states in the system .	in this paper , we present an infinite-state particle gibbs ( pg ) algorithm to resample state trajectories for the ihmm .	1	0	2	-5.996643	5.1293745	1
266-5-10	infinite hidden markov models ( ihmm 's ) are an attractive , nonparametric generalization of the classical hidden markov model which can automatically infer the number of hidden states in the system .	the proposed algorithm uses an efficient proposal optimized for ihmms and leverages ancestor sampling to improve the mixing of the standard pg algorithm .	1	0	3	-5.9542723	5.11968	1
266-5-10	infinite hidden markov models ( ihmm 's ) are an attractive , nonparametric generalization of the classical hidden markov model which can automatically infer the number of hidden states in the system .	our algorithm demonstrates significant convergence improvements on synthetic and real world data sets .	1	0	4	-5.9997163	5.1236486	1
266-5-10	however , due to the infinite-dimensional nature of the transition dynamics , performing inference in the ihmm is difficult .	in this paper , we present an infinite-state particle gibbs ( pg ) algorithm to resample state trajectories for the ihmm .	1	1	2	-5.854085	5.242564	1
266-5-10	the proposed algorithm uses an efficient proposal optimized for ihmms and leverages ancestor sampling to improve the mixing of the standard pg algorithm .	however , due to the infinite-dimensional nature of the transition dynamics , performing inference in the ihmm is difficult .	0	3	1	5.3954253	-4.74695	0
266-5-10	however , due to the infinite-dimensional nature of the transition dynamics , performing inference in the ihmm is difficult .	our algorithm demonstrates significant convergence improvements on synthetic and real world data sets .	1	1	4	-5.9673595	5.081605	1
266-5-10	in this paper , we present an infinite-state particle gibbs ( pg ) algorithm to resample state trajectories for the ihmm .	the proposed algorithm uses an efficient proposal optimized for ihmms and leverages ancestor sampling to improve the mixing of the standard pg algorithm .	1	2	3	-5.871969	5.215465	1
266-5-10	in this paper , we present an infinite-state particle gibbs ( pg ) algorithm to resample state trajectories for the ihmm .	our algorithm demonstrates significant convergence improvements on synthetic and real world data sets .	1	2	4	-5.9960146	5.167081	1
266-5-10	the proposed algorithm uses an efficient proposal optimized for ihmms and leverages ancestor sampling to improve the mixing of the standard pg algorithm .	our algorithm demonstrates significant convergence improvements on synthetic and real world data sets .	1	3	4	-5.5407214	4.9831533	1
267-8-28	we propose a bayesian mixed-effects model to learn typical scenarios of changes from longitudinal manifold-valued data , namely repeated measurements of the same objects or individuals at several points in time .	the model allows to estimate a group-average trajectory in the space of measurements .	1	0	1	-5.9126687	5.238891	1
267-8-28	we propose a bayesian mixed-effects model to learn typical scenarios of changes from longitudinal manifold-valued data , namely repeated measurements of the same objects or individuals at several points in time .	random variations of this trajectory result from spatiotemporal transformations , which allow changes in the direction of the trajectory and in the pace at which trajectories are followed .	1	0	2	-4.488494	4.1776385	1
267-8-28	we propose a bayesian mixed-effects model to learn typical scenarios of changes from longitudinal manifold-valued data , namely repeated measurements of the same objects or individuals at several points in time .	the use of the tools of riemannian geometry allows to derive a generic algorithm for any kind of data with smooth constraints , which lie therefore on a riemannian manifold .	1	0	3	-5.515616	5.0220776	1
267-8-28	we propose a bayesian mixed-effects model to learn typical scenarios of changes from longitudinal manifold-valued data , namely repeated measurements of the same objects or individuals at several points in time .	stochastic approximations of the expectation-maximization algorithm is used to estimate the model parameters in this highly non-linear setting .	1	0	4	-5.911999	5.243612	1
267-8-28	the method is used to estimate a data-driven model of the progressive impairments of cognitive functions during the onset of alzheimer 's disease .	we propose a bayesian mixed-effects model to learn typical scenarios of changes from longitudinal manifold-valued data , namely repeated measurements of the same objects or individuals at several points in time .	0	5	0	5.5268087	-4.8728914	0
267-8-28	experimental results show that the model correctly put into correspondence the age at which each individual was diagnosed with the disease , thus validating the fact that it effectively estimated a normative scenario of disease progression .	we propose a bayesian mixed-effects model to learn typical scenarios of changes from longitudinal manifold-valued data , namely repeated measurements of the same objects or individuals at several points in time .	0	6	0	5.522022	-4.8292217	0
267-8-28	we propose a bayesian mixed-effects model to learn typical scenarios of changes from longitudinal manifold-valued data , namely repeated measurements of the same objects or individuals at several points in time .	random effects provide unique insights into the variations in the ordering and timing of the succession of cognitive impairments across different individuals .	1	0	7	-5.122414	4.693097	1
267-8-28	random variations of this trajectory result from spatiotemporal transformations , which allow changes in the direction of the trajectory and in the pace at which trajectories are followed .	the model allows to estimate a group-average trajectory in the space of measurements .	0	2	1	2.409708	-2.187224	0
267-8-28	the use of the tools of riemannian geometry allows to derive a generic algorithm for any kind of data with smooth constraints , which lie therefore on a riemannian manifold .	the model allows to estimate a group-average trajectory in the space of measurements .	0	3	1	-1.3997242	1.6891687	1
267-8-28	stochastic approximations of the expectation-maximization algorithm is used to estimate the model parameters in this highly non-linear setting .	the model allows to estimate a group-average trajectory in the space of measurements .	0	4	1	3.7236133	-3.4207602	0
267-8-28	the model allows to estimate a group-average trajectory in the space of measurements .	the method is used to estimate a data-driven model of the progressive impairments of cognitive functions during the onset of alzheimer 's disease .	1	1	5	0.14545363	0.13774836	0
267-8-28	the model allows to estimate a group-average trajectory in the space of measurements .	experimental results show that the model correctly put into correspondence the age at which each individual was diagnosed with the disease , thus validating the fact that it effectively estimated a normative scenario of disease progression .	1	1	6	-4.8634644	4.4620943	1
267-8-28	random effects provide unique insights into the variations in the ordering and timing of the succession of cognitive impairments across different individuals .	the model allows to estimate a group-average trajectory in the space of measurements .	0	7	1	2.4287715	-2.1803188	0
267-8-28	the use of the tools of riemannian geometry allows to derive a generic algorithm for any kind of data with smooth constraints , which lie therefore on a riemannian manifold .	random variations of this trajectory result from spatiotemporal transformations , which allow changes in the direction of the trajectory and in the pace at which trajectories are followed .	0	3	2	0.0013164133	0.28819457	1
267-8-28	stochastic approximations of the expectation-maximization algorithm is used to estimate the model parameters in this highly non-linear setting .	random variations of this trajectory result from spatiotemporal transformations , which allow changes in the direction of the trajectory and in the pace at which trajectories are followed .	0	4	2	2.336774	-2.208144	0
267-8-28	the method is used to estimate a data-driven model of the progressive impairments of cognitive functions during the onset of alzheimer 's disease .	random variations of this trajectory result from spatiotemporal transformations , which allow changes in the direction of the trajectory and in the pace at which trajectories are followed .	0	5	2	4.8040595	-4.4249268	0
267-8-28	experimental results show that the model correctly put into correspondence the age at which each individual was diagnosed with the disease , thus validating the fact that it effectively estimated a normative scenario of disease progression .	random variations of this trajectory result from spatiotemporal transformations , which allow changes in the direction of the trajectory and in the pace at which trajectories are followed .	0	6	2	4.843013	-4.369061	0
267-8-28	random effects provide unique insights into the variations in the ordering and timing of the succession of cognitive impairments across different individuals .	random variations of this trajectory result from spatiotemporal transformations , which allow changes in the direction of the trajectory and in the pace at which trajectories are followed .	0	7	2	3.7127209	-3.4721131	0
267-8-28	stochastic approximations of the expectation-maximization algorithm is used to estimate the model parameters in this highly non-linear setting .	the use of the tools of riemannian geometry allows to derive a generic algorithm for any kind of data with smooth constraints , which lie therefore on a riemannian manifold .	0	4	3	3.1514306	-2.9401217	0
267-8-28	the method is used to estimate a data-driven model of the progressive impairments of cognitive functions during the onset of alzheimer 's disease .	the use of the tools of riemannian geometry allows to derive a generic algorithm for any kind of data with smooth constraints , which lie therefore on a riemannian manifold .	0	5	3	4.9066253	-4.435958	0
267-8-28	experimental results show that the model correctly put into correspondence the age at which each individual was diagnosed with the disease , thus validating the fact that it effectively estimated a normative scenario of disease progression .	the use of the tools of riemannian geometry allows to derive a generic algorithm for any kind of data with smooth constraints , which lie therefore on a riemannian manifold .	0	6	3	4.866845	-4.377716	0
267-8-28	random effects provide unique insights into the variations in the ordering and timing of the succession of cognitive impairments across different individuals .	the use of the tools of riemannian geometry allows to derive a generic algorithm for any kind of data with smooth constraints , which lie therefore on a riemannian manifold .	0	7	3	0.4464602	-0.21404132	0
267-8-28	stochastic approximations of the expectation-maximization algorithm is used to estimate the model parameters in this highly non-linear setting .	the method is used to estimate a data-driven model of the progressive impairments of cognitive functions during the onset of alzheimer 's disease .	1	4	5	-3.5047278	3.4440448	1
267-8-28	experimental results show that the model correctly put into correspondence the age at which each individual was diagnosed with the disease , thus validating the fact that it effectively estimated a normative scenario of disease progression .	stochastic approximations of the expectation-maximization algorithm is used to estimate the model parameters in this highly non-linear setting .	0	6	4	4.3270893	-3.9326844	0
267-8-28	random effects provide unique insights into the variations in the ordering and timing of the succession of cognitive impairments across different individuals .	stochastic approximations of the expectation-maximization algorithm is used to estimate the model parameters in this highly non-linear setting .	0	7	4	-0.025250211	0.35298553	1
267-8-28	experimental results show that the model correctly put into correspondence the age at which each individual was diagnosed with the disease , thus validating the fact that it effectively estimated a normative scenario of disease progression .	the method is used to estimate a data-driven model of the progressive impairments of cognitive functions during the onset of alzheimer 's disease .	0	6	5	3.663756	-3.3991344	0
267-8-28	random effects provide unique insights into the variations in the ordering and timing of the succession of cognitive impairments across different individuals .	the method is used to estimate a data-driven model of the progressive impairments of cognitive functions during the onset of alzheimer 's disease .	0	7	5	-3.5334723	3.4499204	1
267-8-28	random effects provide unique insights into the variations in the ordering and timing of the succession of cognitive impairments across different individuals .	experimental results show that the model correctly put into correspondence the age at which each individual was diagnosed with the disease , thus validating the fact that it effectively estimated a normative scenario of disease progression .	0	7	6	-3.7398555	3.6348822	1
268-7-21	the degree of confidence in one 's choice or decision is a critical aspect of perceptual decision making .	attempts to quantify a decision maker 's confidence by measuring accuracy in a task have yielded limited success because confidence and accuracy are typically not equal .	1	0	1	-5.6815176	5.122718	1
268-7-21	in this paper , we introduce a bayesian framework to model confidence in perceptual decision making .	the degree of confidence in one 's choice or decision is a critical aspect of perceptual decision making .	0	2	0	5.598357	-5.016491	0
268-7-21	the degree of confidence in one 's choice or decision is a critical aspect of perceptual decision making .	we show that this model , based on partially observable markov decision processes ( pomdps ) , is able to predict confidence of a decision maker based only on the data available to the experimenter .	1	0	3	-5.926093	5.175478	1
268-7-21	the degree of confidence in one 's choice or decision is a critical aspect of perceptual decision making .	we test our model on two experiments on confidence-based decision making involving the well-known random dots motion discrimination task .	1	0	4	-5.9435606	5.102613	1
268-7-21	the degree of confidence in one 's choice or decision is a critical aspect of perceptual decision making .	in both experiments , we show that our model 's predictions closely match experimental data .	1	0	5	-5.992795	5.1328316	1
268-7-21	additionally , our model is also consistent with other phenomena such as the hard-easy effect in perceptual decision making .	the degree of confidence in one 's choice or decision is a critical aspect of perceptual decision making .	0	6	0	5.5383673	-4.9508686	0
268-7-21	in this paper , we introduce a bayesian framework to model confidence in perceptual decision making .	attempts to quantify a decision maker 's confidence by measuring accuracy in a task have yielded limited success because confidence and accuracy are typically not equal .	0	2	1	5.382544	-4.749199	0
268-7-21	we show that this model , based on partially observable markov decision processes ( pomdps ) , is able to predict confidence of a decision maker based only on the data available to the experimenter .	attempts to quantify a decision maker 's confidence by measuring accuracy in a task have yielded limited success because confidence and accuracy are typically not equal .	0	3	1	5.577006	-4.9794827	0
268-7-21	we test our model on two experiments on confidence-based decision making involving the well-known random dots motion discrimination task .	attempts to quantify a decision maker 's confidence by measuring accuracy in a task have yielded limited success because confidence and accuracy are typically not equal .	0	4	1	5.6582274	-5.084987	0
268-7-21	in both experiments , we show that our model 's predictions closely match experimental data .	attempts to quantify a decision maker 's confidence by measuring accuracy in a task have yielded limited success because confidence and accuracy are typically not equal .	0	5	1	5.70712	-5.116542	0
268-7-21	attempts to quantify a decision maker 's confidence by measuring accuracy in a task have yielded limited success because confidence and accuracy are typically not equal .	additionally , our model is also consistent with other phenomena such as the hard-easy effect in perceptual decision making .	1	1	6	-5.9835496	5.1535983	1
268-7-21	we show that this model , based on partially observable markov decision processes ( pomdps ) , is able to predict confidence of a decision maker based only on the data available to the experimenter .	in this paper , we introduce a bayesian framework to model confidence in perceptual decision making .	0	3	2	5.511447	-4.895876	0
268-7-21	in this paper , we introduce a bayesian framework to model confidence in perceptual decision making .	we test our model on two experiments on confidence-based decision making involving the well-known random dots motion discrimination task .	1	2	4	-5.952965	5.145335	1
268-7-21	in both experiments , we show that our model 's predictions closely match experimental data .	in this paper , we introduce a bayesian framework to model confidence in perceptual decision making .	0	5	2	5.633622	-4.993161	0
268-7-21	additionally , our model is also consistent with other phenomena such as the hard-easy effect in perceptual decision making .	in this paper , we introduce a bayesian framework to model confidence in perceptual decision making .	0	6	2	5.5076675	-4.816476	0
268-7-21	we test our model on two experiments on confidence-based decision making involving the well-known random dots motion discrimination task .	we show that this model , based on partially observable markov decision processes ( pomdps ) , is able to predict confidence of a decision maker based only on the data available to the experimenter .	0	4	3	4.0518327	-3.6563478	0
268-7-21	in both experiments , we show that our model 's predictions closely match experimental data .	we show that this model , based on partially observable markov decision processes ( pomdps ) , is able to predict confidence of a decision maker based only on the data available to the experimenter .	0	5	3	5.3166585	-4.680479	0
268-7-21	additionally , our model is also consistent with other phenomena such as the hard-easy effect in perceptual decision making .	we show that this model , based on partially observable markov decision processes ( pomdps ) , is able to predict confidence of a decision maker based only on the data available to the experimenter .	0	6	3	4.81623	-4.240598	0
268-7-21	in both experiments , we show that our model 's predictions closely match experimental data .	we test our model on two experiments on confidence-based decision making involving the well-known random dots motion discrimination task .	0	5	4	4.8536167	-4.306544	0
268-7-21	we test our model on two experiments on confidence-based decision making involving the well-known random dots motion discrimination task .	additionally , our model is also consistent with other phenomena such as the hard-easy effect in perceptual decision making .	1	4	6	-4.427082	4.059152	1
268-7-21	in both experiments , we show that our model 's predictions closely match experimental data .	additionally , our model is also consistent with other phenomena such as the hard-easy effect in perceptual decision making .	1	5	6	-2.1411028	2.1391153	1
269-3-3	we argue for a geometry invariant to rescaling of weights that does not affect the output of the network , and suggest path-sgd , which is an approximate steepest descent method with respect to a path-wise regularizer related to max-norm regularization .	we revisit the choice of sgd for training deep neural networks by reconsidering the appropriate geometry in which to optimize the weights .	0	1	0	2.519911	-2.243816	0
269-3-3	path-sgd is easy and efficient to implement and leads to empirical gains over sgd and adagrad .	we revisit the choice of sgd for training deep neural networks by reconsidering the appropriate geometry in which to optimize the weights .	0	2	0	4.42857	-3.9806166	0
269-3-3	path-sgd is easy and efficient to implement and leads to empirical gains over sgd and adagrad .	we argue for a geometry invariant to rescaling of weights that does not affect the output of the network , and suggest path-sgd , which is an approximate steepest descent method with respect to a path-wise regularizer related to max-norm regularization .	0	2	1	4.696411	-4.160987	0
270-8-28	variable screening is a fast dimension reduction technique for assisting high dimensional feature selection .	as a preselection method , it selects a moderate size subset of candidate variables for further refining via feature selection to produce the final model .	1	0	1	-5.861585	5.2046776	1
270-8-28	the performance of variable screening depends on both computational efficiency and the ability to dramatically reduce the number of variables without discarding the important ones .	variable screening is a fast dimension reduction technique for assisting high dimensional feature selection .	0	2	0	5.5197167	-4.9686394	0
270-8-28	variable screening is a fast dimension reduction technique for assisting high dimensional feature selection .	when the data dimension p is substantially larger than the sample size n , variable screening becomes crucial as 1 ) faster feature selection algorithms are needed ; 2 ) conditions guaranteeing selection consistency might fail to hold .	1	0	3	-5.8585873	5.2163105	1
270-8-28	variable screening is a fast dimension reduction technique for assisting high dimensional feature selection .	this article studies a class of linear screening methods and establishes consistency theory for this special class .	1	0	4	-5.81172	5.189013	1
270-8-28	in particular , we prove the restricted diagonally dominant ( rdd ) condition is a necessary and sufficient condition for strong screening consistency .	variable screening is a fast dimension reduction technique for assisting high dimensional feature selection .	0	5	0	5.4800434	-4.898832	0
270-8-28	variable screening is a fast dimension reduction technique for assisting high dimensional feature selection .	as concrete examples , we show two screening methods sis and holp are both strong screening consistent ( subject to additional constraints ) with large probability if n > o ( ( s+/ ) 2 log p ) under random designs .	1	0	6	-5.9361944	5.1655836	1
270-8-28	in addition , we relate the rdd condition to the irrepresentable condition , and highlight limitations of sis .	variable screening is a fast dimension reduction technique for assisting high dimensional feature selection .	0	7	0	5.4575543	-4.938396	0
270-8-28	as a preselection method , it selects a moderate size subset of candidate variables for further refining via feature selection to produce the final model .	the performance of variable screening depends on both computational efficiency and the ability to dramatically reduce the number of variables without discarding the important ones .	1	1	2	3.1771727	-2.9620876	0
270-8-28	as a preselection method , it selects a moderate size subset of candidate variables for further refining via feature selection to produce the final model .	when the data dimension p is substantially larger than the sample size n , variable screening becomes crucial as 1 ) faster feature selection algorithms are needed ; 2 ) conditions guaranteeing selection consistency might fail to hold .	1	1	3	5.053807	-4.4953218	0
270-8-28	as a preselection method , it selects a moderate size subset of candidate variables for further refining via feature selection to produce the final model .	this article studies a class of linear screening methods and establishes consistency theory for this special class .	1	1	4	5.04933	-4.580891	0
270-8-28	in particular , we prove the restricted diagonally dominant ( rdd ) condition is a necessary and sufficient condition for strong screening consistency .	as a preselection method , it selects a moderate size subset of candidate variables for further refining via feature selection to produce the final model .	0	5	1	-5.3314104	4.8961115	1
270-8-28	as a preselection method , it selects a moderate size subset of candidate variables for further refining via feature selection to produce the final model .	as concrete examples , we show two screening methods sis and holp are both strong screening consistent ( subject to additional constraints ) with large probability if n > o ( ( s+/ ) 2 log p ) under random designs .	1	1	6	-5.271224	4.753993	1
270-8-28	as a preselection method , it selects a moderate size subset of candidate variables for further refining via feature selection to produce the final model .	in addition , we relate the rdd condition to the irrepresentable condition , and highlight limitations of sis .	1	1	7	0.76148033	-0.6268984	0
270-8-28	the performance of variable screening depends on both computational efficiency and the ability to dramatically reduce the number of variables without discarding the important ones .	when the data dimension p is substantially larger than the sample size n , variable screening becomes crucial as 1 ) faster feature selection algorithms are needed ; 2 ) conditions guaranteeing selection consistency might fail to hold .	1	2	3	4.8305836	-4.2607512	0
270-8-28	this article studies a class of linear screening methods and establishes consistency theory for this special class .	the performance of variable screening depends on both computational efficiency and the ability to dramatically reduce the number of variables without discarding the important ones .	0	4	2	3.7167776	-3.3384376	0
270-8-28	in particular , we prove the restricted diagonally dominant ( rdd ) condition is a necessary and sufficient condition for strong screening consistency .	the performance of variable screening depends on both computational efficiency and the ability to dramatically reduce the number of variables without discarding the important ones .	0	5	2	2.0862644	-1.869976	0
270-8-28	as concrete examples , we show two screening methods sis and holp are both strong screening consistent ( subject to additional constraints ) with large probability if n > o ( ( s+/ ) 2 log p ) under random designs .	the performance of variable screening depends on both computational efficiency and the ability to dramatically reduce the number of variables without discarding the important ones .	0	6	2	4.9104505	-4.3197412	0
270-8-28	the performance of variable screening depends on both computational efficiency and the ability to dramatically reduce the number of variables without discarding the important ones .	in addition , we relate the rdd condition to the irrepresentable condition , and highlight limitations of sis .	1	2	7	-5.6738596	5.1024294	1
270-8-28	when the data dimension p is substantially larger than the sample size n , variable screening becomes crucial as 1 ) faster feature selection algorithms are needed ; 2 ) conditions guaranteeing selection consistency might fail to hold .	this article studies a class of linear screening methods and establishes consistency theory for this special class .	1	3	4	-5.095659	4.6580887	1
270-8-28	in particular , we prove the restricted diagonally dominant ( rdd ) condition is a necessary and sufficient condition for strong screening consistency .	when the data dimension p is substantially larger than the sample size n , variable screening becomes crucial as 1 ) faster feature selection algorithms are needed ; 2 ) conditions guaranteeing selection consistency might fail to hold .	0	5	3	4.2188253	-3.766759	0
270-8-28	when the data dimension p is substantially larger than the sample size n , variable screening becomes crucial as 1 ) faster feature selection algorithms are needed ; 2 ) conditions guaranteeing selection consistency might fail to hold .	as concrete examples , we show two screening methods sis and holp are both strong screening consistent ( subject to additional constraints ) with large probability if n > o ( ( s+/ ) 2 log p ) under random designs .	1	3	6	-5.982072	5.0499735	1
270-8-28	when the data dimension p is substantially larger than the sample size n , variable screening becomes crucial as 1 ) faster feature selection algorithms are needed ; 2 ) conditions guaranteeing selection consistency might fail to hold .	in addition , we relate the rdd condition to the irrepresentable condition , and highlight limitations of sis .	1	3	7	-5.984577	5.069332	1
270-8-28	in particular , we prove the restricted diagonally dominant ( rdd ) condition is a necessary and sufficient condition for strong screening consistency .	this article studies a class of linear screening methods and establishes consistency theory for this special class .	0	5	4	4.591625	-4.0791535	0
270-8-28	this article studies a class of linear screening methods and establishes consistency theory for this special class .	as concrete examples , we show two screening methods sis and holp are both strong screening consistent ( subject to additional constraints ) with large probability if n > o ( ( s+/ ) 2 log p ) under random designs .	1	4	6	-5.996201	5.147764	1
270-8-28	in addition , we relate the rdd condition to the irrepresentable condition , and highlight limitations of sis .	this article studies a class of linear screening methods and establishes consistency theory for this special class .	0	7	4	3.999583	-3.6290247	0
270-8-28	in particular , we prove the restricted diagonally dominant ( rdd ) condition is a necessary and sufficient condition for strong screening consistency .	as concrete examples , we show two screening methods sis and holp are both strong screening consistent ( subject to additional constraints ) with large probability if n > o ( ( s+/ ) 2 log p ) under random designs .	1	5	6	-5.8800764	5.114866	1
270-8-28	in addition , we relate the rdd condition to the irrepresentable condition , and highlight limitations of sis .	in particular , we prove the restricted diagonally dominant ( rdd ) condition is a necessary and sufficient condition for strong screening consistency .	0	7	5	5.5752325	-4.925932	0
270-8-28	in addition , we relate the rdd condition to the irrepresentable condition , and highlight limitations of sis .	as concrete examples , we show two screening methods sis and holp are both strong screening consistent ( subject to additional constraints ) with large probability if n > o ( ( s+/ ) 2 log p ) under random designs .	0	7	6	-4.6684895	4.3484488	1
271-7-21	we introduce a neural network with a recurrent attention model over a possibly large external memory .	the architecture is a form of memory network but unlike the model in that work , it is trained end-to-end , and hence requires significantly less supervision during training , making it more generally applicable in realistic settings .	1	0	1	-5.8687606	5.192135	1
271-7-21	we introduce a neural network with a recurrent attention model over a possibly large external memory .	it can also be seen as an extension of rnnsearch to the case where multiple computational steps ( hops ) are performed per output symbol .	1	0	2	-5.8634114	5.083364	1
271-7-21	the flexibility of the model allows us to apply it to tasks as diverse as ( synthetic ) question answering and to language modeling .	we introduce a neural network with a recurrent attention model over a possibly large external memory .	0	3	0	5.4717417	-4.8080196	0
271-7-21	for the former our approach is competitive with memory networks , but with less supervision .	we introduce a neural network with a recurrent attention model over a possibly large external memory .	0	4	0	4.248269	-3.799271	0
271-7-21	we introduce a neural network with a recurrent attention model over a possibly large external memory .	for the latter , on the penn treebank and text8 datasets our approach demonstrates comparable performance to rnns and lstms .	1	0	5	-5.988128	5.1332254	1
271-7-21	we introduce a neural network with a recurrent attention model over a possibly large external memory .	in both cases we show that the key concept of multiple computational hops yields improved results .	1	0	6	-5.9089823	4.9818273	1
271-7-21	the architecture is a form of memory network but unlike the model in that work , it is trained end-to-end , and hence requires significantly less supervision during training , making it more generally applicable in realistic settings .	it can also be seen as an extension of rnnsearch to the case where multiple computational steps ( hops ) are performed per output symbol .	1	1	2	-2.3502903	2.4188693	1
271-7-21	the flexibility of the model allows us to apply it to tasks as diverse as ( synthetic ) question answering and to language modeling .	the architecture is a form of memory network but unlike the model in that work , it is trained end-to-end , and hence requires significantly less supervision during training , making it more generally applicable in realistic settings .	0	3	1	3.6590636	-3.3928895	0
271-7-21	the architecture is a form of memory network but unlike the model in that work , it is trained end-to-end , and hence requires significantly less supervision during training , making it more generally applicable in realistic settings .	for the former our approach is competitive with memory networks , but with less supervision .	1	1	4	2.4176552	-2.1713355	0
271-7-21	for the latter , on the penn treebank and text8 datasets our approach demonstrates comparable performance to rnns and lstms .	the architecture is a form of memory network but unlike the model in that work , it is trained end-to-end , and hence requires significantly less supervision during training , making it more generally applicable in realistic settings .	0	5	1	4.3244176	-3.9495878	0
271-7-21	in both cases we show that the key concept of multiple computational hops yields improved results .	the architecture is a form of memory network but unlike the model in that work , it is trained end-to-end , and hence requires significantly less supervision during training , making it more generally applicable in realistic settings .	0	6	1	4.0814266	-3.7786946	0
271-7-21	the flexibility of the model allows us to apply it to tasks as diverse as ( synthetic ) question answering and to language modeling .	it can also be seen as an extension of rnnsearch to the case where multiple computational steps ( hops ) are performed per output symbol .	0	3	2	2.6892807	-2.5421898	0
271-7-21	for the former our approach is competitive with memory networks , but with less supervision .	it can also be seen as an extension of rnnsearch to the case where multiple computational steps ( hops ) are performed per output symbol .	0	4	2	-4.8711877	4.5628943	1
271-7-21	it can also be seen as an extension of rnnsearch to the case where multiple computational steps ( hops ) are performed per output symbol .	for the latter , on the penn treebank and text8 datasets our approach demonstrates comparable performance to rnns and lstms .	1	2	5	-4.600717	4.2693353	1
271-7-21	it can also be seen as an extension of rnnsearch to the case where multiple computational steps ( hops ) are performed per output symbol .	in both cases we show that the key concept of multiple computational hops yields improved results .	1	2	6	-4.9775186	4.6141667	1
271-7-21	the flexibility of the model allows us to apply it to tasks as diverse as ( synthetic ) question answering and to language modeling .	for the former our approach is competitive with memory networks , but with less supervision .	1	3	4	3.9106612	-3.604321	0
271-7-21	for the latter , on the penn treebank and text8 datasets our approach demonstrates comparable performance to rnns and lstms .	the flexibility of the model allows us to apply it to tasks as diverse as ( synthetic ) question answering and to language modeling .	0	5	3	3.1329792	-2.9687042	0
271-7-21	the flexibility of the model allows us to apply it to tasks as diverse as ( synthetic ) question answering and to language modeling .	in both cases we show that the key concept of multiple computational hops yields improved results .	1	3	6	-2.7310805	2.6717837	1
271-7-21	for the latter , on the penn treebank and text8 datasets our approach demonstrates comparable performance to rnns and lstms .	for the former our approach is competitive with memory networks , but with less supervision .	0	5	4	4.692964	-4.209649	0
271-7-21	in both cases we show that the key concept of multiple computational hops yields improved results .	for the former our approach is competitive with memory networks , but with less supervision .	0	6	4	4.1477866	-3.7996597	0
271-7-21	in both cases we show that the key concept of multiple computational hops yields improved results .	for the latter , on the penn treebank and text8 datasets our approach demonstrates comparable performance to rnns and lstms .	0	6	5	-1.9467607	1.9927037	1
272-10-45	in this work , we demonstrate that , beyond its advantages for efficient computation , the spectral domain also provides a powerful representation in which to model and train convolutional neural networks ( cnns ) .	discrete fourier transforms provide a significant speedup in the computation of convolutions in deep learning .	0	1	0	5.615274	-4.9727635	0
272-10-45	discrete fourier transforms provide a significant speedup in the computation of convolutions in deep learning .	we employ spectral representations to introduce a number of innovations to cnn design .	1	0	2	-6.0295568	5.2007666	1
272-10-45	discrete fourier transforms provide a significant speedup in the computation of convolutions in deep learning .	first , we propose spectral pooling , which performs dimensionality reduction by truncating the representation in the frequency domain .	1	0	3	-5.944024	5.2105093	1
272-10-45	this approach preserves considerably more information per parameter than other pooling strategies and enables flexibility in the choice of pooling output dimensionality .	discrete fourier transforms provide a significant speedup in the computation of convolutions in deep learning .	0	4	0	5.666461	-5.0614767	0
272-10-45	discrete fourier transforms provide a significant speedup in the computation of convolutions in deep learning .	this representation also enables a new form of stochastic regularization by randomized modification of resolution .	1	0	5	-5.9032865	5.168191	1
272-10-45	discrete fourier transforms provide a significant speedup in the computation of convolutions in deep learning .	we show that these methods achieve competitive results on classification and approximation tasks , without using any dropout or max-pooling .	1	0	6	-5.9817877	5.170722	1
272-10-45	discrete fourier transforms provide a significant speedup in the computation of convolutions in deep learning .	finally , we demonstrate the effectiveness of complex-coefficient spectral parameterization of convolutional filters .	1	0	7	-5.908725	5.1655383	1
272-10-45	while this leaves the underlying model unchanged , it results in a representation that greatly facilitates optimization .	discrete fourier transforms provide a significant speedup in the computation of convolutions in deep learning .	0	8	0	5.591254	-4.971581	0
272-10-45	discrete fourier transforms provide a significant speedup in the computation of convolutions in deep learning .	we observe on a variety of popular cnn configurations that this leads to significantly faster convergence during training .	1	0	9	-5.9429555	5.1617293	1
272-10-45	we employ spectral representations to introduce a number of innovations to cnn design .	in this work , we demonstrate that , beyond its advantages for efficient computation , the spectral domain also provides a powerful representation in which to model and train convolutional neural networks ( cnns ) .	0	2	1	1.575498	-1.1015776	0
272-10-45	first , we propose spectral pooling , which performs dimensionality reduction by truncating the representation in the frequency domain .	in this work , we demonstrate that , beyond its advantages for efficient computation , the spectral domain also provides a powerful representation in which to model and train convolutional neural networks ( cnns ) .	0	3	1	-1.33612	1.5298947	1
272-10-45	in this work , we demonstrate that , beyond its advantages for efficient computation , the spectral domain also provides a powerful representation in which to model and train convolutional neural networks ( cnns ) .	this approach preserves considerably more information per parameter than other pooling strategies and enables flexibility in the choice of pooling output dimensionality .	1	1	4	-2.7709725	2.854004	1
272-10-45	this representation also enables a new form of stochastic regularization by randomized modification of resolution .	in this work , we demonstrate that , beyond its advantages for efficient computation , the spectral domain also provides a powerful representation in which to model and train convolutional neural networks ( cnns ) .	0	5	1	3.0645096	-2.8362594	0
272-10-45	we show that these methods achieve competitive results on classification and approximation tasks , without using any dropout or max-pooling .	in this work , we demonstrate that , beyond its advantages for efficient computation , the spectral domain also provides a powerful representation in which to model and train convolutional neural networks ( cnns ) .	0	6	1	4.9803476	-4.443757	0
272-10-45	in this work , we demonstrate that , beyond its advantages for efficient computation , the spectral domain also provides a powerful representation in which to model and train convolutional neural networks ( cnns ) .	finally , we demonstrate the effectiveness of complex-coefficient spectral parameterization of convolutional filters .	1	1	7	-6.0016327	5.2158184	1
272-10-45	in this work , we demonstrate that , beyond its advantages for efficient computation , the spectral domain also provides a powerful representation in which to model and train convolutional neural networks ( cnns ) .	while this leaves the underlying model unchanged , it results in a representation that greatly facilitates optimization .	1	1	8	-2.5320108	2.6066332	1
272-10-45	we observe on a variety of popular cnn configurations that this leads to significantly faster convergence during training .	in this work , we demonstrate that , beyond its advantages for efficient computation , the spectral domain also provides a powerful representation in which to model and train convolutional neural networks ( cnns ) .	0	9	1	5.4296546	-4.773241	0
272-10-45	we employ spectral representations to introduce a number of innovations to cnn design .	first , we propose spectral pooling , which performs dimensionality reduction by truncating the representation in the frequency domain .	1	2	3	-5.7237744	5.251981	1
272-10-45	we employ spectral representations to introduce a number of innovations to cnn design .	this approach preserves considerably more information per parameter than other pooling strategies and enables flexibility in the choice of pooling output dimensionality .	1	2	4	-3.6080365	3.5384614	1
272-10-45	we employ spectral representations to introduce a number of innovations to cnn design .	this representation also enables a new form of stochastic regularization by randomized modification of resolution .	1	2	5	-5.8157268	5.185464	1
272-10-45	we show that these methods achieve competitive results on classification and approximation tasks , without using any dropout or max-pooling .	we employ spectral representations to introduce a number of innovations to cnn design .	0	6	2	4.687517	-4.196546	0
272-10-45	we employ spectral representations to introduce a number of innovations to cnn design .	finally , we demonstrate the effectiveness of complex-coefficient spectral parameterization of convolutional filters .	1	2	7	-5.9348683	5.2355456	1
272-10-45	while this leaves the underlying model unchanged , it results in a representation that greatly facilitates optimization .	we employ spectral representations to introduce a number of innovations to cnn design .	0	8	2	-0.56646603	0.8765239	1
272-10-45	we employ spectral representations to introduce a number of innovations to cnn design .	we observe on a variety of popular cnn configurations that this leads to significantly faster convergence during training .	1	2	9	-5.7775917	5.1989737	1
272-10-45	this approach preserves considerably more information per parameter than other pooling strategies and enables flexibility in the choice of pooling output dimensionality .	first , we propose spectral pooling , which performs dimensionality reduction by truncating the representation in the frequency domain .	0	4	3	4.5731454	-4.0772943	0
272-10-45	first , we propose spectral pooling , which performs dimensionality reduction by truncating the representation in the frequency domain .	this representation also enables a new form of stochastic regularization by randomized modification of resolution .	1	3	5	-5.3979025	4.9134493	1
272-10-45	we show that these methods achieve competitive results on classification and approximation tasks , without using any dropout or max-pooling .	first , we propose spectral pooling , which performs dimensionality reduction by truncating the representation in the frequency domain .	0	6	3	4.9800386	-4.416769	0
272-10-45	finally , we demonstrate the effectiveness of complex-coefficient spectral parameterization of convolutional filters .	first , we propose spectral pooling , which performs dimensionality reduction by truncating the representation in the frequency domain .	0	7	3	5.3479233	-4.7095003	0
272-10-45	while this leaves the underlying model unchanged , it results in a representation that greatly facilitates optimization .	first , we propose spectral pooling , which performs dimensionality reduction by truncating the representation in the frequency domain .	0	8	3	3.539044	-3.313373	0
272-10-45	first , we propose spectral pooling , which performs dimensionality reduction by truncating the representation in the frequency domain .	we observe on a variety of popular cnn configurations that this leads to significantly faster convergence during training .	1	3	9	-5.4801087	5.004285	1
272-10-45	this approach preserves considerably more information per parameter than other pooling strategies and enables flexibility in the choice of pooling output dimensionality .	this representation also enables a new form of stochastic regularization by randomized modification of resolution .	1	4	5	-2.4398487	2.6001546	1
272-10-45	this approach preserves considerably more information per parameter than other pooling strategies and enables flexibility in the choice of pooling output dimensionality .	we show that these methods achieve competitive results on classification and approximation tasks , without using any dropout or max-pooling .	1	4	6	-4.5705786	4.2585745	1
272-10-45	this approach preserves considerably more information per parameter than other pooling strategies and enables flexibility in the choice of pooling output dimensionality .	finally , we demonstrate the effectiveness of complex-coefficient spectral parameterization of convolutional filters .	1	4	7	-5.8803105	5.1744146	1
272-10-45	while this leaves the underlying model unchanged , it results in a representation that greatly facilitates optimization .	this approach preserves considerably more information per parameter than other pooling strategies and enables flexibility in the choice of pooling output dimensionality .	0	8	4	-2.8414667	2.910654	1
272-10-45	this approach preserves considerably more information per parameter than other pooling strategies and enables flexibility in the choice of pooling output dimensionality .	we observe on a variety of popular cnn configurations that this leads to significantly faster convergence during training .	1	4	9	-4.408868	4.1816406	1
272-10-45	we show that these methods achieve competitive results on classification and approximation tasks , without using any dropout or max-pooling .	this representation also enables a new form of stochastic regularization by randomized modification of resolution .	0	6	5	3.3892581	-3.1710587	0
272-10-45	this representation also enables a new form of stochastic regularization by randomized modification of resolution .	finally , we demonstrate the effectiveness of complex-coefficient spectral parameterization of convolutional filters .	1	5	7	-5.689467	5.0298996	1
272-10-45	this representation also enables a new form of stochastic regularization by randomized modification of resolution .	while this leaves the underlying model unchanged , it results in a representation that greatly facilitates optimization .	1	5	8	4.634225	-4.1630807	0
272-10-45	this representation also enables a new form of stochastic regularization by randomized modification of resolution .	we observe on a variety of popular cnn configurations that this leads to significantly faster convergence during training .	1	5	9	-2.9529567	2.9604025	1
272-10-45	finally , we demonstrate the effectiveness of complex-coefficient spectral parameterization of convolutional filters .	we show that these methods achieve competitive results on classification and approximation tasks , without using any dropout or max-pooling .	0	7	6	3.337665	-3.1745505	0
272-10-45	we show that these methods achieve competitive results on classification and approximation tasks , without using any dropout or max-pooling .	while this leaves the underlying model unchanged , it results in a representation that greatly facilitates optimization .	1	6	8	4.240286	-3.910276	0
272-10-45	we observe on a variety of popular cnn configurations that this leads to significantly faster convergence during training .	we show that these methods achieve competitive results on classification and approximation tasks , without using any dropout or max-pooling .	0	9	6	-3.207535	3.1406877	1
272-10-45	while this leaves the underlying model unchanged , it results in a representation that greatly facilitates optimization .	finally , we demonstrate the effectiveness of complex-coefficient spectral parameterization of convolutional filters .	0	8	7	-5.8403854	4.9502687	1
272-10-45	finally , we demonstrate the effectiveness of complex-coefficient spectral parameterization of convolutional filters .	we observe on a variety of popular cnn configurations that this leads to significantly faster convergence during training .	1	7	9	4.3764796	-3.9900298	0
272-10-45	we observe on a variety of popular cnn configurations that this leads to significantly faster convergence during training .	while this leaves the underlying model unchanged , it results in a representation that greatly facilitates optimization .	0	9	8	4.0825377	-3.7904053	0
273-4-6	generalizing from the batch setting for boosting , the notion of a weak learning algorithm is modeled as an online learning algorithm with linear loss functions that competes with a base class of regression functions , while a strong learning algorithm is an online learning algorithm with smooth convex loss functions that competes with a larger class of regression functions .	we extend the theory of boosting for regression problems to the online learning setting .	0	1	0	-2.5552187	2.6663094	1
273-4-6	our main result is an online gradient boosting algorithm that converts a weak online learning algorithm into a strong one where the larger class of functions is the linear span of the base class .	we extend the theory of boosting for regression problems to the online learning setting .	0	2	0	4.9540253	-4.4210987	0
273-4-6	we also give a simpler boosting algorithm that converts a weak online learning algorithm into a strong one where the larger class of functions is the convex hull of the base class , and prove its optimality .	we extend the theory of boosting for regression problems to the online learning setting .	0	3	0	5.3280816	-4.735761	0
273-4-6	[CLS] generalizing from the batch setting for boosting, the notion of a weak learning algorithm is modeled as an online learning algorithm with linear loss functions that competes with a base class of regression functions, while a strong learning algorithm is an online learning algorithm with smooth convex loss functions that competes with a larger class of regression	our main result is an online gradient boosting algorithm that converts a weak online learning algorithm into a strong one where the larger class of functions is the linear span of the base class .	1	1	2	-3.186835	3.1426172	1
273-4-6	[CLS] generalizing from the batch setting for boosting, the notion of a weak learning algorithm is modeled as an online learning algorithm with linear loss functions that competes with a base class of regression functions, while a strong learning algorithm is an online learning algorithm with smooth convex loss functions that competes with a	we also give a simpler boosting algorithm that converts a weak online learning algorithm into a strong one where the larger class of functions is the convex hull of the base class , and prove its optimality .	1	1	3	-5.690791	5.10741	1
273-4-6	we also give a simpler boosting algorithm that converts a weak online learning algorithm into a strong one where the larger class of functions is the convex hull of the base class , and prove its optimality .	our main result is an online gradient boosting algorithm that converts a weak online learning algorithm into a strong one where the larger class of functions is the linear span of the base class .	0	3	2	4.9412403	-4.449948	0
274-6-15	the multi-layered model is designed by constructing a hierarchy of temporal sigmoid belief networks ( tsbns ) , defined as a sequential stack of sigmoid belief networks ( sbns ) .	deep dynamic generative models are developed to learn sequential dependencies in time-series data .	0	1	0	5.2690983	-4.651021	0
274-6-15	deep dynamic generative models are developed to learn sequential dependencies in time-series data .	each sbn has a contextual hidden state , inherited from the previous sbns in the sequence , and is used to regulate its hidden bias .	1	0	2	-5.7251616	5.2025127	1
274-6-15	deep dynamic generative models are developed to learn sequential dependencies in time-series data .	scalable learning and inference algorithms are derived by introducing a recognition model that yields fast sampling from the variational posterior .	1	0	3	-5.9373617	5.1001887	1
274-6-15	deep dynamic generative models are developed to learn sequential dependencies in time-series data .	this recognition model is trained jointly with the generative model , by maximizing its variational lower bound on the log-likelihood .	1	0	4	-5.9567986	5.20828	1
274-6-15	deep dynamic generative models are developed to learn sequential dependencies in time-series data .	experimental results on bouncing balls , polyphonic music , motion capture , and text streams show that the proposed approach achieves state-of-the-art predictive performance , and has the capacity to synthesize various sequences .	1	0	5	-6.028942	5.180439	1
274-6-15	the multi-layered model is designed by constructing a hierarchy of temporal sigmoid belief networks ( tsbns ) , defined as a sequential stack of sigmoid belief networks ( sbns ) .	each sbn has a contextual hidden state , inherited from the previous sbns in the sequence , and is used to regulate its hidden bias .	1	1	2	-5.766591	5.1891623	1
274-6-15	scalable learning and inference algorithms are derived by introducing a recognition model that yields fast sampling from the variational posterior .	the multi-layered model is designed by constructing a hierarchy of temporal sigmoid belief networks ( tsbns ) , defined as a sequential stack of sigmoid belief networks ( sbns ) .	0	3	1	4.954803	-4.3351192	0
274-6-15	the multi-layered model is designed by constructing a hierarchy of temporal sigmoid belief networks ( tsbns ) , defined as a sequential stack of sigmoid belief networks ( sbns ) .	this recognition model is trained jointly with the generative model , by maximizing its variational lower bound on the log-likelihood .	1	1	4	-5.6881704	5.1212955	1
274-6-15	experimental results on bouncing balls , polyphonic music , motion capture , and text streams show that the proposed approach achieves state-of-the-art predictive performance , and has the capacity to synthesize various sequences .	the multi-layered model is designed by constructing a hierarchy of temporal sigmoid belief networks ( tsbns ) , defined as a sequential stack of sigmoid belief networks ( sbns ) .	0	5	1	5.3899403	-4.738841	0
274-6-15	scalable learning and inference algorithms are derived by introducing a recognition model that yields fast sampling from the variational posterior .	each sbn has a contextual hidden state , inherited from the previous sbns in the sequence , and is used to regulate its hidden bias .	0	3	2	4.464592	-4.103387	0
274-6-15	this recognition model is trained jointly with the generative model , by maximizing its variational lower bound on the log-likelihood .	each sbn has a contextual hidden state , inherited from the previous sbns in the sequence , and is used to regulate its hidden bias .	0	4	2	3.0255742	-2.7816496	0
274-6-15	each sbn has a contextual hidden state , inherited from the previous sbns in the sequence , and is used to regulate its hidden bias .	experimental results on bouncing balls , polyphonic music , motion capture , and text streams show that the proposed approach achieves state-of-the-art predictive performance , and has the capacity to synthesize various sequences .	1	2	5	-6.009249	5.1220503	1
274-6-15	this recognition model is trained jointly with the generative model , by maximizing its variational lower bound on the log-likelihood .	scalable learning and inference algorithms are derived by introducing a recognition model that yields fast sampling from the variational posterior .	0	4	3	3.3813102	-3.0953877	0
274-6-15	experimental results on bouncing balls , polyphonic music , motion capture , and text streams show that the proposed approach achieves state-of-the-art predictive performance , and has the capacity to synthesize various sequences .	scalable learning and inference algorithms are derived by introducing a recognition model that yields fast sampling from the variational posterior .	0	5	3	5.0272274	-4.5258703	0
274-6-15	this recognition model is trained jointly with the generative model , by maximizing its variational lower bound on the log-likelihood .	experimental results on bouncing balls , polyphonic music , motion capture , and text streams show that the proposed approach achieves state-of-the-art predictive performance , and has the capacity to synthesize various sequences .	1	4	5	-5.7951174	5.059599	1
275-8-28	therefore , a key step in understanding neural systems is to reliably distinguish cell types .	many neural circuits are composed of numerous distinct cell types that perform different operations on their inputs , and send their outputs to distinct targets .	0	1	0	5.378231	-4.6329308	0
275-8-28	many neural circuits are composed of numerous distinct cell types that perform different operations on their inputs , and send their outputs to distinct targets .	an important example is the retina , for which present-day techniques for identifying cell types are accurate , but very labor-intensive .	1	0	2	-5.5931177	5.072134	1
275-8-28	many neural circuits are composed of numerous distinct cell types that perform different operations on their inputs , and send their outputs to distinct targets .	here , we develop automated classifiers for functional identification of retinal ganglion cells , the output neurons of the retina , based solely on recorded voltage patterns on a large scale array .	1	0	3	-5.878114	5.0874896	1
275-8-28	many neural circuits are composed of numerous distinct cell types that perform different operations on their inputs , and send their outputs to distinct targets .	we use per-cell classifiers based on features extracted from electrophysiological images ( spatiotemporal voltage waveforms ) and interspike intervals ( autocorrelations ) .	1	0	4	-5.9255686	5.018083	1
275-8-28	many neural circuits are composed of numerous distinct cell types that perform different operations on their inputs , and send their outputs to distinct targets .	these classifiers achieve high performance in distinguishing between the major ganglion cell classes of the primate retina , but fail in achieving the same accuracy in predicting cell polarities ( on vs. off ) .	1	0	5	-5.979912	5.098354	1
275-8-28	many neural circuits are composed of numerous distinct cell types that perform different operations on their inputs , and send their outputs to distinct targets .	we then show how to use indicators of functional coupling within populations of ganglion cells ( cross-correlation ) to infer cell polarities with a matrix completion algorithm .	1	0	6	-5.9193325	4.9926167	1
275-8-28	this can result in accurate , fully automated methods for cell type classification .	many neural circuits are composed of numerous distinct cell types that perform different operations on their inputs , and send their outputs to distinct targets .	0	7	0	5.7447195	-5.110421	0
275-8-28	an important example is the retina , for which present-day techniques for identifying cell types are accurate , but very labor-intensive .	therefore , a key step in understanding neural systems is to reliably distinguish cell types .	0	2	1	3.0941217	-2.6956363	0
275-8-28	therefore , a key step in understanding neural systems is to reliably distinguish cell types .	here , we develop automated classifiers for functional identification of retinal ganglion cells , the output neurons of the retina , based solely on recorded voltage patterns on a large scale array .	1	1	3	-5.447136	5.0691004	1
275-8-28	we use per-cell classifiers based on features extracted from electrophysiological images ( spatiotemporal voltage waveforms ) and interspike intervals ( autocorrelations ) .	therefore , a key step in understanding neural systems is to reliably distinguish cell types .	0	4	1	5.245367	-4.7205143	0
275-8-28	these classifiers achieve high performance in distinguishing between the major ganglion cell classes of the primate retina , but fail in achieving the same accuracy in predicting cell polarities ( on vs. off ) .	therefore , a key step in understanding neural systems is to reliably distinguish cell types .	0	5	1	5.37325	-4.7627983	0
275-8-28	we then show how to use indicators of functional coupling within populations of ganglion cells ( cross-correlation ) to infer cell polarities with a matrix completion algorithm .	therefore , a key step in understanding neural systems is to reliably distinguish cell types .	0	6	1	5.6234775	-4.9873385	0
275-8-28	therefore , a key step in understanding neural systems is to reliably distinguish cell types .	this can result in accurate , fully automated methods for cell type classification .	1	1	7	-5.312126	4.893119	1
275-8-28	an important example is the retina , for which present-day techniques for identifying cell types are accurate , but very labor-intensive .	here , we develop automated classifiers for functional identification of retinal ganglion cells , the output neurons of the retina , based solely on recorded voltage patterns on a large scale array .	1	2	3	-5.139592	4.803499	1
275-8-28	an important example is the retina , for which present-day techniques for identifying cell types are accurate , but very labor-intensive .	we use per-cell classifiers based on features extracted from electrophysiological images ( spatiotemporal voltage waveforms ) and interspike intervals ( autocorrelations ) .	1	2	4	-5.879495	5.0453887	1
275-8-28	these classifiers achieve high performance in distinguishing between the major ganglion cell classes of the primate retina , but fail in achieving the same accuracy in predicting cell polarities ( on vs. off ) .	an important example is the retina , for which present-day techniques for identifying cell types are accurate , but very labor-intensive .	0	5	2	5.294492	-4.7033362	0
275-8-28	we then show how to use indicators of functional coupling within populations of ganglion cells ( cross-correlation ) to infer cell polarities with a matrix completion algorithm .	an important example is the retina , for which present-day techniques for identifying cell types are accurate , but very labor-intensive .	0	6	2	5.533646	-4.842604	0
275-8-28	an important example is the retina , for which present-day techniques for identifying cell types are accurate , but very labor-intensive .	this can result in accurate , fully automated methods for cell type classification .	1	2	7	-3.9329522	3.755434	1
275-8-28	we use per-cell classifiers based on features extracted from electrophysiological images ( spatiotemporal voltage waveforms ) and interspike intervals ( autocorrelations ) .	here , we develop automated classifiers for functional identification of retinal ganglion cells , the output neurons of the retina , based solely on recorded voltage patterns on a large scale array .	0	4	3	5.31686	-4.6276827	0
275-8-28	these classifiers achieve high performance in distinguishing between the major ganglion cell classes of the primate retina , but fail in achieving the same accuracy in predicting cell polarities ( on vs. off ) .	here , we develop automated classifiers for functional identification of retinal ganglion cells , the output neurons of the retina , based solely on recorded voltage patterns on a large scale array .	0	5	3	5.3725433	-4.852754	0
275-8-28	here , we develop automated classifiers for functional identification of retinal ganglion cells , the output neurons of the retina , based solely on recorded voltage patterns on a large scale array .	we then show how to use indicators of functional coupling within populations of ganglion cells ( cross-correlation ) to infer cell polarities with a matrix completion algorithm .	1	3	6	-5.9598904	5.0980644	1
275-8-28	this can result in accurate , fully automated methods for cell type classification .	here , we develop automated classifiers for functional identification of retinal ganglion cells , the output neurons of the retina , based solely on recorded voltage patterns on a large scale array .	0	7	3	0.2233561	0.011117935	0
275-8-28	these classifiers achieve high performance in distinguishing between the major ganglion cell classes of the primate retina , but fail in achieving the same accuracy in predicting cell polarities ( on vs. off ) .	we use per-cell classifiers based on features extracted from electrophysiological images ( spatiotemporal voltage waveforms ) and interspike intervals ( autocorrelations ) .	0	5	4	4.8246775	-4.4140577	0
275-8-28	we use per-cell classifiers based on features extracted from electrophysiological images ( spatiotemporal voltage waveforms ) and interspike intervals ( autocorrelations ) .	we then show how to use indicators of functional coupling within populations of ganglion cells ( cross-correlation ) to infer cell polarities with a matrix completion algorithm .	1	4	6	-5.5130653	5.0066137	1
275-8-28	this can result in accurate , fully automated methods for cell type classification .	we use per-cell classifiers based on features extracted from electrophysiological images ( spatiotemporal voltage waveforms ) and interspike intervals ( autocorrelations ) .	0	7	4	-1.4418396	1.7150761	1
275-8-28	we then show how to use indicators of functional coupling within populations of ganglion cells ( cross-correlation ) to infer cell polarities with a matrix completion algorithm .	these classifiers achieve high performance in distinguishing between the major ganglion cell classes of the primate retina , but fail in achieving the same accuracy in predicting cell polarities ( on vs. off ) .	0	6	5	-1.735612	1.9062538	1
275-8-28	these classifiers achieve high performance in distinguishing between the major ganglion cell classes of the primate retina , but fail in achieving the same accuracy in predicting cell polarities ( on vs. off ) .	this can result in accurate , fully automated methods for cell type classification .	1	5	7	1.7520978	-1.4805446	0
275-8-28	this can result in accurate , fully automated methods for cell type classification .	we then show how to use indicators of functional coupling within populations of ganglion cells ( cross-correlation ) to infer cell polarities with a matrix completion algorithm .	0	7	6	-3.7825313	3.7534513	1
276-6-15	the dynamics of simple decisions are well understood and modeled as a class of random walk models .	however , most real-life decisions include a dynamically-changing influence of additional information we call context .	1	0	1	3.3900852	-3.1978145	0
276-6-15	in this work , we describe a computational theory of decision making under dynamically shifting context .	the dynamics of simple decisions are well understood and modeled as a class of random walk models .	0	2	0	-2.9601085	2.884235	1
276-6-15	we show how the model generalizes the dominant existing model of fixed-context decision making and can be built up from a weighted combination of fixed-context decisions evolving simultaneously .	the dynamics of simple decisions are well understood and modeled as a class of random walk models .	0	3	0	4.5138497	-4.0636363	0
276-6-15	we also show how the model generalizes recent work on the control of attention in the flanker task .	the dynamics of simple decisions are well understood and modeled as a class of random walk models .	0	4	0	5.3706636	-4.7677217	0
276-6-15	finally , we show how the model recovers qualitative data patterns in another task of longstanding psychological interest , the ax continuous performance test , using the same model parameters .	the dynamics of simple decisions are well understood and modeled as a class of random walk models .	0	5	0	5.566284	-4.962447	0
276-6-15	in this work , we describe a computational theory of decision making under dynamically shifting context .	however , most real-life decisions include a dynamically-changing influence of additional information we call context .	0	2	1	4.199829	-3.8678904	0
276-6-15	however , most real-life decisions include a dynamically-changing influence of additional information we call context .	we show how the model generalizes the dominant existing model of fixed-context decision making and can be built up from a weighted combination of fixed-context decisions evolving simultaneously .	1	1	3	-5.9874854	5.1745687	1
276-6-15	however , most real-life decisions include a dynamically-changing influence of additional information we call context .	we also show how the model generalizes recent work on the control of attention in the flanker task .	1	1	4	-5.9875326	5.075023	1
276-6-15	however , most real-life decisions include a dynamically-changing influence of additional information we call context .	finally , we show how the model recovers qualitative data patterns in another task of longstanding psychological interest , the ax continuous performance test , using the same model parameters .	1	1	5	-5.990155	5.1244392	1
276-6-15	we show how the model generalizes the dominant existing model of fixed-context decision making and can be built up from a weighted combination of fixed-context decisions evolving simultaneously .	in this work , we describe a computational theory of decision making under dynamically shifting context .	0	3	2	5.604232	-4.9552693	0
276-6-15	we also show how the model generalizes recent work on the control of attention in the flanker task .	in this work , we describe a computational theory of decision making under dynamically shifting context .	0	4	2	5.485319	-4.8399196	0
276-6-15	finally , we show how the model recovers qualitative data patterns in another task of longstanding psychological interest , the ax continuous performance test , using the same model parameters .	in this work , we describe a computational theory of decision making under dynamically shifting context .	0	5	2	5.579073	-4.9516835	0
276-6-15	we also show how the model generalizes recent work on the control of attention in the flanker task .	we show how the model generalizes the dominant existing model of fixed-context decision making and can be built up from a weighted combination of fixed-context decisions evolving simultaneously .	0	4	3	5.130874	-4.534255	0
276-6-15	we show how the model generalizes the dominant existing model of fixed-context decision making and can be built up from a weighted combination of fixed-context decisions evolving simultaneously .	finally , we show how the model recovers qualitative data patterns in another task of longstanding psychological interest , the ax continuous performance test , using the same model parameters .	1	3	5	-5.90537	5.215823	1
276-6-15	finally , we show how the model recovers qualitative data patterns in another task of longstanding psychological interest , the ax continuous performance test , using the same model parameters .	we also show how the model generalizes recent work on the control of attention in the flanker task .	0	5	4	2.086162	-1.8285651	0
277-5-10	we propose a method for combining two sources of astronomical data , spectroscopy and photometry , that carry information about sources of light ( e.g. , stars , galaxies , and quasars ) at extremely different spectral resolutions .	our model treats the spectral energy distribution ( sed ) of the radiation from a source as a latent variable that jointly explains both photometric and spectroscopic observations .	1	0	1	-5.8614435	5.1743503	1
277-5-10	we place a flexible , nonparametric prior over the sed of a light source that admits a physically interpretable decomposition , and allows us to tractably perform inference .	we propose a method for combining two sources of astronomical data , spectroscopy and photometry , that carry information about sources of light ( e.g. , stars , galaxies , and quasars ) at extremely different spectral resolutions .	0	2	0	5.2363586	-4.5455265	0
277-5-10	we propose a method for combining two sources of astronomical data , spectroscopy and photometry , that carry information about sources of light ( e.g. , stars , galaxies , and quasars ) at extremely different spectral resolutions .	we use our model to predict the distribution of the redshift of a quasar from five-band ( low spectral resolution ) photometric data , the so called `` photoz '' problem .	1	0	3	-5.957821	5.1832294	1
277-5-10	our method shows that tools from machine learning and bayesian statistics allow us to leverage multiple resolutions of information to make accurate predictions with well-characterized uncertainties .	we propose a method for combining two sources of astronomical data , spectroscopy and photometry , that carry information about sources of light ( e.g. , stars , galaxies , and quasars ) at extremely different spectral resolutions .	0	4	0	5.6825953	-5.01676	0
277-5-10	our model treats the spectral energy distribution ( sed ) of the radiation from a source as a latent variable that jointly explains both photometric and spectroscopic observations .	we place a flexible , nonparametric prior over the sed of a light source that admits a physically interpretable decomposition , and allows us to tractably perform inference .	1	1	2	-5.6161227	5.1513996	1
277-5-10	we use our model to predict the distribution of the redshift of a quasar from five-band ( low spectral resolution ) photometric data , the so called `` photoz '' problem .	our model treats the spectral energy distribution ( sed ) of the radiation from a source as a latent variable that jointly explains both photometric and spectroscopic observations .	0	3	1	2.5073628	-2.1509864	0
277-5-10	our model treats the spectral energy distribution ( sed ) of the radiation from a source as a latent variable that jointly explains both photometric and spectroscopic observations .	our method shows that tools from machine learning and bayesian statistics allow us to leverage multiple resolutions of information to make accurate predictions with well-characterized uncertainties .	1	1	4	-5.689949	5.12644	1
277-5-10	we use our model to predict the distribution of the redshift of a quasar from five-band ( low spectral resolution ) photometric data , the so called `` photoz '' problem .	we place a flexible , nonparametric prior over the sed of a light source that admits a physically interpretable decomposition , and allows us to tractably perform inference .	0	3	2	3.1891794	-2.9346654	0
277-5-10	we place a flexible , nonparametric prior over the sed of a light source that admits a physically interpretable decomposition , and allows us to tractably perform inference .	our method shows that tools from machine learning and bayesian statistics allow us to leverage multiple resolutions of information to make accurate predictions with well-characterized uncertainties .	1	2	4	-5.65269	5.0367703	1
277-5-10	we use our model to predict the distribution of the redshift of a quasar from five-band ( low spectral resolution ) photometric data , the so called `` photoz '' problem .	our method shows that tools from machine learning and bayesian statistics allow us to leverage multiple resolutions of information to make accurate predictions with well-characterized uncertainties .	1	3	4	-5.0630856	4.659233	1
278-5-10	this paper argues it is dangerous to think of these quick wins as coming for free .	machine learning offers a fantastically powerful toolkit for building useful complex prediction systems quickly .	0	1	0	5.6840305	-5.0450926	0
278-5-10	machine learning offers a fantastically powerful toolkit for building useful complex prediction systems quickly .	using the software engineering framework of technical debt , we find it is common to incur massive ongoing maintenance costs in real-world ml systems .	1	0	2	-5.9777803	5.1576824	1
278-5-10	we explore several ml-specific risk factors to account for in system design .	machine learning offers a fantastically powerful toolkit for building useful complex prediction systems quickly .	0	3	0	5.6900225	-5.019401	0
278-5-10	these include boundary erosion , entanglement , hidden feedback loops , undeclared consumers , data dependencies , configuration issues , changes in the external world , and a variety of system-level anti-patterns .	machine learning offers a fantastically powerful toolkit for building useful complex prediction systems quickly .	0	4	0	5.686533	-5.022519	0
278-5-10	this paper argues it is dangerous to think of these quick wins as coming for free .	using the software engineering framework of technical debt , we find it is common to incur massive ongoing maintenance costs in real-world ml systems .	1	1	2	-5.9646635	5.1714497	1
278-5-10	this paper argues it is dangerous to think of these quick wins as coming for free .	we explore several ml-specific risk factors to account for in system design .	1	1	3	-5.951267	5.148458	1
278-5-10	these include boundary erosion , entanglement , hidden feedback loops , undeclared consumers , data dependencies , configuration issues , changes in the external world , and a variety of system-level anti-patterns .	this paper argues it is dangerous to think of these quick wins as coming for free .	0	4	1	-3.2566242	3.1693494	1
278-5-10	using the software engineering framework of technical debt , we find it is common to incur massive ongoing maintenance costs in real-world ml systems .	we explore several ml-specific risk factors to account for in system design .	1	2	3	4.7535305	-4.17822	0
278-5-10	these include boundary erosion , entanglement , hidden feedback loops , undeclared consumers , data dependencies , configuration issues , changes in the external world , and a variety of system-level anti-patterns .	using the software engineering framework of technical debt , we find it is common to incur massive ongoing maintenance costs in real-world ml systems .	0	4	2	-4.689047	4.3961887	1
278-5-10	we explore several ml-specific risk factors to account for in system design .	these include boundary erosion , entanglement , hidden feedback loops , undeclared consumers , data dependencies , configuration issues , changes in the external world , and a variety of system-level anti-patterns .	1	3	4	-1.798943	1.9222304	1
279-6-15	state-of-the-art causal learning algorithms generally need to find the global causal structures in the form of complete partial directed acyclic graphs ( cpdag ) in order to identify direct causes and effects of a target variable .	we focus on the discovery and identification of direct causes and effects of a target variable in a causal network .	0	1	0	3.1962395	-2.5387099	0
279-6-15	while these algorithms are effective , it is often unnecessary and wasteful to find the global structures when we are only interested in the local structure of one target variable ( such as class labels ) .	we focus on the discovery and identification of direct causes and effects of a target variable in a causal network .	0	2	0	5.36537	-4.8555317	0
279-6-15	we propose a new local causal discovery algorithm , called causal markov blanket ( cmb ) , to identify the direct causes and effects of a target variable based on markov blanket discovery .	we focus on the discovery and identification of direct causes and effects of a target variable in a causal network .	0	3	0	5.0659566	-4.509866	0
279-6-15	we focus on the discovery and identification of direct causes and effects of a target variable in a causal network .	cmb is designed to conduct causal discovery among multiple variables , but focuses only on finding causal relationships between a specific target variable and other variables .	1	0	4	-5.6256104	5.1228247	1
279-6-15	under standard assumptions , we show both theoretically and experimentally that the proposed local causal discovery algorithm can obtain the comparable identification accuracy as global methods but significantly improve their efficiency , often by more than one order of magnitude .	we focus on the discovery and identification of direct causes and effects of a target variable in a causal network .	0	5	0	5.7387447	-5.100424	0
279-6-15	state-of-the-art causal learning algorithms generally need to find the global causal structures in the form of complete partial directed acyclic graphs ( cpdag ) in order to identify direct causes and effects of a target variable .	while these algorithms are effective , it is often unnecessary and wasteful to find the global structures when we are only interested in the local structure of one target variable ( such as class labels ) .	1	1	2	-4.6318307	4.364567	1
279-6-15	we propose a new local causal discovery algorithm , called causal markov blanket ( cmb ) , to identify the direct causes and effects of a target variable based on markov blanket discovery .	state-of-the-art causal learning algorithms generally need to find the global causal structures in the form of complete partial directed acyclic graphs ( cpdag ) in order to identify direct causes and effects of a target variable .	0	3	1	5.5714293	-4.926201	0
279-6-15	cmb is designed to conduct causal discovery among multiple variables , but focuses only on finding causal relationships between a specific target variable and other variables .	state-of-the-art causal learning algorithms generally need to find the global causal structures in the form of complete partial directed acyclic graphs ( cpdag ) in order to identify direct causes and effects of a target variable .	0	4	1	5.1344194	-4.5722437	0
279-6-15	state-of-the-art causal learning algorithms generally need to find the global causal structures in the form of complete partial directed acyclic graphs ( cpdag ) in order to identify direct causes and effects of a target variable .	under standard assumptions , we show both theoretically and experimentally that the proposed local causal discovery algorithm can obtain the comparable identification accuracy as global methods but significantly improve their efficiency , often by more than one order of magnitude .	1	1	5	-5.8829966	5.205656	1
279-6-15	we propose a new local causal discovery algorithm , called causal markov blanket ( cmb ) , to identify the direct causes and effects of a target variable based on markov blanket discovery .	while these algorithms are effective , it is often unnecessary and wasteful to find the global structures when we are only interested in the local structure of one target variable ( such as class labels ) .	0	3	2	4.2813425	-3.9453607	0
279-6-15	cmb is designed to conduct causal discovery among multiple variables , but focuses only on finding causal relationships between a specific target variable and other variables .	while these algorithms are effective , it is often unnecessary and wasteful to find the global structures when we are only interested in the local structure of one target variable ( such as class labels ) .	0	4	2	-1.5133367	1.71083	1
279-6-15	while these algorithms are effective , it is often unnecessary and wasteful to find the global structures when we are only interested in the local structure of one target variable ( such as class labels ) .	under standard assumptions , we show both theoretically and experimentally that the proposed local causal discovery algorithm can obtain the comparable identification accuracy as global methods but significantly improve their efficiency , often by more than one order of magnitude .	1	2	5	-6.003462	5.171455	1
279-6-15	we propose a new local causal discovery algorithm , called causal markov blanket ( cmb ) , to identify the direct causes and effects of a target variable based on markov blanket discovery .	cmb is designed to conduct causal discovery among multiple variables , but focuses only on finding causal relationships between a specific target variable and other variables .	1	3	4	-5.7030997	5.167003	1
279-6-15	we propose a new local causal discovery algorithm , called causal markov blanket ( cmb ) , to identify the direct causes and effects of a target variable based on markov blanket discovery .	under standard assumptions , we show both theoretically and experimentally that the proposed local causal discovery algorithm can obtain the comparable identification accuracy as global methods but significantly improve their efficiency , often by more than one order of magnitude .	1	3	5	-6.001721	5.165118	1
279-6-15	under standard assumptions , we show both theoretically and experimentally that the proposed local causal discovery algorithm can obtain the comparable identification accuracy as global methods but significantly improve their efficiency , often by more than one order of magnitude .	cmb is designed to conduct causal discovery among multiple variables , but focuses only on finding causal relationships between a specific target variable and other variables .	0	5	4	5.4481726	-4.8431764	0
280-5-10	in particular , we make two contributions : ( i ) for parameter estimation , we propose a novel high dimensional em algorithm which naturally incorporates sparsity structure into parameter estimation .	we provide a general theory of the expectation-maximization ( em ) algorithm for inferring high dimensional latent variable models .	0	1	0	5.595045	-4.9614406	0
280-5-10	with an appropriate initialization , this algorithm converges at a geometric rate and attains an estimator with the ( near- ) optimal statistical rate of convergence .	we provide a general theory of the expectation-maximization ( em ) algorithm for inferring high dimensional latent variable models .	0	2	0	5.3904448	-4.8702817	0
280-5-10	( ii ) based on the obtained estimator , we propose a new inferential procedure for testing hypotheses for low dimensional components of high dimensional parameters .	we provide a general theory of the expectation-maximization ( em ) algorithm for inferring high dimensional latent variable models .	0	3	0	5.5752378	-4.9864283	0
280-5-10	we provide a general theory of the expectation-maximization ( em ) algorithm for inferring high dimensional latent variable models .	for a broad family of statistical models , our framework establishes the first computationally feasible approach for optimal estimation and asymptotic inference in high dimensions .	1	0	4	-5.9705057	5.2372026	1
280-5-10	in particular , we make two contributions : ( i ) for parameter estimation , we propose a novel high dimensional em algorithm which naturally incorporates sparsity structure into parameter estimation .	with an appropriate initialization , this algorithm converges at a geometric rate and attains an estimator with the ( near- ) optimal statistical rate of convergence .	1	1	2	-4.4583287	4.179088	1
280-5-10	( ii ) based on the obtained estimator , we propose a new inferential procedure for testing hypotheses for low dimensional components of high dimensional parameters .	in particular , we make two contributions : ( i ) for parameter estimation , we propose a novel high dimensional em algorithm which naturally incorporates sparsity structure into parameter estimation .	0	3	1	4.377453	-3.9429774	0
280-5-10	for a broad family of statistical models , our framework establishes the first computationally feasible approach for optimal estimation and asymptotic inference in high dimensions .	in particular , we make two contributions : ( i ) for parameter estimation , we propose a novel high dimensional em algorithm which naturally incorporates sparsity structure into parameter estimation .	0	4	1	3.005298	-2.7719648	0
280-5-10	with an appropriate initialization , this algorithm converges at a geometric rate and attains an estimator with the ( near- ) optimal statistical rate of convergence .	( ii ) based on the obtained estimator , we propose a new inferential procedure for testing hypotheses for low dimensional components of high dimensional parameters .	1	2	3	-4.2555842	3.9732862	1
280-5-10	for a broad family of statistical models , our framework establishes the first computationally feasible approach for optimal estimation and asymptotic inference in high dimensions .	with an appropriate initialization , this algorithm converges at a geometric rate and attains an estimator with the ( near- ) optimal statistical rate of convergence .	0	4	2	2.298334	-2.1225388	0
280-5-10	( ii ) based on the obtained estimator , we propose a new inferential procedure for testing hypotheses for low dimensional components of high dimensional parameters .	for a broad family of statistical models , our framework establishes the first computationally feasible approach for optimal estimation and asymptotic inference in high dimensions .	1	3	4	1.0213642	-0.7891438	0
281-3-3	in order to analyze this problem we introduce the notion of -strategic buyer , a more natural notion of strategic behavior than what has been considered in the past .	we present a revenue optimization algorithm for posted-price auctions when facing a buyer with random valuations who seeks to optimize his -discounted surplus .	0	1	0	5.3364205	-4.795718	0
281-3-3	[CLS] we improve upon the previous state - of - the - art and achieve an optimal regret bound in o ( log t + 1 / log ( 1 / ) ) pwhen the seller selects prices from a finite set e t + t 1 / 4 / log ( 1 / ) ) when the prices offered and provide a regret	we present a revenue optimization algorithm for posted-price auctions when facing a buyer with random valuations who seeks to optimize his -discounted surplus .	0	2	0	5.600027	-5.041829	0
281-3-3	[CLS] we improve upon the previous state - of - the - art and achieve an optimal regret bound in o ( log t + 1 / log ( 1 / ) ) pwhen the seller selects prices from a finite set e t + t 1 / 4 / log ( 1 / ) ) when the prices offered and provide a regret bound	in order to analyze this problem we introduce the notion of -strategic buyer , a more natural notion of strategic behavior than what has been considered in the past .	0	2	1	3.1035419	-2.7991047	0
282-6-15	the dcign model is composed of multiple layers of convolution and de-convolution operators and is trained using the stochastic gradient variational bayes ( sgvb ) algorithm .	this paper presents the deep convolution inverse graphics network ( dcign ) , a model that aims to learn an interpretable representation of images , disentangled with respect to three-dimensional scene structure and viewing transformations such as depth rotations and lighting variations .	0	1	0	5.56679	-4.907091	0
282-6-15	we propose a training procedure to encourage neurons in the graphics code layer to represent a specific transformation ( e.g .	this paper presents the deep convolution inverse graphics network ( dcign ) , a model that aims to learn an interpretable representation of images , disentangled with respect to three-dimensional scene structure and viewing transformations such as depth rotations and lighting variations .	0	2	0	5.4986963	-4.838395	0
282-6-15	pose or light ) .	this paper presents the deep convolution inverse graphics network ( dcign ) , a model that aims to learn an interpretable representation of images , disentangled with respect to three-dimensional scene structure and viewing transformations such as depth rotations and lighting variations .	0	3	0	4.6124544	-4.1140566	0
282-6-15	this paper presents the deep convolution inverse graphics network ( dcign ) , a model that aims to learn an interpretable representation of images , disentangled with respect to three-dimensional scene structure and viewing transformations such as depth rotations and lighting variations .	given a single input image , our model can generate new images of the same object with variations in pose and lighting .	1	0	4	-5.820101	5.2372665	1
282-6-15	this paper presents the deep convolution inverse graphics network ( dcign ) , a model that aims to learn an interpretable representation of images , disentangled with respect to three-dimensional scene structure and viewing transformations such as depth rotations and lighting variations .	we present qualitative and quantitative tests of the model 's efficacy at learning a 3d rendering engine for varied object classes including faces and chairs .	1	0	5	-5.939252	5.1601605	1
282-6-15	we propose a training procedure to encourage neurons in the graphics code layer to represent a specific transformation ( e.g .	the dcign model is composed of multiple layers of convolution and de-convolution operators and is trained using the stochastic gradient variational bayes ( sgvb ) algorithm .	0	2	1	-5.143347	4.6750755	1
282-6-15	the dcign model is composed of multiple layers of convolution and de-convolution operators and is trained using the stochastic gradient variational bayes ( sgvb ) algorithm .	pose or light ) .	1	1	3	3.0778062	-2.8463242	0
282-6-15	the dcign model is composed of multiple layers of convolution and de-convolution operators and is trained using the stochastic gradient variational bayes ( sgvb ) algorithm .	given a single input image , our model can generate new images of the same object with variations in pose and lighting .	1	1	4	3.2397132	-2.9346886	0
282-6-15	we present qualitative and quantitative tests of the model 's efficacy at learning a 3d rendering engine for varied object classes including faces and chairs .	the dcign model is composed of multiple layers of convolution and de-convolution operators and is trained using the stochastic gradient variational bayes ( sgvb ) algorithm .	0	5	1	5.216203	-4.565223	0
282-6-15	we propose a training procedure to encourage neurons in the graphics code layer to represent a specific transformation ( e.g .	pose or light ) .	1	2	3	-3.3269148	3.013935	1
282-6-15	we propose a training procedure to encourage neurons in the graphics code layer to represent a specific transformation ( e.g .	given a single input image , our model can generate new images of the same object with variations in pose and lighting .	1	2	4	-2.8534904	2.7561903	1
282-6-15	we present qualitative and quantitative tests of the model 's efficacy at learning a 3d rendering engine for varied object classes including faces and chairs .	we propose a training procedure to encourage neurons in the graphics code layer to represent a specific transformation ( e.g .	0	5	2	5.4599648	-4.721368	0
282-6-15	pose or light ) .	given a single input image , our model can generate new images of the same object with variations in pose and lighting .	1	3	4	-2.5744753	2.575849	1
282-6-15	we present qualitative and quantitative tests of the model 's efficacy at learning a 3d rendering engine for varied object classes including faces and chairs .	pose or light ) .	0	5	3	5.1860876	-4.6274443	0
282-6-15	we present qualitative and quantitative tests of the model 's efficacy at learning a 3d rendering engine for varied object classes including faces and chairs .	given a single input image , our model can generate new images of the same object with variations in pose and lighting .	0	5	4	5.4076643	-4.6968117	0
283-6-15	motivated by the problem of robust factorization of a low-rank tensor , we study the question of sparse and low-rank tensor decomposition .	we present an efficient computational algorithm that modifies leurgans ' algoirthm for tensor factorization .	1	0	1	-5.8561807	5.153165	1
283-6-15	our method relies on a reduction of the problem to sparse and low-rank matrix decomposition via the notion of tensor contraction .	motivated by the problem of robust factorization of a low-rank tensor , we study the question of sparse and low-rank tensor decomposition .	0	2	0	5.605647	-5.051712	0
283-6-15	motivated by the problem of robust factorization of a low-rank tensor , we study the question of sparse and low-rank tensor decomposition .	we use well-understood convex techniques for solving the reduced matrix sub-problem which then allows us to perform the full decomposition of the tensor .	1	0	3	-5.802144	5.1891394	1
283-6-15	we delineate situations where the problem is recoverable and provide theoretical guarantees for our algorithm .	motivated by the problem of robust factorization of a low-rank tensor , we study the question of sparse and low-rank tensor decomposition .	0	4	0	5.6351137	-5.0908747	0
283-6-15	motivated by the problem of robust factorization of a low-rank tensor , we study the question of sparse and low-rank tensor decomposition .	we validate our algorithm with numerical experiments .	1	0	5	-5.9668813	5.1243377	1
283-6-15	we present an efficient computational algorithm that modifies leurgans ' algoirthm for tensor factorization .	our method relies on a reduction of the problem to sparse and low-rank matrix decomposition via the notion of tensor contraction .	1	1	2	-3.5435886	3.3862462	1
283-6-15	we present an efficient computational algorithm that modifies leurgans ' algoirthm for tensor factorization .	we use well-understood convex techniques for solving the reduced matrix sub-problem which then allows us to perform the full decomposition of the tensor .	1	1	3	-2.2967088	2.40064	1
283-6-15	we delineate situations where the problem is recoverable and provide theoretical guarantees for our algorithm .	we present an efficient computational algorithm that modifies leurgans ' algoirthm for tensor factorization .	0	4	1	4.7909307	-4.255784	0
283-6-15	we present an efficient computational algorithm that modifies leurgans ' algoirthm for tensor factorization .	we validate our algorithm with numerical experiments .	1	1	5	-5.9042273	5.1615257	1
283-6-15	our method relies on a reduction of the problem to sparse and low-rank matrix decomposition via the notion of tensor contraction .	we use well-understood convex techniques for solving the reduced matrix sub-problem which then allows us to perform the full decomposition of the tensor .	1	2	3	-3.6293445	3.4693615	1
283-6-15	our method relies on a reduction of the problem to sparse and low-rank matrix decomposition via the notion of tensor contraction .	we delineate situations where the problem is recoverable and provide theoretical guarantees for our algorithm .	1	2	4	-5.1258764	4.696701	1
283-6-15	we validate our algorithm with numerical experiments .	our method relies on a reduction of the problem to sparse and low-rank matrix decomposition via the notion of tensor contraction .	0	5	2	5.353088	-4.763339	0
283-6-15	we delineate situations where the problem is recoverable and provide theoretical guarantees for our algorithm .	we use well-understood convex techniques for solving the reduced matrix sub-problem which then allows us to perform the full decomposition of the tensor .	0	4	3	4.3377056	-3.9849365	0
283-6-15	we use well-understood convex techniques for solving the reduced matrix sub-problem which then allows us to perform the full decomposition of the tensor .	we validate our algorithm with numerical experiments .	1	3	5	-5.7961497	5.079496	1
283-6-15	we delineate situations where the problem is recoverable and provide theoretical guarantees for our algorithm .	we validate our algorithm with numerical experiments .	1	4	5	-4.5356207	4.2254477	1
284-9-36	we consider an adversarial formulation of the problem of predicting a time series with square loss .	the aim is to predict an arbitrary sequence of vectors almost as well as the best smooth comparator sequence in retrospect .	1	0	1	-5.7246037	5.1084166	1
284-9-36	our approach allows natural measures of smoothness such as the squared norm of increments .	we consider an adversarial formulation of the problem of predicting a time series with square loss .	0	2	0	5.587759	-5.0557146	0
284-9-36	more generally , we consider a linear time series model and penalize the comparator sequence through the energy of the implied driving noise terms .	we consider an adversarial formulation of the problem of predicting a time series with square loss .	0	3	0	5.564444	-4.979431	0
284-9-36	we consider an adversarial formulation of the problem of predicting a time series with square loss .	we derive the minimax strategy for all problems of this type and show that it can be implemented efficiently .	1	0	4	-5.930896	5.1785965	1
284-9-36	the optimal predictions are linear in the previous observations .	we consider an adversarial formulation of the problem of predicting a time series with square loss .	0	5	0	5.6351624	-5.1123986	0
284-9-36	we consider an adversarial formulation of the problem of predicting a time series with square loss .	we obtain an explicit expression for the regret in terms of the parameters defining the problem .	1	0	6	-5.975875	5.1183815	1
284-9-36	for typical , simple definitions of smoothness , the computation of the optimal predictions involves only sparse matrices .	we consider an adversarial formulation of the problem of predicting a time series with square loss .	0	7	0	5.606187	-5.083662	0
284-9-36	we consider an adversarial formulation of the problem of predicting a time series with square loss .	in the case of norm-constrained data , where the smoothness is defined in terms of the squared norm of the comparator 's increments , we show that the regret grows as t / t , where t is the length of the game and t is an increasing limit on comparator smoothness .	1	0	8	-5.8551636	5.1923647	1
284-9-36	the aim is to predict an arbitrary sequence of vectors almost as well as the best smooth comparator sequence in retrospect .	our approach allows natural measures of smoothness such as the squared norm of increments .	1	1	2	-4.9042187	4.50143	1
284-9-36	the aim is to predict an arbitrary sequence of vectors almost as well as the best smooth comparator sequence in retrospect .	more generally , we consider a linear time series model and penalize the comparator sequence through the energy of the implied driving noise terms .	1	1	3	-5.3653107	4.9300528	1
284-9-36	the aim is to predict an arbitrary sequence of vectors almost as well as the best smooth comparator sequence in retrospect .	we derive the minimax strategy for all problems of this type and show that it can be implemented efficiently .	1	1	4	-5.0449734	4.656417	1
284-9-36	the aim is to predict an arbitrary sequence of vectors almost as well as the best smooth comparator sequence in retrospect .	the optimal predictions are linear in the previous observations .	1	1	5	-5.246832	4.780452	1
284-9-36	we obtain an explicit expression for the regret in terms of the parameters defining the problem .	the aim is to predict an arbitrary sequence of vectors almost as well as the best smooth comparator sequence in retrospect .	0	6	1	3.2376072	-3.078067	0
284-9-36	the aim is to predict an arbitrary sequence of vectors almost as well as the best smooth comparator sequence in retrospect .	for typical , simple definitions of smoothness , the computation of the optimal predictions involves only sparse matrices .	1	1	7	-2.6124053	2.6076393	1
284-9-36	the aim is to predict an arbitrary sequence of vectors almost as well as the best smooth comparator sequence in retrospect .	in the case of norm-constrained data , where the smoothness is defined in terms of the squared norm of the comparator 's increments , we show that the regret grows as t / t , where t is the length of the game and t is an increasing limit on comparator smoothness .	1	1	8	-4.31359	4.0144606	1
284-9-36	our approach allows natural measures of smoothness such as the squared norm of increments .	more generally , we consider a linear time series model and penalize the comparator sequence through the energy of the implied driving noise terms .	1	2	3	2.7140806	-2.5279236	0
284-9-36	our approach allows natural measures of smoothness such as the squared norm of increments .	we derive the minimax strategy for all problems of this type and show that it can be implemented efficiently .	1	2	4	3.2565284	-3.0510106	0
284-9-36	the optimal predictions are linear in the previous observations .	our approach allows natural measures of smoothness such as the squared norm of increments .	0	5	2	-2.8989968	2.9482896	1
284-9-36	we obtain an explicit expression for the regret in terms of the parameters defining the problem .	our approach allows natural measures of smoothness such as the squared norm of increments .	0	6	2	-4.653137	4.3353376	1
284-9-36	for typical , simple definitions of smoothness , the computation of the optimal predictions involves only sparse matrices .	our approach allows natural measures of smoothness such as the squared norm of increments .	0	7	2	-6.0017285	5.0564737	1
284-9-36	in the case of norm-constrained data , where the smoothness is defined in terms of the squared norm of the comparator 's increments , we show that the regret grows as t / t , where t is the length of the game and t is an increasing limit on comparator smoothness .	our approach allows natural measures of smoothness such as the squared norm of increments .	0	8	2	-1.9321796	2.0697684	1
284-9-36	more generally , we consider a linear time series model and penalize the comparator sequence through the energy of the implied driving noise terms .	we derive the minimax strategy for all problems of this type and show that it can be implemented efficiently .	1	3	4	-3.2190285	3.177723	1
284-9-36	the optimal predictions are linear in the previous observations .	more generally , we consider a linear time series model and penalize the comparator sequence through the energy of the implied driving noise terms .	0	5	3	2.988833	-2.7650998	0
284-9-36	we obtain an explicit expression for the regret in terms of the parameters defining the problem .	more generally , we consider a linear time series model and penalize the comparator sequence through the energy of the implied driving noise terms .	0	6	3	-3.9769363	3.8593612	1
284-9-36	more generally , we consider a linear time series model and penalize the comparator sequence through the energy of the implied driving noise terms .	for typical , simple definitions of smoothness , the computation of the optimal predictions involves only sparse matrices .	1	3	7	3.796099	-3.4784214	0
284-9-36	more generally , we consider a linear time series model and penalize the comparator sequence through the energy of the implied driving noise terms .	in the case of norm-constrained data , where the smoothness is defined in terms of the squared norm of the comparator 's increments , we show that the regret grows as t / t , where t is the length of the game and t is an increasing limit on comparator smoothness .	1	3	8	-1.8609741	1.9744742	1
284-9-36	the optimal predictions are linear in the previous observations .	we derive the minimax strategy for all problems of this type and show that it can be implemented efficiently .	0	5	4	-2.678209	2.7925231	1
284-9-36	we derive the minimax strategy for all problems of this type and show that it can be implemented efficiently .	we obtain an explicit expression for the regret in terms of the parameters defining the problem .	1	4	6	0.68722814	-0.6285	0
284-9-36	for typical , simple definitions of smoothness , the computation of the optimal predictions involves only sparse matrices .	we derive the minimax strategy for all problems of this type and show that it can be implemented efficiently .	0	7	4	-6.013727	5.1402335	1
284-9-36	in the case of norm-constrained data , where the smoothness is defined in terms of the squared norm of the comparator 's increments , we show that the regret grows as t / t , where t is the length of the game and t is an increasing limit on comparator smoothness .	we derive the minimax strategy for all problems of this type and show that it can be implemented efficiently .	0	8	4	-1.943408	2.1016865	1
284-9-36	we obtain an explicit expression for the regret in terms of the parameters defining the problem .	the optimal predictions are linear in the previous observations .	0	6	5	-1.7754653	1.8957657	1
284-9-36	for typical , simple definitions of smoothness , the computation of the optimal predictions involves only sparse matrices .	the optimal predictions are linear in the previous observations .	0	7	5	-4.7293863	4.374202	1
284-9-36	in the case of norm-constrained data , where the smoothness is defined in terms of the squared norm of the comparator 's increments , we show that the regret grows as t / t , where t is the length of the game and t is an increasing limit on comparator smoothness .	the optimal predictions are linear in the previous observations .	0	8	5	-2.4441257	2.4971986	1
284-9-36	for typical , simple definitions of smoothness , the computation of the optimal predictions involves only sparse matrices .	we obtain an explicit expression for the regret in terms of the parameters defining the problem .	0	7	6	-5.9199576	4.963378	1
284-9-36	we obtain an explicit expression for the regret in terms of the parameters defining the problem .	in the case of norm-constrained data , where the smoothness is defined in terms of the squared norm of the comparator 's increments , we show that the regret grows as t / t , where t is the length of the game and t is an increasing limit on comparator smoothness .	1	6	8	-2.5311408	2.5908883	1
284-9-36	in the case of norm-constrained data , where the smoothness is defined in terms of the squared norm of the comparator 's increments , we show that the regret grows as t / t , where t is the length of the game and t is an increasing limit on comparator smoothness .	for typical , simple definitions of smoothness , the computation of the optimal predictions involves only sparse matrices .	0	8	7	4.980825	-4.4482017	0
285-6-15	we investigate the problem of learning an unknown probability distribution over a discrete population from random samples .	our goal is to design efficient algorithms that simultaneously achieve low error in total variation norm while guaranteeing differential privacy to the individuals of the population .	1	0	1	-5.8276305	5.18572	1
285-6-15	we investigate the problem of learning an unknown probability distribution over a discrete population from random samples .	we describe a general approach that yields near sample-optimal and computationally efficient differentially private estimators for a wide range of well-studied and natural distribution families .	1	0	2	-5.9485054	5.2115536	1
285-6-15	our theoretical results show that for a wide variety of structured distributions there exist private estimation algorithms that are nearly as efficient -- both in terms of sample size and running time -- as their non-private counterparts .	we investigate the problem of learning an unknown probability distribution over a discrete population from random samples .	0	3	0	5.6812344	-5.071505	0
285-6-15	we complement our theoretical guarantees with an experimental evaluation .	we investigate the problem of learning an unknown probability distribution over a discrete population from random samples .	0	4	0	5.612871	-4.9874682	0
285-6-15	our experiments illustrate the speed and accuracy of our private estimators on both synthetic mixture models and a large public data set .	we investigate the problem of learning an unknown probability distribution over a discrete population from random samples .	0	5	0	5.60853	-5.0413094	0
285-6-15	we describe a general approach that yields near sample-optimal and computationally efficient differentially private estimators for a wide range of well-studied and natural distribution families .	our goal is to design efficient algorithms that simultaneously achieve low error in total variation norm while guaranteeing differential privacy to the individuals of the population .	0	2	1	-4.4803615	4.1426544	1
285-6-15	our theoretical results show that for a wide variety of structured distributions there exist private estimation algorithms that are nearly as efficient -- both in terms of sample size and running time -- as their non-private counterparts .	our goal is to design efficient algorithms that simultaneously achieve low error in total variation norm while guaranteeing differential privacy to the individuals of the population .	0	3	1	5.25529	-4.6966186	0
285-6-15	our goal is to design efficient algorithms that simultaneously achieve low error in total variation norm while guaranteeing differential privacy to the individuals of the population .	we complement our theoretical guarantees with an experimental evaluation .	1	1	4	-5.9366636	5.150614	1
285-6-15	our goal is to design efficient algorithms that simultaneously achieve low error in total variation norm while guaranteeing differential privacy to the individuals of the population .	our experiments illustrate the speed and accuracy of our private estimators on both synthetic mixture models and a large public data set .	1	1	5	-5.9780407	5.1294146	1
285-6-15	we describe a general approach that yields near sample-optimal and computationally efficient differentially private estimators for a wide range of well-studied and natural distribution families .	our theoretical results show that for a wide variety of structured distributions there exist private estimation algorithms that are nearly as efficient -- both in terms of sample size and running time -- as their non-private counterparts .	1	2	3	-5.8415527	5.24399	1
285-6-15	we complement our theoretical guarantees with an experimental evaluation .	we describe a general approach that yields near sample-optimal and computationally efficient differentially private estimators for a wide range of well-studied and natural distribution families .	0	4	2	5.4189854	-4.759276	0
285-6-15	our experiments illustrate the speed and accuracy of our private estimators on both synthetic mixture models and a large public data set .	we describe a general approach that yields near sample-optimal and computationally efficient differentially private estimators for a wide range of well-studied and natural distribution families .	0	5	2	5.5484924	-4.9019556	0
285-6-15	our theoretical results show that for a wide variety of structured distributions there exist private estimation algorithms that are nearly as efficient -- both in terms of sample size and running time -- as their non-private counterparts .	we complement our theoretical guarantees with an experimental evaluation .	1	3	4	-1.5545255	1.7175775	1
285-6-15	our theoretical results show that for a wide variety of structured distributions there exist private estimation algorithms that are nearly as efficient -- both in terms of sample size and running time -- as their non-private counterparts .	our experiments illustrate the speed and accuracy of our private estimators on both synthetic mixture models and a large public data set .	1	3	5	-5.323065	4.8746443	1
285-6-15	we complement our theoretical guarantees with an experimental evaluation .	our experiments illustrate the speed and accuracy of our private estimators on both synthetic mixture models and a large public data set .	1	4	5	-5.146641	4.7040434	1
286-6-15	we investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational bayesian inference ( sgvb ) of a posterior over model parameters , while retaining parallelizability .	this local reparameterization translates uncertainty about global parameters into local noise that is independent across datapoints in the minibatch .	1	0	1	-5.888588	5.2281413	1
286-6-15	we investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational bayesian inference ( sgvb ) of a posterior over model parameters , while retaining parallelizability .	such parameterizations can be trivially parallelized and have variance that is inversely proportional to the minibatch size , generally leading to much faster convergence .	1	0	2	-5.7470207	5.1856365	1
286-6-15	additionally , we explore a connection with dropout : gaussian dropout objectives correspond to sgvb with local reparameterization , a scale-invariant prior and proportionally fixed posterior variance .	we investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational bayesian inference ( sgvb ) of a posterior over model parameters , while retaining parallelizability .	0	3	0	5.572156	-4.8920183	0
286-6-15	we investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational bayesian inference ( sgvb ) of a posterior over model parameters , while retaining parallelizability .	our method allows inference of more flexibly parameterized posteriors ; specifically , we propose variational dropout , a generalization of gaussian dropout where the dropout rates are learned , often leading to better models .	1	0	4	-6.0201607	5.170919	1
286-6-15	the method is demonstrated through several experiments .	we investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational bayesian inference ( sgvb ) of a posterior over model parameters , while retaining parallelizability .	0	5	0	5.559434	-4.958749	0
286-6-15	this local reparameterization translates uncertainty about global parameters into local noise that is independent across datapoints in the minibatch .	such parameterizations can be trivially parallelized and have variance that is inversely proportional to the minibatch size , generally leading to much faster convergence .	1	1	2	-2.9773962	2.9443245	1
286-6-15	this local reparameterization translates uncertainty about global parameters into local noise that is independent across datapoints in the minibatch .	additionally , we explore a connection with dropout : gaussian dropout objectives correspond to sgvb with local reparameterization , a scale-invariant prior and proportionally fixed posterior variance .	1	1	3	-2.2516468	2.3323364	1
286-6-15	this local reparameterization translates uncertainty about global parameters into local noise that is independent across datapoints in the minibatch .	our method allows inference of more flexibly parameterized posteriors ; specifically , we propose variational dropout , a generalization of gaussian dropout where the dropout rates are learned , often leading to better models .	1	1	4	-2.2823906	2.3594203	1
286-6-15	the method is demonstrated through several experiments .	this local reparameterization translates uncertainty about global parameters into local noise that is independent across datapoints in the minibatch .	0	5	1	5.049633	-4.539759	0
286-6-15	such parameterizations can be trivially parallelized and have variance that is inversely proportional to the minibatch size , generally leading to much faster convergence .	additionally , we explore a connection with dropout : gaussian dropout objectives correspond to sgvb with local reparameterization , a scale-invariant prior and proportionally fixed posterior variance .	1	2	3	-5.2794333	4.8934307	1
286-6-15	our method allows inference of more flexibly parameterized posteriors ; specifically , we propose variational dropout , a generalization of gaussian dropout where the dropout rates are learned , often leading to better models .	such parameterizations can be trivially parallelized and have variance that is inversely proportional to the minibatch size , generally leading to much faster convergence .	0	4	2	2.1242723	-2.0146587	0
286-6-15	the method is demonstrated through several experiments .	such parameterizations can be trivially parallelized and have variance that is inversely proportional to the minibatch size , generally leading to much faster convergence .	0	5	2	5.395607	-4.8282046	0
286-6-15	additionally , we explore a connection with dropout : gaussian dropout objectives correspond to sgvb with local reparameterization , a scale-invariant prior and proportionally fixed posterior variance .	our method allows inference of more flexibly parameterized posteriors ; specifically , we propose variational dropout , a generalization of gaussian dropout where the dropout rates are learned , often leading to better models .	1	3	4	2.8294458	-2.5839949	0
286-6-15	the method is demonstrated through several experiments .	additionally , we explore a connection with dropout : gaussian dropout objectives correspond to sgvb with local reparameterization , a scale-invariant prior and proportionally fixed posterior variance .	0	5	3	4.2887044	-3.8664186	0
286-6-15	our method allows inference of more flexibly parameterized posteriors ; specifically , we propose variational dropout , a generalization of gaussian dropout where the dropout rates are learned , often leading to better models .	the method is demonstrated through several experiments .	1	4	5	-5.3556147	4.8127728	1
287-5-10	metric learning seeks a transformation of the feature space that enhances prediction quality for a given task .	in this work we provide pac-style sample complexity rates for supervised metric learning .	1	0	1	-5.902403	5.1510925	1
287-5-10	we give matching lower- and upper-bounds showing that sample complexity scales with the representation dimension when no assumptions are made about the underlying data distribution .	metric learning seeks a transformation of the feature space that enhances prediction quality for a given task .	0	2	0	5.660671	-5.059106	0
287-5-10	metric learning seeks a transformation of the feature space that enhances prediction quality for a given task .	in addition , by leveraging the structure of the data distribution , we provide rates fine-tuned to a specific notion of the intrinsic complexity of a given dataset , allowing us to relax the dependence on representation dimension .	1	0	3	-5.9841437	5.1655245	1
287-5-10	metric learning seeks a transformation of the feature space that enhances prediction quality for a given task .	we show both theoretically and empirically that augmenting the metric learning optimization criterion with a simple norm-based regularization is important and can help adapt to a dataset 's intrinsic complexity yielding better generalization , thus partly explaining the empirical success of similar regularizations reported in previous works .	1	0	4	-5.924739	5.1286016	1
287-5-10	we give matching lower- and upper-bounds showing that sample complexity scales with the representation dimension when no assumptions are made about the underlying data distribution .	in this work we provide pac-style sample complexity rates for supervised metric learning .	0	2	1	5.255086	-4.671379	0
287-5-10	in addition , by leveraging the structure of the data distribution , we provide rates fine-tuned to a specific notion of the intrinsic complexity of a given dataset , allowing us to relax the dependence on representation dimension .	in this work we provide pac-style sample complexity rates for supervised metric learning .	0	3	1	5.3055	-4.65841	0
287-5-10	we show both theoretically and empirically that augmenting the metric learning optimization criterion with a simple norm-based regularization is important and can help adapt to a dataset 's intrinsic complexity yielding better generalization , thus partly explaining the empirical success of similar regularizations reported in previous works .	in this work we provide pac-style sample complexity rates for supervised metric learning .	0	4	1	3.9925985	-3.5326939	0
287-5-10	we give matching lower- and upper-bounds showing that sample complexity scales with the representation dimension when no assumptions are made about the underlying data distribution .	in addition , by leveraging the structure of the data distribution , we provide rates fine-tuned to a specific notion of the intrinsic complexity of a given dataset , allowing us to relax the dependence on representation dimension .	1	2	3	-2.6200476	2.661206	1
287-5-10	we show both theoretically and empirically that augmenting the metric learning optimization criterion with a simple norm-based regularization is important and can help adapt to a dataset 's intrinsic complexity yielding better generalization , thus partly explaining the empirical success of similar regularizations reported in previous works .	we give matching lower- and upper-bounds showing that sample complexity scales with the representation dimension when no assumptions are made about the underlying data distribution .	0	4	2	1.5882049	-1.3465246	0
287-5-10	[CLS] we show both theoretically and empirically that augmenting the metric learning optimization criterion with a simple norm - based regularization is important and can help adapt to a dataset's intrinsic complexity yielding better generalization, thus partly explaining the empirical success of similar regularizations reported	in addition , by leveraging the structure of the data distribution , we provide rates fine-tuned to a specific notion of the intrinsic complexity of a given dataset , allowing us to relax the dependence on representation dimension .	0	4	3	-1.997879	2.0851645	1
288-4-6	stochastic attention-based models have been shown to improve computational efficiency at test time , but they remain difficult to train because of intractable posterior inference and high variance in the stochastic gradient estimates .	despite their success , convolutional neural networks are computationally expensive because they must examine all image locations .	0	1	0	4.969074	-4.321147	0
288-4-6	despite their success , convolutional neural networks are computationally expensive because they must examine all image locations .	borrowing techniques from the literature on training deep generative models , we present the wake-sleep recurrent attention model , a method for training stochastic attention networks which improves posterior inference and which reduces the variability in the stochastic gradients .	1	0	2	-5.987856	5.129978	1
288-4-6	despite their success , convolutional neural networks are computationally expensive because they must examine all image locations .	we show that our method can greatly speed up the training time for stochastic attention networks in the domains of image classification and caption generation .	1	0	3	-5.8831472	4.9359307	1
288-4-6	borrowing techniques from the literature on training deep generative models , we present the wake-sleep recurrent attention model , a method for training stochastic attention networks which improves posterior inference and which reduces the variability in the stochastic gradients .	stochastic attention-based models have been shown to improve computational efficiency at test time , but they remain difficult to train because of intractable posterior inference and high variance in the stochastic gradient estimates .	0	2	1	5.3998528	-4.7634487	0
288-4-6	stochastic attention-based models have been shown to improve computational efficiency at test time , but they remain difficult to train because of intractable posterior inference and high variance in the stochastic gradient estimates .	we show that our method can greatly speed up the training time for stochastic attention networks in the domains of image classification and caption generation .	1	1	3	-5.957953	5.161496	1
288-4-6	borrowing techniques from the literature on training deep generative models , we present the wake-sleep recurrent attention model , a method for training stochastic attention networks which improves posterior inference and which reduces the variability in the stochastic gradients .	we show that our method can greatly speed up the training time for stochastic attention networks in the domains of image classification and caption generation .	1	2	3	-5.718273	5.121791	1
289-7-21	however , many modern applications such as gene network discovery and social interactions analysis often involve high-dimensional noisy data with outliers or heavier tails than the gaussian distribution .	gaussian graphical models ( ggms ) are popular tools for studying network structures .	0	1	0	5.5453386	-4.996048	0
289-7-21	gaussian graphical models ( ggms ) are popular tools for studying network structures .	in this paper , we propose the trimmed graphical lasso for robust estimation of sparse ggms .	1	0	2	-5.8717065	5.0818996	1
289-7-21	our method guards against outliers by an implicit trimming mechanism akin to the popular least trimmed squares method used for linear regression .	gaussian graphical models ( ggms ) are popular tools for studying network structures .	0	3	0	5.674358	-5.080423	0
289-7-21	we provide a rigorous statistical analysis of our estimator in the high-dimensional setting .	gaussian graphical models ( ggms ) are popular tools for studying network structures .	0	4	0	5.6757936	-5.065346	0
289-7-21	in contrast , existing approaches for robust sparse ggms estimation lack statistical guarantees .	gaussian graphical models ( ggms ) are popular tools for studying network structures .	0	5	0	5.7286077	-5.0919228	0
289-7-21	our theoretical results are complemented by experiments on simulated and real gene expression data which further demonstrate the value of our approach .	gaussian graphical models ( ggms ) are popular tools for studying network structures .	0	6	0	5.695093	-5.094339	0
289-7-21	however , many modern applications such as gene network discovery and social interactions analysis often involve high-dimensional noisy data with outliers or heavier tails than the gaussian distribution .	in this paper , we propose the trimmed graphical lasso for robust estimation of sparse ggms .	1	1	2	-5.897709	5.2095623	1
289-7-21	our method guards against outliers by an implicit trimming mechanism akin to the popular least trimmed squares method used for linear regression .	however , many modern applications such as gene network discovery and social interactions analysis often involve high-dimensional noisy data with outliers or heavier tails than the gaussian distribution .	0	3	1	5.633665	-5.014711	0
289-7-21	however , many modern applications such as gene network discovery and social interactions analysis often involve high-dimensional noisy data with outliers or heavier tails than the gaussian distribution .	we provide a rigorous statistical analysis of our estimator in the high-dimensional setting .	1	1	4	-5.9375086	5.104624	1
289-7-21	in contrast , existing approaches for robust sparse ggms estimation lack statistical guarantees .	however , many modern applications such as gene network discovery and social interactions analysis often involve high-dimensional noisy data with outliers or heavier tails than the gaussian distribution .	0	5	1	4.7956977	-4.3168235	0
289-7-21	our theoretical results are complemented by experiments on simulated and real gene expression data which further demonstrate the value of our approach .	however , many modern applications such as gene network discovery and social interactions analysis often involve high-dimensional noisy data with outliers or heavier tails than the gaussian distribution .	0	6	1	5.6626945	-5.0512757	0
289-7-21	our method guards against outliers by an implicit trimming mechanism akin to the popular least trimmed squares method used for linear regression .	in this paper , we propose the trimmed graphical lasso for robust estimation of sparse ggms .	0	3	2	5.573428	-4.933078	0
289-7-21	in this paper , we propose the trimmed graphical lasso for robust estimation of sparse ggms .	we provide a rigorous statistical analysis of our estimator in the high-dimensional setting .	1	2	4	-5.961754	5.1222024	1
289-7-21	in contrast , existing approaches for robust sparse ggms estimation lack statistical guarantees .	in this paper , we propose the trimmed graphical lasso for robust estimation of sparse ggms .	0	5	2	-5.723165	5.2364664	1
289-7-21	in this paper , we propose the trimmed graphical lasso for robust estimation of sparse ggms .	our theoretical results are complemented by experiments on simulated and real gene expression data which further demonstrate the value of our approach .	1	2	6	-6.0023828	5.161603	1
289-7-21	we provide a rigorous statistical analysis of our estimator in the high-dimensional setting .	our method guards against outliers by an implicit trimming mechanism akin to the popular least trimmed squares method used for linear regression .	0	4	3	2.6219757	-2.4389772	0
289-7-21	in contrast , existing approaches for robust sparse ggms estimation lack statistical guarantees .	our method guards against outliers by an implicit trimming mechanism akin to the popular least trimmed squares method used for linear regression .	0	5	3	-5.7967615	5.237158	1
289-7-21	our method guards against outliers by an implicit trimming mechanism akin to the popular least trimmed squares method used for linear regression .	our theoretical results are complemented by experiments on simulated and real gene expression data which further demonstrate the value of our approach .	1	3	6	-5.811699	5.143323	1
289-7-21	in contrast , existing approaches for robust sparse ggms estimation lack statistical guarantees .	we provide a rigorous statistical analysis of our estimator in the high-dimensional setting .	0	5	4	-5.9195995	5.072568	1
289-7-21	we provide a rigorous statistical analysis of our estimator in the high-dimensional setting .	our theoretical results are complemented by experiments on simulated and real gene expression data which further demonstrate the value of our approach .	1	4	6	-5.8891973	5.1193123	1
289-7-21	our theoretical results are complemented by experiments on simulated and real gene expression data which further demonstrate the value of our approach .	in contrast , existing approaches for robust sparse ggms estimation lack statistical guarantees .	0	6	5	5.5667276	-4.943008	0
290-7-21	we consider the problem of testing whether two unequal-sized samples were drawn from identical distributions , versus distributions that differ significantly .	specifically , given a target error parameter > 0 , m1 independent draws from an unknown distribution p with discrete support , and m2 draws from an unknown distribution q of discrete support , we describe a test for distinguishing the case that p = q from the case that ||p - q||1 .	1	0	1	-5.7502604	5.2178726	1
290-7-21	we consider the problem of testing whether two unequal-sized samples were drawn from identical distributions , versus distributions that differ significantly .	if p and q are supported on at most n elements , then our test is successful with high probability provided m1 n2/3 /4/3 and m2 = max { mn 2 , 2n } .	1	0	2	-5.888136	5.2174587	1
290-7-21	we consider the problem of testing whether two unequal-sized samples were drawn from identical distributions , versus distributions that differ significantly .	we show that this tradeoff is information the1 oretically optimal throughout this range in the dependencies on all parameters , n , m1 , and , to constant factors for worst-case distributions .	1	0	3	-5.8916826	5.2103224	1
290-7-21	we consider the problem of testing whether two unequal-sized samples were drawn from identical distributions , versus distributions that differ significantly .	as a consequence , we obtain an algorithm for estimating the mixing time of a markov chain on n 3/2 mix ) queries to a `` next node '' orastates up to a log n factor that uses o ( n cle .	1	0	4	-5.924988	5.2204866	1
290-7-21	the core of our testing algorithm is a relatively simple statistic that seems to perform well in practice , both on synthetic and on natural language data .	we consider the problem of testing whether two unequal-sized samples were drawn from identical distributions , versus distributions that differ significantly .	0	5	0	5.594795	-5.002647	0
290-7-21	we believe that this statistic might prove to be a useful primitive within larger machine learning and natural language processing systems .	we consider the problem of testing whether two unequal-sized samples were drawn from identical distributions , versus distributions that differ significantly .	0	6	0	5.613883	-5.0544577	0
290-7-21	if p and q are supported on at most n elements , then our test is successful with high probability provided m1 n2/3 /4/3 and m2 = max { mn 2 , 2n } .	[CLS] specifically, given a target error parameter > 0, m1 independent draws from an unknown distribution p with discrete support, and m2 draws from an unknown distribution q of discrete support, we describe a test for distinguishing the case that p = q from the case that | | p - q	0	2	1	5.089018	-4.561075	0
290-7-21	we show that this tradeoff is information the1 oretically optimal throughout this range in the dependencies on all parameters , n , m1 , and , to constant factors for worst-case distributions .	[CLS] specifically, given a target error parameter > 0, m1 independent draws from an unknown distribution p with discrete support, and m2 draws from an unknown distribution q of discrete support, we describe a test for distinguishing the case that p = q from the case that | | p - q | | 1	0	3	1	3.7764485	-3.4323707	0
290-7-21	[CLS] specifically, given a target error parameter > 0, m1 independent draws from an unknown distribution p with discrete support, and m2 draws from an unknown distribution q of discrete support, we describe a test for distinguishing the case that p = q from the case	[CLS] as a consequence, we obtain an algorithm for estimating the mixing time of a markov chain on n 3 / 2 mix ) queries to a ` ` next node'' orastates up to a log n factor that uses o (	1	1	4	-5.6321244	5.029129	1
290-7-21	the core of our testing algorithm is a relatively simple statistic that seems to perform well in practice , both on synthetic and on natural language data .	specifically , given a target error parameter > 0 , m1 independent draws from an unknown distribution p with discrete support , and m2 draws from an unknown distribution q of discrete support , we describe a test for distinguishing the case that p = q from the case that ||p - q||1 .	0	5	1	4.689115	-4.235056	0
290-7-21	we believe that this statistic might prove to be a useful primitive within larger machine learning and natural language processing systems .	specifically , given a target error parameter > 0 , m1 independent draws from an unknown distribution p with discrete support , and m2 draws from an unknown distribution q of discrete support , we describe a test for distinguishing the case that p = q from the case that ||p - q||1 .	0	6	1	5.066572	-4.5811377	0
290-7-21	if p and q are supported on at most n elements , then our test is successful with high probability provided m1 n2/3 /4/3 and m2 = max { mn 2 , 2n } .	we show that this tradeoff is information the1 oretically optimal throughout this range in the dependencies on all parameters , n , m1 , and , to constant factors for worst-case distributions .	1	2	3	2.4305117	-2.2915046	0
290-7-21	as a consequence , we obtain an algorithm for estimating the mixing time of a markov chain on n 3/2 mix ) queries to a `` next node '' orastates up to a log n factor that uses o ( n cle .	if p and q are supported on at most n elements , then our test is successful with high probability provided m1 n2/3 /4/3 and m2 = max { mn 2 , 2n } .	0	4	2	3.305428	-3.1424415	0
290-7-21	the core of our testing algorithm is a relatively simple statistic that seems to perform well in practice , both on synthetic and on natural language data .	if p and q are supported on at most n elements , then our test is successful with high probability provided m1 n2/3 /4/3 and m2 = max { mn 2 , 2n } .	0	5	2	-1.827337	1.874635	1
290-7-21	we believe that this statistic might prove to be a useful primitive within larger machine learning and natural language processing systems .	if p and q are supported on at most n elements , then our test is successful with high probability provided m1 n2/3 /4/3 and m2 = max { mn 2 , 2n } .	0	6	2	4.327759	-4.046688	0
290-7-21	as a consequence , we obtain an algorithm for estimating the mixing time of a markov chain on n 3/2 mix ) queries to a `` next node '' orastates up to a log n factor that uses o ( n cle .	we show that this tradeoff is information the1 oretically optimal throughout this range in the dependencies on all parameters , n , m1 , and , to constant factors for worst-case distributions .	0	4	3	3.9514022	-3.6803985	0
290-7-21	the core of our testing algorithm is a relatively simple statistic that seems to perform well in practice , both on synthetic and on natural language data .	we show that this tradeoff is information the1 oretically optimal throughout this range in the dependencies on all parameters , n , m1 , and , to constant factors for worst-case distributions .	0	5	3	2.218506	-2.0608454	0
290-7-21	we believe that this statistic might prove to be a useful primitive within larger machine learning and natural language processing systems .	we show that this tradeoff is information the1 oretically optimal throughout this range in the dependencies on all parameters , n , m1 , and , to constant factors for worst-case distributions .	0	6	3	4.394736	-4.10915	0
290-7-21	as a consequence , we obtain an algorithm for estimating the mixing time of a markov chain on n 3/2 mix ) queries to a `` next node '' orastates up to a log n factor that uses o ( n cle .	the core of our testing algorithm is a relatively simple statistic that seems to perform well in practice , both on synthetic and on natural language data .	1	4	5	1.8259056	-1.7683159	0
290-7-21	we believe that this statistic might prove to be a useful primitive within larger machine learning and natural language processing systems .	as a consequence , we obtain an algorithm for estimating the mixing time of a markov chain on n 3/2 mix ) queries to a `` next node '' orastates up to a log n factor that uses o ( n cle .	0	6	4	3.206594	-3.025032	0
290-7-21	we believe that this statistic might prove to be a useful primitive within larger machine learning and natural language processing systems .	the core of our testing algorithm is a relatively simple statistic that seems to perform well in practice , both on synthetic and on natural language data .	0	6	5	5.106776	-4.4899454	0
291-6-15	this paper investigates the problem of estimating a jaccard index matrix when there are missing observations in data samples .	the jaccard index is a standard statistics for comparing the pairwise similarity between data samples .	0	1	0	4.317697	-3.640151	0
291-6-15	starting from a jaccard index matrix approximated from the incomplete data , our method calibrates the matrix to meet the requirement of positive semi-definiteness and other constraints , through a simple alternating projection algorithm .	the jaccard index is a standard statistics for comparing the pairwise similarity between data samples .	0	2	0	5.361163	-4.77639	0
291-6-15	compared with conventional approaches that estimate the similarity matrix based on the imputed data , our method has a strong advantage in that the calibrated matrix is guaranteed to be closer to the unknown ground truth in the frobenius norm than the un-calibrated matrix ( except in special cases they are identical ) .	the jaccard index is a standard statistics for comparing the pairwise similarity between data samples .	0	3	0	5.5397964	-4.948941	0
291-6-15	we carried out a series of empirical experiments and the results confirmed our theoretical justification .	the jaccard index is a standard statistics for comparing the pairwise similarity between data samples .	0	4	0	5.557988	-5.009171	0
291-6-15	the jaccard index is a standard statistics for comparing the pairwise similarity between data samples .	the evaluation also reported significantly improved results in real learning tasks on benchmark datasets .	1	0	5	-6.0259676	5.2005134	1
291-6-15	starting from a jaccard index matrix approximated from the incomplete data , our method calibrates the matrix to meet the requirement of positive semi-definiteness and other constraints , through a simple alternating projection algorithm .	this paper investigates the problem of estimating a jaccard index matrix when there are missing observations in data samples .	0	2	1	5.4061575	-4.844642	0
291-6-15	compared with conventional approaches that estimate the similarity matrix based on the imputed data , our method has a strong advantage in that the calibrated matrix is guaranteed to be closer to the unknown ground truth in the frobenius norm than the un-calibrated matrix ( except in special cases they are identical ) .	this paper investigates the problem of estimating a jaccard index matrix when there are missing observations in data samples .	0	3	1	5.639976	-5.059026	0
291-6-15	we carried out a series of empirical experiments and the results confirmed our theoretical justification .	this paper investigates the problem of estimating a jaccard index matrix when there are missing observations in data samples .	0	4	1	5.5902853	-5.0617743	0
291-6-15	the evaluation also reported significantly improved results in real learning tasks on benchmark datasets .	this paper investigates the problem of estimating a jaccard index matrix when there are missing observations in data samples .	0	5	1	5.5686464	-5.0194016	0
291-6-15	[CLS] compared with conventional approaches that estimate the similarity matrix based on the imputed data, our method has a strong advantage in that the calibrated matrix is guaranteed to be closer to the unknown ground truth in the frobenius norm than the un - calibrated matrix (	starting from a jaccard index matrix approximated from the incomplete data , our method calibrates the matrix to meet the requirement of positive semi-definiteness and other constraints , through a simple alternating projection algorithm .	0	3	2	4.3936625	-3.9689674	0
291-6-15	starting from a jaccard index matrix approximated from the incomplete data , our method calibrates the matrix to meet the requirement of positive semi-definiteness and other constraints , through a simple alternating projection algorithm .	we carried out a series of empirical experiments and the results confirmed our theoretical justification .	1	2	4	-5.9613824	5.0952473	1
291-6-15	starting from a jaccard index matrix approximated from the incomplete data , our method calibrates the matrix to meet the requirement of positive semi-definiteness and other constraints , through a simple alternating projection algorithm .	the evaluation also reported significantly improved results in real learning tasks on benchmark datasets .	1	2	5	-5.9520774	5.1775093	1
291-6-15	compared with conventional approaches that estimate the similarity matrix based on the imputed data , our method has a strong advantage in that the calibrated matrix is guaranteed to be closer to the unknown ground truth in the frobenius norm than the un-calibrated matrix ( except in special cases they are identical ) .	we carried out a series of empirical experiments and the results confirmed our theoretical justification .	1	3	4	-5.675754	5.104844	1
291-6-15	the evaluation also reported significantly improved results in real learning tasks on benchmark datasets .	compared with conventional approaches that estimate the similarity matrix based on the imputed data , our method has a strong advantage in that the calibrated matrix is guaranteed to be closer to the unknown ground truth in the frobenius norm than the un-calibrated matrix ( except in special cases they are identical ) .	0	5	3	4.8250966	-4.3448496	0
291-6-15	the evaluation also reported significantly improved results in real learning tasks on benchmark datasets .	we carried out a series of empirical experiments and the results confirmed our theoretical justification .	0	5	4	-0.37166703	0.58378625	1
292-9-36	like other importance sampling-based methods , performance is critically dependent on the proposal distribution : a bad proposal can lead to arbitrarily inaccurate estimates of the target distribution .	sequential monte carlo ( smc ) , or particle filtering , is a popular class of methods for sampling from an intractable target distribution using a sequence of simpler intermediate distributions .	0	1	0	4.8913994	-4.3543754	0
292-9-36	this paper presents a new method for automatically adapting the proposal using an approximation of the kullback-leibler divergence between the true posterior and the proposal distribution .	sequential monte carlo ( smc ) , or particle filtering , is a popular class of methods for sampling from an intractable target distribution using a sequence of simpler intermediate distributions .	0	2	0	5.543094	-4.9468927	0
292-9-36	the method is very flexible , applicable to any parameterized proposal distribution and it supports online and batch variants .	sequential monte carlo ( smc ) , or particle filtering , is a popular class of methods for sampling from an intractable target distribution using a sequence of simpler intermediate distributions .	0	3	0	5.595206	-5.0182104	0
292-9-36	we use the new framework to adapt powerful proposal distributions with rich parameterizations based upon neural networks leading to neural adaptive sequential monte carlo ( nasmc ) .	sequential monte carlo ( smc ) , or particle filtering , is a popular class of methods for sampling from an intractable target distribution using a sequence of simpler intermediate distributions .	0	4	0	5.451722	-4.8666334	0
292-9-36	sequential monte carlo ( smc ) , or particle filtering , is a popular class of methods for sampling from an intractable target distribution using a sequence of simpler intermediate distributions .	experiments indicate that nasmc significantly improves inference in a non-linear state space model outperforming adaptive proposal methods including the extended kalman and unscented particle filters .	1	0	5	-5.9772806	5.168448	1
292-9-36	experiments also indicate that improved inference translates into improved parameter learning when nasmc is used as a subroutine of particle marginal metropolis hastings .	sequential monte carlo ( smc ) , or particle filtering , is a popular class of methods for sampling from an intractable target distribution using a sequence of simpler intermediate distributions .	0	6	0	5.581909	-4.9813538	0
292-9-36	sequential monte carlo ( smc ) , or particle filtering , is a popular class of methods for sampling from an intractable target distribution using a sequence of simpler intermediate distributions .	finally we show that nasmc is able to train a latent variable recurrent neural network ( lv-rnn ) achieving results that compete with the state-of-the-art for polymorphic music modelling .	1	0	7	-5.938471	5.1904197	1
292-9-36	nasmc can be seen as bridging the gap between adaptive smc methods and the recent work in scalable , black-box variational inference .	sequential monte carlo ( smc ) , or particle filtering , is a popular class of methods for sampling from an intractable target distribution using a sequence of simpler intermediate distributions .	0	8	0	5.632512	-5.0109296	0
292-9-36	this paper presents a new method for automatically adapting the proposal using an approximation of the kullback-leibler divergence between the true posterior and the proposal distribution .	like other importance sampling-based methods , performance is critically dependent on the proposal distribution : a bad proposal can lead to arbitrarily inaccurate estimates of the target distribution .	0	2	1	4.8992314	-4.3169346	0
292-9-36	the method is very flexible , applicable to any parameterized proposal distribution and it supports online and batch variants .	like other importance sampling-based methods , performance is critically dependent on the proposal distribution : a bad proposal can lead to arbitrarily inaccurate estimates of the target distribution .	0	3	1	5.244603	-4.633626	0
292-9-36	like other importance sampling-based methods , performance is critically dependent on the proposal distribution : a bad proposal can lead to arbitrarily inaccurate estimates of the target distribution .	we use the new framework to adapt powerful proposal distributions with rich parameterizations based upon neural networks leading to neural adaptive sequential monte carlo ( nasmc ) .	1	1	4	-5.9665527	5.064086	1
292-9-36	like other importance sampling-based methods , performance is critically dependent on the proposal distribution : a bad proposal can lead to arbitrarily inaccurate estimates of the target distribution .	experiments indicate that nasmc significantly improves inference in a non-linear state space model outperforming adaptive proposal methods including the extended kalman and unscented particle filters .	1	1	5	-6.011933	5.1169586	1
292-9-36	experiments also indicate that improved inference translates into improved parameter learning when nasmc is used as a subroutine of particle marginal metropolis hastings .	like other importance sampling-based methods , performance is critically dependent on the proposal distribution : a bad proposal can lead to arbitrarily inaccurate estimates of the target distribution .	0	6	1	5.4429502	-4.8225455	0
292-9-36	like other importance sampling-based methods , performance is critically dependent on the proposal distribution : a bad proposal can lead to arbitrarily inaccurate estimates of the target distribution .	finally we show that nasmc is able to train a latent variable recurrent neural network ( lv-rnn ) achieving results that compete with the state-of-the-art for polymorphic music modelling .	1	1	7	-5.9557347	5.045332	1
292-9-36	like other importance sampling-based methods , performance is critically dependent on the proposal distribution : a bad proposal can lead to arbitrarily inaccurate estimates of the target distribution .	nasmc can be seen as bridging the gap between adaptive smc methods and the recent work in scalable , black-box variational inference .	1	1	8	-6.0128384	5.1440077	1
292-9-36	this paper presents a new method for automatically adapting the proposal using an approximation of the kullback-leibler divergence between the true posterior and the proposal distribution .	the method is very flexible , applicable to any parameterized proposal distribution and it supports online and batch variants .	1	2	3	-5.8592825	5.2159314	1
292-9-36	we use the new framework to adapt powerful proposal distributions with rich parameterizations based upon neural networks leading to neural adaptive sequential monte carlo ( nasmc ) .	this paper presents a new method for automatically adapting the proposal using an approximation of the kullback-leibler divergence between the true posterior and the proposal distribution .	0	4	2	4.2910533	-3.8922503	0
292-9-36	experiments indicate that nasmc significantly improves inference in a non-linear state space model outperforming adaptive proposal methods including the extended kalman and unscented particle filters .	this paper presents a new method for automatically adapting the proposal using an approximation of the kullback-leibler divergence between the true posterior and the proposal distribution .	0	5	2	5.376226	-4.8351684	0
292-9-36	experiments also indicate that improved inference translates into improved parameter learning when nasmc is used as a subroutine of particle marginal metropolis hastings .	this paper presents a new method for automatically adapting the proposal using an approximation of the kullback-leibler divergence between the true posterior and the proposal distribution .	0	6	2	5.209476	-4.642623	0
292-9-36	this paper presents a new method for automatically adapting the proposal using an approximation of the kullback-leibler divergence between the true posterior and the proposal distribution .	finally we show that nasmc is able to train a latent variable recurrent neural network ( lv-rnn ) achieving results that compete with the state-of-the-art for polymorphic music modelling .	1	2	7	-5.943589	5.161293	1
292-9-36	nasmc can be seen as bridging the gap between adaptive smc methods and the recent work in scalable , black-box variational inference .	this paper presents a new method for automatically adapting the proposal using an approximation of the kullback-leibler divergence between the true posterior and the proposal distribution .	0	8	2	-1.423025	1.6281784	1
292-9-36	we use the new framework to adapt powerful proposal distributions with rich parameterizations based upon neural networks leading to neural adaptive sequential monte carlo ( nasmc ) .	the method is very flexible , applicable to any parameterized proposal distribution and it supports online and batch variants .	0	4	3	-2.8832412	2.851119	1
292-9-36	the method is very flexible , applicable to any parameterized proposal distribution and it supports online and batch variants .	experiments indicate that nasmc significantly improves inference in a non-linear state space model outperforming adaptive proposal methods including the extended kalman and unscented particle filters .	1	3	5	-4.8221655	4.408348	1
292-9-36	experiments also indicate that improved inference translates into improved parameter learning when nasmc is used as a subroutine of particle marginal metropolis hastings .	the method is very flexible , applicable to any parameterized proposal distribution and it supports online and batch variants .	0	6	3	3.8279495	-3.5970287	0
292-9-36	the method is very flexible , applicable to any parameterized proposal distribution and it supports online and batch variants .	finally we show that nasmc is able to train a latent variable recurrent neural network ( lv-rnn ) achieving results that compete with the state-of-the-art for polymorphic music modelling .	1	3	7	-5.5403748	4.9780455	1
292-9-36	nasmc can be seen as bridging the gap between adaptive smc methods and the recent work in scalable , black-box variational inference .	the method is very flexible , applicable to any parameterized proposal distribution and it supports online and batch variants .	0	8	3	-3.4264588	3.413589	1
292-9-36	we use the new framework to adapt powerful proposal distributions with rich parameterizations based upon neural networks leading to neural adaptive sequential monte carlo ( nasmc ) .	experiments indicate that nasmc significantly improves inference in a non-linear state space model outperforming adaptive proposal methods including the extended kalman and unscented particle filters .	1	4	5	-5.9974213	5.1677685	1
292-9-36	we use the new framework to adapt powerful proposal distributions with rich parameterizations based upon neural networks leading to neural adaptive sequential monte carlo ( nasmc ) .	experiments also indicate that improved inference translates into improved parameter learning when nasmc is used as a subroutine of particle marginal metropolis hastings .	1	4	6	-5.9630656	5.108095	1
292-9-36	finally we show that nasmc is able to train a latent variable recurrent neural network ( lv-rnn ) achieving results that compete with the state-of-the-art for polymorphic music modelling .	we use the new framework to adapt powerful proposal distributions with rich parameterizations based upon neural networks leading to neural adaptive sequential monte carlo ( nasmc ) .	0	7	4	5.4944806	-4.8988276	0
292-9-36	we use the new framework to adapt powerful proposal distributions with rich parameterizations based upon neural networks leading to neural adaptive sequential monte carlo ( nasmc ) .	nasmc can be seen as bridging the gap between adaptive smc methods and the recent work in scalable , black-box variational inference .	1	4	8	-5.7213826	5.12531	1
292-9-36	experiments also indicate that improved inference translates into improved parameter learning when nasmc is used as a subroutine of particle marginal metropolis hastings .	experiments indicate that nasmc significantly improves inference in a non-linear state space model outperforming adaptive proposal methods including the extended kalman and unscented particle filters .	0	6	5	2.9669623	-2.8345568	0
292-9-36	finally we show that nasmc is able to train a latent variable recurrent neural network ( lv-rnn ) achieving results that compete with the state-of-the-art for polymorphic music modelling .	experiments indicate that nasmc significantly improves inference in a non-linear state space model outperforming adaptive proposal methods including the extended kalman and unscented particle filters .	0	7	5	2.7499719	-2.6208222	0
292-9-36	nasmc can be seen as bridging the gap between adaptive smc methods and the recent work in scalable , black-box variational inference .	experiments indicate that nasmc significantly improves inference in a non-linear state space model outperforming adaptive proposal methods including the extended kalman and unscented particle filters .	0	8	5	-5.7565413	5.080271	1
292-9-36	experiments also indicate that improved inference translates into improved parameter learning when nasmc is used as a subroutine of particle marginal metropolis hastings .	finally we show that nasmc is able to train a latent variable recurrent neural network ( lv-rnn ) achieving results that compete with the state-of-the-art for polymorphic music modelling .	1	6	7	-2.3936975	2.3400707	1
292-9-36	nasmc can be seen as bridging the gap between adaptive smc methods and the recent work in scalable , black-box variational inference .	experiments also indicate that improved inference translates into improved parameter learning when nasmc is used as a subroutine of particle marginal metropolis hastings .	0	8	6	-5.7209783	5.0415444	1
292-9-36	nasmc can be seen as bridging the gap between adaptive smc methods and the recent work in scalable , black-box variational inference .	finally we show that nasmc is able to train a latent variable recurrent neural network ( lv-rnn ) achieving results that compete with the state-of-the-art for polymorphic music modelling .	0	8	7	-5.8559704	5.0483303	1
293-5-10	this algorithm divides the problem of estimating the stochastic gradients over multiple variational parameters into smaller sub-tasks so that each sub-task explores intelligently the most relevant part of the variational distribution .	we introduce local expectation gradients which is a general purpose stochastic variational inference algorithm for constructing stochastic gradients by sampling from the variational distribution .	0	1	0	4.7477865	-4.2192917	0
293-5-10	this is achieved by performing an exact expectation over the single random variable that most correlates with the variational parameter of interest resulting in a rao-blackwellized estimate that has low variance .	we introduce local expectation gradients which is a general purpose stochastic variational inference algorithm for constructing stochastic gradients by sampling from the variational distribution .	0	2	0	4.3912344	-3.9768274	0
293-5-10	our method works efficiently for both continuous and discrete random variables .	we introduce local expectation gradients which is a general purpose stochastic variational inference algorithm for constructing stochastic gradients by sampling from the variational distribution .	0	3	0	5.0974646	-4.552636	0
293-5-10	we introduce local expectation gradients which is a general purpose stochastic variational inference algorithm for constructing stochastic gradients by sampling from the variational distribution .	furthermore , the proposed algorithm has interesting similarities with gibbs sampling but at the same time , unlike gibbs sampling , can be trivially parallelized .	1	0	4	-5.9721622	5.0957375	1
293-5-10	this algorithm divides the problem of estimating the stochastic gradients over multiple variational parameters into smaller sub-tasks so that each sub-task explores intelligently the most relevant part of the variational distribution .	this is achieved by performing an exact expectation over the single random variable that most correlates with the variational parameter of interest resulting in a rao-blackwellized estimate that has low variance .	1	1	2	-5.466752	5.059613	1
293-5-10	our method works efficiently for both continuous and discrete random variables .	this algorithm divides the problem of estimating the stochastic gradients over multiple variational parameters into smaller sub-tasks so that each sub-task explores intelligently the most relevant part of the variational distribution .	0	3	1	3.9276748	-3.6362607	0
293-5-10	furthermore , the proposed algorithm has interesting similarities with gibbs sampling but at the same time , unlike gibbs sampling , can be trivially parallelized .	this algorithm divides the problem of estimating the stochastic gradients over multiple variational parameters into smaller sub-tasks so that each sub-task explores intelligently the most relevant part of the variational distribution .	0	4	1	4.9847317	-4.467695	0
293-5-10	our method works efficiently for both continuous and discrete random variables .	this is achieved by performing an exact expectation over the single random variable that most correlates with the variational parameter of interest resulting in a rao-blackwellized estimate that has low variance .	0	3	2	0.23195842	-0.03937906	0
293-5-10	furthermore , the proposed algorithm has interesting similarities with gibbs sampling but at the same time , unlike gibbs sampling , can be trivially parallelized .	this is achieved by performing an exact expectation over the single random variable that most correlates with the variational parameter of interest resulting in a rao-blackwellized estimate that has low variance .	0	4	2	3.9901352	-3.7471166	0
293-5-10	our method works efficiently for both continuous and discrete random variables .	furthermore , the proposed algorithm has interesting similarities with gibbs sampling but at the same time , unlike gibbs sampling , can be trivially parallelized .	1	3	4	-4.512205	4.1682515	1
294-9-36	remarkable recent progress has been made in this direction through development of algorithms like sag , svrg , saga .	we study optimization algorithms based on variance reduction for stochastic gradient descent ( sgd ) .	0	1	0	5.2636547	-4.6660547	0
294-9-36	these algorithms have been shown to outperform sgd , both theoretically and empirically .	we study optimization algorithms based on variance reduction for stochastic gradient descent ( sgd ) .	0	2	0	5.4712515	-4.8674536	0
294-9-36	we study optimization algorithms based on variance reduction for stochastic gradient descent ( sgd ) .	however , asynchronous versions of these algorithms -- a crucial requirement for modern large-scale applications -- have not been studied .	1	0	3	-5.540161	4.9610996	1
294-9-36	we bridge this gap by presenting a unifying framework for many variance reduction techniques .	we study optimization algorithms based on variance reduction for stochastic gradient descent ( sgd ) .	0	4	0	5.236017	-4.6905103	0
294-9-36	we study optimization algorithms based on variance reduction for stochastic gradient descent ( sgd ) .	subsequently , we propose an asynchronous algorithm grounded in our framework , and prove its fast convergence .	1	0	5	-5.9869757	5.16042	1
294-9-36	an important consequence of our general approach is that it yields asynchronous versions of variance reduction algorithms such as svrg and saga as a byproduct .	we study optimization algorithms based on variance reduction for stochastic gradient descent ( sgd ) .	0	6	0	5.5261383	-4.925838	0
294-9-36	we study optimization algorithms based on variance reduction for stochastic gradient descent ( sgd ) .	our method achieves near linear speedup in sparse settings common to machine learning .	1	0	7	-6.0109076	5.212794	1
294-9-36	we study optimization algorithms based on variance reduction for stochastic gradient descent ( sgd ) .	we demonstrate the empirical performance of our method through a concrete realization of asynchronous svrg .	1	0	8	-6.00167	5.175912	1
294-9-36	remarkable recent progress has been made in this direction through development of algorithms like sag , svrg , saga .	these algorithms have been shown to outperform sgd , both theoretically and empirically .	1	1	2	-5.856042	5.2132726	1
294-9-36	however , asynchronous versions of these algorithms -- a crucial requirement for modern large-scale applications -- have not been studied .	remarkable recent progress has been made in this direction through development of algorithms like sag , svrg , saga .	0	3	1	4.045197	-3.6555774	0
294-9-36	we bridge this gap by presenting a unifying framework for many variance reduction techniques .	remarkable recent progress has been made in this direction through development of algorithms like sag , svrg , saga .	0	4	1	5.13976	-4.584276	0
294-9-36	remarkable recent progress has been made in this direction through development of algorithms like sag , svrg , saga .	subsequently , we propose an asynchronous algorithm grounded in our framework , and prove its fast convergence .	1	1	5	-5.990839	5.11158	1
294-9-36	remarkable recent progress has been made in this direction through development of algorithms like sag , svrg , saga .	an important consequence of our general approach is that it yields asynchronous versions of variance reduction algorithms such as svrg and saga as a byproduct .	1	1	6	-5.9922748	5.0917187	1
294-9-36	remarkable recent progress has been made in this direction through development of algorithms like sag , svrg , saga .	our method achieves near linear speedup in sparse settings common to machine learning .	1	1	7	-6.0208225	5.1767883	1
294-9-36	remarkable recent progress has been made in this direction through development of algorithms like sag , svrg , saga .	we demonstrate the empirical performance of our method through a concrete realization of asynchronous svrg .	1	1	8	-5.9493513	5.062724	1
294-9-36	these algorithms have been shown to outperform sgd , both theoretically and empirically .	however , asynchronous versions of these algorithms -- a crucial requirement for modern large-scale applications -- have not been studied .	1	2	3	1.8872982	-1.4504046	0
294-9-36	we bridge this gap by presenting a unifying framework for many variance reduction techniques .	these algorithms have been shown to outperform sgd , both theoretically and empirically .	0	4	2	-0.28356975	0.49774605	1
294-9-36	subsequently , we propose an asynchronous algorithm grounded in our framework , and prove its fast convergence .	these algorithms have been shown to outperform sgd , both theoretically and empirically .	0	5	2	1.8905413	-1.8499722	0
294-9-36	these algorithms have been shown to outperform sgd , both theoretically and empirically .	an important consequence of our general approach is that it yields asynchronous versions of variance reduction algorithms such as svrg and saga as a byproduct .	1	2	6	-5.661999	5.0296307	1
294-9-36	our method achieves near linear speedup in sparse settings common to machine learning .	these algorithms have been shown to outperform sgd , both theoretically and empirically .	0	7	2	3.6834097	-3.4445786	0
294-9-36	these algorithms have been shown to outperform sgd , both theoretically and empirically .	we demonstrate the empirical performance of our method through a concrete realization of asynchronous svrg .	1	2	8	-5.8872128	5.073943	1
294-9-36	however , asynchronous versions of these algorithms -- a crucial requirement for modern large-scale applications -- have not been studied .	we bridge this gap by presenting a unifying framework for many variance reduction techniques .	1	3	4	-5.939213	5.215252	1
294-9-36	however , asynchronous versions of these algorithms -- a crucial requirement for modern large-scale applications -- have not been studied .	subsequently , we propose an asynchronous algorithm grounded in our framework , and prove its fast convergence .	1	3	5	-5.89974	5.21481	1
294-9-36	an important consequence of our general approach is that it yields asynchronous versions of variance reduction algorithms such as svrg and saga as a byproduct .	however , asynchronous versions of these algorithms -- a crucial requirement for modern large-scale applications -- have not been studied .	0	6	3	5.397547	-4.735333	0
294-9-36	our method achieves near linear speedup in sparse settings common to machine learning .	however , asynchronous versions of these algorithms -- a crucial requirement for modern large-scale applications -- have not been studied .	0	7	3	5.57185	-4.9268894	0
294-9-36	however , asynchronous versions of these algorithms -- a crucial requirement for modern large-scale applications -- have not been studied .	we demonstrate the empirical performance of our method through a concrete realization of asynchronous svrg .	1	3	8	-5.936702	5.158597	1
294-9-36	we bridge this gap by presenting a unifying framework for many variance reduction techniques .	subsequently , we propose an asynchronous algorithm grounded in our framework , and prove its fast convergence .	1	4	5	-5.8138995	5.2078576	1
294-9-36	we bridge this gap by presenting a unifying framework for many variance reduction techniques .	an important consequence of our general approach is that it yields asynchronous versions of variance reduction algorithms such as svrg and saga as a byproduct .	1	4	6	-5.997446	5.1703753	1
294-9-36	we bridge this gap by presenting a unifying framework for many variance reduction techniques .	our method achieves near linear speedup in sparse settings common to machine learning .	1	4	7	-5.979668	5.199776	1
294-9-36	we demonstrate the empirical performance of our method through a concrete realization of asynchronous svrg .	we bridge this gap by presenting a unifying framework for many variance reduction techniques .	0	8	4	5.588723	-4.9363294	0
294-9-36	subsequently , we propose an asynchronous algorithm grounded in our framework , and prove its fast convergence .	an important consequence of our general approach is that it yields asynchronous versions of variance reduction algorithms such as svrg and saga as a byproduct .	1	5	6	-5.430789	4.8799477	1
294-9-36	our method achieves near linear speedup in sparse settings common to machine learning .	subsequently , we propose an asynchronous algorithm grounded in our framework , and prove its fast convergence .	0	7	5	4.382832	-3.8868768	0
294-9-36	we demonstrate the empirical performance of our method through a concrete realization of asynchronous svrg .	subsequently , we propose an asynchronous algorithm grounded in our framework , and prove its fast convergence .	0	8	5	4.8377156	-4.19962	0
294-9-36	our method achieves near linear speedup in sparse settings common to machine learning .	an important consequence of our general approach is that it yields asynchronous versions of variance reduction algorithms such as svrg and saga as a byproduct .	0	7	6	-3.4457705	3.3006659	1
294-9-36	we demonstrate the empirical performance of our method through a concrete realization of asynchronous svrg .	an important consequence of our general approach is that it yields asynchronous versions of variance reduction algorithms such as svrg and saga as a byproduct .	0	8	6	-3.4494095	3.248292	1
294-9-36	our method achieves near linear speedup in sparse settings common to machine learning .	we demonstrate the empirical performance of our method through a concrete realization of asynchronous svrg .	1	7	8	-2.8759222	2.8905365	1
295-6-15	active learning methods automatically adapt data collection by selecting the most informative samples in order to accelerate machine learning .	because of this , real-world testing and comparing active learning algorithms requires collecting new datasets ( adaptively ) , rather than simply applying algorithms to benchmark datasets , as is the norm in ( passive ) machine learning research .	1	0	1	-3.276359	3.2451088	1
295-6-15	to facilitate the development , testing and deployment of active learning for real applications , we have built an open-source software system for large-scale active learning research and experimentation .	active learning methods automatically adapt data collection by selecting the most informative samples in order to accelerate machine learning .	0	2	0	4.5054703	-3.9144049	0
295-6-15	the system , called next , provides a unique platform for real-world , reproducible active learning research .	active learning methods automatically adapt data collection by selecting the most informative samples in order to accelerate machine learning .	0	3	0	5.188985	-4.5774517	0
295-6-15	this paper details the challenges of building the system and demonstrates its capabilities with several experiments .	active learning methods automatically adapt data collection by selecting the most informative samples in order to accelerate machine learning .	0	4	0	5.1273737	-4.4836044	0
295-6-15	active learning methods automatically adapt data collection by selecting the most informative samples in order to accelerate machine learning .	the results show how experimentation can help expose strengths and weaknesses of active learning algorithms , in sometimes unexpected and enlightening ways .	1	0	5	-5.9861813	5.0908127	1
295-6-15	to facilitate the development , testing and deployment of active learning for real applications , we have built an open-source software system for large-scale active learning research and experimentation .	because of this , real-world testing and comparing active learning algorithms requires collecting new datasets ( adaptively ) , rather than simply applying algorithms to benchmark datasets , as is the norm in ( passive ) machine learning research .	0	2	1	4.532608	-4.044731	0
295-6-15	the system , called next , provides a unique platform for real-world , reproducible active learning research .	because of this , real-world testing and comparing active learning algorithms requires collecting new datasets ( adaptively ) , rather than simply applying algorithms to benchmark datasets , as is the norm in ( passive ) machine learning research .	0	3	1	4.8189707	-4.271742	0
295-6-15	because of this , real-world testing and comparing active learning algorithms requires collecting new datasets ( adaptively ) , rather than simply applying algorithms to benchmark datasets , as is the norm in ( passive ) machine learning research .	this paper details the challenges of building the system and demonstrates its capabilities with several experiments .	1	1	4	-5.953611	5.2408924	1
295-6-15	because of this , real-world testing and comparing active learning algorithms requires collecting new datasets ( adaptively ) , rather than simply applying algorithms to benchmark datasets , as is the norm in ( passive ) machine learning research .	the results show how experimentation can help expose strengths and weaknesses of active learning algorithms , in sometimes unexpected and enlightening ways .	1	1	5	-5.9823885	5.1747904	1
295-6-15	to facilitate the development , testing and deployment of active learning for real applications , we have built an open-source software system for large-scale active learning research and experimentation .	the system , called next , provides a unique platform for real-world , reproducible active learning research .	1	2	3	-5.8445587	5.258807	1
295-6-15	this paper details the challenges of building the system and demonstrates its capabilities with several experiments .	to facilitate the development , testing and deployment of active learning for real applications , we have built an open-source software system for large-scale active learning research and experimentation .	0	4	2	3.6788712	-3.397176	0
295-6-15	to facilitate the development , testing and deployment of active learning for real applications , we have built an open-source software system for large-scale active learning research and experimentation .	the results show how experimentation can help expose strengths and weaknesses of active learning algorithms , in sometimes unexpected and enlightening ways .	1	2	5	-5.839302	5.244429	1
295-6-15	this paper details the challenges of building the system and demonstrates its capabilities with several experiments .	the system , called next , provides a unique platform for real-world , reproducible active learning research .	0	4	3	2.4482603	-2.2764435	0
295-6-15	the system , called next , provides a unique platform for real-world , reproducible active learning research .	the results show how experimentation can help expose strengths and weaknesses of active learning algorithms , in sometimes unexpected and enlightening ways .	1	3	5	-4.6402044	4.2627645	1
295-6-15	the results show how experimentation can help expose strengths and weaknesses of active learning algorithms , in sometimes unexpected and enlightening ways .	this paper details the challenges of building the system and demonstrates its capabilities with several experiments .	0	5	4	5.5519896	-5.0092025	0
296-10-45	this signal processing problem arises in numerous imaging problems , ranging from astronomy to biology to spectroscopy , where it is common to take ( coarse ) fourier measurements of an object .	super-resolution is the problem of recovering a superposition of point sources using bandlimited measurements , which may be corrupted with noise .	0	1	0	4.767361	-4.2837086	0
296-10-45	super-resolution is the problem of recovering a superposition of point sources using bandlimited measurements , which may be corrupted with noise .	of particular interest is in obtaining estimation procedures which are robust to noise , with the following desirable statistical and computational properties : we seek to use coarse fourier measurements ( bounded by some cutoff frequency ) ; we hope to take a ( quantifiably ) small number of measurements ; we desire our algorithm to run quickly .	1	0	2	-5.948889	5.2079773	1
296-10-45	super-resolution is the problem of recovering a superposition of point sources using bandlimited measurements , which may be corrupted with noise .	suppose we have k point sources in d dimensions , where the points are separated by at least from each other ( in euclidean distance ) .	1	0	3	-1.0435187	1.3695979	1
296-10-45	super-resolution is the problem of recovering a superposition of point sources using bandlimited measurements , which may be corrupted with noise .	this work provides an algorithm with the following favorable guarantees : * the algorithm uses fourier measurements , whose frequencies are bounded by o ( 1/ ) ( up to log factors ) .	1	0	4	-5.9048033	5.184265	1
296-10-45	p previous algorithms require a cutoff frequency which may be as large as ( d/ ) .	super-resolution is the problem of recovering a superposition of point sources using bandlimited measurements , which may be corrupted with noise .	0	5	0	5.3214006	-4.7345896	0
296-10-45	* the number of measurements taken by and the computational complexity of our algorithm are bounded by a polynomial in both the number of points k and the dimension d , with no dependence on the separation .	super-resolution is the problem of recovering a superposition of point sources using bandlimited measurements , which may be corrupted with noise .	0	6	0	5.4069924	-4.8877597	0
296-10-45	in contrast , previous algorithms depended inverse polynomially on the minimal separation and exponentially on the dimension for both of these quantities .	super-resolution is the problem of recovering a superposition of point sources using bandlimited measurements , which may be corrupted with noise .	0	7	0	5.2759476	-4.688196	0
296-10-45	super-resolution is the problem of recovering a superposition of point sources using bandlimited measurements , which may be corrupted with noise .	our estimation procedure itself is simple : we take random bandlimited measurements ( as opposed to taking an exponential number of measurements on the hypergrid ) .	1	0	8	-5.9487925	5.1908116	1
296-10-45	furthermore , our analysis and algorithm are elementary ( based on concentration bounds for sampling and the singular value decomposition ) .	super-resolution is the problem of recovering a superposition of point sources using bandlimited measurements , which may be corrupted with noise .	0	9	0	5.3728204	-4.8198833	0
296-10-45	this signal processing problem arises in numerous imaging problems , ranging from astronomy to biology to spectroscopy , where it is common to take ( coarse ) fourier measurements of an object .	of particular interest is in obtaining estimation procedures which are robust to noise , with the following desirable statistical and computational properties : we seek to use coarse fourier measurements ( bounded by some cutoff frequency ) ; we hope to take a ( quantifiably ) small number of measurements ; we desire our algorithm to run quickly .	1	1	2	-6.0364404	5.1855865	1
296-10-45	suppose we have k point sources in d dimensions , where the points are separated by at least from each other ( in euclidean distance ) .	this signal processing problem arises in numerous imaging problems , ranging from astronomy to biology to spectroscopy , where it is common to take ( coarse ) fourier measurements of an object .	0	3	1	-2.507509	2.610818	1
296-10-45	this work provides an algorithm with the following favorable guarantees : * the algorithm uses fourier measurements , whose frequencies are bounded by o ( 1/ ) ( up to log factors ) .	this signal processing problem arises in numerous imaging problems , ranging from astronomy to biology to spectroscopy , where it is common to take ( coarse ) fourier measurements of an object .	0	4	1	5.4316053	-4.824299	0
296-10-45	p previous algorithms require a cutoff frequency which may be as large as ( d/ ) .	this signal processing problem arises in numerous imaging problems , ranging from astronomy to biology to spectroscopy , where it is common to take ( coarse ) fourier measurements of an object .	0	5	1	5.2205763	-4.5986223	0
296-10-45	this signal processing problem arises in numerous imaging problems , ranging from astronomy to biology to spectroscopy , where it is common to take ( coarse ) fourier measurements of an object .	* the number of measurements taken by and the computational complexity of our algorithm are bounded by a polynomial in both the number of points k and the dimension d , with no dependence on the separation .	1	1	6	-5.982643	5.2480755	1
296-10-45	in contrast , previous algorithms depended inverse polynomially on the minimal separation and exponentially on the dimension for both of these quantities .	this signal processing problem arises in numerous imaging problems , ranging from astronomy to biology to spectroscopy , where it is common to take ( coarse ) fourier measurements of an object .	0	7	1	5.298806	-4.685044	0
296-10-45	our estimation procedure itself is simple : we take random bandlimited measurements ( as opposed to taking an exponential number of measurements on the hypergrid ) .	this signal processing problem arises in numerous imaging problems , ranging from astronomy to biology to spectroscopy , where it is common to take ( coarse ) fourier measurements of an object .	0	8	1	5.5582905	-5.0186024	0
296-10-45	this signal processing problem arises in numerous imaging problems , ranging from astronomy to biology to spectroscopy , where it is common to take ( coarse ) fourier measurements of an object .	furthermore , our analysis and algorithm are elementary ( based on concentration bounds for sampling and the singular value decomposition ) .	1	1	9	-6.028775	5.2165184	1
296-10-45	of particular interest is in obtaining estimation procedures which are robust to noise , with the following desirable statistical and computational properties : we seek to use coarse fourier measurements ( bounded by some cutoff frequency ) ; we hope to take a ( quantifiably ) small number of measurements ; we desire our algorithm to run quickly .	suppose we have k point sources in d dimensions , where the points are separated by at least from each other ( in euclidean distance ) .	1	2	3	5.579981	-4.972104	0
296-10-45	[CLS] of particular interest is in obtaining estimation procedures which are robust to noise, with the following desirable statistical and computational properties : we seek to use coarse fourier measurements ( bounded by some cutoff frequency ) ; we hope to take a ( quantifiably ) small number of measurements ; we desire our algorithm to run quickly	this work provides an algorithm with the following favorable guarantees : * the algorithm uses fourier measurements , whose frequencies are bounded by o ( 1/ ) ( up to log factors ) .	1	2	4	-0.7517951	0.9500246	1
296-10-45	p previous algorithms require a cutoff frequency which may be as large as ( d/ ) .	of particular interest is in obtaining estimation procedures which are robust to noise , with the following desirable statistical and computational properties : we seek to use coarse fourier measurements ( bounded by some cutoff frequency ) ; we hope to take a ( quantifiably ) small number of measurements ; we desire our algorithm to run quickly .	0	5	2	-5.6366277	5.114659	1
296-10-45	* the number of measurements taken by and the computational complexity of our algorithm are bounded by a polynomial in both the number of points k and the dimension d , with no dependence on the separation .	[CLS] of particular interest is in obtaining estimation procedures which are robust to noise, with the following desirable statistical and computational properties : we seek to use coarse fourier measurements ( bounded by some cutoff frequency ) ; we hope to take a ( quantifiably ) small number of measurements ; we desire our algorithm	0	6	2	-2.1818771	2.1981628	1
296-10-45	in contrast , previous algorithms depended inverse polynomially on the minimal separation and exponentially on the dimension for both of these quantities .	of particular interest is in obtaining estimation procedures which are robust to noise , with the following desirable statistical and computational properties : we seek to use coarse fourier measurements ( bounded by some cutoff frequency ) ; we hope to take a ( quantifiably ) small number of measurements ; we desire our algorithm to run quickly .	0	7	2	-4.8419604	4.429729	1
296-10-45	our estimation procedure itself is simple : we take random bandlimited measurements ( as opposed to taking an exponential number of measurements on the hypergrid ) .	of particular interest is in obtaining estimation procedures which are robust to noise , with the following desirable statistical and computational properties : we seek to use coarse fourier measurements ( bounded by some cutoff frequency ) ; we hope to take a ( quantifiably ) small number of measurements ; we desire our algorithm to run quickly .	0	8	2	2.4029644	-2.2692585	0
296-10-45	of particular interest is in obtaining estimation procedures which are robust to noise , with the following desirable statistical and computational properties : we seek to use coarse fourier measurements ( bounded by some cutoff frequency ) ; we hope to take a ( quantifiably ) small number of measurements ; we desire our algorithm to run quickly .	furthermore , our analysis and algorithm are elementary ( based on concentration bounds for sampling and the singular value decomposition ) .	1	2	9	-4.426946	4.117341	1
296-10-45	this work provides an algorithm with the following favorable guarantees : * the algorithm uses fourier measurements , whose frequencies are bounded by o ( 1/ ) ( up to log factors ) .	suppose we have k point sources in d dimensions , where the points are separated by at least from each other ( in euclidean distance ) .	0	4	3	5.6518497	-5.046167	0
296-10-45	suppose we have k point sources in d dimensions , where the points are separated by at least from each other ( in euclidean distance ) .	p previous algorithms require a cutoff frequency which may be as large as ( d/ ) .	1	3	5	-5.7688246	5.18391	1
296-10-45	* the number of measurements taken by and the computational complexity of our algorithm are bounded by a polynomial in both the number of points k and the dimension d , with no dependence on the separation .	suppose we have k point sources in d dimensions , where the points are separated by at least from each other ( in euclidean distance ) .	0	6	3	5.6209545	-5.0540333	0
296-10-45	in contrast , previous algorithms depended inverse polynomially on the minimal separation and exponentially on the dimension for both of these quantities .	suppose we have k point sources in d dimensions , where the points are separated by at least from each other ( in euclidean distance ) .	0	7	3	5.5441103	-4.91157	0
296-10-45	our estimation procedure itself is simple : we take random bandlimited measurements ( as opposed to taking an exponential number of measurements on the hypergrid ) .	suppose we have k point sources in d dimensions , where the points are separated by at least from each other ( in euclidean distance ) .	0	8	3	5.6447372	-5.069275	0
296-10-45	furthermore , our analysis and algorithm are elementary ( based on concentration bounds for sampling and the singular value decomposition ) .	suppose we have k point sources in d dimensions , where the points are separated by at least from each other ( in euclidean distance ) .	0	9	3	5.6099005	-5.0026712	0
296-10-45	p previous algorithms require a cutoff frequency which may be as large as ( d/ ) .	this work provides an algorithm with the following favorable guarantees : * the algorithm uses fourier measurements , whose frequencies are bounded by o ( 1/ ) ( up to log factors ) .	0	5	4	-5.9381065	5.1871166	1
296-10-45	this work provides an algorithm with the following favorable guarantees : * the algorithm uses fourier measurements , whose frequencies are bounded by o ( 1/ ) ( up to log factors ) .	* the number of measurements taken by and the computational complexity of our algorithm are bounded by a polynomial in both the number of points k and the dimension d , with no dependence on the separation .	1	4	6	-4.954074	4.546986	1
296-10-45	this work provides an algorithm with the following favorable guarantees : * the algorithm uses fourier measurements , whose frequencies are bounded by o ( 1/ ) ( up to log factors ) .	in contrast , previous algorithms depended inverse polynomially on the minimal separation and exponentially on the dimension for both of these quantities .	1	4	7	3.757483	-3.4573321	0
296-10-45	our estimation procedure itself is simple : we take random bandlimited measurements ( as opposed to taking an exponential number of measurements on the hypergrid ) .	this work provides an algorithm with the following favorable guarantees : * the algorithm uses fourier measurements , whose frequencies are bounded by o ( 1/ ) ( up to log factors ) .	0	8	4	2.5527654	-2.3881526	0
296-10-45	furthermore , our analysis and algorithm are elementary ( based on concentration bounds for sampling and the singular value decomposition ) .	this work provides an algorithm with the following favorable guarantees : * the algorithm uses fourier measurements , whose frequencies are bounded by o ( 1/ ) ( up to log factors ) .	0	9	4	3.8847709	-3.5346835	0
296-10-45	p previous algorithms require a cutoff frequency which may be as large as ( d/ ) .	* the number of measurements taken by and the computational complexity of our algorithm are bounded by a polynomial in both the number of points k and the dimension d , with no dependence on the separation .	1	5	6	-5.898565	5.247678	1
296-10-45	p previous algorithms require a cutoff frequency which may be as large as ( d/ ) .	in contrast , previous algorithms depended inverse polynomially on the minimal separation and exponentially on the dimension for both of these quantities .	1	5	7	-2.0149171	2.1268268	1
296-10-45	our estimation procedure itself is simple : we take random bandlimited measurements ( as opposed to taking an exponential number of measurements on the hypergrid ) .	p previous algorithms require a cutoff frequency which may be as large as ( d/ ) .	0	8	5	4.4634423	-4.017333	0
296-10-45	p previous algorithms require a cutoff frequency which may be as large as ( d/ ) .	furthermore , our analysis and algorithm are elementary ( based on concentration bounds for sampling and the singular value decomposition ) .	1	5	9	-5.9934444	5.175108	1
296-10-45	in contrast , previous algorithms depended inverse polynomially on the minimal separation and exponentially on the dimension for both of these quantities .	* the number of measurements taken by and the computational complexity of our algorithm are bounded by a polynomial in both the number of points k and the dimension d , with no dependence on the separation .	0	7	6	-4.421815	4.0995183	1
296-10-45	* the number of measurements taken by and the computational complexity of our algorithm are bounded by a polynomial in both the number of points k and the dimension d , with no dependence on the separation .	our estimation procedure itself is simple : we take random bandlimited measurements ( as opposed to taking an exponential number of measurements on the hypergrid ) .	1	6	8	0.9562572	-0.809004	0
296-10-45	* the number of measurements taken by and the computational complexity of our algorithm are bounded by a polynomial in both the number of points k and the dimension d , with no dependence on the separation .	furthermore , our analysis and algorithm are elementary ( based on concentration bounds for sampling and the singular value decomposition ) .	1	6	9	-5.2403164	4.8044205	1
296-10-45	in contrast , previous algorithms depended inverse polynomially on the minimal separation and exponentially on the dimension for both of these quantities .	our estimation procedure itself is simple : we take random bandlimited measurements ( as opposed to taking an exponential number of measurements on the hypergrid ) .	1	7	8	-5.0975256	4.7328005	1
296-10-45	in contrast , previous algorithms depended inverse polynomially on the minimal separation and exponentially on the dimension for both of these quantities .	furthermore , our analysis and algorithm are elementary ( based on concentration bounds for sampling and the singular value decomposition ) .	1	7	9	-5.9371467	5.1284227	1
296-10-45	furthermore , our analysis and algorithm are elementary ( based on concentration bounds for sampling and the singular value decomposition ) .	our estimation procedure itself is simple : we take random bandlimited measurements ( as opposed to taking an exponential number of measurements on the hypergrid ) .	0	9	8	3.4051342	-3.1983953	0
297-6-15	stochastic gradient descent ( sgd ) is a ubiquitous algorithm for a variety of machine learning problems .	researchers and industry have developed several techniques to optimize sgd 's runtime performance , including asynchronous execution and reduced precision .	1	0	1	-5.8332205	5.103367	1
297-6-15	our main result is a martingale-based analysis that enables us to capture the rich noise models that may arise from such techniques .	stochastic gradient descent ( sgd ) is a ubiquitous algorithm for a variety of machine learning problems .	0	2	0	5.6240315	-5.0598927	0
297-6-15	specifically , we use our new analysis in three ways : ( 1 ) we derive convergence rates for the convex case ( h ogwild ! )	stochastic gradient descent ( sgd ) is a ubiquitous algorithm for a variety of machine learning problems .	0	3	0	5.616819	-5.0274296	0
297-6-15	with relaxed assumptions on the sparsity of the problem ; ( 2 ) we analyze asynchronous sgd algorithms for non-convex matrix problems including matrix completion ; and ( 3 ) we design and analyze an asynchronous sgd algorithm , called b uckwild ! , that uses lower-precision arithmetic .	stochastic gradient descent ( sgd ) is a ubiquitous algorithm for a variety of machine learning problems .	0	4	0	5.667322	-5.0532646	0
297-6-15	stochastic gradient descent ( sgd ) is a ubiquitous algorithm for a variety of machine learning problems .	we show experimentally that our algorithms run efficiently for a variety of problems on modern hardware .	1	0	5	-5.978879	5.1432743	1
297-6-15	researchers and industry have developed several techniques to optimize sgd 's runtime performance , including asynchronous execution and reduced precision .	our main result is a martingale-based analysis that enables us to capture the rich noise models that may arise from such techniques .	1	1	2	-5.956783	5.0884185	1
297-6-15	specifically , we use our new analysis in three ways : ( 1 ) we derive convergence rates for the convex case ( h ogwild ! )	researchers and industry have developed several techniques to optimize sgd 's runtime performance , including asynchronous execution and reduced precision .	0	3	1	5.410802	-4.8648257	0
297-6-15	researchers and industry have developed several techniques to optimize sgd 's runtime performance , including asynchronous execution and reduced precision .	[CLS] with relaxed assumptions on the sparsity of the problem ; ( 2 ) we analyze asynchronous sgd algorithms for non - convex matrix problems including matrix completion ; and ( 3 ) we design and analyze an asynchronous sgd algorithm, called b uckwild!, that uses lower - precision arithmetic	1	1	4	-5.9323688	5.1473627	1
297-6-15	researchers and industry have developed several techniques to optimize sgd 's runtime performance , including asynchronous execution and reduced precision .	we show experimentally that our algorithms run efficiently for a variety of problems on modern hardware .	1	1	5	-5.981935	5.1509204	1
297-6-15	our main result is a martingale-based analysis that enables us to capture the rich noise models that may arise from such techniques .	specifically , we use our new analysis in three ways : ( 1 ) we derive convergence rates for the convex case ( h ogwild ! )	1	2	3	0.38874704	-0.111536816	0
297-6-15	with relaxed assumptions on the sparsity of the problem ; ( 2 ) we analyze asynchronous sgd algorithms for non-convex matrix problems including matrix completion ; and ( 3 ) we design and analyze an asynchronous sgd algorithm , called b uckwild ! , that uses lower-precision arithmetic .	our main result is a martingale-based analysis that enables us to capture the rich noise models that may arise from such techniques .	0	4	2	-2.538133	2.5961237	1
297-6-15	our main result is a martingale-based analysis that enables us to capture the rich noise models that may arise from such techniques .	we show experimentally that our algorithms run efficiently for a variety of problems on modern hardware .	1	2	5	-4.1816044	3.9401126	1
297-6-15	specifically , we use our new analysis in three ways : ( 1 ) we derive convergence rates for the convex case ( h ogwild ! )	with relaxed assumptions on the sparsity of the problem ; ( 2 ) we analyze asynchronous sgd algorithms for non-convex matrix problems including matrix completion ; and ( 3 ) we design and analyze an asynchronous sgd algorithm , called b uckwild ! , that uses lower-precision arithmetic .	1	3	4	-5.4252977	4.968883	1
297-6-15	specifically , we use our new analysis in three ways : ( 1 ) we derive convergence rates for the convex case ( h ogwild ! )	we show experimentally that our algorithms run efficiently for a variety of problems on modern hardware .	1	3	5	-5.8422575	5.0578914	1
297-6-15	we show experimentally that our algorithms run efficiently for a variety of problems on modern hardware .	with relaxed assumptions on the sparsity of the problem ; ( 2 ) we analyze asynchronous sgd algorithms for non-convex matrix problems including matrix completion ; and ( 3 ) we design and analyze an asynchronous sgd algorithm , called b uckwild ! , that uses lower-precision arithmetic .	0	5	4	4.7064347	-4.2817044	0
298-7-21	successful approaches include both generative models of natural images as well as discriminative training of deep neural networks .	in recent years , approaches based on machine learning have achieved state-of-theart performance on image restoration problems .	0	1	0	4.5756006	-3.991315	0
298-7-21	in recent years , approaches based on machine learning have achieved state-of-theart performance on image restoration problems .	discriminative training of feed forward architectures allows explicit control over the computational cost of performing restoration and therefore often leads to better performance at the same cost at run time .	1	0	2	-5.8174057	5.2042155	1
298-7-21	in recent years , approaches based on machine learning have achieved state-of-theart performance on image restoration problems .	in contrast , generative models have the advantage that they can be trained once and then adapted to any image restoration task by a simple use of bayes ' rule .	1	0	3	-5.9816484	5.2232385	1
298-7-21	in this paper we show how to combine the strengths of both approaches by training a discriminative , feed-forward architecture to predict the state of latent variables in a generative model of natural images .	in recent years , approaches based on machine learning have achieved state-of-theart performance on image restoration problems .	0	4	0	5.648555	-5.012007	0
298-7-21	in recent years , approaches based on machine learning have achieved state-of-theart performance on image restoration problems .	we apply this idea to the very successful gaussian mixture model ( gmm ) of natural images .	1	0	5	-5.9228497	5.0628943	1
298-7-21	in recent years , approaches based on machine learning have achieved state-of-theart performance on image restoration problems .	we show that it is possible to achieve comparable performance as the original gmm but with two orders of magnitude improvement in run time while maintaining the advantage of generative models .	1	0	6	-5.993812	5.194979	1
298-7-21	successful approaches include both generative models of natural images as well as discriminative training of deep neural networks .	discriminative training of feed forward architectures allows explicit control over the computational cost of performing restoration and therefore often leads to better performance at the same cost at run time .	1	1	2	-5.1524525	4.71219	1
298-7-21	in contrast , generative models have the advantage that they can be trained once and then adapted to any image restoration task by a simple use of bayes ' rule .	successful approaches include both generative models of natural images as well as discriminative training of deep neural networks .	0	3	1	5.1932507	-4.5034385	0
298-7-21	successful approaches include both generative models of natural images as well as discriminative training of deep neural networks .	in this paper we show how to combine the strengths of both approaches by training a discriminative , feed-forward architecture to predict the state of latent variables in a generative model of natural images .	1	1	4	-5.853757	4.8991613	1
298-7-21	successful approaches include both generative models of natural images as well as discriminative training of deep neural networks .	we apply this idea to the very successful gaussian mixture model ( gmm ) of natural images .	1	1	5	-5.717194	5.0106316	1
298-7-21	we show that it is possible to achieve comparable performance as the original gmm but with two orders of magnitude improvement in run time while maintaining the advantage of generative models .	successful approaches include both generative models of natural images as well as discriminative training of deep neural networks .	0	6	1	5.55702	-4.9242554	0
298-7-21	in contrast , generative models have the advantage that they can be trained once and then adapted to any image restoration task by a simple use of bayes ' rule .	discriminative training of feed forward architectures allows explicit control over the computational cost of performing restoration and therefore often leads to better performance at the same cost at run time .	0	3	2	2.01162	-1.7215545	0
298-7-21	discriminative training of feed forward architectures allows explicit control over the computational cost of performing restoration and therefore often leads to better performance at the same cost at run time .	in this paper we show how to combine the strengths of both approaches by training a discriminative , feed-forward architecture to predict the state of latent variables in a generative model of natural images .	1	2	4	-2.6148949	2.63613	1
298-7-21	discriminative training of feed forward architectures allows explicit control over the computational cost of performing restoration and therefore often leads to better performance at the same cost at run time .	we apply this idea to the very successful gaussian mixture model ( gmm ) of natural images .	1	2	5	-3.4470596	3.3923774	1
298-7-21	we show that it is possible to achieve comparable performance as the original gmm but with two orders of magnitude improvement in run time while maintaining the advantage of generative models .	discriminative training of feed forward architectures allows explicit control over the computational cost of performing restoration and therefore often leads to better performance at the same cost at run time .	0	6	2	4.7820554	-4.271845	0
298-7-21	in contrast , generative models have the advantage that they can be trained once and then adapted to any image restoration task by a simple use of bayes ' rule .	in this paper we show how to combine the strengths of both approaches by training a discriminative , feed-forward architecture to predict the state of latent variables in a generative model of natural images .	1	3	4	-2.2777762	2.3288717	1
298-7-21	we apply this idea to the very successful gaussian mixture model ( gmm ) of natural images .	in contrast , generative models have the advantage that they can be trained once and then adapted to any image restoration task by a simple use of bayes ' rule .	0	5	3	2.5857592	-2.245733	0
298-7-21	we show that it is possible to achieve comparable performance as the original gmm but with two orders of magnitude improvement in run time while maintaining the advantage of generative models .	in contrast , generative models have the advantage that they can be trained once and then adapted to any image restoration task by a simple use of bayes ' rule .	0	6	3	4.8488493	-4.340266	0
298-7-21	in this paper we show how to combine the strengths of both approaches by training a discriminative , feed-forward architecture to predict the state of latent variables in a generative model of natural images .	we apply this idea to the very successful gaussian mixture model ( gmm ) of natural images .	1	4	5	-3.9513626	3.786223	1
298-7-21	we show that it is possible to achieve comparable performance as the original gmm but with two orders of magnitude improvement in run time while maintaining the advantage of generative models .	in this paper we show how to combine the strengths of both approaches by training a discriminative , feed-forward architecture to predict the state of latent variables in a generative model of natural images .	0	6	4	5.507039	-4.9138737	0
298-7-21	we apply this idea to the very successful gaussian mixture model ( gmm ) of natural images .	we show that it is possible to achieve comparable performance as the original gmm but with two orders of magnitude improvement in run time while maintaining the advantage of generative models .	1	5	6	-5.9941473	5.1000586	1
299-10-45	we introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence .	such problems can not be trivially addressed by existent approaches such as sequence-to-sequence and neural turing machines , because the number of target classes in each step of the output depends on the length of the input , which is variable .	1	0	1	-4.2661924	3.985907	1
299-10-45	problems such as sorting variable sized sequences , and various combinatorial optimization problems belong to this class .	we introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence .	0	2	0	1.395416	-0.9060222	0
299-10-45	we introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence .	our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention .	1	0	3	-5.587882	5.09023	1
299-10-45	it differs from the previous attention attempts in that , instead of using attention to blend hidden units of an encoder to a context vector at each decoder step , it uses attention as a pointer to select a member of the input sequence as the output .	we introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence .	0	4	0	5.300252	-4.7508965	0
299-10-45	we call this architecture a pointer net ( ptr-net ) .	we introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence .	0	5	0	5.5234346	-4.912258	0
299-10-45	we introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence .	we show ptr-nets can be used to learn approximate solutions to three challenging geometric problems - finding planar convex hulls , computing delaunay triangulations , and the planar travelling salesman problem - using training examples alone .	1	0	6	-5.974935	5.233278	1
299-10-45	ptr-nets not only improve over sequence-to-sequence with input attention , but also allow us to generalize to variable size output dictionaries .	we introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence .	0	7	0	5.4919763	-4.842375	0
299-10-45	we introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence .	we show that the learnt models generalize beyond the maximum lengths they were trained on .	1	0	8	-6.001956	5.1874285	1
299-10-45	we introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence .	we hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems .	1	0	9	-5.863681	4.987188	1
299-10-45	such problems can not be trivially addressed by existent approaches such as sequence-to-sequence and neural turing machines , because the number of target classes in each step of the output depends on the length of the input , which is variable .	problems such as sorting variable sized sequences , and various combinatorial optimization problems belong to this class .	1	1	2	4.8329363	-4.2752104	0
299-10-45	such problems can not be trivially addressed by existent approaches such as sequence-to-sequence and neural turing machines , because the number of target classes in each step of the output depends on the length of the input , which is variable .	our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention .	1	1	3	-4.115065	3.95843	1
299-10-45	such problems can not be trivially addressed by existent approaches such as sequence-to-sequence and neural turing machines , because the number of target classes in each step of the output depends on the length of the input , which is variable .	[CLS] it differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output	1	1	4	-4.846202	4.5129457	1
299-10-45	we call this architecture a pointer net ( ptr-net ) .	such problems can not be trivially addressed by existent approaches such as sequence-to-sequence and neural turing machines , because the number of target classes in each step of the output depends on the length of the input , which is variable .	0	5	1	3.1976051	-2.9820518	0
299-10-45	such problems can not be trivially addressed by existent approaches such as sequence-to-sequence and neural turing machines , because the number of target classes in each step of the output depends on the length of the input , which is variable .	we show ptr-nets can be used to learn approximate solutions to three challenging geometric problems - finding planar convex hulls , computing delaunay triangulations , and the planar travelling salesman problem - using training examples alone .	1	1	6	-5.745721	5.1389394	1
299-10-45	such problems can not be trivially addressed by existent approaches such as sequence-to-sequence and neural turing machines , because the number of target classes in each step of the output depends on the length of the input , which is variable .	ptr-nets not only improve over sequence-to-sequence with input attention , but also allow us to generalize to variable size output dictionaries .	1	1	7	-5.7673287	5.110237	1
299-10-45	we show that the learnt models generalize beyond the maximum lengths they were trained on .	such problems can not be trivially addressed by existent approaches such as sequence-to-sequence and neural turing machines , because the number of target classes in each step of the output depends on the length of the input , which is variable .	0	8	1	4.507489	-4.0626216	0
299-10-45	we hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems .	such problems can not be trivially addressed by existent approaches such as sequence-to-sequence and neural turing machines , because the number of target classes in each step of the output depends on the length of the input , which is variable .	0	9	1	5.222417	-4.6768656	0
299-10-45	problems such as sorting variable sized sequences , and various combinatorial optimization problems belong to this class .	our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention .	1	2	3	-4.722304	4.4228177	1
299-10-45	it differs from the previous attention attempts in that , instead of using attention to blend hidden units of an encoder to a context vector at each decoder step , it uses attention as a pointer to select a member of the input sequence as the output .	problems such as sorting variable sized sequences , and various combinatorial optimization problems belong to this class .	0	4	2	4.5328817	-4.053986	0
299-10-45	we call this architecture a pointer net ( ptr-net ) .	problems such as sorting variable sized sequences , and various combinatorial optimization problems belong to this class .	0	5	2	4.1169763	-3.72601	0
299-10-45	we show ptr-nets can be used to learn approximate solutions to three challenging geometric problems - finding planar convex hulls , computing delaunay triangulations , and the planar travelling salesman problem - using training examples alone .	problems such as sorting variable sized sequences , and various combinatorial optimization problems belong to this class .	0	6	2	4.852338	-4.344793	0
299-10-45	problems such as sorting variable sized sequences , and various combinatorial optimization problems belong to this class .	ptr-nets not only improve over sequence-to-sequence with input attention , but also allow us to generalize to variable size output dictionaries .	1	2	7	-5.8581896	5.1863575	1
299-10-45	we show that the learnt models generalize beyond the maximum lengths they were trained on .	problems such as sorting variable sized sequences , and various combinatorial optimization problems belong to this class .	0	8	2	5.1122003	-4.6179533	0
299-10-45	problems such as sorting variable sized sequences , and various combinatorial optimization problems belong to this class .	we hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems .	1	2	9	-5.861416	4.9579744	1
299-10-45	it differs from the previous attention attempts in that , instead of using attention to blend hidden units of an encoder to a context vector at each decoder step , it uses attention as a pointer to select a member of the input sequence as the output .	our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention .	0	4	3	-1.541699	1.7367859	1
299-10-45	we call this architecture a pointer net ( ptr-net ) .	our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention .	0	5	3	-1.1630543	1.380732	1
299-10-45	we show ptr-nets can be used to learn approximate solutions to three challenging geometric problems - finding planar convex hulls , computing delaunay triangulations , and the planar travelling salesman problem - using training examples alone .	our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention .	0	6	3	3.148479	-2.8884716	0
299-10-45	ptr-nets not only improve over sequence-to-sequence with input attention , but also allow us to generalize to variable size output dictionaries .	our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention .	0	7	3	3.4049954	-3.16264	0
299-10-45	our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention .	we show that the learnt models generalize beyond the maximum lengths they were trained on .	1	3	8	-2.2098653	2.2440786	1
299-10-45	our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention .	we hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems .	1	3	9	-5.855717	5.02893	1
299-10-45	we call this architecture a pointer net ( ptr-net ) .	it differs from the previous attention attempts in that , instead of using attention to blend hidden units of an encoder to a context vector at each decoder step , it uses attention as a pointer to select a member of the input sequence as the output .	0	5	4	3.013187	-2.850079	0
299-10-45	[CLS] it differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the	[CLS] we show ptr - nets can be used to learn approximate solutions to three challenging geometric problems - finding planar convex hulls, computing delaunay triangulations, and the planar travelling salesman problem - using training examples alone. [SEP]	1	4	6	-2.5693111	2.6671708	1
299-10-45	ptr-nets not only improve over sequence-to-sequence with input attention , but also allow us to generalize to variable size output dictionaries .	it differs from the previous attention attempts in that , instead of using attention to blend hidden units of an encoder to a context vector at each decoder step , it uses attention as a pointer to select a member of the input sequence as the output .	0	7	4	4.3356304	-3.971302	0
299-10-45	it differs from the previous attention attempts in that , instead of using attention to blend hidden units of an encoder to a context vector at each decoder step , it uses attention as a pointer to select a member of the input sequence as the output .	we show that the learnt models generalize beyond the maximum lengths they were trained on .	1	4	8	-4.9965143	4.617347	1
299-10-45	it differs from the previous attention attempts in that , instead of using attention to blend hidden units of an encoder to a context vector at each decoder step , it uses attention as a pointer to select a member of the input sequence as the output .	we hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems .	1	4	9	-5.875637	5.0289636	1
299-10-45	we call this architecture a pointer net ( ptr-net ) .	we show ptr-nets can be used to learn approximate solutions to three challenging geometric problems - finding planar convex hulls , computing delaunay triangulations , and the planar travelling salesman problem - using training examples alone .	1	5	6	-5.9850807	5.115036	1
299-10-45	we call this architecture a pointer net ( ptr-net ) .	ptr-nets not only improve over sequence-to-sequence with input attention , but also allow us to generalize to variable size output dictionaries .	1	5	7	-6.0009065	5.1228456	1
299-10-45	we show that the learnt models generalize beyond the maximum lengths they were trained on .	we call this architecture a pointer net ( ptr-net ) .	0	8	5	4.0417733	-3.6911325	0
299-10-45	we call this architecture a pointer net ( ptr-net ) .	we hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems .	1	5	9	-5.7013354	4.800558	1
299-10-45	ptr-nets not only improve over sequence-to-sequence with input attention , but also allow us to generalize to variable size output dictionaries .	we show ptr-nets can be used to learn approximate solutions to three challenging geometric problems - finding planar convex hulls , computing delaunay triangulations , and the planar travelling salesman problem - using training examples alone .	0	7	6	1.0788584	-0.65097266	0
299-10-45	we show that the learnt models generalize beyond the maximum lengths they were trained on .	we show ptr-nets can be used to learn approximate solutions to three challenging geometric problems - finding planar convex hulls , computing delaunay triangulations , and the planar travelling salesman problem - using training examples alone .	0	8	6	3.954637	-3.6001937	0
299-10-45	we show ptr-nets can be used to learn approximate solutions to three challenging geometric problems - finding planar convex hulls , computing delaunay triangulations , and the planar travelling salesman problem - using training examples alone .	we hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems .	1	6	9	-5.7963343	5.0619287	1
299-10-45	we show that the learnt models generalize beyond the maximum lengths they were trained on .	ptr-nets not only improve over sequence-to-sequence with input attention , but also allow us to generalize to variable size output dictionaries .	0	8	7	-1.9027878	2.0105224	1
299-10-45	ptr-nets not only improve over sequence-to-sequence with input attention , but also allow us to generalize to variable size output dictionaries .	we hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems .	1	7	9	-5.6541805	4.9501553	1
299-10-45	we hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems .	we show that the learnt models generalize beyond the maximum lengths they were trained on .	0	9	8	4.748521	-4.30755	0
300-8-28	an associative memory is a structure learned from a dataset m of vectors ( signals ) in a way such that , given a noisy version of one of the vectors as input , the nearest valid vector from m ( nearest neighbor ) is provided as output , preferably via a fast iterative algorithm .	traditionally , binary ( or q-ary ) hopfield neural networks are used to model the above structure .	1	0	1	-5.473298	4.9938974	1
300-8-28	in this paper , for the first time , we propose a model of associative memory based on sparse recovery of signals .	an associative memory is a structure learned from a dataset m of vectors ( signals ) in a way such that , given a noisy version of one of the vectors as input , the nearest valid vector from m ( nearest neighbor ) is provided as output , preferably via a fast iterative algorithm .	0	2	0	4.4455605	-3.957398	0
300-8-28	our basic premise is simple .	an associative memory is a structure learned from a dataset m of vectors ( signals ) in a way such that , given a noisy version of one of the vectors as input , the nearest valid vector from m ( nearest neighbor ) is provided as output , preferably via a fast iterative algorithm .	0	3	0	5.039577	-4.476512	0
300-8-28	an associative memory is a structure learned from a dataset m of vectors ( signals ) in a way such that , given a noisy version of one of the vectors as input , the nearest valid vector from m ( nearest neighbor ) is provided as output , preferably via a fast iterative algorithm .	for a dataset , we learn a set of linear constraints that every vector in the dataset must satisfy .	1	0	4	-3.392281	3.157017	1
300-8-28	an associative memory is a structure learned from a dataset m of vectors ( signals ) in a way such that , given a noisy version of one of the vectors as input , the nearest valid vector from m ( nearest neighbor ) is provided as output , preferably via a fast iterative algorithm .	provided these linear constraints possess some special properties , it is possible to cast the task of finding nearest neighbor as a sparse recovery problem .	1	0	5	-6.000762	5.209115	1
300-8-28	assuming generic random models for the dataset , we show that it is possible to store super-polynomial or exponential number of n-length vectors in a neural network of size o ( n ) .	[CLS] an associative memory is a structure learned from a dataset m of vectors ( signals ) in a way such that, given a noisy version of one of the vectors as input, the nearest valid vector from m ( nearest neighbor ) is provided as output, preferably via a fast	0	6	0	3.6604936	-3.3265622	0
300-8-28	an associative memory is a structure learned from a dataset m of vectors ( signals ) in a way such that , given a noisy version of one of the vectors as input , the nearest valid vector from m ( nearest neighbor ) is provided as output , preferably via a fast iterative algorithm .	furthermore , given a noisy version of one of the stored vectors corrupted in near-linear number of coordinates , the vector can be correctly recalled using a neurally feasible algorithm .	1	0	7	-5.9801607	5.1767607	1
300-8-28	traditionally , binary ( or q-ary ) hopfield neural networks are used to model the above structure .	in this paper , for the first time , we propose a model of associative memory based on sparse recovery of signals .	1	1	2	-5.845499	4.984198	1
300-8-28	traditionally , binary ( or q-ary ) hopfield neural networks are used to model the above structure .	our basic premise is simple .	1	1	3	-5.857954	4.9309926	1
300-8-28	traditionally , binary ( or q-ary ) hopfield neural networks are used to model the above structure .	for a dataset , we learn a set of linear constraints that every vector in the dataset must satisfy .	1	1	4	-3.9475884	3.843655	1
300-8-28	traditionally , binary ( or q-ary ) hopfield neural networks are used to model the above structure .	provided these linear constraints possess some special properties , it is possible to cast the task of finding nearest neighbor as a sparse recovery problem .	1	1	5	-6.0196986	5.1485815	1
300-8-28	traditionally , binary ( or q-ary ) hopfield neural networks are used to model the above structure .	assuming generic random models for the dataset , we show that it is possible to store super-polynomial or exponential number of n-length vectors in a neural network of size o ( n ) .	1	1	6	-4.4439874	4.217747	1
300-8-28	furthermore , given a noisy version of one of the stored vectors corrupted in near-linear number of coordinates , the vector can be correctly recalled using a neurally feasible algorithm .	traditionally , binary ( or q-ary ) hopfield neural networks are used to model the above structure .	0	7	1	5.561819	-4.9254684	0
300-8-28	in this paper , for the first time , we propose a model of associative memory based on sparse recovery of signals .	our basic premise is simple .	1	2	3	-5.865834	5.1139736	1
300-8-28	for a dataset , we learn a set of linear constraints that every vector in the dataset must satisfy .	in this paper , for the first time , we propose a model of associative memory based on sparse recovery of signals .	0	4	2	5.05141	-4.5048904	0
300-8-28	in this paper , for the first time , we propose a model of associative memory based on sparse recovery of signals .	provided these linear constraints possess some special properties , it is possible to cast the task of finding nearest neighbor as a sparse recovery problem .	1	2	5	-4.633705	4.3284883	1
300-8-28	assuming generic random models for the dataset , we show that it is possible to store super-polynomial or exponential number of n-length vectors in a neural network of size o ( n ) .	in this paper , for the first time , we propose a model of associative memory based on sparse recovery of signals .	0	6	2	5.3089743	-4.722464	0
300-8-28	in this paper , for the first time , we propose a model of associative memory based on sparse recovery of signals .	furthermore , given a noisy version of one of the stored vectors corrupted in near-linear number of coordinates , the vector can be correctly recalled using a neurally feasible algorithm .	1	2	7	-5.522383	5.0232916	1
300-8-28	for a dataset , we learn a set of linear constraints that every vector in the dataset must satisfy .	our basic premise is simple .	0	4	3	-4.6117363	4.283904	1
300-8-28	provided these linear constraints possess some special properties , it is possible to cast the task of finding nearest neighbor as a sparse recovery problem .	our basic premise is simple .	0	5	3	-1.9499513	1.9999186	1
300-8-28	our basic premise is simple .	assuming generic random models for the dataset , we show that it is possible to store super-polynomial or exponential number of n-length vectors in a neural network of size o ( n ) .	1	3	6	3.7132158	-3.4092288	0
300-8-28	our basic premise is simple .	furthermore , given a noisy version of one of the stored vectors corrupted in near-linear number of coordinates , the vector can be correctly recalled using a neurally feasible algorithm .	1	3	7	-4.823166	4.4929557	1
300-8-28	for a dataset , we learn a set of linear constraints that every vector in the dataset must satisfy .	provided these linear constraints possess some special properties , it is possible to cast the task of finding nearest neighbor as a sparse recovery problem .	1	4	5	-5.9241805	5.127493	1
300-8-28	assuming generic random models for the dataset , we show that it is possible to store super-polynomial or exponential number of n-length vectors in a neural network of size o ( n ) .	for a dataset , we learn a set of linear constraints that every vector in the dataset must satisfy .	0	6	4	4.8493156	-4.2403708	0
300-8-28	for a dataset , we learn a set of linear constraints that every vector in the dataset must satisfy .	furthermore , given a noisy version of one of the stored vectors corrupted in near-linear number of coordinates , the vector can be correctly recalled using a neurally feasible algorithm .	1	4	7	-5.950794	5.191959	1
300-8-28	provided these linear constraints possess some special properties , it is possible to cast the task of finding nearest neighbor as a sparse recovery problem .	assuming generic random models for the dataset , we show that it is possible to store super-polynomial or exponential number of n-length vectors in a neural network of size o ( n ) .	1	5	6	3.6439295	-3.3366838	0
300-8-28	provided these linear constraints possess some special properties , it is possible to cast the task of finding nearest neighbor as a sparse recovery problem .	furthermore , given a noisy version of one of the stored vectors corrupted in near-linear number of coordinates , the vector can be correctly recalled using a neurally feasible algorithm .	1	5	7	-5.3198333	4.906148	1
300-8-28	furthermore , given a noisy version of one of the stored vectors corrupted in near-linear number of coordinates , the vector can be correctly recalled using a neurally feasible algorithm .	assuming generic random models for the dataset , we show that it is possible to store super-polynomial or exponential number of n-length vectors in a neural network of size o ( n ) .	0	7	6	5.4499965	-4.8355646	0
301-5-10	but for real data these algorithms require additional ad-hoc heuristics , and even then often produce unusable results .	spectral inference provides fast algorithms and provable optimality for latent topic analysis .	0	1	0	5.093952	-4.453924	0
301-5-10	spectral inference provides fast algorithms and provable optimality for latent topic analysis .	we explain this poor performance by casting the problem of topic inference in the framework of joint stochastic matrix factorization ( jsmf ) and showing that previous methods violate the theoretical conditions necessary for a good solution to exist .	1	0	2	-4.8979454	4.5520773	1
301-5-10	we then propose a novel rectification method that learns high quality topics and their interactions even on small , noisy data .	spectral inference provides fast algorithms and provable optimality for latent topic analysis .	0	3	0	3.2746966	-3.0132198	0
301-5-10	spectral inference provides fast algorithms and provable optimality for latent topic analysis .	this method achieves results comparable to probabilistic techniques in several domains while maintaining scalability and provable optimality .	1	0	4	-5.822198	4.9488564	1
301-5-10	but for real data these algorithms require additional ad-hoc heuristics , and even then often produce unusable results .	we explain this poor performance by casting the problem of topic inference in the framework of joint stochastic matrix factorization ( jsmf ) and showing that previous methods violate the theoretical conditions necessary for a good solution to exist .	1	1	2	-4.3178744	4.1285143	1
301-5-10	but for real data these algorithms require additional ad-hoc heuristics , and even then often produce unusable results .	we then propose a novel rectification method that learns high quality topics and their interactions even on small , noisy data .	1	1	3	-5.897537	5.125049	1
301-5-10	this method achieves results comparable to probabilistic techniques in several domains while maintaining scalability and provable optimality .	but for real data these algorithms require additional ad-hoc heuristics , and even then often produce unusable results .	0	4	1	5.08028	-4.5490894	0
301-5-10	we then propose a novel rectification method that learns high quality topics and their interactions even on small , noisy data .	we explain this poor performance by casting the problem of topic inference in the framework of joint stochastic matrix factorization ( jsmf ) and showing that previous methods violate the theoretical conditions necessary for a good solution to exist .	0	3	2	5.252451	-4.655425	0
301-5-10	this method achieves results comparable to probabilistic techniques in several domains while maintaining scalability and provable optimality .	we explain this poor performance by casting the problem of topic inference in the framework of joint stochastic matrix factorization ( jsmf ) and showing that previous methods violate the theoretical conditions necessary for a good solution to exist .	0	4	2	5.3596735	-4.84573	0
301-5-10	we then propose a novel rectification method that learns high quality topics and their interactions even on small , noisy data .	this method achieves results comparable to probabilistic techniques in several domains while maintaining scalability and provable optimality .	1	3	4	-5.4255514	4.9040775	1
302-3-3	given a directed acyclic graph g , and a set of values y on the vertices , the isotonic regression of y is a vector x that respects the partial order described by g , and minimizes x - y , for a specified norm .	this paper gives improved algorithms for computing the isotonic regression for all weighted p -norms with rigorous performance guarantees .	1	0	1	-5.9308195	5.1253204	1
302-3-3	given a directed acyclic graph g , and a set of values y on the vertices , the isotonic regression of y is a vector x that respects the partial order described by g , and minimizes x - y , for a specified norm .	our algorithms are quite practical , and variants of them can be implemented to run fast in practice .	1	0	2	-5.9864426	5.0738587	1
302-3-3	our algorithms are quite practical , and variants of them can be implemented to run fast in practice .	this paper gives improved algorithms for computing the isotonic regression for all weighted p -norms with rigorous performance guarantees .	0	2	1	5.4872684	-4.861745	0
303-6-15	convex approximations are typically optimized in their place to avoid np-hard empirical risk minimization problems .	multivariate loss functions are used to assess performance in many modern prediction tasks , including information retrieval and ranking applications .	0	1	0	5.6338434	-5.0692286	0
303-6-15	multivariate loss functions are used to assess performance in many modern prediction tasks , including information retrieval and ranking applications .	we propose to approximate the training data instead of the loss function by posing multivariate prediction as an adversarial game between a loss-minimizing prediction player and a loss-maximizing evaluation player constrained to match specified properties of training data .	1	0	2	-5.8754177	5.2841706	1
303-6-15	this avoids the non-convexity of empirical risk minimization , but game sizes are exponential in the number of predicted variables .	multivariate loss functions are used to assess performance in many modern prediction tasks , including information retrieval and ranking applications .	0	3	0	5.629691	-5.067648	0
303-6-15	we overcome this intractability using the double oracle constraint generation method .	multivariate loss functions are used to assess performance in many modern prediction tasks , including information retrieval and ranking applications .	0	4	0	5.627726	-5.0611315	0
303-6-15	we demonstrate the efficiency and predictive performance of our approach on tasks evaluated using the precision at k , the f-score and the discounted cumulative gain .	multivariate loss functions are used to assess performance in many modern prediction tasks , including information retrieval and ranking applications .	0	5	0	5.6649084	-5.1038427	0
303-6-15	we propose to approximate the training data instead of the loss function by posing multivariate prediction as an adversarial game between a loss-minimizing prediction player and a loss-maximizing evaluation player constrained to match specified properties of training data .	convex approximations are typically optimized in their place to avoid np-hard empirical risk minimization problems .	0	2	1	-1.9096197	2.07301	1
303-6-15	this avoids the non-convexity of empirical risk minimization , but game sizes are exponential in the number of predicted variables .	convex approximations are typically optimized in their place to avoid np-hard empirical risk minimization problems .	0	3	1	4.0089364	-3.67828	0
303-6-15	we overcome this intractability using the double oracle constraint generation method .	convex approximations are typically optimized in their place to avoid np-hard empirical risk minimization problems .	0	4	1	3.7315512	-3.4334378	0
303-6-15	we demonstrate the efficiency and predictive performance of our approach on tasks evaluated using the precision at k , the f-score and the discounted cumulative gain .	convex approximations are typically optimized in their place to avoid np-hard empirical risk minimization problems .	0	5	1	5.228599	-4.667675	0
303-6-15	we propose to approximate the training data instead of the loss function by posing multivariate prediction as an adversarial game between a loss-minimizing prediction player and a loss-maximizing evaluation player constrained to match specified properties of training data .	this avoids the non-convexity of empirical risk minimization , but game sizes are exponential in the number of predicted variables .	1	2	3	-5.6375875	5.12817	1
303-6-15	we propose to approximate the training data instead of the loss function by posing multivariate prediction as an adversarial game between a loss-minimizing prediction player and a loss-maximizing evaluation player constrained to match specified properties of training data .	we overcome this intractability using the double oracle constraint generation method .	1	2	4	-4.3679676	4.1081905	1
303-6-15	we demonstrate the efficiency and predictive performance of our approach on tasks evaluated using the precision at k , the f-score and the discounted cumulative gain .	we propose to approximate the training data instead of the loss function by posing multivariate prediction as an adversarial game between a loss-minimizing prediction player and a loss-maximizing evaluation player constrained to match specified properties of training data .	0	5	2	5.551374	-4.8713894	0
303-6-15	this avoids the non-convexity of empirical risk minimization , but game sizes are exponential in the number of predicted variables .	we overcome this intractability using the double oracle constraint generation method .	1	3	4	1.4004138	-1.1532457	0
303-6-15	we demonstrate the efficiency and predictive performance of our approach on tasks evaluated using the precision at k , the f-score and the discounted cumulative gain .	this avoids the non-convexity of empirical risk minimization , but game sizes are exponential in the number of predicted variables .	0	5	3	4.651244	-4.269469	0
303-6-15	we demonstrate the efficiency and predictive performance of our approach on tasks evaluated using the precision at k , the f-score and the discounted cumulative gain .	we overcome this intractability using the double oracle constraint generation method .	0	5	4	5.2424664	-4.6506577	0
304-5-10	asynchronous parallel implementations of stochastic gradient ( sg ) have been broadly used in solving deep neural network and received many successes in practice recently .	however , existing theories can not explain their convergence and speedup properties , mainly due to the nonconvexity of most deep learning formulations and the asynchronous parallel mechanism .	1	0	1	-5.720977	5.1615663	1
304-5-10	asynchronous parallel implementations of stochastic gradient ( sg ) have been broadly used in solving deep neural network and received many successes in practice recently .	to fill the gaps in theory and provide theoretical supports , this paper studies two asynchronous parallel implementations of sg : one is over a computer network and the other is on a shared memory system .	1	0	2	-5.959424	5.151131	1
304-5-10	asynchronous parallel implementations of stochastic gradient ( sg ) have been broadly used in solving deep neural network and received many successes in practice recently .	we establish an ergodic convergence rate o ( 1/ k ) for both algorithms and prove that the linear speedup is achievable if the number of workers is bounded by k ( k is the total number of iterations ) .	1	0	3	-5.9680204	5.1822276	1
304-5-10	our results generalize and improve existing analysis for convex minimization .	asynchronous parallel implementations of stochastic gradient ( sg ) have been broadly used in solving deep neural network and received many successes in practice recently .	0	4	0	5.5880632	-4.9735956	0
304-5-10	however , existing theories can not explain their convergence and speedup properties , mainly due to the nonconvexity of most deep learning formulations and the asynchronous parallel mechanism .	to fill the gaps in theory and provide theoretical supports , this paper studies two asynchronous parallel implementations of sg : one is over a computer network and the other is on a shared memory system .	1	1	2	-5.410152	5.078024	1
304-5-10	however , existing theories can not explain their convergence and speedup properties , mainly due to the nonconvexity of most deep learning formulations and the asynchronous parallel mechanism .	we establish an ergodic convergence rate o ( 1/ k ) for both algorithms and prove that the linear speedup is achievable if the number of workers is bounded by k ( k is the total number of iterations ) .	1	1	3	-5.6733475	5.152515	1
304-5-10	our results generalize and improve existing analysis for convex minimization .	however , existing theories can not explain their convergence and speedup properties , mainly due to the nonconvexity of most deep learning formulations and the asynchronous parallel mechanism .	0	4	1	5.517185	-4.900939	0
304-5-10	to fill the gaps in theory and provide theoretical supports , this paper studies two asynchronous parallel implementations of sg : one is over a computer network and the other is on a shared memory system .	we establish an ergodic convergence rate o ( 1/ k ) for both algorithms and prove that the linear speedup is achievable if the number of workers is bounded by k ( k is the total number of iterations ) .	1	2	3	-5.708897	5.187171	1
304-5-10	to fill the gaps in theory and provide theoretical supports , this paper studies two asynchronous parallel implementations of sg : one is over a computer network and the other is on a shared memory system .	our results generalize and improve existing analysis for convex minimization .	1	2	4	-6.0005865	5.1513624	1
304-5-10	we establish an ergodic convergence rate o ( 1/ k ) for both algorithms and prove that the linear speedup is achievable if the number of workers is bounded by k ( k is the total number of iterations ) .	our results generalize and improve existing analysis for convex minimization .	1	3	4	-5.4412518	4.9454308	1
305-3-3	we introduce embed to control ( e2c ) , a method for model learning and control of non-linear dynamical systems from raw pixel images .	e2c consists of a deep generative model , belonging to the family of variational autoencoders , that learns to generate image trajectories from a latent space in which the dynamics is constrained to be locally linear .	1	0	1	-5.7995853	5.206159	1
305-3-3	our model is derived directly from an optimal control formulation in latent space , supports long-term prediction of image sequences and exhibits strong performance on a variety of complex control problems .	we introduce embed to control ( e2c ) , a method for model learning and control of non-linear dynamical systems from raw pixel images .	0	2	0	5.3040867	-4.675602	0
305-3-3	our model is derived directly from an optimal control formulation in latent space , supports long-term prediction of image sequences and exhibits strong performance on a variety of complex control problems .	e2c consists of a deep generative model , belonging to the family of variational autoencoders , that learns to generate image trajectories from a latent space in which the dynamics is constrained to be locally linear .	0	2	1	4.3523817	-3.9425771	0
306-5-10	2 ) it is efficiently implementable with an erm oracle .	we develop a new active learning algorithm for the streaming setting satisfying three important properties : 1 ) it provably works for any classifier representation and classification problem including those with severe noise .	0	1	0	3.909512	-3.5810218	0
306-5-10	we develop a new active learning algorithm for the streaming setting satisfying three important properties : 1 ) it provably works for any classifier representation and classification problem including those with severe noise .	3 ) it is more aggressive than all previous approaches satisfying 1 and 2 .	1	0	2	-5.777102	5.2057247	1
306-5-10	we develop a new active learning algorithm for the streaming setting satisfying three important properties : 1 ) it provably works for any classifier representation and classification problem including those with severe noise .	to do this , we create an algorithm based on a newly defined optimization problem and analyze it .	1	0	3	-2.700388	2.7438102	1
306-5-10	we also conduct the first experimental analysis of all efficient agnostic active learning algorithms , evaluating their strengths and weaknesses in different settings .	we develop a new active learning algorithm for the streaming setting satisfying three important properties : 1 ) it provably works for any classifier representation and classification problem including those with severe noise .	0	4	0	4.6240845	-4.1308217	0
306-5-10	2 ) it is efficiently implementable with an erm oracle .	3 ) it is more aggressive than all previous approaches satisfying 1 and 2 .	1	1	2	-2.685144	2.6865284	1
306-5-10	2 ) it is efficiently implementable with an erm oracle .	to do this , we create an algorithm based on a newly defined optimization problem and analyze it .	1	1	3	3.2109368	-2.9670386	0
306-5-10	2 ) it is efficiently implementable with an erm oracle .	we also conduct the first experimental analysis of all efficient agnostic active learning algorithms , evaluating their strengths and weaknesses in different settings .	1	1	4	-3.4351048	3.3005767	1
306-5-10	to do this , we create an algorithm based on a newly defined optimization problem and analyze it .	3 ) it is more aggressive than all previous approaches satisfying 1 and 2 .	0	3	2	-3.785479	3.6043353	1
306-5-10	we also conduct the first experimental analysis of all efficient agnostic active learning algorithms , evaluating their strengths and weaknesses in different settings .	3 ) it is more aggressive than all previous approaches satisfying 1 and 2 .	0	4	2	1.1695101	-0.989095	0
306-5-10	to do this , we create an algorithm based on a newly defined optimization problem and analyze it .	we also conduct the first experimental analysis of all efficient agnostic active learning algorithms , evaluating their strengths and weaknesses in different settings .	1	3	4	-5.442011	4.931988	1
307-5-10	this higher-level abstraction improves generalization in different prediction settings , but computing predictions often becomes intractable in large decision spaces .	recent machine learning methods for sequential behavior prediction estimate the motives of behavior rather than the behavior itself .	0	1	0	5.6235123	-4.985772	0
307-5-10	recent machine learning methods for sequential behavior prediction estimate the motives of behavior rather than the behavior itself .	we propose the softstar algorithm , a softened heuristic-guided search technique for the maximum entropy inverse optimal control model of sequential behavior .	1	0	2	-5.9853363	5.165876	1
307-5-10	recent machine learning methods for sequential behavior prediction estimate the motives of behavior rather than the behavior itself .	this approach supports probabilistic search with bounded approximation error at a significantly reduced computational cost when compared to sampling based methods .	1	0	3	-6.0205216	5.1460314	1
307-5-10	we present the algorithm , analyze approximation guarantees , and compare performance with simulation-based inference on two distinct complex decision tasks .	recent machine learning methods for sequential behavior prediction estimate the motives of behavior rather than the behavior itself .	0	4	0	5.7354536	-5.1276064	0
307-5-10	we propose the softstar algorithm , a softened heuristic-guided search technique for the maximum entropy inverse optimal control model of sequential behavior .	this higher-level abstraction improves generalization in different prediction settings , but computing predictions often becomes intractable in large decision spaces .	0	2	1	-3.8534791	3.6421924	1
307-5-10	this approach supports probabilistic search with bounded approximation error at a significantly reduced computational cost when compared to sampling based methods .	this higher-level abstraction improves generalization in different prediction settings , but computing predictions often becomes intractable in large decision spaces .	0	3	1	-1.3136833	1.5014781	1
307-5-10	this higher-level abstraction improves generalization in different prediction settings , but computing predictions often becomes intractable in large decision spaces .	we present the algorithm , analyze approximation guarantees , and compare performance with simulation-based inference on two distinct complex decision tasks .	1	1	4	-4.095584	4.0040026	1
307-5-10	we propose the softstar algorithm , a softened heuristic-guided search technique for the maximum entropy inverse optimal control model of sequential behavior .	this approach supports probabilistic search with bounded approximation error at a significantly reduced computational cost when compared to sampling based methods .	1	2	3	-5.649399	5.1177044	1
307-5-10	we present the algorithm , analyze approximation guarantees , and compare performance with simulation-based inference on two distinct complex decision tasks .	we propose the softstar algorithm , a softened heuristic-guided search technique for the maximum entropy inverse optimal control model of sequential behavior .	0	4	2	5.429275	-4.7538595	0
307-5-10	we present the algorithm , analyze approximation guarantees , and compare performance with simulation-based inference on two distinct complex decision tasks .	this approach supports probabilistic search with bounded approximation error at a significantly reduced computational cost when compared to sampling based methods .	0	4	3	3.7505794	-3.4856813	0
308-5-10	syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades .	as a result , the most accurate parsers are domain specific , complex , and inefficient .	1	0	1	-5.9087048	5.183404	1
308-5-10	in this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset , when trained on a large synthetic corpus that was annotated using existing parsers .	syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades .	0	2	0	5.683566	-5.0772634	0
308-5-10	it also matches the performance of standard parsers when trained only on a small human-annotated dataset , which shows that this model is highly data-efficient , in contrast to sequence-to-sequence models without the attention mechanism .	syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades .	0	3	0	5.5942936	-5.014083	0
308-5-10	our parser is also fast , processing over a hundred sentences per second with an unoptimized cpu implementation .	syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades .	0	4	0	5.602745	-5.0539856	0
308-5-10	in this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset , when trained on a large synthetic corpus that was annotated using existing parsers .	as a result , the most accurate parsers are domain specific , complex , and inefficient .	0	2	1	5.09324	-4.4659367	0
308-5-10	as a result , the most accurate parsers are domain specific , complex , and inefficient .	it also matches the performance of standard parsers when trained only on a small human-annotated dataset , which shows that this model is highly data-efficient , in contrast to sequence-to-sequence models without the attention mechanism .	1	1	3	-5.756965	5.1744003	1
308-5-10	our parser is also fast , processing over a hundred sentences per second with an unoptimized cpu implementation .	as a result , the most accurate parsers are domain specific , complex , and inefficient .	0	4	1	5.346505	-4.7589645	0
308-5-10	[CLS] it also matches the performance of standard parsers when trained only on a small human - annotated dataset, which shows that this model is highly data - efficient, in contrast to sequence - to - sequence models without the attention mechanism.	[CLS] in this paper we show that the domain agnostic attention - enhanced sequence - to - sequence model achieves state - of - the - art results on the most widely used syntactic constituency parsing dataset, when trained on a large synthetic	0	3	2	3.5315824	-3.2350407	0
308-5-10	in this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset , when trained on a large synthetic corpus that was annotated using existing parsers .	our parser is also fast , processing over a hundred sentences per second with an unoptimized cpu implementation .	1	2	4	-4.2341585	3.896504	1
308-5-10	it also matches the performance of standard parsers when trained only on a small human-annotated dataset , which shows that this model is highly data-efficient , in contrast to sequence-to-sequence models without the attention mechanism .	our parser is also fast , processing over a hundred sentences per second with an unoptimized cpu implementation .	1	3	4	1.9680101	-1.8329682	0
309-5-10	trace regression models have received considerable attention in the context of matrix completion , quantum state tomography , and compressed sensing .	estimation of the underlying matrix from regularization-based approaches promoting low-rankedness , notably nuclear norm regularization , have enjoyed great popularity .	1	0	1	-5.333623	4.942839	1
309-5-10	in this paper , we argue that such regularization may no longer be necessary if the underlying matrix is symmetric positive semidefinite ( spd ) and the design satisfies certain conditions .	trace regression models have received considerable attention in the context of matrix completion , quantum state tomography , and compressed sensing .	0	2	0	5.608489	-5.010046	0
309-5-10	in this situation , simple least squares estimation subject to an spd constraint may perform as well as regularization-based approaches with a proper choice of regularization parameter , which entails knowledge of the noise level and/or tuning .	trace regression models have received considerable attention in the context of matrix completion , quantum state tomography , and compressed sensing .	0	3	0	5.566065	-4.9318247	0
309-5-10	trace regression models have received considerable attention in the context of matrix completion , quantum state tomography , and compressed sensing .	by contrast , constrained least squares estimation comes without any tuning parameter and may hence be preferred due to its simplicity .	1	0	4	-5.9451375	5.2488437	1
309-5-10	in this paper , we argue that such regularization may no longer be necessary if the underlying matrix is symmetric positive semidefinite ( spd ) and the design satisfies certain conditions .	estimation of the underlying matrix from regularization-based approaches promoting low-rankedness , notably nuclear norm regularization , have enjoyed great popularity .	0	2	1	5.5462723	-4.9313855	0
309-5-10	estimation of the underlying matrix from regularization-based approaches promoting low-rankedness , notably nuclear norm regularization , have enjoyed great popularity .	in this situation , simple least squares estimation subject to an spd constraint may perform as well as regularization-based approaches with a proper choice of regularization parameter , which entails knowledge of the noise level and/or tuning .	1	1	3	-5.9848866	5.086197	1
309-5-10	by contrast , constrained least squares estimation comes without any tuning parameter and may hence be preferred due to its simplicity .	estimation of the underlying matrix from regularization-based approaches promoting low-rankedness , notably nuclear norm regularization , have enjoyed great popularity .	0	4	1	5.581709	-4.942718	0
309-5-10	in this paper , we argue that such regularization may no longer be necessary if the underlying matrix is symmetric positive semidefinite ( spd ) and the design satisfies certain conditions .	in this situation , simple least squares estimation subject to an spd constraint may perform as well as regularization-based approaches with a proper choice of regularization parameter , which entails knowledge of the noise level and/or tuning .	1	2	3	-3.9035993	3.8384285	1
309-5-10	by contrast , constrained least squares estimation comes without any tuning parameter and may hence be preferred due to its simplicity .	in this paper , we argue that such regularization may no longer be necessary if the underlying matrix is symmetric positive semidefinite ( spd ) and the design satisfies certain conditions .	0	4	2	-2.259842	2.3774185	1
309-5-10	by contrast , constrained least squares estimation comes without any tuning parameter and may hence be preferred due to its simplicity .	in this situation , simple least squares estimation subject to an spd constraint may perform as well as regularization-based approaches with a proper choice of regularization parameter , which entails knowledge of the noise level and/or tuning .	0	4	3	0.784456	-0.4652886	0
310-5-10	we first introduce fully-connected winner-take-all autoencoders which use mini-batch statistics to directly enforce a lifetime sparsity in the activations of the hidden units .	in this paper , we propose a winner-take-all method for learning hierarchical sparse representations in an unsupervised fashion .	0	1	0	5.4897346	-4.842928	0
310-5-10	in this paper , we propose a winner-take-all method for learning hierarchical sparse representations in an unsupervised fashion .	we then propose the convolutional winner-take-all autoencoder which combines the benefits of convolutional architectures and autoencoders for learning shift-invariant sparse representations .	1	0	2	-5.9748807	5.1991243	1
310-5-10	in this paper , we propose a winner-take-all method for learning hierarchical sparse representations in an unsupervised fashion .	we describe a way to train convolutional autoencoders layer by layer , where in addition to lifetime sparsity , a spatial sparsity within each feature map is achieved using winner-take-all activation functions .	1	0	3	-5.872122	5.214296	1
310-5-10	in this paper , we propose a winner-take-all method for learning hierarchical sparse representations in an unsupervised fashion .	we will show that winner-take-all autoencoders can be used to to learn deep sparse representations from the mnist , cifar-10 , imagenet , street view house numbers and toronto face datasets , and achieve competitive classification performance .	1	0	4	-5.917838	5.218976	1
310-5-10	we first introduce fully-connected winner-take-all autoencoders which use mini-batch statistics to directly enforce a lifetime sparsity in the activations of the hidden units .	we then propose the convolutional winner-take-all autoencoder which combines the benefits of convolutional architectures and autoencoders for learning shift-invariant sparse representations .	1	1	2	-5.502079	5.037344	1
310-5-10	we first introduce fully-connected winner-take-all autoencoders which use mini-batch statistics to directly enforce a lifetime sparsity in the activations of the hidden units .	we describe a way to train convolutional autoencoders layer by layer , where in addition to lifetime sparsity , a spatial sparsity within each feature map is achieved using winner-take-all activation functions .	1	1	3	-3.3700953	3.281756	1
310-5-10	we first introduce fully-connected winner-take-all autoencoders which use mini-batch statistics to directly enforce a lifetime sparsity in the activations of the hidden units .	we will show that winner-take-all autoencoders can be used to to learn deep sparse representations from the mnist , cifar-10 , imagenet , street view house numbers and toronto face datasets , and achieve competitive classification performance .	1	1	4	-5.9331355	5.1719737	1
310-5-10	we describe a way to train convolutional autoencoders layer by layer , where in addition to lifetime sparsity , a spatial sparsity within each feature map is achieved using winner-take-all activation functions .	we then propose the convolutional winner-take-all autoencoder which combines the benefits of convolutional architectures and autoencoders for learning shift-invariant sparse representations .	0	3	2	-2.1523764	2.3093646	1
310-5-10	we will show that winner-take-all autoencoders can be used to to learn deep sparse representations from the mnist , cifar-10 , imagenet , street view house numbers and toronto face datasets , and achieve competitive classification performance .	we then propose the convolutional winner-take-all autoencoder which combines the benefits of convolutional architectures and autoencoders for learning shift-invariant sparse representations .	0	4	2	4.8367615	-4.2959394	0
310-5-10	[CLS] we will show that winner - take - all autoencoders can be used to to learn deep sparse representations from the mnist, cifar - 10, imagenet, street view house numbers and toronto face datasets, and achieve competitive	[CLS] we describe a way to train convolutional autoencoders layer by layer, where in addition to lifetime sparsity, a spatial sparsity within each feature map is achieved using winner - take - all activation functions. [SEP]	0	4	3	3.3306222	-2.9815388	0
311-6-15	the model is composed of a poisson distribution to model observed vectors of counts , as well as a deep hierarchy of hidden binary units .	we propose a new deep architecture for topic modeling , based on poisson factor analysis ( pfa ) modules .	0	1	0	4.197015	-3.7442546	0
311-6-15	we propose a new deep architecture for topic modeling , based on poisson factor analysis ( pfa ) modules .	rather than using logistic functions to characterize the probability that a latent binary unit is on , we employ a bernoulli-poisson link , which allows pfa modules to be used repeatedly in the deep architecture .	1	0	2	-5.857527	5.20584	1
311-6-15	we also describe an approach to build discriminative topic models , by adapting pfa modules .	we propose a new deep architecture for topic modeling , based on poisson factor analysis ( pfa ) modules .	0	3	0	5.5176177	-4.825329	0
311-6-15	we propose a new deep architecture for topic modeling , based on poisson factor analysis ( pfa ) modules .	we derive efficient inference via mcmc and stochastic variational methods , that scale with the number of non-zeros in the data and binary units , yielding significant efficiency , relative to models based on logistic links .	1	0	4	-5.814918	5.2060966	1
311-6-15	we propose a new deep architecture for topic modeling , based on poisson factor analysis ( pfa ) modules .	experiments on several corpora demonstrate the advantages of our model when compared to related deep models .	1	0	5	-5.9620466	5.2121177	1
311-6-15	the model is composed of a poisson distribution to model observed vectors of counts , as well as a deep hierarchy of hidden binary units .	rather than using logistic functions to characterize the probability that a latent binary unit is on , we employ a bernoulli-poisson link , which allows pfa modules to be used repeatedly in the deep architecture .	1	1	2	-5.48309	4.999097	1
311-6-15	we also describe an approach to build discriminative topic models , by adapting pfa modules .	the model is composed of a poisson distribution to model observed vectors of counts , as well as a deep hierarchy of hidden binary units .	0	3	1	4.5357213	-4.1035624	0
311-6-15	we derive efficient inference via mcmc and stochastic variational methods , that scale with the number of non-zeros in the data and binary units , yielding significant efficiency , relative to models based on logistic links .	the model is composed of a poisson distribution to model observed vectors of counts , as well as a deep hierarchy of hidden binary units .	0	4	1	5.284976	-4.6618395	0
311-6-15	the model is composed of a poisson distribution to model observed vectors of counts , as well as a deep hierarchy of hidden binary units .	experiments on several corpora demonstrate the advantages of our model when compared to related deep models .	1	1	5	-5.9942136	5.126073	1
311-6-15	rather than using logistic functions to characterize the probability that a latent binary unit is on , we employ a bernoulli-poisson link , which allows pfa modules to be used repeatedly in the deep architecture .	we also describe an approach to build discriminative topic models , by adapting pfa modules .	1	2	3	-5.0144825	4.62054	1
311-6-15	we derive efficient inference via mcmc and stochastic variational methods , that scale with the number of non-zeros in the data and binary units , yielding significant efficiency , relative to models based on logistic links .	rather than using logistic functions to characterize the probability that a latent binary unit is on , we employ a bernoulli-poisson link , which allows pfa modules to be used repeatedly in the deep architecture .	0	4	2	4.6538815	-4.1323366	0
311-6-15	experiments on several corpora demonstrate the advantages of our model when compared to related deep models .	rather than using logistic functions to characterize the probability that a latent binary unit is on , we employ a bernoulli-poisson link , which allows pfa modules to be used repeatedly in the deep architecture .	0	5	2	5.3669376	-4.7026434	0
311-6-15	we derive efficient inference via mcmc and stochastic variational methods , that scale with the number of non-zeros in the data and binary units , yielding significant efficiency , relative to models based on logistic links .	we also describe an approach to build discriminative topic models , by adapting pfa modules .	0	4	3	-1.6767441	1.871599	1
311-6-15	experiments on several corpora demonstrate the advantages of our model when compared to related deep models .	we also describe an approach to build discriminative topic models , by adapting pfa modules .	0	5	3	4.1097455	-3.7493284	0
311-6-15	experiments on several corpora demonstrate the advantages of our model when compared to related deep models .	we derive efficient inference via mcmc and stochastic variational methods , that scale with the number of non-zeros in the data and binary units , yielding significant efficiency , relative to models based on logistic links .	0	5	4	5.125153	-4.5531983	0
312-4-6	most bayesian optimization methods require auxiliary optimization : an additional non-convex global optimization problem , which can be time-consuming and hard to implement in practice .	this paper presents a bayesian optimization method with exponential convergence without the need of auxiliary optimization and without the -cover sampling .	0	1	0	-5.801094	5.2360144	1
312-4-6	this paper presents a bayesian optimization method with exponential convergence without the need of auxiliary optimization and without the -cover sampling .	also , the existing bayesian optimization method with exponential convergence requires access to the -cover sampling , which was considered to be impractical .	1	0	2	3.229553	-2.9683743	0
312-4-6	this paper presents a bayesian optimization method with exponential convergence without the need of auxiliary optimization and without the -cover sampling .	our approach eliminates both requirements and achieves an exponential convergence rate .	1	0	3	-5.8994756	5.231695	1
312-4-6	most bayesian optimization methods require auxiliary optimization : an additional non-convex global optimization problem , which can be time-consuming and hard to implement in practice .	also , the existing bayesian optimization method with exponential convergence requires access to the -cover sampling , which was considered to be impractical .	1	1	2	-5.6380644	5.1364994	1
312-4-6	our approach eliminates both requirements and achieves an exponential convergence rate .	most bayesian optimization methods require auxiliary optimization : an additional non-convex global optimization problem , which can be time-consuming and hard to implement in practice .	0	3	1	5.579131	-4.9667826	0
312-4-6	our approach eliminates both requirements and achieves an exponential convergence rate .	also , the existing bayesian optimization method with exponential convergence requires access to the -cover sampling , which was considered to be impractical .	0	3	2	4.8627005	-4.3548627	0
313-7-21	however , in many real-world applications , an interactive learning agent operates for a fixed or bounded period of time , for example tutoring students for exams or handling customer service requests .	recently , there has been significant progress in understanding reinforcement learning in discounted infinite-horizon markov decision processes ( mdps ) by deriving tight sample complexity bounds .	0	1	0	3.3992915	-2.765152	0
313-7-21	recently , there has been significant progress in understanding reinforcement learning in discounted infinite-horizon markov decision processes ( mdps ) by deriving tight sample complexity bounds .	such scenarios can often be better treated as episodic fixed-horizon mdps , for which only looser bounds on the sample complexity exist .	1	0	2	-5.8989773	5.19589	1
313-7-21	recently , there has been significant progress in understanding reinforcement learning in discounted infinite-horizon markov decision processes ( mdps ) by deriving tight sample complexity bounds .	a natural notion of sample complexity in this setting is the number of episodes required to guarantee a certain performance with high probability ( pac guarantee ) .	1	0	3	-5.7505827	5.207611	1
313-7-21	recently , there has been significant progress in understanding reinforcement learning in discounted infinite-horizon markov decision processes ( mdps ) by deriving tight sample complexity bounds .	[CLS] in this paper, we derive an 2 2 2 1 | s | | a | h | s | | a | h upper pac bound o ( ln 1 ) and a lower pac bound ( ln + c ) 2 2 that match up to log - terms and an additional linear dependency on the number of states	1	0	4	-5.910208	5.171424	1
313-7-21	recently , there has been significant progress in understanding reinforcement learning in discounted infinite-horizon markov decision processes ( mdps ) by deriving tight sample complexity bounds .	the lower bound is the first of its kind for this setting .	1	0	5	-5.928721	5.17281	1
313-7-21	our upper bound leverages bernstein 's inequality to improve on previous bounds for episodic finitehorizon mdps which have a time-horizon dependency of at least h 3 .	recently , there has been significant progress in understanding reinforcement learning in discounted infinite-horizon markov decision processes ( mdps ) by deriving tight sample complexity bounds .	0	6	0	5.7301493	-5.094844	0
313-7-21	however , in many real-world applications , an interactive learning agent operates for a fixed or bounded period of time , for example tutoring students for exams or handling customer service requests .	such scenarios can often be better treated as episodic fixed-horizon mdps , for which only looser bounds on the sample complexity exist .	1	1	2	-5.9632874	5.163398	1
313-7-21	however , in many real-world applications , an interactive learning agent operates for a fixed or bounded period of time , for example tutoring students for exams or handling customer service requests .	a natural notion of sample complexity in this setting is the number of episodes required to guarantee a certain performance with high probability ( pac guarantee ) .	1	1	3	-5.8532667	5.2424555	1
313-7-21	however , in many real-world applications , an interactive learning agent operates for a fixed or bounded period of time , for example tutoring students for exams or handling customer service requests .	[CLS] in this paper, we derive an 2 2 2 1 | s | | a | h | s | | a | h upper pac bound o ( ln 1 ) and a lower pac bound ( ln + c ) 2 2 that match up to log - terms and an additional linear dependency on the	1	1	4	-5.956908	5.1875134	1
313-7-21	however , in many real-world applications , an interactive learning agent operates for a fixed or bounded period of time , for example tutoring students for exams or handling customer service requests .	the lower bound is the first of its kind for this setting .	1	1	5	-5.9926877	5.164791	1
313-7-21	however , in many real-world applications , an interactive learning agent operates for a fixed or bounded period of time , for example tutoring students for exams or handling customer service requests .	our upper bound leverages bernstein 's inequality to improve on previous bounds for episodic finitehorizon mdps which have a time-horizon dependency of at least h 3 .	1	1	6	-5.934696	5.2223597	1
313-7-21	such scenarios can often be better treated as episodic fixed-horizon mdps , for which only looser bounds on the sample complexity exist .	a natural notion of sample complexity in this setting is the number of episodes required to guarantee a certain performance with high probability ( pac guarantee ) .	1	2	3	1.1261452	-0.76099104	0
313-7-21	in this paper , we derive an 2 2 2 1 |s| |a|h |s||a|h upper pac bound o ( ln 1 ) and a lower pac bound ( ln +c ) 2 2 that match up to log-terms and an additional linear dependency on the number of states |s| .	such scenarios can often be better treated as episodic fixed-horizon mdps , for which only looser bounds on the sample complexity exist .	0	4	2	4.828128	-4.3117294	0
313-7-21	the lower bound is the first of its kind for this setting .	such scenarios can often be better treated as episodic fixed-horizon mdps , for which only looser bounds on the sample complexity exist .	0	5	2	4.617897	-4.1592226	0
313-7-21	such scenarios can often be better treated as episodic fixed-horizon mdps , for which only looser bounds on the sample complexity exist .	our upper bound leverages bernstein 's inequality to improve on previous bounds for episodic finitehorizon mdps which have a time-horizon dependency of at least h 3 .	1	2	6	-5.9165053	5.2061577	1
313-7-21	a natural notion of sample complexity in this setting is the number of episodes required to guarantee a certain performance with high probability ( pac guarantee ) .	in this paper , we derive an 2 2 2 1 |s| |a|h |s||a|h upper pac bound o ( ln 1 ) and a lower pac bound ( ln +c ) 2 2 that match up to log-terms and an additional linear dependency on the number of states |s| .	1	3	4	-5.9408855	5.195028	1
313-7-21	the lower bound is the first of its kind for this setting .	a natural notion of sample complexity in this setting is the number of episodes required to guarantee a certain performance with high probability ( pac guarantee ) .	0	5	3	3.762485	-3.4468057	0
313-7-21	our upper bound leverages bernstein 's inequality to improve on previous bounds for episodic finitehorizon mdps which have a time-horizon dependency of at least h 3 .	a natural notion of sample complexity in this setting is the number of episodes required to guarantee a certain performance with high probability ( pac guarantee ) .	0	6	3	4.5351257	-4.024308	0
313-7-21	the lower bound is the first of its kind for this setting .	in this paper , we derive an 2 2 2 1 |s| |a|h |s||a|h upper pac bound o ( ln 1 ) and a lower pac bound ( ln +c ) 2 2 that match up to log-terms and an additional linear dependency on the number of states |s| .	0	5	4	4.511396	-4.0334024	0
313-7-21	[CLS] in this paper, we derive an 2 2 2 1 | s | | a | h | s | | a | h upper pac bound o ( ln 1 ) and a lower pac bound ( ln + c ) 2 2 that match up to log - terms and an additional linear dependency on	our upper bound leverages bernstein 's inequality to improve on previous bounds for episodic finitehorizon mdps which have a time-horizon dependency of at least h 3 .	1	4	6	-4.4088764	4.0831776	1
313-7-21	our upper bound leverages bernstein 's inequality to improve on previous bounds for episodic finitehorizon mdps which have a time-horizon dependency of at least h 3 .	the lower bound is the first of its kind for this setting .	0	6	5	-3.6839294	3.5258496	1
314-6-15	even finding a single latent variable setting that satisfies the constraints could be difficult ; for instance , the observed output may be the result of a latent database query or graphics program which must be inferred .	for weakly-supervised problems with deterministic constraints between the latent variables and observed output , learning necessitates performing inference over latent variables conditioned on the output , which can be intractable no matter how simple the model family is .	0	1	0	4.2169156	-3.7313337	0
314-6-15	for weakly-supervised problems with deterministic constraints between the latent variables and observed output , learning necessitates performing inference over latent variables conditioned on the output , which can be intractable no matter how simple the model family is .	here , the difficulty lies in not the model but the supervision , and poor approximations at this stage could lead to following the wrong learning signal entirely .	1	0	2	-4.3378077	4.1026425	1
314-6-15	in this paper , we develop a rigorous approach to relaxing the supervision , which yields asymptotically consistent parameter estimates despite altering the supervision .	for weakly-supervised problems with deterministic constraints between the latent variables and observed output , learning necessitates performing inference over latent variables conditioned on the output , which can be intractable no matter how simple the model family is .	0	3	0	4.9096503	-4.369938	0
314-6-15	for weakly-supervised problems with deterministic constraints between the latent variables and observed output , learning necessitates performing inference over latent variables conditioned on the output , which can be intractable no matter how simple the model family is .	our approach parameterizes a family of increasingly accurate relaxations , and jointly optimizes both the model and relaxation parameters , while formulating constraints between these parameters to ensure efficient inference .	1	0	4	-4.201106	4.062913	1
314-6-15	these efficiency constraints allow us to learn in otherwise intractable settings , while asymptotic consistency ensures that we always follow a valid learning signal .	for weakly-supervised problems with deterministic constraints between the latent variables and observed output , learning necessitates performing inference over latent variables conditioned on the output , which can be intractable no matter how simple the model family is .	0	5	0	4.885262	-4.3072743	0
314-6-15	here , the difficulty lies in not the model but the supervision , and poor approximations at this stage could lead to following the wrong learning signal entirely .	even finding a single latent variable setting that satisfies the constraints could be difficult ; for instance , the observed output may be the result of a latent database query or graphics program which must be inferred .	0	2	1	3.2969265	-3.0947871	0
314-6-15	in this paper , we develop a rigorous approach to relaxing the supervision , which yields asymptotically consistent parameter estimates despite altering the supervision .	even finding a single latent variable setting that satisfies the constraints could be difficult ; for instance , the observed output may be the result of a latent database query or graphics program which must be inferred .	0	3	1	4.7792544	-4.2345257	0
314-6-15	even finding a single latent variable setting that satisfies the constraints could be difficult ; for instance , the observed output may be the result of a latent database query or graphics program which must be inferred .	our approach parameterizes a family of increasingly accurate relaxations , and jointly optimizes both the model and relaxation parameters , while formulating constraints between these parameters to ensure efficient inference .	1	1	4	-4.5044594	4.3148956	1
314-6-15	these efficiency constraints allow us to learn in otherwise intractable settings , while asymptotic consistency ensures that we always follow a valid learning signal .	even finding a single latent variable setting that satisfies the constraints could be difficult ; for instance , the observed output may be the result of a latent database query or graphics program which must be inferred .	0	5	1	3.0157666	-2.8242784	0
314-6-15	in this paper , we develop a rigorous approach to relaxing the supervision , which yields asymptotically consistent parameter estimates despite altering the supervision .	here , the difficulty lies in not the model but the supervision , and poor approximations at this stage could lead to following the wrong learning signal entirely .	0	3	2	5.1067247	-4.49205	0
314-6-15	here , the difficulty lies in not the model but the supervision , and poor approximations at this stage could lead to following the wrong learning signal entirely .	our approach parameterizes a family of increasingly accurate relaxations , and jointly optimizes both the model and relaxation parameters , while formulating constraints between these parameters to ensure efficient inference .	1	2	4	-5.438255	5.0140314	1
314-6-15	here , the difficulty lies in not the model but the supervision , and poor approximations at this stage could lead to following the wrong learning signal entirely .	these efficiency constraints allow us to learn in otherwise intractable settings , while asymptotic consistency ensures that we always follow a valid learning signal .	1	2	5	-5.2752	4.8079863	1
314-6-15	in this paper , we develop a rigorous approach to relaxing the supervision , which yields asymptotically consistent parameter estimates despite altering the supervision .	our approach parameterizes a family of increasingly accurate relaxations , and jointly optimizes both the model and relaxation parameters , while formulating constraints between these parameters to ensure efficient inference .	1	3	4	-5.2526865	4.8781967	1
314-6-15	in this paper , we develop a rigorous approach to relaxing the supervision , which yields asymptotically consistent parameter estimates despite altering the supervision .	these efficiency constraints allow us to learn in otherwise intractable settings , while asymptotic consistency ensures that we always follow a valid learning signal .	1	3	5	-4.083349	3.858596	1
314-6-15	these efficiency constraints allow us to learn in otherwise intractable settings , while asymptotic consistency ensures that we always follow a valid learning signal .	our approach parameterizes a family of increasingly accurate relaxations , and jointly optimizes both the model and relaxation parameters , while formulating constraints between these parameters to ensure efficient inference .	0	5	4	4.846249	-4.282644	0
315-6-15	our algorithm matches up to a constant factor the best-known bounds for the number of edges ( or constraints ) needed for perfect recovery and its running time is linear in the number of edges used .	we present an algorithm for recovering planted solutions in two well-known models , the stochastic block model and planted constraint satisfaction problems ( csp ) , via a common generalization in terms of random bipartite graphs .	0	1	0	5.472382	-4.807395	0
315-6-15	the time complexity is significantly better than both spectral and sdp-based approaches .	we present an algorithm for recovering planted solutions in two well-known models , the stochastic block model and planted constraint satisfaction problems ( csp ) , via a common generalization in terms of random bipartite graphs .	0	2	0	5.387657	-4.753327	0
315-6-15	the main contribution of the algorithm is in the case of unequal sizes in the bipartition that arises in our reduction from the planted csp .	we present an algorithm for recovering planted solutions in two well-known models , the stochastic block model and planted constraint satisfaction problems ( csp ) , via a common generalization in terms of random bipartite graphs .	0	3	0	5.616189	-4.990566	0
315-6-15	here our algorithm succeeds at a significantly lower density than the spectral approaches , surpassing a barrier based on the spectral norm of a random matrix .	we present an algorithm for recovering planted solutions in two well-known models , the stochastic block model and planted constraint satisfaction problems ( csp ) , via a common generalization in terms of random bipartite graphs .	0	4	0	5.400565	-4.7917614	0
315-6-15	we present an algorithm for recovering planted solutions in two well-known models , the stochastic block model and planted constraint satisfaction problems ( csp ) , via a common generalization in terms of random bipartite graphs .	[CLS] other significant features of the algorithm and analysis include ( i ) the critical use of power iteration with subsampling, which might be of independent interest ; its analysis requires keeping track of multiple norms of an evolving solution ( ii ) the algorithm can be implemented statistically	1	0	5	-5.941062	5.1585793	1
315-6-15	the time complexity is significantly better than both spectral and sdp-based approaches .	our algorithm matches up to a constant factor the best-known bounds for the number of edges ( or constraints ) needed for perfect recovery and its running time is linear in the number of edges used .	0	2	1	1.5440891	-1.3491621	0
315-6-15	the main contribution of the algorithm is in the case of unequal sizes in the bipartition that arises in our reduction from the planted csp .	our algorithm matches up to a constant factor the best-known bounds for the number of edges ( or constraints ) needed for perfect recovery and its running time is linear in the number of edges used .	0	3	1	-1.0057563	1.2380763	1
315-6-15	here our algorithm succeeds at a significantly lower density than the spectral approaches , surpassing a barrier based on the spectral norm of a random matrix .	our algorithm matches up to a constant factor the best-known bounds for the number of edges ( or constraints ) needed for perfect recovery and its running time is linear in the number of edges used .	0	4	1	-2.5342503	2.5767765	1
315-6-15	[CLS] other significant features of the algorithm and analysis include ( i ) the critical use of power iteration with subsampling, which might be of independent interest ; its analysis requires keeping track of multiple norms of an evolving solution ( ii ) the algorithm can be implemented statistically, i. e.,	our algorithm matches up to a constant factor the best-known bounds for the number of edges ( or constraints ) needed for perfect recovery and its running time is linear in the number of edges used .	0	5	1	-2.3605738	2.445614	1
315-6-15	the time complexity is significantly better than both spectral and sdp-based approaches .	the main contribution of the algorithm is in the case of unequal sizes in the bipartition that arises in our reduction from the planted csp .	1	2	3	1.822642	-1.7183038	0
315-6-15	the time complexity is significantly better than both spectral and sdp-based approaches .	here our algorithm succeeds at a significantly lower density than the spectral approaches , surpassing a barrier based on the spectral norm of a random matrix .	1	2	4	-0.8807365	1.1295159	1
315-6-15	the time complexity is significantly better than both spectral and sdp-based approaches .	[CLS] other significant features of the algorithm and analysis include ( i ) the critical use of power iteration with subsampling, which might be of independent interest ; its analysis requires keeping track of multiple norms of an evolving solution ( ii ) the algorithm can be implemented statistically, i. e., with very limited access to the input distribution ( iii ) the algorithm is extremely simple to implement and runs in linear time	1	2	5	0.36654472	-0.09914379	0
315-6-15	here our algorithm succeeds at a significantly lower density than the spectral approaches , surpassing a barrier based on the spectral norm of a random matrix .	the main contribution of the algorithm is in the case of unequal sizes in the bipartition that arises in our reduction from the planted csp .	0	4	3	-2.720449	2.6884165	1
315-6-15	[CLS] other significant features of the algorithm and analysis include ( i ) the critical use of power iteration with subsampling, which might be of independent interest ; its analysis requires keeping track of multiple norms of an evolving solution ( ii ) the algorithm can be implemented statistically, i. e., with very limited access to the input distribution	the main contribution of the algorithm is in the case of unequal sizes in the bipartition that arises in our reduction from the planted csp .	0	5	3	-3.6097353	3.4430175	1
315-6-15	[CLS] other significant features of the algorithm and analysis include ( i ) the critical use of power iteration with subsampling, which might be of independent interest ; its analysis requires keeping track of multiple norms of an evolving solution ( ii ) the algorithm can be implemented statistically, i. e., with very limited access to the input distribution ( iii ) the	here our algorithm succeeds at a significantly lower density than the spectral approaches , surpassing a barrier based on the spectral norm of a random matrix .	0	5	4	-2.7698085	2.7567205	1
316-4-6	we study accelerated mirror descent dynamics in continuous and discrete time .	combining the original continuous-time motivation of mirror descent with a recent ode interpretation of nesterov 's accelerated method , we propose a family of continuous-time descent dynamics for convex functions with lipschitz gradients , such that the solution trajectories converge to the optimum at a o ( 1/t2 ) rate .	1	0	1	-5.9314556	5.11492	1
316-4-6	we then show that a large family of first-order accelerated methods can be obtained as a discretization of the ode , and these methods converge at a o ( 1/k 2 ) rate .	we study accelerated mirror descent dynamics in continuous and discrete time .	0	2	0	5.5312033	-5.0166597	0
316-4-6	this connection between accelerated mirror descent and the ode provides an intuitive approach to the design and analysis of accelerated first-order algorithms .	we study accelerated mirror descent dynamics in continuous and discrete time .	0	3	0	2.965767	-2.648992	0
316-4-6	we then show that a large family of first-order accelerated methods can be obtained as a discretization of the ode , and these methods converge at a o ( 1/k 2 ) rate .	[CLS] combining the original continuous - time motivation of mirror descent with a recent ode interpretation of nesterov's accelerated method, we propose a family of continuous - time descent dynamics for convex functions with lipschitz gradients, such that the solution trajectories converge to the optimum	0	2	1	4.9228506	-4.3680716	0
316-4-6	combining the original continuous-time motivation of mirror descent with a recent ode interpretation of nesterov 's accelerated method , we propose a family of continuous-time descent dynamics for convex functions with lipschitz gradients , such that the solution trajectories converge to the optimum at a o ( 1/t2 ) rate .	this connection between accelerated mirror descent and the ode provides an intuitive approach to the design and analysis of accelerated first-order algorithms .	1	1	3	-5.191758	4.7851467	1
316-4-6	we then show that a large family of first-order accelerated methods can be obtained as a discretization of the ode , and these methods converge at a o ( 1/k 2 ) rate .	this connection between accelerated mirror descent and the ode provides an intuitive approach to the design and analysis of accelerated first-order algorithms .	1	2	3	2.1321256	-1.8588929	0
317-5-10	bayesian nonparametric models , such as gaussian processes , provide a compelling framework for automatic statistical modelling : these models have a high degree of flexibility , and automatically calibrated complexity .	however , automating human expertise remains elusive ; for example , gaussian processes with standard kernels struggle on function extrapolation problems that are trivial for human learners .	1	0	1	-2.3645372	2.4774265	1
317-5-10	bayesian nonparametric models , such as gaussian processes , provide a compelling framework for automatic statistical modelling : these models have a high degree of flexibility , and automatically calibrated complexity .	in this paper , we create function extrapolation problems and acquire human responses , and then design a kernel learning framework to reverse engineer the inductive biases of human learners across a set of behavioral experiments .	1	0	2	-5.431409	5.0393977	1
317-5-10	bayesian nonparametric models , such as gaussian processes , provide a compelling framework for automatic statistical modelling : these models have a high degree of flexibility , and automatically calibrated complexity .	we use the learned kernels to gain psychological insights and to extrapolate in humanlike ways that go beyond traditional stationary and polynomial kernels .	1	0	3	-6.0064836	5.188217	1
317-5-10	bayesian nonparametric models , such as gaussian processes , provide a compelling framework for automatic statistical modelling : these models have a high degree of flexibility , and automatically calibrated complexity .	finally , we investigate occam 's razor in human and gaussian process based function learning .	1	0	4	-5.9213734	5.018978	1
317-5-10	however , automating human expertise remains elusive ; for example , gaussian processes with standard kernels struggle on function extrapolation problems that are trivial for human learners .	in this paper , we create function extrapolation problems and acquire human responses , and then design a kernel learning framework to reverse engineer the inductive biases of human learners across a set of behavioral experiments .	1	1	2	-5.8758907	5.249016	1
317-5-10	we use the learned kernels to gain psychological insights and to extrapolate in humanlike ways that go beyond traditional stationary and polynomial kernels .	however , automating human expertise remains elusive ; for example , gaussian processes with standard kernels struggle on function extrapolation problems that are trivial for human learners .	0	3	1	5.513114	-4.895719	0
317-5-10	finally , we investigate occam 's razor in human and gaussian process based function learning .	however , automating human expertise remains elusive ; for example , gaussian processes with standard kernels struggle on function extrapolation problems that are trivial for human learners .	0	4	1	5.5135536	-4.933729	0
317-5-10	we use the learned kernels to gain psychological insights and to extrapolate in humanlike ways that go beyond traditional stationary and polynomial kernels .	in this paper , we create function extrapolation problems and acquire human responses , and then design a kernel learning framework to reverse engineer the inductive biases of human learners across a set of behavioral experiments .	0	3	2	5.387582	-4.7472277	0
317-5-10	finally , we investigate occam 's razor in human and gaussian process based function learning .	in this paper , we create function extrapolation problems and acquire human responses , and then design a kernel learning framework to reverse engineer the inductive biases of human learners across a set of behavioral experiments .	0	4	2	5.18231	-4.5261517	0
317-5-10	we use the learned kernels to gain psychological insights and to extrapolate in humanlike ways that go beyond traditional stationary and polynomial kernels .	finally , we investigate occam 's razor in human and gaussian process based function learning .	1	3	4	-5.113532	4.6596913	1
318-5-10	[CLS] motivated by vision - based reinforcement learning ( rl ) problems, in particular atari games from the recent benchmark aracade learning environment ( ale ), we consider spatio - temporal prediction problems where future image - frames depend on control variables or actions	[CLS] while not composed of natural scenes, frames in atari games are high - dimensional in size, can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly, can involve entry and departure of objects	1	0	1	2.8851705	-2.4230733	0
318-5-10	motivated by vision-based reinforcement learning ( rl ) problems , in particular atari games from the recent benchmark aracade learning environment ( ale ) , we consider spatio-temporal prediction problems where future image-frames depend on control variables or actions as well as previous frames .	we propose and evaluate two deep neural network architectures that consist of encoding , actionconditional transformation , and decoding layers based on convolutional neural networks and recurrent neural networks .	1	0	2	-5.9589477	5.097222	1
318-5-10	experimental results show that the proposed architectures are able to generate visually-realistic frames that are also useful for control over approximately 100-step action-conditional futures in some games .	motivated by vision-based reinforcement learning ( rl ) problems , in particular atari games from the recent benchmark aracade learning environment ( ale ) , we consider spatio-temporal prediction problems where future image-frames depend on control variables or actions as well as previous frames .	0	3	0	5.5482807	-4.9863663	0
318-5-10	motivated by vision-based reinforcement learning ( rl ) problems , in particular atari games from the recent benchmark aracade learning environment ( ale ) , we consider spatio-temporal prediction problems where future image-frames depend on control variables or actions as well as previous frames .	to the best of our knowledge , this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs .	1	0	4	-5.9413323	5.0529494	1
318-5-10	we propose and evaluate two deep neural network architectures that consist of encoding , actionconditional transformation , and decoding layers based on convolutional neural networks and recurrent neural networks .	[CLS] while not composed of natural scenes, frames in atari games are high - dimensional in size, can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly, can involve entry and departure of objects, and can involve deep partial observa	0	2	1	4.285534	-3.8656988	0
318-5-10	while not composed of natural scenes , frames in atari games are high-dimensional in size , can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly , can involve entry and departure of objects , and can involve deep partial observability .	experimental results show that the proposed architectures are able to generate visually-realistic frames that are also useful for control over approximately 100-step action-conditional futures in some games .	1	1	3	-5.960444	5.023674	1
318-5-10	to the best of our knowledge , this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs .	while not composed of natural scenes , frames in atari games are high-dimensional in size , can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly , can involve entry and departure of objects , and can involve deep partial observability .	0	4	1	5.535831	-4.8719788	0
318-5-10	we propose and evaluate two deep neural network architectures that consist of encoding , actionconditional transformation , and decoding layers based on convolutional neural networks and recurrent neural networks .	experimental results show that the proposed architectures are able to generate visually-realistic frames that are also useful for control over approximately 100-step action-conditional futures in some games .	1	2	3	-6.0014997	5.1463375	1
318-5-10	we propose and evaluate two deep neural network architectures that consist of encoding , actionconditional transformation , and decoding layers based on convolutional neural networks and recurrent neural networks .	to the best of our knowledge , this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs .	1	2	4	-5.878483	5.122044	1
318-5-10	experimental results show that the proposed architectures are able to generate visually-realistic frames that are also useful for control over approximately 100-step action-conditional futures in some games .	to the best of our knowledge , this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs .	1	3	4	3.79223	-3.4993582	0
319-11-55	independent component analysis ( ica ) is a popular model for blind signal separation .	the ica model assumes that a number of independent source signals are linearly mixed to form the observed signals .	1	0	1	-5.8309083	5.158353	1
319-11-55	we propose a new algorithm , pegi ( for pseudo-euclidean gradient iteration ) , for provable model recovery for ica with gaussian noise .	independent component analysis ( ica ) is a popular model for blind signal separation .	0	2	0	5.6393876	-5.026636	0
319-11-55	the main technical innovation of the algorithm is to use a fixed point iteration in a pseudo-euclidean ( indefinite `` inner product '' ) space .	independent component analysis ( ica ) is a popular model for blind signal separation .	0	3	0	5.586457	-5.052224	0
319-11-55	the use of this indefinite `` inner product '' resolves technical issues common to several existing algorithms for noisy ica .	independent component analysis ( ica ) is a popular model for blind signal separation .	0	4	0	5.605984	-5.0319877	0
319-11-55	this leads to an algorithm which is conceptually simple , efficient and accurate in testing .	independent component analysis ( ica ) is a popular model for blind signal separation .	0	5	0	5.618108	-5.0544705	0
319-11-55	independent component analysis ( ica ) is a popular model for blind signal separation .	our second contribution is combining pegi with the analysis of objectives for optimal recovery in the noisy ica model .	1	0	6	-5.834148	5.1332355	1
319-11-55	it has been observed that the direct approach of demixing with the inverse of the mixing matrix is suboptimal for signal recovery in terms of the natural signal to interference plus noise ratio ( sinr ) criterion .	independent component analysis ( ica ) is a popular model for blind signal separation .	0	7	0	5.67109	-5.069007	0
319-11-55	independent component analysis ( ica ) is a popular model for blind signal separation .	there have been several partial solutions proposed in the ica literature .	1	0	8	-5.865423	5.0885086	1
319-11-55	independent component analysis ( ica ) is a popular model for blind signal separation .	it turns out that any solution to the mixing matrix reconstruction problem can be used to construct an sinr-optimal ica demixing , despite the fact that sinr itself can not be computed from data .	1	0	9	-5.8187475	5.081409	1
319-11-55	that allows us to obtain a practical and provably sinr-optimal recovery method for ica with arbitrary gaussian noise .	independent component analysis ( ica ) is a popular model for blind signal separation .	0	10	0	5.5779576	-5.0041237	0
319-11-55	we propose a new algorithm , pegi ( for pseudo-euclidean gradient iteration ) , for provable model recovery for ica with gaussian noise .	the ica model assumes that a number of independent source signals are linearly mixed to form the observed signals .	0	2	1	4.4013977	-3.9437985	0
319-11-55	the main technical innovation of the algorithm is to use a fixed point iteration in a pseudo-euclidean ( indefinite `` inner product '' ) space .	the ica model assumes that a number of independent source signals are linearly mixed to form the observed signals .	0	3	1	4.7813797	-4.2240677	0
319-11-55	the ica model assumes that a number of independent source signals are linearly mixed to form the observed signals .	the use of this indefinite `` inner product '' resolves technical issues common to several existing algorithms for noisy ica .	1	1	4	-5.977818	5.0369177	1
319-11-55	this leads to an algorithm which is conceptually simple , efficient and accurate in testing .	the ica model assumes that a number of independent source signals are linearly mixed to form the observed signals .	0	5	1	5.2604866	-4.6040287	0
319-11-55	the ica model assumes that a number of independent source signals are linearly mixed to form the observed signals .	our second contribution is combining pegi with the analysis of objectives for optimal recovery in the noisy ica model .	1	1	6	-5.8996506	5.1616936	1
319-11-55	the ica model assumes that a number of independent source signals are linearly mixed to form the observed signals .	it has been observed that the direct approach of demixing with the inverse of the mixing matrix is suboptimal for signal recovery in terms of the natural signal to interference plus noise ratio ( sinr ) criterion .	1	1	7	-4.9858327	4.5817075	1
319-11-55	there have been several partial solutions proposed in the ica literature .	the ica model assumes that a number of independent source signals are linearly mixed to form the observed signals .	0	8	1	0.83866405	-0.6700908	0
319-11-55	the ica model assumes that a number of independent source signals are linearly mixed to form the observed signals .	it turns out that any solution to the mixing matrix reconstruction problem can be used to construct an sinr-optimal ica demixing , despite the fact that sinr itself can not be computed from data .	1	1	9	-5.9306273	4.9450703	1
319-11-55	that allows us to obtain a practical and provably sinr-optimal recovery method for ica with arbitrary gaussian noise .	the ica model assumes that a number of independent source signals are linearly mixed to form the observed signals .	0	10	1	5.405162	-4.738057	0
319-11-55	we propose a new algorithm , pegi ( for pseudo-euclidean gradient iteration ) , for provable model recovery for ica with gaussian noise .	the main technical innovation of the algorithm is to use a fixed point iteration in a pseudo-euclidean ( indefinite `` inner product '' ) space .	1	2	3	-5.6017118	5.096339	1
319-11-55	the use of this indefinite `` inner product '' resolves technical issues common to several existing algorithms for noisy ica .	we propose a new algorithm , pegi ( for pseudo-euclidean gradient iteration ) , for provable model recovery for ica with gaussian noise .	0	4	2	-2.1106598	2.2128124	1
319-11-55	we propose a new algorithm , pegi ( for pseudo-euclidean gradient iteration ) , for provable model recovery for ica with gaussian noise .	this leads to an algorithm which is conceptually simple , efficient and accurate in testing .	1	2	5	-5.361786	4.932384	1
319-11-55	we propose a new algorithm , pegi ( for pseudo-euclidean gradient iteration ) , for provable model recovery for ica with gaussian noise .	our second contribution is combining pegi with the analysis of objectives for optimal recovery in the noisy ica model .	1	2	6	-5.9377384	5.2107983	1
319-11-55	it has been observed that the direct approach of demixing with the inverse of the mixing matrix is suboptimal for signal recovery in terms of the natural signal to interference plus noise ratio ( sinr ) criterion .	we propose a new algorithm , pegi ( for pseudo-euclidean gradient iteration ) , for provable model recovery for ica with gaussian noise .	0	7	2	-4.733659	4.463686	1
319-11-55	there have been several partial solutions proposed in the ica literature .	we propose a new algorithm , pegi ( for pseudo-euclidean gradient iteration ) , for provable model recovery for ica with gaussian noise .	0	8	2	-5.7076545	5.233146	1
319-11-55	we propose a new algorithm , pegi ( for pseudo-euclidean gradient iteration ) , for provable model recovery for ica with gaussian noise .	it turns out that any solution to the mixing matrix reconstruction problem can be used to construct an sinr-optimal ica demixing , despite the fact that sinr itself can not be computed from data .	1	2	9	-5.11835	4.738741	1
319-11-55	that allows us to obtain a practical and provably sinr-optimal recovery method for ica with arbitrary gaussian noise .	we propose a new algorithm , pegi ( for pseudo-euclidean gradient iteration ) , for provable model recovery for ica with gaussian noise .	0	10	2	4.5697346	-4.0374217	0
319-11-55	the use of this indefinite `` inner product '' resolves technical issues common to several existing algorithms for noisy ica .	the main technical innovation of the algorithm is to use a fixed point iteration in a pseudo-euclidean ( indefinite `` inner product '' ) space .	0	4	3	4.720668	-4.221469	0
319-11-55	the main technical innovation of the algorithm is to use a fixed point iteration in a pseudo-euclidean ( indefinite `` inner product '' ) space .	this leads to an algorithm which is conceptually simple , efficient and accurate in testing .	1	3	5	-1.2150145	1.4701823	1
319-11-55	our second contribution is combining pegi with the analysis of objectives for optimal recovery in the noisy ica model .	the main technical innovation of the algorithm is to use a fixed point iteration in a pseudo-euclidean ( indefinite `` inner product '' ) space .	0	6	3	1.1002108	-0.9901459	0
319-11-55	it has been observed that the direct approach of demixing with the inverse of the mixing matrix is suboptimal for signal recovery in terms of the natural signal to interference plus noise ratio ( sinr ) criterion .	the main technical innovation of the algorithm is to use a fixed point iteration in a pseudo-euclidean ( indefinite `` inner product '' ) space .	0	7	3	-5.4031515	4.951435	1
319-11-55	there have been several partial solutions proposed in the ica literature .	the main technical innovation of the algorithm is to use a fixed point iteration in a pseudo-euclidean ( indefinite `` inner product '' ) space .	0	8	3	-5.7369227	5.1569266	1
319-11-55	it turns out that any solution to the mixing matrix reconstruction problem can be used to construct an sinr-optimal ica demixing , despite the fact that sinr itself can not be computed from data .	the main technical innovation of the algorithm is to use a fixed point iteration in a pseudo-euclidean ( indefinite `` inner product '' ) space .	0	9	3	1.304974	-1.1568263	0
319-11-55	that allows us to obtain a practical and provably sinr-optimal recovery method for ica with arbitrary gaussian noise .	the main technical innovation of the algorithm is to use a fixed point iteration in a pseudo-euclidean ( indefinite `` inner product '' ) space .	0	10	3	3.2747815	-3.1119804	0
319-11-55	the use of this indefinite `` inner product '' resolves technical issues common to several existing algorithms for noisy ica .	this leads to an algorithm which is conceptually simple , efficient and accurate in testing .	1	4	5	-3.7245724	3.6569583	1
319-11-55	the use of this indefinite `` inner product '' resolves technical issues common to several existing algorithms for noisy ica .	our second contribution is combining pegi with the analysis of objectives for optimal recovery in the noisy ica model .	1	4	6	-1.7577732	1.8997991	1
319-11-55	the use of this indefinite `` inner product '' resolves technical issues common to several existing algorithms for noisy ica .	it has been observed that the direct approach of demixing with the inverse of the mixing matrix is suboptimal for signal recovery in terms of the natural signal to interference plus noise ratio ( sinr ) criterion .	1	4	7	3.8634305	-3.5163639	0
319-11-55	there have been several partial solutions proposed in the ica literature .	the use of this indefinite `` inner product '' resolves technical issues common to several existing algorithms for noisy ica .	0	8	4	-5.6387777	5.0761404	1
319-11-55	it turns out that any solution to the mixing matrix reconstruction problem can be used to construct an sinr-optimal ica demixing , despite the fact that sinr itself can not be computed from data .	the use of this indefinite `` inner product '' resolves technical issues common to several existing algorithms for noisy ica .	0	9	4	1.1347888	-1.0384722	0
319-11-55	the use of this indefinite `` inner product '' resolves technical issues common to several existing algorithms for noisy ica .	that allows us to obtain a practical and provably sinr-optimal recovery method for ica with arbitrary gaussian noise .	1	4	10	-5.4454575	4.891058	1
319-11-55	this leads to an algorithm which is conceptually simple , efficient and accurate in testing .	our second contribution is combining pegi with the analysis of objectives for optimal recovery in the noisy ica model .	1	5	6	2.6646762	-2.481083	0
319-11-55	this leads to an algorithm which is conceptually simple , efficient and accurate in testing .	it has been observed that the direct approach of demixing with the inverse of the mixing matrix is suboptimal for signal recovery in terms of the natural signal to interference plus noise ratio ( sinr ) criterion .	1	5	7	4.6039925	-4.130398	0
319-11-55	this leads to an algorithm which is conceptually simple , efficient and accurate in testing .	there have been several partial solutions proposed in the ica literature .	1	5	8	5.0407567	-4.532111	0
319-11-55	it turns out that any solution to the mixing matrix reconstruction problem can be used to construct an sinr-optimal ica demixing , despite the fact that sinr itself can not be computed from data .	this leads to an algorithm which is conceptually simple , efficient and accurate in testing .	0	9	5	-1.8679299	2.0774965	1
319-11-55	this leads to an algorithm which is conceptually simple , efficient and accurate in testing .	that allows us to obtain a practical and provably sinr-optimal recovery method for ica with arbitrary gaussian noise .	1	5	10	-2.7460365	2.7653706	1
319-11-55	our second contribution is combining pegi with the analysis of objectives for optimal recovery in the noisy ica model .	it has been observed that the direct approach of demixing with the inverse of the mixing matrix is suboptimal for signal recovery in terms of the natural signal to interference plus noise ratio ( sinr ) criterion .	1	6	7	4.758165	-4.2369337	0
319-11-55	there have been several partial solutions proposed in the ica literature .	our second contribution is combining pegi with the analysis of objectives for optimal recovery in the noisy ica model .	0	8	6	-5.9544272	5.1884594	1
319-11-55	our second contribution is combining pegi with the analysis of objectives for optimal recovery in the noisy ica model .	it turns out that any solution to the mixing matrix reconstruction problem can be used to construct an sinr-optimal ica demixing , despite the fact that sinr itself can not be computed from data .	1	6	9	-3.2167802	3.1696944	1
319-11-55	our second contribution is combining pegi with the analysis of objectives for optimal recovery in the noisy ica model .	that allows us to obtain a practical and provably sinr-optimal recovery method for ica with arbitrary gaussian noise .	1	6	10	-4.388284	4.0877542	1
319-11-55	there have been several partial solutions proposed in the ica literature .	it has been observed that the direct approach of demixing with the inverse of the mixing matrix is suboptimal for signal recovery in terms of the natural signal to interference plus noise ratio ( sinr ) criterion .	0	8	7	-3.667604	3.589449	1
319-11-55	it turns out that any solution to the mixing matrix reconstruction problem can be used to construct an sinr-optimal ica demixing , despite the fact that sinr itself can not be computed from data .	it has been observed that the direct approach of demixing with the inverse of the mixing matrix is suboptimal for signal recovery in terms of the natural signal to interference plus noise ratio ( sinr ) criterion .	0	9	7	5.3506927	-4.768268	0
319-11-55	it has been observed that the direct approach of demixing with the inverse of the mixing matrix is suboptimal for signal recovery in terms of the natural signal to interference plus noise ratio ( sinr ) criterion .	that allows us to obtain a practical and provably sinr-optimal recovery method for ica with arbitrary gaussian noise .	1	7	10	-5.9569774	5.043852	1
319-11-55	there have been several partial solutions proposed in the ica literature .	it turns out that any solution to the mixing matrix reconstruction problem can be used to construct an sinr-optimal ica demixing , despite the fact that sinr itself can not be computed from data .	1	8	9	-5.8614397	5.2267914	1
319-11-55	that allows us to obtain a practical and provably sinr-optimal recovery method for ica with arbitrary gaussian noise .	there have been several partial solutions proposed in the ica literature .	0	10	8	5.32653	-4.717289	0
319-11-55	it turns out that any solution to the mixing matrix reconstruction problem can be used to construct an sinr-optimal ica demixing , despite the fact that sinr itself can not be computed from data .	that allows us to obtain a practical and provably sinr-optimal recovery method for ica with arbitrary gaussian noise .	1	9	10	-4.125416	3.9598985	1
320-10-45	i.e. , its corresponding utility , measured according to a suitable utility function , should be comparable to that of the whole dataset .	how can one find a subset , ideally as small as possible , that well represents a massive dataset ?	0	1	0	5.215424	-4.670227	0
320-10-45	in this paper , we formalize this challenge as a submodular cover problem .	how can one find a subset , ideally as small as possible , that well represents a massive dataset ?	0	2	0	5.330317	-4.6940174	0
320-10-45	here , the utility is assumed to exhibit submodularity , a natural diminishing returns condition prevalent in many data summarization applications .	how can one find a subset , ideally as small as possible , that well represents a massive dataset ?	0	3	0	5.4453154	-4.8051014	0
320-10-45	the classical greedy algorithm is known to provide solutions with logarithmic approximation guarantees compared to the optimum solution .	how can one find a subset , ideally as small as possible , that well represents a massive dataset ?	0	4	0	5.2673306	-4.6825256	0
320-10-45	how can one find a subset , ideally as small as possible , that well represents a massive dataset ?	however , this sequential , centralized approach is impractical for truly large-scale problems .	1	0	5	-5.7474127	5.188923	1
320-10-45	in this work , we develop the first distributed algorithm - d is c over - for submodular set cover that is easily implementable using mapreduce-style computations .	how can one find a subset , ideally as small as possible , that well represents a massive dataset ?	0	6	0	5.5707803	-4.9109516	0
320-10-45	how can one find a subset , ideally as small as possible , that well represents a massive dataset ?	we theoretically analyze our approach , and present approximation guarantees for the solutions returned by d is c over .	1	0	7	-5.9892597	5.192005	1
320-10-45	we also study a natural trade-off between the communication cost and the number of rounds required to obtain such a solution .	how can one find a subset , ideally as small as possible , that well represents a massive dataset ?	0	8	0	5.555328	-4.900902	0
320-10-45	in our extensive experiments , we demonstrate the effectiveness of our approach on several applications , including active set selection , exemplar based clustering , and vertex cover on tens of millions of data points using spark .	how can one find a subset , ideally as small as possible , that well represents a massive dataset ?	0	9	0	5.657666	-4.98221	0
320-10-45	i.e. , its corresponding utility , measured according to a suitable utility function , should be comparable to that of the whole dataset .	in this paper , we formalize this challenge as a submodular cover problem .	1	1	2	-3.4095511	3.3638787	1
320-10-45	here , the utility is assumed to exhibit submodularity , a natural diminishing returns condition prevalent in many data summarization applications .	i.e. , its corresponding utility , measured according to a suitable utility function , should be comparable to that of the whole dataset .	0	3	1	4.518441	-4.0878825	0
320-10-45	i.e. , its corresponding utility , measured according to a suitable utility function , should be comparable to that of the whole dataset .	the classical greedy algorithm is known to provide solutions with logarithmic approximation guarantees compared to the optimum solution .	1	1	4	-2.156991	2.2671037	1
320-10-45	i.e. , its corresponding utility , measured according to a suitable utility function , should be comparable to that of the whole dataset .	however , this sequential , centralized approach is impractical for truly large-scale problems .	1	1	5	0.42231274	-0.14445911	0
320-10-45	in this work , we develop the first distributed algorithm - d is c over - for submodular set cover that is easily implementable using mapreduce-style computations .	i.e. , its corresponding utility , measured according to a suitable utility function , should be comparable to that of the whole dataset .	0	6	1	3.8777966	-3.5381012	0
320-10-45	i.e. , its corresponding utility , measured according to a suitable utility function , should be comparable to that of the whole dataset .	we theoretically analyze our approach , and present approximation guarantees for the solutions returned by d is c over .	1	1	7	-5.844767	5.1107903	1
320-10-45	i.e. , its corresponding utility , measured according to a suitable utility function , should be comparable to that of the whole dataset .	we also study a natural trade-off between the communication cost and the number of rounds required to obtain such a solution .	1	1	8	-5.913237	5.0049195	1
320-10-45	i.e. , its corresponding utility , measured according to a suitable utility function , should be comparable to that of the whole dataset .	in our extensive experiments , we demonstrate the effectiveness of our approach on several applications , including active set selection , exemplar based clustering , and vertex cover on tens of millions of data points using spark .	1	1	9	-5.906345	4.95989	1
320-10-45	here , the utility is assumed to exhibit submodularity , a natural diminishing returns condition prevalent in many data summarization applications .	in this paper , we formalize this challenge as a submodular cover problem .	0	3	2	-2.759742	2.786515	1
320-10-45	in this paper , we formalize this challenge as a submodular cover problem .	the classical greedy algorithm is known to provide solutions with logarithmic approximation guarantees compared to the optimum solution .	1	2	4	3.1601224	-2.9109714	0
320-10-45	however , this sequential , centralized approach is impractical for truly large-scale problems .	in this paper , we formalize this challenge as a submodular cover problem .	0	5	2	-5.649243	5.194372	1
320-10-45	in this paper , we formalize this challenge as a submodular cover problem .	in this work , we develop the first distributed algorithm - d is c over - for submodular set cover that is easily implementable using mapreduce-style computations .	1	2	6	-5.8470707	5.2446456	1
320-10-45	we theoretically analyze our approach , and present approximation guarantees for the solutions returned by d is c over .	in this paper , we formalize this challenge as a submodular cover problem .	0	7	2	5.554907	-4.908273	0
320-10-45	we also study a natural trade-off between the communication cost and the number of rounds required to obtain such a solution .	in this paper , we formalize this challenge as a submodular cover problem .	0	8	2	5.3025503	-4.6392546	0
320-10-45	in our extensive experiments , we demonstrate the effectiveness of our approach on several applications , including active set selection , exemplar based clustering , and vertex cover on tens of millions of data points using spark .	in this paper , we formalize this challenge as a submodular cover problem .	0	9	2	5.629909	-4.985834	0
320-10-45	here , the utility is assumed to exhibit submodularity , a natural diminishing returns condition prevalent in many data summarization applications .	the classical greedy algorithm is known to provide solutions with logarithmic approximation guarantees compared to the optimum solution .	1	3	4	2.2539406	-2.078741	0
320-10-45	here , the utility is assumed to exhibit submodularity , a natural diminishing returns condition prevalent in many data summarization applications .	however , this sequential , centralized approach is impractical for truly large-scale problems .	1	3	5	3.570804	-3.2960567	0
320-10-45	in this work , we develop the first distributed algorithm - d is c over - for submodular set cover that is easily implementable using mapreduce-style computations .	here , the utility is assumed to exhibit submodularity , a natural diminishing returns condition prevalent in many data summarization applications .	0	6	3	3.5645628	-3.2565815	0
320-10-45	we theoretically analyze our approach , and present approximation guarantees for the solutions returned by d is c over .	here , the utility is assumed to exhibit submodularity , a natural diminishing returns condition prevalent in many data summarization applications .	0	7	3	4.4390354	-4.0405483	0
320-10-45	here , the utility is assumed to exhibit submodularity , a natural diminishing returns condition prevalent in many data summarization applications .	we also study a natural trade-off between the communication cost and the number of rounds required to obtain such a solution .	1	3	8	-5.9424953	5.1796446	1
320-10-45	in our extensive experiments , we demonstrate the effectiveness of our approach on several applications , including active set selection , exemplar based clustering , and vertex cover on tens of millions of data points using spark .	here , the utility is assumed to exhibit submodularity , a natural diminishing returns condition prevalent in many data summarization applications .	0	9	3	5.252427	-4.613682	0
320-10-45	however , this sequential , centralized approach is impractical for truly large-scale problems .	the classical greedy algorithm is known to provide solutions with logarithmic approximation guarantees compared to the optimum solution .	0	5	4	2.6467948	-2.4363902	0
320-10-45	the classical greedy algorithm is known to provide solutions with logarithmic approximation guarantees compared to the optimum solution .	in this work , we develop the first distributed algorithm - d is c over - for submodular set cover that is easily implementable using mapreduce-style computations .	1	4	6	-3.8939333	3.7431269	1
320-10-45	we theoretically analyze our approach , and present approximation guarantees for the solutions returned by d is c over .	the classical greedy algorithm is known to provide solutions with logarithmic approximation guarantees compared to the optimum solution .	0	7	4	5.033249	-4.5192747	0
320-10-45	the classical greedy algorithm is known to provide solutions with logarithmic approximation guarantees compared to the optimum solution .	we also study a natural trade-off between the communication cost and the number of rounds required to obtain such a solution .	1	4	8	-5.971446	5.2052274	1
320-10-45	in our extensive experiments , we demonstrate the effectiveness of our approach on several applications , including active set selection , exemplar based clustering , and vertex cover on tens of millions of data points using spark .	the classical greedy algorithm is known to provide solutions with logarithmic approximation guarantees compared to the optimum solution .	0	9	4	5.5752106	-4.9134307	0
320-10-45	however , this sequential , centralized approach is impractical for truly large-scale problems .	in this work , we develop the first distributed algorithm - d is c over - for submodular set cover that is easily implementable using mapreduce-style computations .	1	5	6	-5.832054	5.254393	1
320-10-45	however , this sequential , centralized approach is impractical for truly large-scale problems .	we theoretically analyze our approach , and present approximation guarantees for the solutions returned by d is c over .	1	5	7	-5.9714794	5.154796	1
320-10-45	however , this sequential , centralized approach is impractical for truly large-scale problems .	we also study a natural trade-off between the communication cost and the number of rounds required to obtain such a solution .	1	5	8	-5.9729776	5.10146	1
320-10-45	however , this sequential , centralized approach is impractical for truly large-scale problems .	in our extensive experiments , we demonstrate the effectiveness of our approach on several applications , including active set selection , exemplar based clustering , and vertex cover on tens of millions of data points using spark .	1	5	9	-5.9451747	5.1487265	1
320-10-45	in this work , we develop the first distributed algorithm - d is c over - for submodular set cover that is easily implementable using mapreduce-style computations .	we theoretically analyze our approach , and present approximation guarantees for the solutions returned by d is c over .	1	6	7	-5.939607	5.221889	1
320-10-45	in this work , we develop the first distributed algorithm - d is c over - for submodular set cover that is easily implementable using mapreduce-style computations .	we also study a natural trade-off between the communication cost and the number of rounds required to obtain such a solution .	1	6	8	-5.840102	5.160074	1
320-10-45	in this work , we develop the first distributed algorithm - d is c over - for submodular set cover that is easily implementable using mapreduce-style computations .	in our extensive experiments , we demonstrate the effectiveness of our approach on several applications , including active set selection , exemplar based clustering , and vertex cover on tens of millions of data points using spark .	1	6	9	-5.998554	5.2135105	1
320-10-45	we also study a natural trade-off between the communication cost and the number of rounds required to obtain such a solution .	we theoretically analyze our approach , and present approximation guarantees for the solutions returned by d is c over .	0	8	7	2.9981785	-2.832496	0
320-10-45	we theoretically analyze our approach , and present approximation guarantees for the solutions returned by d is c over .	in our extensive experiments , we demonstrate the effectiveness of our approach on several applications , including active set selection , exemplar based clustering , and vertex cover on tens of millions of data points using spark .	1	7	9	-5.757641	5.091249	1
320-10-45	we also study a natural trade-off between the communication cost and the number of rounds required to obtain such a solution .	in our extensive experiments , we demonstrate the effectiveness of our approach on several applications , including active set selection , exemplar based clustering , and vertex cover on tens of millions of data points using spark .	1	8	9	-3.152654	3.1168346	1
321-5-10	the algorithm uses random walks to embed the graph in a space of measures , after which a modification of k-means in that space is applied .	we present a new algorithm for community detection .	0	1	0	5.6561055	-5.118922	0
321-5-10	the algorithm is therefore fast and easily parallelizable .	we present a new algorithm for community detection .	0	2	0	5.6429377	-5.0525994	0
321-5-10	we present a new algorithm for community detection .	we evaluate the algorithm on standard random graph benchmarks , including some overlapping community benchmarks , and find its performance to be better or at least as good as previously known algorithms .	1	0	3	-5.925074	5.2228947	1
321-5-10	we also prove a linear time ( in number of edges ) guarantee for the algorithm on a p , q-stochastic 1 block model with where p c * n - 2 + and p - q c	we present a new algorithm for community detection .	0	4	0	5.5287447	-4.9474783	0
321-5-10	the algorithm uses random walks to embed the graph in a space of measures , after which a modification of k-means in that space is applied .	the algorithm is therefore fast and easily parallelizable .	1	1	2	-4.132243	3.8685527	1
321-5-10	the algorithm uses random walks to embed the graph in a space of measures , after which a modification of k-means in that space is applied .	we evaluate the algorithm on standard random graph benchmarks , including some overlapping community benchmarks , and find its performance to be better or at least as good as previously known algorithms .	1	1	3	-5.652048	5.12146	1
321-5-10	we also prove a linear time ( in number of edges ) guarantee for the algorithm on a p , q-stochastic 1 block model with where p c * n - 2 + and p - q c	the algorithm uses random walks to embed the graph in a space of measures , after which a modification of k-means in that space is applied .	0	4	1	4.443854	-4.003139	0
321-5-10	the algorithm is therefore fast and easily parallelizable .	we evaluate the algorithm on standard random graph benchmarks , including some overlapping community benchmarks , and find its performance to be better or at least as good as previously known algorithms .	1	2	3	-4.2508383	4.02267	1
321-5-10	the algorithm is therefore fast and easily parallelizable .	we also prove a linear time ( in number of edges ) guarantee for the algorithm on a p , q-stochastic 1 block model with where p c * n - 2 + and p - q c	1	2	4	-2.055098	2.1709025	1
321-5-10	we also prove a linear time ( in number of edges ) guarantee for the algorithm on a p , q-stochastic 1 block model with where p c * n - 2 + and p - q c	we evaluate the algorithm on standard random graph benchmarks , including some overlapping community benchmarks , and find its performance to be better or at least as good as previously known algorithms .	0	4	3	-3.577725	3.414059	1
322-4-6	we show that important existing methods , such as krylov bases and bellman-errorbased methods are a special case of the general framework we develop .	we provide a theoretical framework for analyzing basis function construction for linear value function approximation in markov decision processes ( mdps ) .	0	1	0	5.613238	-5.0310903	0
322-4-6	we provide a general algorithmic framework for computing basis function refinements which `` respect '' the dynamics of the environment , and we derive approximation error bounds that apply for any algorithm respecting this general framework .	we provide a theoretical framework for analyzing basis function construction for linear value function approximation in markov decision processes ( mdps ) .	0	2	0	5.6872954	-5.050247	0
322-4-6	we provide a theoretical framework for analyzing basis function construction for linear value function approximation in markov decision processes ( mdps ) .	we also show how , using ideas related to bisimulation metrics , one can translate basis refinement into a process of finding `` prototypes '' that are diverse enough to represent the given mdp .	1	0	3	-5.892047	5.15517	1
322-4-6	we show that important existing methods , such as krylov bases and bellman-errorbased methods are a special case of the general framework we develop .	we provide a general algorithmic framework for computing basis function refinements which `` respect '' the dynamics of the environment , and we derive approximation error bounds that apply for any algorithm respecting this general framework .	1	1	2	-3.249996	3.1624405	1
322-4-6	we also show how , using ideas related to bisimulation metrics , one can translate basis refinement into a process of finding `` prototypes '' that are diverse enough to represent the given mdp .	we show that important existing methods , such as krylov bases and bellman-errorbased methods are a special case of the general framework we develop .	0	3	1	4.67103	-4.1968756	0
322-4-6	we also show how , using ideas related to bisimulation metrics , one can translate basis refinement into a process of finding `` prototypes '' that are diverse enough to represent the given mdp .	we provide a general algorithmic framework for computing basis function refinements which `` respect '' the dynamics of the environment , and we derive approximation error bounds that apply for any algorithm respecting this general framework .	0	3	2	4.5530148	-4.0621195	0
323-4-6	for structured estimation problems with atomic norms , recent advances in the literature express sample complexity and estimation error bounds in terms of certain geometric measures , in particular gaussian width of the unit norm ball , gaussian width of a spherical cap induced by a tangent cone , and a restricted norm compatibility constant .	however , given an atomic norm , bounding these geometric measures can be difficult .	1	0	1	-4.1174765	3.915731	1
323-4-6	in this paper , we present general upper bounds for such geometric measures , which only require simple information of the atomic norm under consideration , and we establish tightness of these bounds by providing the corresponding lower bounds .	[CLS] for structured estimation problems with atomic norms, recent advances in the literature express sample complexity and estimation error bounds in terms of certain geometric measures, in particular gaussian width of the unit norm ball, gaussian width of a spherical cap induced by a tangent cone, and a restricted	0	2	0	4.2196345	-3.8208134	0
323-4-6	for structured estimation problems with atomic norms , recent advances in the literature express sample complexity and estimation error bounds in terms of certain geometric measures , in particular gaussian width of the unit norm ball , gaussian width of a spherical cap induced by a tangent cone , and a restricted norm compatibility constant .	we show applications of our analysis to certain atomic norms , especially k-support norm , for which existing result is incomplete .	1	0	3	-6.005165	5.112436	1
323-4-6	in this paper , we present general upper bounds for such geometric measures , which only require simple information of the atomic norm under consideration , and we establish tightness of these bounds by providing the corresponding lower bounds .	however , given an atomic norm , bounding these geometric measures can be difficult .	0	2	1	5.3525367	-4.66272	0
323-4-6	however , given an atomic norm , bounding these geometric measures can be difficult .	we show applications of our analysis to certain atomic norms , especially k-support norm , for which existing result is incomplete .	1	1	3	-5.899555	5.069864	1
323-4-6	in this paper , we present general upper bounds for such geometric measures , which only require simple information of the atomic norm under consideration , and we establish tightness of these bounds by providing the corresponding lower bounds .	we show applications of our analysis to certain atomic norms , especially k-support norm , for which existing result is incomplete .	1	2	3	-5.9828253	5.0933447	1
324-10-45	in tandem , a focus has been on devising scalable variants that subsample the data and use stochastic gradients in place of full-data gradients in the dynamic simulations .	many recent markov chain monte carlo ( mcmc ) samplers leverage continuous dynamics to define a transition kernel that efficiently explores a target distribution .	0	1	0	5.538168	-4.947501	0
324-10-45	however , such stochastic gradient mcmc samplers have lagged behind their full-data counterparts in terms of the complexity of dynamics considered since proving convergence in the presence of the stochastic gradient noise is nontrivial .	many recent markov chain monte carlo ( mcmc ) samplers leverage continuous dynamics to define a transition kernel that efficiently explores a target distribution .	0	2	0	5.7094245	-5.1234274	0
324-10-45	even with simple dynamics , significant physical intuition is often required to modify the dynamical system to account for the stochastic gradient noise .	many recent markov chain monte carlo ( mcmc ) samplers leverage continuous dynamics to define a transition kernel that efficiently explores a target distribution .	0	3	0	4.9925475	-4.4312115	0
324-10-45	many recent markov chain monte carlo ( mcmc ) samplers leverage continuous dynamics to define a transition kernel that efficiently explores a target distribution .	in this paper , we provide a general recipe for constructing mcmc samplers -- including stochastic gradient versions -- based on continuous markov processes specified via two matrices .	1	0	4	-5.917801	5.1206484	1
324-10-45	we constructively prove that the framework is complete .	many recent markov chain monte carlo ( mcmc ) samplers leverage continuous dynamics to define a transition kernel that efficiently explores a target distribution .	0	5	0	5.700902	-5.1170135	0
324-10-45	that is , any continuous markov process that provides samples from the target distribution can be written in our framework .	many recent markov chain monte carlo ( mcmc ) samplers leverage continuous dynamics to define a transition kernel that efficiently explores a target distribution .	0	6	0	5.7059755	-5.1323805	0
324-10-45	many recent markov chain monte carlo ( mcmc ) samplers leverage continuous dynamics to define a transition kernel that efficiently explores a target distribution .	we show how previous continuous-dynamic samplers can be trivially `` reinvented '' in our framework , avoiding the complicated sampler-specific proofs .	1	0	7	-5.9240828	5.157495	1
324-10-45	we likewise use our recipe to straightforwardly propose a new state-adaptive sampler : stochastic gradient riemann hamiltonian monte carlo ( sgrhmc ) .	many recent markov chain monte carlo ( mcmc ) samplers leverage continuous dynamics to define a transition kernel that efficiently explores a target distribution .	0	8	0	5.5991716	-5.0079155	0
324-10-45	many recent markov chain monte carlo ( mcmc ) samplers leverage continuous dynamics to define a transition kernel that efficiently explores a target distribution .	our experiments on simulated data and a streaming wikipedia analysis demonstrate that the proposed sgrhmc sampler inherits the benefits of riemann hmc , with the scalability of stochastic gradient methods .	1	0	9	-5.9556375	5.097821	1
324-10-45	in tandem , a focus has been on devising scalable variants that subsample the data and use stochastic gradients in place of full-data gradients in the dynamic simulations .	however , such stochastic gradient mcmc samplers have lagged behind their full-data counterparts in terms of the complexity of dynamics considered since proving convergence in the presence of the stochastic gradient noise is nontrivial .	1	1	2	-0.70992297	0.9086604	1
324-10-45	in tandem , a focus has been on devising scalable variants that subsample the data and use stochastic gradients in place of full-data gradients in the dynamic simulations .	even with simple dynamics , significant physical intuition is often required to modify the dynamical system to account for the stochastic gradient noise .	1	1	3	4.9424148	-4.3567495	0
324-10-45	in tandem , a focus has been on devising scalable variants that subsample the data and use stochastic gradients in place of full-data gradients in the dynamic simulations .	in this paper , we provide a general recipe for constructing mcmc samplers -- including stochastic gradient versions -- based on continuous markov processes specified via two matrices .	1	1	4	-5.594203	5.176208	1
324-10-45	in tandem , a focus has been on devising scalable variants that subsample the data and use stochastic gradients in place of full-data gradients in the dynamic simulations .	we constructively prove that the framework is complete .	1	1	5	-5.5399714	5.070137	1
324-10-45	in tandem , a focus has been on devising scalable variants that subsample the data and use stochastic gradients in place of full-data gradients in the dynamic simulations .	that is , any continuous markov process that provides samples from the target distribution can be written in our framework .	1	1	6	-5.0597425	4.693971	1
324-10-45	in tandem , a focus has been on devising scalable variants that subsample the data and use stochastic gradients in place of full-data gradients in the dynamic simulations .	we show how previous continuous-dynamic samplers can be trivially `` reinvented '' in our framework , avoiding the complicated sampler-specific proofs .	1	1	7	-5.8235893	5.158197	1
324-10-45	we likewise use our recipe to straightforwardly propose a new state-adaptive sampler : stochastic gradient riemann hamiltonian monte carlo ( sgrhmc ) .	in tandem , a focus has been on devising scalable variants that subsample the data and use stochastic gradients in place of full-data gradients in the dynamic simulations .	0	8	1	5.144972	-4.527464	0
324-10-45	in tandem , a focus has been on devising scalable variants that subsample the data and use stochastic gradients in place of full-data gradients in the dynamic simulations .	our experiments on simulated data and a streaming wikipedia analysis demonstrate that the proposed sgrhmc sampler inherits the benefits of riemann hmc , with the scalability of stochastic gradient methods .	1	1	9	-5.967597	5.061515	1
324-10-45	however , such stochastic gradient mcmc samplers have lagged behind their full-data counterparts in terms of the complexity of dynamics considered since proving convergence in the presence of the stochastic gradient noise is nontrivial .	even with simple dynamics , significant physical intuition is often required to modify the dynamical system to account for the stochastic gradient noise .	1	2	3	4.3287773	-3.804842	0
324-10-45	however , such stochastic gradient mcmc samplers have lagged behind their full-data counterparts in terms of the complexity of dynamics considered since proving convergence in the presence of the stochastic gradient noise is nontrivial .	in this paper , we provide a general recipe for constructing mcmc samplers -- including stochastic gradient versions -- based on continuous markov processes specified via two matrices .	1	2	4	-5.8324556	5.225729	1
324-10-45	we constructively prove that the framework is complete .	however , such stochastic gradient mcmc samplers have lagged behind their full-data counterparts in terms of the complexity of dynamics considered since proving convergence in the presence of the stochastic gradient noise is nontrivial .	0	5	2	5.376525	-4.7842016	0
324-10-45	that is , any continuous markov process that provides samples from the target distribution can be written in our framework .	however , such stochastic gradient mcmc samplers have lagged behind their full-data counterparts in terms of the complexity of dynamics considered since proving convergence in the presence of the stochastic gradient noise is nontrivial .	0	6	2	5.1561594	-4.6513777	0
324-10-45	however , such stochastic gradient mcmc samplers have lagged behind their full-data counterparts in terms of the complexity of dynamics considered since proving convergence in the presence of the stochastic gradient noise is nontrivial .	we show how previous continuous-dynamic samplers can be trivially `` reinvented '' in our framework , avoiding the complicated sampler-specific proofs .	1	2	7	-5.921957	5.1801596	1
324-10-45	we likewise use our recipe to straightforwardly propose a new state-adaptive sampler : stochastic gradient riemann hamiltonian monte carlo ( sgrhmc ) .	however , such stochastic gradient mcmc samplers have lagged behind their full-data counterparts in terms of the complexity of dynamics considered since proving convergence in the presence of the stochastic gradient noise is nontrivial .	0	8	2	5.080158	-4.4794817	0
324-10-45	our experiments on simulated data and a streaming wikipedia analysis demonstrate that the proposed sgrhmc sampler inherits the benefits of riemann hmc , with the scalability of stochastic gradient methods .	however , such stochastic gradient mcmc samplers have lagged behind their full-data counterparts in terms of the complexity of dynamics considered since proving convergence in the presence of the stochastic gradient noise is nontrivial .	0	9	2	5.6203	-4.9981756	0
324-10-45	even with simple dynamics , significant physical intuition is often required to modify the dynamical system to account for the stochastic gradient noise .	in this paper , we provide a general recipe for constructing mcmc samplers -- including stochastic gradient versions -- based on continuous markov processes specified via two matrices .	1	3	4	-5.9546766	5.168891	1
324-10-45	even with simple dynamics , significant physical intuition is often required to modify the dynamical system to account for the stochastic gradient noise .	we constructively prove that the framework is complete .	1	3	5	-5.981929	5.2353253	1
324-10-45	even with simple dynamics , significant physical intuition is often required to modify the dynamical system to account for the stochastic gradient noise .	that is , any continuous markov process that provides samples from the target distribution can be written in our framework .	1	3	6	-5.925394	5.2451677	1
324-10-45	we show how previous continuous-dynamic samplers can be trivially `` reinvented '' in our framework , avoiding the complicated sampler-specific proofs .	even with simple dynamics , significant physical intuition is often required to modify the dynamical system to account for the stochastic gradient noise .	0	7	3	5.5050573	-4.827292	0
324-10-45	we likewise use our recipe to straightforwardly propose a new state-adaptive sampler : stochastic gradient riemann hamiltonian monte carlo ( sgrhmc ) .	even with simple dynamics , significant physical intuition is often required to modify the dynamical system to account for the stochastic gradient noise .	0	8	3	5.513713	-4.899008	0
324-10-45	our experiments on simulated data and a streaming wikipedia analysis demonstrate that the proposed sgrhmc sampler inherits the benefits of riemann hmc , with the scalability of stochastic gradient methods .	even with simple dynamics , significant physical intuition is often required to modify the dynamical system to account for the stochastic gradient noise .	0	9	3	5.736079	-5.08737	0
324-10-45	in this paper , we provide a general recipe for constructing mcmc samplers -- including stochastic gradient versions -- based on continuous markov processes specified via two matrices .	we constructively prove that the framework is complete .	1	4	5	-5.579927	5.068735	1
324-10-45	that is , any continuous markov process that provides samples from the target distribution can be written in our framework .	in this paper , we provide a general recipe for constructing mcmc samplers -- including stochastic gradient versions -- based on continuous markov processes specified via two matrices .	0	6	4	5.177063	-4.6793222	0
324-10-45	we show how previous continuous-dynamic samplers can be trivially `` reinvented '' in our framework , avoiding the complicated sampler-specific proofs .	in this paper , we provide a general recipe for constructing mcmc samplers -- including stochastic gradient versions -- based on continuous markov processes specified via two matrices .	0	7	4	5.5281076	-4.8619757	0
324-10-45	we likewise use our recipe to straightforwardly propose a new state-adaptive sampler : stochastic gradient riemann hamiltonian monte carlo ( sgrhmc ) .	in this paper , we provide a general recipe for constructing mcmc samplers -- including stochastic gradient versions -- based on continuous markov processes specified via two matrices .	0	8	4	5.3489485	-4.69936	0
324-10-45	our experiments on simulated data and a streaming wikipedia analysis demonstrate that the proposed sgrhmc sampler inherits the benefits of riemann hmc , with the scalability of stochastic gradient methods .	in this paper , we provide a general recipe for constructing mcmc samplers -- including stochastic gradient versions -- based on continuous markov processes specified via two matrices .	0	9	4	5.6022053	-4.9853177	0
324-10-45	that is , any continuous markov process that provides samples from the target distribution can be written in our framework .	we constructively prove that the framework is complete .	0	6	5	-2.7147722	2.668702	1
324-10-45	we show how previous continuous-dynamic samplers can be trivially `` reinvented '' in our framework , avoiding the complicated sampler-specific proofs .	we constructively prove that the framework is complete .	0	7	5	1.8265725	-1.6702284	0
324-10-45	we constructively prove that the framework is complete .	we likewise use our recipe to straightforwardly propose a new state-adaptive sampler : stochastic gradient riemann hamiltonian monte carlo ( sgrhmc ) .	1	5	8	-5.1416698	4.6905107	1
324-10-45	we constructively prove that the framework is complete .	our experiments on simulated data and a streaming wikipedia analysis demonstrate that the proposed sgrhmc sampler inherits the benefits of riemann hmc , with the scalability of stochastic gradient methods .	1	5	9	-5.8425426	5.1259527	1
324-10-45	that is , any continuous markov process that provides samples from the target distribution can be written in our framework .	we show how previous continuous-dynamic samplers can be trivially `` reinvented '' in our framework , avoiding the complicated sampler-specific proofs .	1	6	7	-2.9695613	2.919489	1
324-10-45	we likewise use our recipe to straightforwardly propose a new state-adaptive sampler : stochastic gradient riemann hamiltonian monte carlo ( sgrhmc ) .	that is , any continuous markov process that provides samples from the target distribution can be written in our framework .	0	8	6	3.9261546	-3.6260335	0
324-10-45	our experiments on simulated data and a streaming wikipedia analysis demonstrate that the proposed sgrhmc sampler inherits the benefits of riemann hmc , with the scalability of stochastic gradient methods .	that is , any continuous markov process that provides samples from the target distribution can be written in our framework .	0	9	6	5.2750936	-4.6759396	0
324-10-45	we likewise use our recipe to straightforwardly propose a new state-adaptive sampler : stochastic gradient riemann hamiltonian monte carlo ( sgrhmc ) .	we show how previous continuous-dynamic samplers can be trivially `` reinvented '' in our framework , avoiding the complicated sampler-specific proofs .	0	8	7	3.7571614	-3.4813633	0
324-10-45	we show how previous continuous-dynamic samplers can be trivially `` reinvented '' in our framework , avoiding the complicated sampler-specific proofs .	our experiments on simulated data and a streaming wikipedia analysis demonstrate that the proposed sgrhmc sampler inherits the benefits of riemann hmc , with the scalability of stochastic gradient methods .	1	7	9	-5.0355034	4.5972524	1
324-10-45	we likewise use our recipe to straightforwardly propose a new state-adaptive sampler : stochastic gradient riemann hamiltonian monte carlo ( sgrhmc ) .	our experiments on simulated data and a streaming wikipedia analysis demonstrate that the proposed sgrhmc sampler inherits the benefits of riemann hmc , with the scalability of stochastic gradient methods .	1	8	9	-5.7874775	5.1688533	1
325-7-21	bandit convex optimization is one of the fundamental problems in the field of online learning .	the best algorithm for the general bandit convex optimizae 5/6 ) , while the best known lower bound tion problem guarantees a regret of o ( t 1/2 is ( t ) .	1	0	1	-5.7975574	5.136817	1
325-7-21	many attempts have been made to bridge the huge gap between these bounds .	bandit convex optimization is one of the fundamental problems in the field of online learning .	0	2	0	5.605438	-5.082348	0
325-7-21	bandit convex optimization is one of the fundamental problems in the field of online learning .	a particularly interesting special case of this problem assumes that the loss functions are smooth .	1	0	3	-5.972002	5.1651673	1
325-7-21	in this case , the best known algorithm guarantees a ree 2/3 ) .	bandit convex optimization is one of the fundamental problems in the field of online learning .	0	4	0	5.55035	-4.996684	0
325-7-21	we present an efficient algorithm for the bandit smooth convex gret of o ( t e 5/8 ) .	bandit convex optimization is one of the fundamental problems in the field of online learning .	0	5	0	5.623241	-5.0412545	0
325-7-21	bandit convex optimization is one of the fundamental problems in the field of online learning .	our result rules out optimization problem that guarantees a regret of o ( t an ( t 2/3 ) lower bound and takes a significant step towards the resolution of this open problem .	1	0	6	-5.837225	5.1046767	1
325-7-21	many attempts have been made to bridge the huge gap between these bounds .	the best algorithm for the general bandit convex optimizae 5/6 ) , while the best known lower bound tion problem guarantees a regret of o ( t 1/2 is ( t ) .	0	2	1	-5.6568475	5.1234097	1
325-7-21	a particularly interesting special case of this problem assumes that the loss functions are smooth .	the best algorithm for the general bandit convex optimizae 5/6 ) , while the best known lower bound tion problem guarantees a regret of o ( t 1/2 is ( t ) .	0	3	1	-3.4780047	3.388577	1
325-7-21	in this case , the best known algorithm guarantees a ree 2/3 ) .	the best algorithm for the general bandit convex optimizae 5/6 ) , while the best known lower bound tion problem guarantees a regret of o ( t 1/2 is ( t ) .	0	4	1	3.9964242	-3.664389	0
325-7-21	the best algorithm for the general bandit convex optimizae 5/6 ) , while the best known lower bound tion problem guarantees a regret of o ( t 1/2 is ( t ) .	we present an efficient algorithm for the bandit smooth convex gret of o ( t e 5/8 ) .	1	1	5	-2.8383584	2.9075882	1
325-7-21	our result rules out optimization problem that guarantees a regret of o ( t an ( t 2/3 ) lower bound and takes a significant step towards the resolution of this open problem .	the best algorithm for the general bandit convex optimizae 5/6 ) , while the best known lower bound tion problem guarantees a regret of o ( t 1/2 is ( t ) .	0	6	1	4.03076	-3.7173696	0
325-7-21	many attempts have been made to bridge the huge gap between these bounds .	a particularly interesting special case of this problem assumes that the loss functions are smooth .	1	2	3	-3.8180192	3.7758858	1
325-7-21	many attempts have been made to bridge the huge gap between these bounds .	in this case , the best known algorithm guarantees a ree 2/3 ) .	1	2	4	-5.967685	5.2263823	1
325-7-21	we present an efficient algorithm for the bandit smooth convex gret of o ( t e 5/8 ) .	many attempts have been made to bridge the huge gap between these bounds .	0	5	2	5.35104	-4.7368813	0
325-7-21	our result rules out optimization problem that guarantees a regret of o ( t an ( t 2/3 ) lower bound and takes a significant step towards the resolution of this open problem .	many attempts have been made to bridge the huge gap between these bounds .	0	6	2	5.495035	-4.870301	0
325-7-21	a particularly interesting special case of this problem assumes that the loss functions are smooth .	in this case , the best known algorithm guarantees a ree 2/3 ) .	1	3	4	-5.5484676	5.103845	1
325-7-21	we present an efficient algorithm for the bandit smooth convex gret of o ( t e 5/8 ) .	a particularly interesting special case of this problem assumes that the loss functions are smooth .	0	5	3	4.719435	-4.2002273	0
325-7-21	our result rules out optimization problem that guarantees a regret of o ( t an ( t 2/3 ) lower bound and takes a significant step towards the resolution of this open problem .	a particularly interesting special case of this problem assumes that the loss functions are smooth .	0	6	3	4.9114113	-4.3170652	0
325-7-21	we present an efficient algorithm for the bandit smooth convex gret of o ( t e 5/8 ) .	in this case , the best known algorithm guarantees a ree 2/3 ) .	0	5	4	-2.2137537	2.3072004	1
325-7-21	our result rules out optimization problem that guarantees a regret of o ( t an ( t 2/3 ) lower bound and takes a significant step towards the resolution of this open problem .	in this case , the best known algorithm guarantees a ree 2/3 ) .	0	6	4	2.4014153	-2.2600818	0
325-7-21	our result rules out optimization problem that guarantees a regret of o ( t an ( t 2/3 ) lower bound and takes a significant step towards the resolution of this open problem .	we present an efficient algorithm for the bandit smooth convex gret of o ( t e 5/8 ) .	0	6	5	4.321065	-3.9055886	0
326-8-28	we design an online algorithm to classify the vertices of a graph .	underpinning the algorithm is the probability distribution of an ising model isomorphic to the graph .	1	0	1	-5.8608627	5.1842117	1
326-8-28	we design an online algorithm to classify the vertices of a graph .	each classification is based on predicting the label with maximum marginal probability in the limit of zero-temperature with respect to the labels and vertices seen so far .	1	0	2	-5.8588295	5.1987286	1
326-8-28	we design an online algorithm to classify the vertices of a graph .	computing these classifications is unfortunately based on a # p complete problem .	1	0	3	-4.3479304	4.101636	1
326-8-28	we design an online algorithm to classify the vertices of a graph .	this motivates us to develop an algorithm for which we give a sequential guarantee in the online mistake bound framework .	1	0	4	-5.9740915	5.110974	1
326-8-28	our algorithm is optimal when the graph is a tree matching the prior results in .	we design an online algorithm to classify the vertices of a graph .	0	5	0	5.6335278	-5.0234194	0
326-8-28	we design an online algorithm to classify the vertices of a graph .	for a general graph , the algorithm exploits the additional connectivity over a tree to provide a per-cluster bound .	1	0	6	-5.8651247	5.1730485	1
326-8-28	we design an online algorithm to classify the vertices of a graph .	the algorithm is efficient , as the cumulative time to sequentially predict all of the vertices of the graph is quadratic in the size of the graph .	1	0	7	-5.88673	5.204324	1
326-8-28	underpinning the algorithm is the probability distribution of an ising model isomorphic to the graph .	each classification is based on predicting the label with maximum marginal probability in the limit of zero-temperature with respect to the labels and vertices seen so far .	1	1	2	2.9590328	-2.7807534	0
326-8-28	computing these classifications is unfortunately based on a # p complete problem .	underpinning the algorithm is the probability distribution of an ising model isomorphic to the graph .	0	3	1	-5.987975	5.214744	1
326-8-28	this motivates us to develop an algorithm for which we give a sequential guarantee in the online mistake bound framework .	underpinning the algorithm is the probability distribution of an ising model isomorphic to the graph .	0	4	1	-4.5046344	4.1448774	1
326-8-28	our algorithm is optimal when the graph is a tree matching the prior results in .	underpinning the algorithm is the probability distribution of an ising model isomorphic to the graph .	0	5	1	-4.6796694	4.319702	1
326-8-28	underpinning the algorithm is the probability distribution of an ising model isomorphic to the graph .	for a general graph , the algorithm exploits the additional connectivity over a tree to provide a per-cluster bound .	1	1	6	-0.63921154	0.9069636	1
326-8-28	underpinning the algorithm is the probability distribution of an ising model isomorphic to the graph .	the algorithm is efficient , as the cumulative time to sequentially predict all of the vertices of the graph is quadratic in the size of the graph .	1	1	7	-2.3789737	2.432069	1
326-8-28	each classification is based on predicting the label with maximum marginal probability in the limit of zero-temperature with respect to the labels and vertices seen so far .	computing these classifications is unfortunately based on a # p complete problem .	1	2	3	3.2095456	-2.9044507	0
326-8-28	this motivates us to develop an algorithm for which we give a sequential guarantee in the online mistake bound framework .	each classification is based on predicting the label with maximum marginal probability in the limit of zero-temperature with respect to the labels and vertices seen so far .	0	4	2	2.8544328	-2.662875	0
326-8-28	our algorithm is optimal when the graph is a tree matching the prior results in .	each classification is based on predicting the label with maximum marginal probability in the limit of zero-temperature with respect to the labels and vertices seen so far .	0	5	2	2.8448465	-2.6774514	0
326-8-28	each classification is based on predicting the label with maximum marginal probability in the limit of zero-temperature with respect to the labels and vertices seen so far .	for a general graph , the algorithm exploits the additional connectivity over a tree to provide a per-cluster bound .	1	2	6	-5.7449865	5.1153517	1
326-8-28	the algorithm is efficient , as the cumulative time to sequentially predict all of the vertices of the graph is quadratic in the size of the graph .	each classification is based on predicting the label with maximum marginal probability in the limit of zero-temperature with respect to the labels and vertices seen so far .	0	7	2	4.306898	-3.940515	0
326-8-28	this motivates us to develop an algorithm for which we give a sequential guarantee in the online mistake bound framework .	computing these classifications is unfortunately based on a # p complete problem .	0	4	3	5.165065	-4.5109277	0
326-8-28	our algorithm is optimal when the graph is a tree matching the prior results in .	computing these classifications is unfortunately based on a # p complete problem .	0	5	3	5.0569577	-4.5260386	0
326-8-28	for a general graph , the algorithm exploits the additional connectivity over a tree to provide a per-cluster bound .	computing these classifications is unfortunately based on a # p complete problem .	0	6	3	5.113002	-4.536789	0
326-8-28	computing these classifications is unfortunately based on a # p complete problem .	the algorithm is efficient , as the cumulative time to sequentially predict all of the vertices of the graph is quadratic in the size of the graph .	1	3	7	-5.879322	5.2446384	1
326-8-28	our algorithm is optimal when the graph is a tree matching the prior results in .	this motivates us to develop an algorithm for which we give a sequential guarantee in the online mistake bound framework .	0	5	4	4.0304694	-3.6110935	0
326-8-28	for a general graph , the algorithm exploits the additional connectivity over a tree to provide a per-cluster bound .	this motivates us to develop an algorithm for which we give a sequential guarantee in the online mistake bound framework .	0	6	4	4.13396	-3.6658382	0
326-8-28	the algorithm is efficient , as the cumulative time to sequentially predict all of the vertices of the graph is quadratic in the size of the graph .	this motivates us to develop an algorithm for which we give a sequential guarantee in the online mistake bound framework .	0	7	4	4.787496	-4.221588	0
326-8-28	our algorithm is optimal when the graph is a tree matching the prior results in .	for a general graph , the algorithm exploits the additional connectivity over a tree to provide a per-cluster bound .	1	5	6	-4.385313	4.14715	1
326-8-28	our algorithm is optimal when the graph is a tree matching the prior results in .	the algorithm is efficient , as the cumulative time to sequentially predict all of the vertices of the graph is quadratic in the size of the graph .	1	5	7	-4.0275927	3.7936819	1
326-8-28	the algorithm is efficient , as the cumulative time to sequentially predict all of the vertices of the graph is quadratic in the size of the graph .	for a general graph , the algorithm exploits the additional connectivity over a tree to provide a per-cluster bound .	0	7	6	0.3210892	0.04618244	0
327-6-15	it supports stochastic control by treating stochasticity in the bellman equation as a deterministic function of exogenous noise .	we present a unified framework for learning continuous control policies using backpropagation .	0	1	0	4.927459	-4.4051833	0
327-6-15	we present a unified framework for learning continuous control policies using backpropagation .	the product is a spectrum of general policy gradient algorithms that range from model-free methods with value functions to model-based methods without value functions .	1	0	2	-5.6743383	5.12899	1
327-6-15	we use learned models but only require observations from the environment instead of observations from model-predicted trajectories , minimizing the impact of compounded model errors .	we present a unified framework for learning continuous control policies using backpropagation .	0	3	0	4.718605	-4.1499667	0
327-6-15	we present a unified framework for learning continuous control policies using backpropagation .	we apply these algorithms first to a toy stochastic control problem and then to several physics-based control problems in simulation .	1	0	4	-5.980418	5.1883197	1
327-6-15	we present a unified framework for learning continuous control policies using backpropagation .	one of these variants , svg ( 1 ) , shows the effectiveness of learning models , value functions , and policies simultaneously in continuous domains .	1	0	5	-5.660508	5.182375	1
327-6-15	it supports stochastic control by treating stochasticity in the bellman equation as a deterministic function of exogenous noise .	the product is a spectrum of general policy gradient algorithms that range from model-free methods with value functions to model-based methods without value functions .	1	1	2	-1.0161674	1.264528	1
327-6-15	it supports stochastic control by treating stochasticity in the bellman equation as a deterministic function of exogenous noise .	we use learned models but only require observations from the environment instead of observations from model-predicted trajectories , minimizing the impact of compounded model errors .	1	1	3	-2.8698974	2.9079838	1
327-6-15	it supports stochastic control by treating stochasticity in the bellman equation as a deterministic function of exogenous noise .	we apply these algorithms first to a toy stochastic control problem and then to several physics-based control problems in simulation .	1	1	4	-5.35205	4.8699546	1
327-6-15	it supports stochastic control by treating stochasticity in the bellman equation as a deterministic function of exogenous noise .	one of these variants , svg ( 1 ) , shows the effectiveness of learning models , value functions , and policies simultaneously in continuous domains .	1	1	5	-2.382453	2.4955792	1
327-6-15	we use learned models but only require observations from the environment instead of observations from model-predicted trajectories , minimizing the impact of compounded model errors .	the product is a spectrum of general policy gradient algorithms that range from model-free methods with value functions to model-based methods without value functions .	0	3	2	3.3719976	-3.1051526	0
327-6-15	we apply these algorithms first to a toy stochastic control problem and then to several physics-based control problems in simulation .	the product is a spectrum of general policy gradient algorithms that range from model-free methods with value functions to model-based methods without value functions .	0	4	2	5.2709036	-4.6533327	0
327-6-15	the product is a spectrum of general policy gradient algorithms that range from model-free methods with value functions to model-based methods without value functions .	one of these variants , svg ( 1 ) , shows the effectiveness of learning models , value functions , and policies simultaneously in continuous domains .	1	2	5	-1.804672	1.9371593	1
327-6-15	we use learned models but only require observations from the environment instead of observations from model-predicted trajectories , minimizing the impact of compounded model errors .	we apply these algorithms first to a toy stochastic control problem and then to several physics-based control problems in simulation .	1	3	4	-4.1673484	3.9310558	1
327-6-15	we use learned models but only require observations from the environment instead of observations from model-predicted trajectories , minimizing the impact of compounded model errors .	one of these variants , svg ( 1 ) , shows the effectiveness of learning models , value functions , and policies simultaneously in continuous domains .	1	3	5	2.7362428	-2.5558352	0
327-6-15	we apply these algorithms first to a toy stochastic control problem and then to several physics-based control problems in simulation .	one of these variants , svg ( 1 ) , shows the effectiveness of learning models , value functions , and policies simultaneously in continuous domains .	1	4	5	4.613508	-4.1093445	0
328-6-15	this work aims to address the problem of image-based question-answering ( qa ) with new models and datasets .	in our work , we propose to use neural networks and visual semantic embeddings , without intermediate stages such as object detection and image segmentation , to predict answers to simple questions about images .	1	0	1	-5.4632607	4.996174	1
328-6-15	this work aims to address the problem of image-based question-answering ( qa ) with new models and datasets .	our model performs 1.8 times better than the only published results on an existing image qa dataset .	1	0	2	-5.875936	5.215207	1
328-6-15	this work aims to address the problem of image-based question-answering ( qa ) with new models and datasets .	we also present a question generation algorithm that converts image descriptions , which are widely available , into qa form .	1	0	3	-6.008106	5.1910114	1
328-6-15	we used this algorithm to produce an order-of-magnitude larger dataset , with more evenly distributed answers .	this work aims to address the problem of image-based question-answering ( qa ) with new models and datasets .	0	4	0	5.5541625	-5.009151	0
328-6-15	a suite of baseline results on this new dataset are also presented .	this work aims to address the problem of image-based question-answering ( qa ) with new models and datasets .	0	5	0	5.5119686	-4.962565	0
328-6-15	our model performs 1.8 times better than the only published results on an existing image qa dataset .	in our work , we propose to use neural networks and visual semantic embeddings , without intermediate stages such as object detection and image segmentation , to predict answers to simple questions about images .	0	2	1	5.1929207	-4.543001	0
328-6-15	in our work , we propose to use neural networks and visual semantic embeddings , without intermediate stages such as object detection and image segmentation , to predict answers to simple questions about images .	we also present a question generation algorithm that converts image descriptions , which are widely available , into qa form .	1	1	3	-5.8655815	5.1790957	1
328-6-15	in our work , we propose to use neural networks and visual semantic embeddings , without intermediate stages such as object detection and image segmentation , to predict answers to simple questions about images .	we used this algorithm to produce an order-of-magnitude larger dataset , with more evenly distributed answers .	1	1	4	-5.9374385	5.2246795	1
328-6-15	in our work , we propose to use neural networks and visual semantic embeddings , without intermediate stages such as object detection and image segmentation , to predict answers to simple questions about images .	a suite of baseline results on this new dataset are also presented .	1	1	5	-5.9721	5.131801	1
328-6-15	we also present a question generation algorithm that converts image descriptions , which are widely available , into qa form .	our model performs 1.8 times better than the only published results on an existing image qa dataset .	0	3	2	-4.3438377	4.0537176	1
328-6-15	we used this algorithm to produce an order-of-magnitude larger dataset , with more evenly distributed answers .	our model performs 1.8 times better than the only published results on an existing image qa dataset .	0	4	2	-5.0698338	4.608819	1
328-6-15	a suite of baseline results on this new dataset are also presented .	our model performs 1.8 times better than the only published results on an existing image qa dataset .	0	5	2	2.4958925	-2.3731842	0
328-6-15	we also present a question generation algorithm that converts image descriptions , which are widely available , into qa form .	we used this algorithm to produce an order-of-magnitude larger dataset , with more evenly distributed answers .	1	3	4	-2.224133	2.339003	1
328-6-15	we also present a question generation algorithm that converts image descriptions , which are widely available , into qa form .	a suite of baseline results on this new dataset are also presented .	1	3	5	-5.493977	4.966558	1
328-6-15	we used this algorithm to produce an order-of-magnitude larger dataset , with more evenly distributed answers .	a suite of baseline results on this new dataset are also presented .	1	4	5	-5.782893	5.0209093	1
329-7-21	to be effective in practice , such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand , and also set their respective hyperparameters .	the success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts .	0	1	0	5.6823263	-5.086812	0
329-7-21	recent work has started to tackle this automated machine learning ( automl ) problem with the help of efficient bayesian optimization methods .	the success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts .	0	2	0	5.594672	-4.9899926	0
329-7-21	building on this , we introduce a robust new automl system based on scikit-learn ( using 15 classifiers , 14 feature preprocessing methods , and 4 data preprocessing methods , giving rise to a structured hypothesis space with 110 hyperparameters ) .	the success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts .	0	3	0	5.69295	-5.08025	0
329-7-21	this system , which we dub auto - sklearn , improves on existing automl methods by automatically taking into account past performance on similar datasets , and by constructing ensembles from the models evaluated during the optimization .	the success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts .	0	4	0	5.697899	-5.129903	0
329-7-21	our system won the first phase of the ongoing chalearn automl challenge , and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in automl .	the success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts .	0	5	0	5.6865225	-5.08719	0
329-7-21	the success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts .	we also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of auto - sklearn .	1	0	6	-5.909893	5.1489773	1
329-7-21	to be effective in practice , such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand , and also set their respective hyperparameters .	recent work has started to tackle this automated machine learning ( automl ) problem with the help of efficient bayesian optimization methods .	1	1	2	-1.3359909	1.649827	1
329-7-21	to be effective in practice , such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand , and also set their respective hyperparameters .	building on this , we introduce a robust new automl system based on scikit-learn ( using 15 classifiers , 14 feature preprocessing methods , and 4 data preprocessing methods , giving rise to a structured hypothesis space with 110 hyperparameters ) .	1	1	3	-5.9691906	5.143799	1
329-7-21	to be effective in practice , such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand , and also set their respective hyperparameters .	this system , which we dub auto - sklearn , improves on existing automl methods by automatically taking into account past performance on similar datasets , and by constructing ensembles from the models evaluated during the optimization .	1	1	4	-5.836097	5.247031	1
329-7-21	our system won the first phase of the ongoing chalearn automl challenge , and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in automl .	to be effective in practice , such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand , and also set their respective hyperparameters .	0	5	1	5.491409	-4.8504353	0
329-7-21	to be effective in practice , such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand , and also set their respective hyperparameters .	we also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of auto - sklearn .	1	1	6	-5.9383144	5.0367894	1
329-7-21	building on this , we introduce a robust new automl system based on scikit-learn ( using 15 classifiers , 14 feature preprocessing methods , and 4 data preprocessing methods , giving rise to a structured hypothesis space with 110 hyperparameters ) .	recent work has started to tackle this automated machine learning ( automl ) problem with the help of efficient bayesian optimization methods .	0	3	2	5.6428022	-4.966362	0
329-7-21	recent work has started to tackle this automated machine learning ( automl ) problem with the help of efficient bayesian optimization methods .	this system , which we dub auto - sklearn , improves on existing automl methods by automatically taking into account past performance on similar datasets , and by constructing ensembles from the models evaluated during the optimization .	1	2	4	-5.9320817	5.184196	1
329-7-21	our system won the first phase of the ongoing chalearn automl challenge , and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in automl .	recent work has started to tackle this automated machine learning ( automl ) problem with the help of efficient bayesian optimization methods .	0	5	2	5.5901737	-4.9387755	0
329-7-21	we also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of auto - sklearn .	recent work has started to tackle this automated machine learning ( automl ) problem with the help of efficient bayesian optimization methods .	0	6	2	5.540812	-4.943155	0
329-7-21	[CLS] building on this, we introduce a robust new automl system based on scikit - learn ( using 15 classifiers, 14 feature preprocessing methods, and 4 data preprocessing methods, giving rise to a structured hypothesis space with 110 hyperparameters	this system , which we dub auto - sklearn , improves on existing automl methods by automatically taking into account past performance on similar datasets , and by constructing ensembles from the models evaluated during the optimization .	1	3	4	2.6968932	-2.557139	0
329-7-21	our system won the first phase of the ongoing chalearn automl challenge , and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in automl .	[CLS] building on this, we introduce a robust new automl system based on scikit - learn ( using 15 classifiers, 14 feature preprocessing methods, and 4 data preprocessing methods, giving rise to a structured hypothesis space with 110 hyperparameter	0	5	3	3.5940747	-3.310843	0
329-7-21	we also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of auto - sklearn .	building on this , we introduce a robust new automl system based on scikit-learn ( using 15 classifiers , 14 feature preprocessing methods , and 4 data preprocessing methods , giving rise to a structured hypothesis space with 110 hyperparameters ) .	0	6	3	4.985528	-4.3723917	0
329-7-21	this system , which we dub auto - sklearn , improves on existing automl methods by automatically taking into account past performance on similar datasets , and by constructing ensembles from the models evaluated during the optimization .	our system won the first phase of the ongoing chalearn automl challenge , and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in automl .	1	4	5	-5.729249	5.112672	1
329-7-21	this system , which we dub auto - sklearn , improves on existing automl methods by automatically taking into account past performance on similar datasets , and by constructing ensembles from the models evaluated during the optimization .	we also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of auto - sklearn .	1	4	6	-5.922716	5.034803	1
329-7-21	our system won the first phase of the ongoing chalearn automl challenge , and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in automl .	we also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of auto - sklearn .	1	5	6	-4.5267334	4.2190847	1
330-8-28	deep learning presents notorious computational challenges .	these challenges include , but are not limited to , the non-convexity of learning objectives and estimating the quantities needed for optimization algorithms , such as gradients .	1	0	1	-5.833694	5.182652	1
330-8-28	deep learning presents notorious computational challenges .	while we do not address the non-convexity , we present an optimization solution that exploits the so far unused `` geometry '' in the objective function in order to best make use of the estimated gradients .	1	0	2	-5.9407687	5.129661	1
330-8-28	previous work attempted similar goals with preconditioned methods in the euclidean space , such as l-bfgs , rmsprop , and adagrad .	deep learning presents notorious computational challenges .	0	3	0	5.6468697	-5.089889	0
330-8-28	deep learning presents notorious computational challenges .	in stark contrast , our approach combines a non-euclidean gradient method with preconditioning .	1	0	4	-5.990241	5.1417785	1
330-8-28	we provide evidence that this combination more accurately captures the geometry of the objective function compared to prior work .	deep learning presents notorious computational challenges .	0	5	0	5.6851187	-5.054336	0
330-8-28	we theoretically formalize our arguments and derive novel preconditioned non-euclidean algorithms .	deep learning presents notorious computational challenges .	0	6	0	5.6675982	-5.068157	0
330-8-28	deep learning presents notorious computational challenges .	the results are promising in both computational time and quality when applied to restricted boltzmann machines , feedforward neural nets , and convolutional neural nets .	1	0	7	-5.901219	5.167368	1
330-8-28	these challenges include , but are not limited to , the non-convexity of learning objectives and estimating the quantities needed for optimization algorithms , such as gradients .	while we do not address the non-convexity , we present an optimization solution that exploits the so far unused `` geometry '' in the objective function in order to best make use of the estimated gradients .	1	1	2	-5.987305	5.0588336	1
330-8-28	these challenges include , but are not limited to , the non-convexity of learning objectives and estimating the quantities needed for optimization algorithms , such as gradients .	previous work attempted similar goals with preconditioned methods in the euclidean space , such as l-bfgs , rmsprop , and adagrad .	1	1	3	-2.0387547	2.1729484	1
330-8-28	in stark contrast , our approach combines a non-euclidean gradient method with preconditioning .	these challenges include , but are not limited to , the non-convexity of learning objectives and estimating the quantities needed for optimization algorithms , such as gradients .	0	4	1	4.716609	-4.307226	0
330-8-28	these challenges include , but are not limited to , the non-convexity of learning objectives and estimating the quantities needed for optimization algorithms , such as gradients .	we provide evidence that this combination more accurately captures the geometry of the objective function compared to prior work .	1	1	5	-5.987191	5.2103295	1
330-8-28	we theoretically formalize our arguments and derive novel preconditioned non-euclidean algorithms .	these challenges include , but are not limited to , the non-convexity of learning objectives and estimating the quantities needed for optimization algorithms , such as gradients .	0	6	1	4.822765	-4.340329	0
330-8-28	these challenges include , but are not limited to , the non-convexity of learning objectives and estimating the quantities needed for optimization algorithms , such as gradients .	the results are promising in both computational time and quality when applied to restricted boltzmann machines , feedforward neural nets , and convolutional neural nets .	1	1	7	-6.0255857	5.165028	1
330-8-28	previous work attempted similar goals with preconditioned methods in the euclidean space , such as l-bfgs , rmsprop , and adagrad .	while we do not address the non-convexity , we present an optimization solution that exploits the so far unused `` geometry '' in the objective function in order to best make use of the estimated gradients .	0	3	2	-5.9457545	5.1332464	1
330-8-28	while we do not address the non-convexity , we present an optimization solution that exploits the so far unused `` geometry '' in the objective function in order to best make use of the estimated gradients .	in stark contrast , our approach combines a non-euclidean gradient method with preconditioning .	1	2	4	-1.5318758	1.6896292	1
330-8-28	while we do not address the non-convexity , we present an optimization solution that exploits the so far unused `` geometry '' in the objective function in order to best make use of the estimated gradients .	we provide evidence that this combination more accurately captures the geometry of the objective function compared to prior work .	1	2	5	-5.1571846	4.757085	1
330-8-28	we theoretically formalize our arguments and derive novel preconditioned non-euclidean algorithms .	while we do not address the non-convexity , we present an optimization solution that exploits the so far unused `` geometry '' in the objective function in order to best make use of the estimated gradients .	0	6	2	2.1204746	-1.9299556	0
330-8-28	while we do not address the non-convexity , we present an optimization solution that exploits the so far unused `` geometry '' in the objective function in order to best make use of the estimated gradients .	the results are promising in both computational time and quality when applied to restricted boltzmann machines , feedforward neural nets , and convolutional neural nets .	1	2	7	-5.8970547	5.143738	1
330-8-28	in stark contrast , our approach combines a non-euclidean gradient method with preconditioning .	previous work attempted similar goals with preconditioned methods in the euclidean space , such as l-bfgs , rmsprop , and adagrad .	0	4	3	4.711007	-4.261013	0
330-8-28	we provide evidence that this combination more accurately captures the geometry of the objective function compared to prior work .	previous work attempted similar goals with preconditioned methods in the euclidean space , such as l-bfgs , rmsprop , and adagrad .	0	5	3	5.0909867	-4.489189	0
330-8-28	we theoretically formalize our arguments and derive novel preconditioned non-euclidean algorithms .	previous work attempted similar goals with preconditioned methods in the euclidean space , such as l-bfgs , rmsprop , and adagrad .	0	6	3	4.749424	-4.280851	0
330-8-28	previous work attempted similar goals with preconditioned methods in the euclidean space , such as l-bfgs , rmsprop , and adagrad .	the results are promising in both computational time and quality when applied to restricted boltzmann machines , feedforward neural nets , and convolutional neural nets .	1	3	7	-5.9903727	5.1596317	1
330-8-28	we provide evidence that this combination more accurately captures the geometry of the objective function compared to prior work .	in stark contrast , our approach combines a non-euclidean gradient method with preconditioning .	0	5	4	4.070328	-3.7249227	0
330-8-28	we theoretically formalize our arguments and derive novel preconditioned non-euclidean algorithms .	in stark contrast , our approach combines a non-euclidean gradient method with preconditioning .	0	6	4	2.765005	-2.543279	0
330-8-28	in stark contrast , our approach combines a non-euclidean gradient method with preconditioning .	the results are promising in both computational time and quality when applied to restricted boltzmann machines , feedforward neural nets , and convolutional neural nets .	1	4	7	-5.8823605	5.1347814	1
330-8-28	we provide evidence that this combination more accurately captures the geometry of the objective function compared to prior work .	we theoretically formalize our arguments and derive novel preconditioned non-euclidean algorithms .	1	5	6	3.5631988	-3.3250632	0
330-8-28	we provide evidence that this combination more accurately captures the geometry of the objective function compared to prior work .	the results are promising in both computational time and quality when applied to restricted boltzmann machines , feedforward neural nets , and convolutional neural nets .	1	5	7	-5.485424	4.924899	1
330-8-28	we theoretically formalize our arguments and derive novel preconditioned non-euclidean algorithms .	the results are promising in both computational time and quality when applied to restricted boltzmann machines , feedforward neural nets , and convolutional neural nets .	1	6	7	-5.933659	5.0622063	1
331-4-6	in this paper , we explore the inclusion of latent random variables into the hidden state of a recurrent neural network ( rnn ) by combining the elements of the variational autoencoder .	we argue that through the use of high-level latent random variables , the variational rnn ( vrnn ) 1 can model the kind of variability observed in highly structured sequential data such as natural speech .	1	0	1	-5.9782515	5.17768	1
331-4-6	we empirically evaluate the proposed model against other related sequential models on four speech datasets and one handwriting dataset .	in this paper , we explore the inclusion of latent random variables into the hidden state of a recurrent neural network ( rnn ) by combining the elements of the variational autoencoder .	0	2	0	5.648071	-4.9794374	0
331-4-6	in this paper , we explore the inclusion of latent random variables into the hidden state of a recurrent neural network ( rnn ) by combining the elements of the variational autoencoder .	our results show the important roles that latent random variables can play in the rnn dynamics .	1	0	3	-5.9763412	5.116418	1
331-4-6	we argue that through the use of high-level latent random variables , the variational rnn ( vrnn ) 1 can model the kind of variability observed in highly structured sequential data such as natural speech .	we empirically evaluate the proposed model against other related sequential models on four speech datasets and one handwriting dataset .	1	1	2	-5.941737	5.2103815	1
331-4-6	our results show the important roles that latent random variables can play in the rnn dynamics .	we argue that through the use of high-level latent random variables , the variational rnn ( vrnn ) 1 can model the kind of variability observed in highly structured sequential data such as natural speech .	0	3	1	4.968378	-4.38002	0
331-4-6	our results show the important roles that latent random variables can play in the rnn dynamics .	we empirically evaluate the proposed model against other related sequential models on four speech datasets and one handwriting dataset .	0	3	2	2.7241278	-2.496645	0
332-5-10	[CLS] when each player in a game uses an algorithm from our class, their individual regret decays at o ( t 3 / 4 ), while the sum of utilities converges to an approximate optimum at o ( t 1 ) - an improvement upon the worst case o ( t 1 / 2 )	we show that natural classes of regularized learning algorithms with a form of recency bias achieve faster convergence rates to approximate efficiency and to coarse correlated equilibria in multiplayer normal form games .	0	1	0	-3.5560977	3.4088721	1
332-5-10	we show that natural classes of regularized learning algorithms with a form of recency bias achieve faster convergence rates to approximate efficiency and to coarse correlated equilibria in multiplayer normal form games .	we show a black1/2 box reduction for any algorithm in the class to achieve o ( t ) rates against an adversary , while maintaining the faster rates against algorithms in the class .	1	0	2	0.9665653	-0.6176816	0
332-5-10	our results extend those of rakhlin and shridharan and daskalakis et al .	we show that natural classes of regularized learning algorithms with a form of recency bias achieve faster convergence rates to approximate efficiency and to coarse correlated equilibria in multiplayer normal form games .	0	3	0	-2.9294648	2.9809003	1
332-5-10	we show that natural classes of regularized learning algorithms with a form of recency bias achieve faster convergence rates to approximate efficiency and to coarse correlated equilibria in multiplayer normal form games .	, who only analyzed two-player zero-sum games for specific algorithms .	1	0	4	3.984704	-3.5700989	0
332-5-10	[CLS] when each player in a game uses an algorithm from our class, their individual regret decays at o ( t 3 / 4 ), while the sum of utilities converges to an approximate optimum at o ( t 1 ) - an improvement upon the worst case o ( t 1 / 2 ) rates	we show a black1/2 box reduction for any algorithm in the class to achieve o ( t ) rates against an adversary , while maintaining the faster rates against algorithms in the class .	1	1	2	-4.4235377	4.124916	1
332-5-10	our results extend those of rakhlin and shridharan and daskalakis et al .	when each player in a game uses an algorithm from our class , their individual regret decays at o ( t 3/4 ) , while the sum of utilities converges to an approximate optimum at o ( t 1 ) -an improvement upon the worst case o ( t 1/2 ) rates .	0	3	1	4.1228228	-3.741186	0
332-5-10	when each player in a game uses an algorithm from our class , their individual regret decays at o ( t 3/4 ) , while the sum of utilities converges to an approximate optimum at o ( t 1 ) -an improvement upon the worst case o ( t 1/2 ) rates .	, who only analyzed two-player zero-sum games for specific algorithms .	1	1	4	0.3788105	0.025513113	0
332-5-10	we show a black1/2 box reduction for any algorithm in the class to achieve o ( t ) rates against an adversary , while maintaining the faster rates against algorithms in the class .	our results extend those of rakhlin and shridharan and daskalakis et al .	1	2	3	-2.499013	2.4867573	1
332-5-10	, who only analyzed two-player zero-sum games for specific algorithms .	we show a black1/2 box reduction for any algorithm in the class to achieve o ( t ) rates against an adversary , while maintaining the faster rates against algorithms in the class .	0	4	2	-3.5655031	3.4259503	1
332-5-10	, who only analyzed two-player zero-sum games for specific algorithms .	our results extend those of rakhlin and shridharan and daskalakis et al .	0	4	3	-1.9104025	2.03276	1
333-7-21	they have a fixed input size and typically perceive only small local contexts of the pixels to be classified as foreground or background .	convolutional neural networks ( cnns ) can be shifted across 2d images or 3d videos to segment them .	0	1	0	3.566511	-3.1720245	0
333-7-21	in contrast , multi-dimensional recurrent nns ( md-rnns ) can perceive the entire spatio-temporal context of each pixel in a few sweeps through all pixels , especially when the rnn is a long short-term memory ( lstm ) .	convolutional neural networks ( cnns ) can be shifted across 2d images or 3d videos to segment them .	0	2	0	5.1090164	-4.496191	0
333-7-21	convolutional neural networks ( cnns ) can be shifted across 2d images or 3d videos to segment them .	despite these theoretical advantages , however , unlike cnns , previous md-lstm variants were hard to parallelise on gpus .	1	0	3	-5.947078	5.1466565	1
333-7-21	convolutional neural networks ( cnns ) can be shifted across 2d images or 3d videos to segment them .	here we re-arrange the traditional cuboid order of computations in md-lstm in pyramidal fashion .	1	0	4	-5.627925	5.087219	1
333-7-21	the resulting pyramid-lstm is easy to parallelise , especially for 3d data such as stacks of brain slice images .	convolutional neural networks ( cnns ) can be shifted across 2d images or 3d videos to segment them .	0	5	0	5.48907	-4.8541474	0
333-7-21	convolutional neural networks ( cnns ) can be shifted across 2d images or 3d videos to segment them .	pyramid-lstm achieved best known pixel-wise brain image segmentation results on mrbrains13 ( and competitive results on em-isbi12 ) .	1	0	6	-5.928934	4.993708	1
333-7-21	they have a fixed input size and typically perceive only small local contexts of the pixels to be classified as foreground or background .	in contrast , multi-dimensional recurrent nns ( md-rnns ) can perceive the entire spatio-temporal context of each pixel in a few sweeps through all pixels , especially when the rnn is a long short-term memory ( lstm ) .	1	1	2	-5.719173	5.209273	1
333-7-21	they have a fixed input size and typically perceive only small local contexts of the pixels to be classified as foreground or background .	despite these theoretical advantages , however , unlike cnns , previous md-lstm variants were hard to parallelise on gpus .	1	1	3	-3.994601	3.7961807	1
333-7-21	they have a fixed input size and typically perceive only small local contexts of the pixels to be classified as foreground or background .	here we re-arrange the traditional cuboid order of computations in md-lstm in pyramidal fashion .	1	1	4	-5.5400257	5.061991	1
333-7-21	the resulting pyramid-lstm is easy to parallelise , especially for 3d data such as stacks of brain slice images .	they have a fixed input size and typically perceive only small local contexts of the pixels to be classified as foreground or background .	0	5	1	5.3750763	-4.7490416	0
333-7-21	pyramid-lstm achieved best known pixel-wise brain image segmentation results on mrbrains13 ( and competitive results on em-isbi12 ) .	they have a fixed input size and typically perceive only small local contexts of the pixels to be classified as foreground or background .	0	6	1	5.456808	-4.850579	0
333-7-21	despite these theoretical advantages , however , unlike cnns , previous md-lstm variants were hard to parallelise on gpus .	in contrast , multi-dimensional recurrent nns ( md-rnns ) can perceive the entire spatio-temporal context of each pixel in a few sweeps through all pixels , especially when the rnn is a long short-term memory ( lstm ) .	0	3	2	4.8694158	-4.3196163	0
333-7-21	here we re-arrange the traditional cuboid order of computations in md-lstm in pyramidal fashion .	in contrast , multi-dimensional recurrent nns ( md-rnns ) can perceive the entire spatio-temporal context of each pixel in a few sweeps through all pixels , especially when the rnn is a long short-term memory ( lstm ) .	0	4	2	4.912511	-4.3449993	0
333-7-21	the resulting pyramid-lstm is easy to parallelise , especially for 3d data such as stacks of brain slice images .	in contrast , multi-dimensional recurrent nns ( md-rnns ) can perceive the entire spatio-temporal context of each pixel in a few sweeps through all pixels , especially when the rnn is a long short-term memory ( lstm ) .	0	5	2	5.4548936	-4.873908	0
333-7-21	in contrast , multi-dimensional recurrent nns ( md-rnns ) can perceive the entire spatio-temporal context of each pixel in a few sweeps through all pixels , especially when the rnn is a long short-term memory ( lstm ) .	pyramid-lstm achieved best known pixel-wise brain image segmentation results on mrbrains13 ( and competitive results on em-isbi12 ) .	1	2	6	-5.9484553	5.041208	1
333-7-21	despite these theoretical advantages , however , unlike cnns , previous md-lstm variants were hard to parallelise on gpus .	here we re-arrange the traditional cuboid order of computations in md-lstm in pyramidal fashion .	1	3	4	-5.621377	5.1791897	1
333-7-21	despite these theoretical advantages , however , unlike cnns , previous md-lstm variants were hard to parallelise on gpus .	the resulting pyramid-lstm is easy to parallelise , especially for 3d data such as stacks of brain slice images .	1	3	5	-5.894685	5.21437	1
333-7-21	pyramid-lstm achieved best known pixel-wise brain image segmentation results on mrbrains13 ( and competitive results on em-isbi12 ) .	despite these theoretical advantages , however , unlike cnns , previous md-lstm variants were hard to parallelise on gpus .	0	6	3	5.438334	-4.856596	0
333-7-21	the resulting pyramid-lstm is easy to parallelise , especially for 3d data such as stacks of brain slice images .	here we re-arrange the traditional cuboid order of computations in md-lstm in pyramidal fashion .	0	5	4	4.9963293	-4.4280195	0
333-7-21	here we re-arrange the traditional cuboid order of computations in md-lstm in pyramidal fashion .	pyramid-lstm achieved best known pixel-wise brain image segmentation results on mrbrains13 ( and competitive results on em-isbi12 ) .	1	4	6	-5.913574	5.0451536	1
333-7-21	the resulting pyramid-lstm is easy to parallelise , especially for 3d data such as stacks of brain slice images .	pyramid-lstm achieved best known pixel-wise brain image segmentation results on mrbrains13 ( and competitive results on em-isbi12 ) .	1	5	6	-4.4864726	4.142816	1
334-6-15	hamiltonian monte carlo ( hmc ) is a successful approach for sampling from continuous densities .	however , it has difficulty simulating hamiltonian dynamics with non-smooth functions , leading to poor performance .	1	0	1	-5.898566	5.2315216	1
334-6-15	this paper is motivated by the behavior of hamiltonian dynamics in physical systems like optics .	hamiltonian monte carlo ( hmc ) is a successful approach for sampling from continuous densities .	0	2	0	3.8239346	-2.99096	0
334-6-15	hamiltonian monte carlo ( hmc ) is a successful approach for sampling from continuous densities .	we introduce a modification of the leapfrog discretization of hamiltonian dynamics on piecewise continuous energies , where intersections of the trajectory with discontinuities are detected , and the momentum is reflected or refracted to compensate for the change in energy .	1	0	3	-5.955441	5.2379513	1
334-6-15	hamiltonian monte carlo ( hmc ) is a successful approach for sampling from continuous densities .	we prove that this method preserves the correct stationary distribution when boundaries are affine .	1	0	4	-6.0122623	5.171436	1
334-6-15	hamiltonian monte carlo ( hmc ) is a successful approach for sampling from continuous densities .	experiments show that by reducing the number of rejected samples , this method improves on traditional hmc .	1	0	5	-5.892564	5.0959535	1
334-6-15	however , it has difficulty simulating hamiltonian dynamics with non-smooth functions , leading to poor performance .	this paper is motivated by the behavior of hamiltonian dynamics in physical systems like optics .	1	1	2	3.7281475	-3.26799	0
334-6-15	we introduce a modification of the leapfrog discretization of hamiltonian dynamics on piecewise continuous energies , where intersections of the trajectory with discontinuities are detected , and the momentum is reflected or refracted to compensate for the change in energy .	however , it has difficulty simulating hamiltonian dynamics with non-smooth functions , leading to poor performance .	0	3	1	5.134195	-4.5452595	0
334-6-15	we prove that this method preserves the correct stationary distribution when boundaries are affine .	however , it has difficulty simulating hamiltonian dynamics with non-smooth functions , leading to poor performance .	0	4	1	5.0017543	-4.4980564	0
334-6-15	however , it has difficulty simulating hamiltonian dynamics with non-smooth functions , leading to poor performance .	experiments show that by reducing the number of rejected samples , this method improves on traditional hmc .	1	1	5	-5.972349	5.158805	1
334-6-15	we introduce a modification of the leapfrog discretization of hamiltonian dynamics on piecewise continuous energies , where intersections of the trajectory with discontinuities are detected , and the momentum is reflected or refracted to compensate for the change in energy .	this paper is motivated by the behavior of hamiltonian dynamics in physical systems like optics .	0	3	2	5.5388403	-4.8678064	0
334-6-15	we prove that this method preserves the correct stationary distribution when boundaries are affine .	this paper is motivated by the behavior of hamiltonian dynamics in physical systems like optics .	0	4	2	5.664099	-5.0967474	0
334-6-15	experiments show that by reducing the number of rejected samples , this method improves on traditional hmc .	this paper is motivated by the behavior of hamiltonian dynamics in physical systems like optics .	0	5	2	5.6076336	-5.0385256	0
334-6-15	we introduce a modification of the leapfrog discretization of hamiltonian dynamics on piecewise continuous energies , where intersections of the trajectory with discontinuities are detected , and the momentum is reflected or refracted to compensate for the change in energy .	we prove that this method preserves the correct stationary distribution when boundaries are affine .	1	3	4	-5.391264	4.915788	1
334-6-15	we introduce a modification of the leapfrog discretization of hamiltonian dynamics on piecewise continuous energies , where intersections of the trajectory with discontinuities are detected , and the momentum is reflected or refracted to compensate for the change in energy .	experiments show that by reducing the number of rejected samples , this method improves on traditional hmc .	1	3	5	-5.8280506	5.1605554	1
334-6-15	experiments show that by reducing the number of rejected samples , this method improves on traditional hmc .	we prove that this method preserves the correct stationary distribution when boundaries are affine .	0	5	4	3.7195158	-3.4561124	0
335-5-10	link prediction and clustering are key problems for network-structured data .	while spectral clustering has strong theoretical guarantees under the popular stochastic blockmodel formulation of networks , it can be expensive for large graphs .	1	0	1	-5.859343	5.197854	1
335-5-10	link prediction and clustering are key problems for network-structured data .	on the other hand , the heuristic of predicting links to nodes that share the most common neighbors with the query node is much fast , and works very well in practice .	1	0	2	-5.878625	5.188553	1
335-5-10	we show theoretically that the common neighbors heuristic can extract clusters with high probability when the graph is dense enough , and can do so even in sparser graphs with the addition of a `` cleaning '' step .	link prediction and clustering are key problems for network-structured data .	0	3	0	5.6899915	-5.11333	0
335-5-10	link prediction and clustering are key problems for network-structured data .	empirical results on simulated and real-world data support our conclusions .	1	0	4	-5.935112	5.122864	1
335-5-10	on the other hand , the heuristic of predicting links to nodes that share the most common neighbors with the query node is much fast , and works very well in practice .	while spectral clustering has strong theoretical guarantees under the popular stochastic blockmodel formulation of networks , it can be expensive for large graphs .	0	2	1	5.0350933	-4.425294	0
335-5-10	we show theoretically that the common neighbors heuristic can extract clusters with high probability when the graph is dense enough , and can do so even in sparser graphs with the addition of a `` cleaning '' step .	while spectral clustering has strong theoretical guarantees under the popular stochastic blockmodel formulation of networks , it can be expensive for large graphs .	0	3	1	5.4346695	-4.7868433	0
335-5-10	empirical results on simulated and real-world data support our conclusions .	while spectral clustering has strong theoretical guarantees under the popular stochastic blockmodel formulation of networks , it can be expensive for large graphs .	0	4	1	5.68748	-5.0694222	0
335-5-10	we show theoretically that the common neighbors heuristic can extract clusters with high probability when the graph is dense enough , and can do so even in sparser graphs with the addition of a `` cleaning '' step .	on the other hand , the heuristic of predicting links to nodes that share the most common neighbors with the query node is much fast , and works very well in practice .	0	3	2	3.666716	-3.427213	0
335-5-10	empirical results on simulated and real-world data support our conclusions .	on the other hand , the heuristic of predicting links to nodes that share the most common neighbors with the query node is much fast , and works very well in practice .	0	4	2	5.0739427	-4.568659	0
335-5-10	we show theoretically that the common neighbors heuristic can extract clusters with high probability when the graph is dense enough , and can do so even in sparser graphs with the addition of a `` cleaning '' step .	empirical results on simulated and real-world data support our conclusions .	1	3	4	-5.729826	5.0441766	1
336-5-10	our algorithm provides privacy protection with respect to each training example .	we present a nearly optimal differentially private version of the well known lasso estimator .	0	1	0	5.10414	-4.547222	0
336-5-10	we present a nearly optimal differentially private version of the well known lasso estimator .	the excess risk of our algorithm , compared to the non-private version , is o ( 1/n2/3 ) , assuming all the input data has bounded norm .	1	0	2	-5.95844	5.2139473	1
336-5-10	this is the first differentially private algorithm that achieves such a bound without the polynomial dependence on p under no additional assumptions on the design matrix .	we present a nearly optimal differentially private version of the well known lasso estimator .	0	3	0	5.3625464	-4.7930474	0
336-5-10	in addition , we show that this error bound is nearly optimal amongst all differentially private algorithms .	we present a nearly optimal differentially private version of the well known lasso estimator .	0	4	0	5.0979176	-4.488641	0
336-5-10	the excess risk of our algorithm , compared to the non-private version , is o ( 1/n2/3 ) , assuming all the input data has bounded norm .	our algorithm provides privacy protection with respect to each training example .	0	2	1	0.6396231	-0.35985684	0
336-5-10	our algorithm provides privacy protection with respect to each training example .	this is the first differentially private algorithm that achieves such a bound without the polynomial dependence on p under no additional assumptions on the design matrix .	1	1	3	-0.13099077	0.3546954	1
336-5-10	our algorithm provides privacy protection with respect to each training example .	in addition , we show that this error bound is nearly optimal amongst all differentially private algorithms .	1	1	4	-3.4629936	3.3368173	1
336-5-10	the excess risk of our algorithm , compared to the non-private version , is o ( 1/n2/3 ) , assuming all the input data has bounded norm .	this is the first differentially private algorithm that achieves such a bound without the polynomial dependence on p under no additional assumptions on the design matrix .	1	2	3	-2.1606712	2.2461383	1
336-5-10	the excess risk of our algorithm , compared to the non-private version , is o ( 1/n2/3 ) , assuming all the input data has bounded norm .	in addition , we show that this error bound is nearly optimal amongst all differentially private algorithms .	1	2	4	-4.638163	4.3249984	1
336-5-10	in addition , we show that this error bound is nearly optimal amongst all differentially private algorithms .	this is the first differentially private algorithm that achieves such a bound without the polynomial dependence on p under no additional assumptions on the design matrix .	0	4	3	3.1314328	-2.9770818	0
337-6-15	the pricing mechanisms in these markets are known to be related to optimization algorithms in machine learning and through these connections we have some understanding of how equilibrium market prices relate to the beliefs of the traders in a market .	prediction markets are economic mechanisms for aggregating information about future events through sequential interactions with traders .	0	1	0	5.672782	-5.0544996	0
337-6-15	however , little is known about rates and guarantees for the convergence of these sequential mechanisms , and two recent papers cite this as an important open question .	prediction markets are economic mechanisms for aggregating information about future events through sequential interactions with traders .	0	2	0	5.701955	-5.079736	0
337-6-15	prediction markets are economic mechanisms for aggregating information about future events through sequential interactions with traders .	in this paper we show how some previously studied prediction market trading models can be understood as a natural generalization of randomized coordinate descent which we call randomized subspace descent ( rsd ) .	1	0	3	-5.9060044	5.1953278	1
337-6-15	prediction markets are economic mechanisms for aggregating information about future events through sequential interactions with traders .	we establish convergence rates for rsd and leverage them to prove rates for the two prediction market models above , answering the open questions .	1	0	4	-5.923274	5.1211395	1
337-6-15	our results extend beyond standard centralized markets to arbitrary trade networks .	prediction markets are economic mechanisms for aggregating information about future events through sequential interactions with traders .	0	5	0	5.7205167	-5.126071	0
337-6-15	the pricing mechanisms in these markets are known to be related to optimization algorithms in machine learning and through these connections we have some understanding of how equilibrium market prices relate to the beliefs of the traders in a market .	however , little is known about rates and guarantees for the convergence of these sequential mechanisms , and two recent papers cite this as an important open question .	1	1	2	-3.4895835	3.3778214	1
337-6-15	the pricing mechanisms in these markets are known to be related to optimization algorithms in machine learning and through these connections we have some understanding of how equilibrium market prices relate to the beliefs of the traders in a market .	in this paper we show how some previously studied prediction market trading models can be understood as a natural generalization of randomized coordinate descent which we call randomized subspace descent ( rsd ) .	1	1	3	-4.284762	4.0866385	1
337-6-15	we establish convergence rates for rsd and leverage them to prove rates for the two prediction market models above , answering the open questions .	the pricing mechanisms in these markets are known to be related to optimization algorithms in machine learning and through these connections we have some understanding of how equilibrium market prices relate to the beliefs of the traders in a market .	0	4	1	5.335163	-4.6960993	0
337-6-15	the pricing mechanisms in these markets are known to be related to optimization algorithms in machine learning and through these connections we have some understanding of how equilibrium market prices relate to the beliefs of the traders in a market .	our results extend beyond standard centralized markets to arbitrary trade networks .	1	1	5	-5.830375	5.155018	1
337-6-15	however , little is known about rates and guarantees for the convergence of these sequential mechanisms , and two recent papers cite this as an important open question .	in this paper we show how some previously studied prediction market trading models can be understood as a natural generalization of randomized coordinate descent which we call randomized subspace descent ( rsd ) .	1	2	3	-5.706306	5.205994	1
337-6-15	we establish convergence rates for rsd and leverage them to prove rates for the two prediction market models above , answering the open questions .	however , little is known about rates and guarantees for the convergence of these sequential mechanisms , and two recent papers cite this as an important open question .	0	4	2	5.531458	-4.946734	0
337-6-15	however , little is known about rates and guarantees for the convergence of these sequential mechanisms , and two recent papers cite this as an important open question .	our results extend beyond standard centralized markets to arbitrary trade networks .	1	2	5	-5.942724	5.095561	1
337-6-15	in this paper we show how some previously studied prediction market trading models can be understood as a natural generalization of randomized coordinate descent which we call randomized subspace descent ( rsd ) .	we establish convergence rates for rsd and leverage them to prove rates for the two prediction market models above , answering the open questions .	1	3	4	-5.9672484	5.157517	1
337-6-15	our results extend beyond standard centralized markets to arbitrary trade networks .	in this paper we show how some previously studied prediction market trading models can be understood as a natural generalization of randomized coordinate descent which we call randomized subspace descent ( rsd ) .	0	5	3	5.4964514	-4.896876	0
337-6-15	we establish convergence rates for rsd and leverage them to prove rates for the two prediction market models above , answering the open questions .	our results extend beyond standard centralized markets to arbitrary trade networks .	1	4	5	-3.9923646	3.649053	1
338-5-10	[CLS] the pgbn's hidden layers are jointly trained with an upward - downward gibbs sampler, each iteration of which upward samples dirichlet distributed connection weight vectors starting from the first layer ( bottom data layer ), and then downward samples gamma distributed	[CLS] to infer a multilayer representation of high - dimensional count vectors, we propose the poisson gamma belief network ( pgbn ) that factorizes each of its layers into the product of a connection weight matrix and the nonnegative real hidden units	0	1	0	5.6363096	-4.9663143	0
338-5-10	to infer a multilayer representation of high-dimensional count vectors , we propose the poisson gamma belief network ( pgbn ) that factorizes each of its layers into the product of a connection weight matrix and the nonnegative real hidden units of the next layer .	the gamma-negative binomial process combined with a layer-wise training strategy allows the pgbn to infer the width of each layer given a fixed budget on the width of the first layer .	1	0	2	-5.8836384	5.196666	1
338-5-10	to infer a multilayer representation of high-dimensional count vectors , we propose the poisson gamma belief network ( pgbn ) that factorizes each of its layers into the product of a connection weight matrix and the nonnegative real hidden units of the next layer .	the pgbn with a single hidden layer reduces to poisson factor analysis .	1	0	3	-5.9236784	5.2168255	1
338-5-10	[CLS] to infer a multilayer representation of high - dimensional count vectors, we propose the poisson gamma belief network ( pgbn ) that factorizes each of its layers into the product of a connection weight matrix and the nonnegative real hidden units	[CLS] example results on text analysis illustrate interesting relationships between the width of the first layer and the inferred network structure, and demonstrate that the pgbn, whose hidden units are imposed with correlated gamma priors, can add more layers to increase its performance	1	0	4	-5.874426	5.126485	1
338-5-10	the gamma-negative binomial process combined with a layer-wise training strategy allows the pgbn to infer the width of each layer given a fixed budget on the width of the first layer .	[CLS] the pgbn's hidden layers are jointly trained with an upward - downward gibbs sampler, each iteration of which upward samples dirichlet distributed connection weight vectors starting from the first layer ( bottom data layer ), and then downward samples gamma distributed hidden units starting from the top hidden layer	0	2	1	3.0132053	-2.7673657	0
338-5-10	the pgbn 's hidden layers are jointly trained with an upward-downward gibbs sampler , each iteration of which upward samples dirichlet distributed connection weight vectors starting from the first layer ( bottom data layer ) , and then downward samples gamma distributed hidden units starting from the top hidden layer .	the pgbn with a single hidden layer reduces to poisson factor analysis .	1	1	3	-1.8254489	1.996648	1
338-5-10	[CLS] the pgbn's hidden layers are jointly trained with an upward - downward gibbs sampler, each iteration of which upward samples dirichlet distributed connection weight vectors starting from the first layer ( bottom data layer ), and then downward samples gamma distributed	[CLS] example results on text analysis illustrate interesting relationships between the width of the first layer and the inferred network structure, and demonstrate that the pgbn, whose hidden units are imposed with correlated gamma priors, can add more layers to increase its performance	1	1	4	-5.7756405	5.1338654	1
338-5-10	the pgbn with a single hidden layer reduces to poisson factor analysis .	the gamma-negative binomial process combined with a layer-wise training strategy allows the pgbn to infer the width of each layer given a fixed budget on the width of the first layer .	0	3	2	0.062684305	0.16616563	1
338-5-10	[CLS] example results on text analysis illustrate interesting relationships between the width of the first layer and the inferred network structure, and demonstrate that the pgbn, whose hidden units are imposed with correlated gamma priors, can add more layers to increase its performance gains over poisson factor analysis, given	the gamma-negative binomial process combined with a layer-wise training strategy allows the pgbn to infer the width of each layer given a fixed budget on the width of the first layer .	0	4	2	5.02931	-4.4817953	0
338-5-10	the pgbn with a single hidden layer reduces to poisson factor analysis .	example results on text analysis illustrate interesting relationships between the width of the first layer and the inferred network structure , and demonstrate that the pgbn , whose hidden units are imposed with correlated gamma priors , can add more layers to increase its performance gains over poisson factor analysis , given the same limit on the width of the first layer .	1	3	4	-5.7869897	5.0881367	1
339-8-28	we consider the problem of minimizing a sum of n functions via projected iterations onto a convex parameter set c rp , where n p 1 .	in this regime , algorithms which utilize sub-sampling techniques are known to be effective .	1	0	1	-5.9087377	5.18709	1
339-8-28	in this paper , we use sub-sampling techniques together with low-rank approximation to design a new randomized batch algorithm which possesses comparable convergence rate to newton 's method , yet has much smaller per-iteration cost .	we consider the problem of minimizing a sum of n functions via projected iterations onto a convex parameter set c rp , where n p 1 .	0	2	0	5.6286774	-5.016014	0
339-8-28	we consider the problem of minimizing a sum of n functions via projected iterations onto a convex parameter set c rp , where n p 1 .	the proposed algorithm is robust in terms of starting point and step size , and enjoys a composite convergence rate , namely , quadratic convergence at start and linear convergence when the iterate is close to the minimizer .	1	0	3	-5.8822746	5.17463	1
339-8-28	we consider the problem of minimizing a sum of n functions via projected iterations onto a convex parameter set c rp , where n p 1 .	we develop its theoretical analysis which also allows us to select near-optimal algorithm parameters .	1	0	4	-5.9232483	5.1331086	1
339-8-28	our theoretical results can be used to obtain convergence rates of previously proposed sub-sampling based algorithms as well .	we consider the problem of minimizing a sum of n functions via projected iterations onto a convex parameter set c rp , where n p 1 .	0	5	0	5.593977	-5.033149	0
339-8-28	we demonstrate how our results apply to well-known machine learning problems .	we consider the problem of minimizing a sum of n functions via projected iterations onto a convex parameter set c rp , where n p 1 .	0	6	0	5.6293063	-5.074015	0
339-8-28	we consider the problem of minimizing a sum of n functions via projected iterations onto a convex parameter set c rp , where n p 1 .	lastly , we evaluate the performance of our algorithm on several datasets under various scenarios .	1	0	7	-5.97999	5.130341	1
339-8-28	in this paper , we use sub-sampling techniques together with low-rank approximation to design a new randomized batch algorithm which possesses comparable convergence rate to newton 's method , yet has much smaller per-iteration cost .	in this regime , algorithms which utilize sub-sampling techniques are known to be effective .	0	2	1	0.36165982	-0.16617423	0
339-8-28	in this regime , algorithms which utilize sub-sampling techniques are known to be effective .	the proposed algorithm is robust in terms of starting point and step size , and enjoys a composite convergence rate , namely , quadratic convergence at start and linear convergence when the iterate is close to the minimizer .	1	1	3	-5.1844854	4.738373	1
339-8-28	in this regime , algorithms which utilize sub-sampling techniques are known to be effective .	we develop its theoretical analysis which also allows us to select near-optimal algorithm parameters .	1	1	4	-5.896133	5.1421165	1
339-8-28	in this regime , algorithms which utilize sub-sampling techniques are known to be effective .	our theoretical results can be used to obtain convergence rates of previously proposed sub-sampling based algorithms as well .	1	1	5	-5.562838	4.959877	1
339-8-28	we demonstrate how our results apply to well-known machine learning problems .	in this regime , algorithms which utilize sub-sampling techniques are known to be effective .	0	6	1	4.9957495	-4.4412603	0
339-8-28	in this regime , algorithms which utilize sub-sampling techniques are known to be effective .	lastly , we evaluate the performance of our algorithm on several datasets under various scenarios .	1	1	7	-5.9690204	5.1192155	1
339-8-28	in this paper , we use sub-sampling techniques together with low-rank approximation to design a new randomized batch algorithm which possesses comparable convergence rate to newton 's method , yet has much smaller per-iteration cost .	the proposed algorithm is robust in terms of starting point and step size , and enjoys a composite convergence rate , namely , quadratic convergence at start and linear convergence when the iterate is close to the minimizer .	1	2	3	-4.81441	4.422243	1
339-8-28	in this paper , we use sub-sampling techniques together with low-rank approximation to design a new randomized batch algorithm which possesses comparable convergence rate to newton 's method , yet has much smaller per-iteration cost .	we develop its theoretical analysis which also allows us to select near-optimal algorithm parameters .	1	2	4	-5.6345716	5.049197	1
339-8-28	our theoretical results can be used to obtain convergence rates of previously proposed sub-sampling based algorithms as well .	in this paper , we use sub-sampling techniques together with low-rank approximation to design a new randomized batch algorithm which possesses comparable convergence rate to newton 's method , yet has much smaller per-iteration cost .	0	5	2	5.2205873	-4.609127	0
339-8-28	in this paper , we use sub-sampling techniques together with low-rank approximation to design a new randomized batch algorithm which possesses comparable convergence rate to newton 's method , yet has much smaller per-iteration cost .	we demonstrate how our results apply to well-known machine learning problems .	1	2	6	-5.734926	5.0950994	1
339-8-28	lastly , we evaluate the performance of our algorithm on several datasets under various scenarios .	in this paper , we use sub-sampling techniques together with low-rank approximation to design a new randomized batch algorithm which possesses comparable convergence rate to newton 's method , yet has much smaller per-iteration cost .	0	7	2	5.488529	-4.8421955	0
339-8-28	we develop its theoretical analysis which also allows us to select near-optimal algorithm parameters .	the proposed algorithm is robust in terms of starting point and step size , and enjoys a composite convergence rate , namely , quadratic convergence at start and linear convergence when the iterate is close to the minimizer .	0	4	3	3.831778	-3.5470526	0
339-8-28	the proposed algorithm is robust in terms of starting point and step size , and enjoys a composite convergence rate , namely , quadratic convergence at start and linear convergence when the iterate is close to the minimizer .	our theoretical results can be used to obtain convergence rates of previously proposed sub-sampling based algorithms as well .	1	3	5	-5.6298904	5.005048	1
339-8-28	we demonstrate how our results apply to well-known machine learning problems .	the proposed algorithm is robust in terms of starting point and step size , and enjoys a composite convergence rate , namely , quadratic convergence at start and linear convergence when the iterate is close to the minimizer .	0	6	3	4.6560497	-4.202071	0
339-8-28	the proposed algorithm is robust in terms of starting point and step size , and enjoys a composite convergence rate , namely , quadratic convergence at start and linear convergence when the iterate is close to the minimizer .	lastly , we evaluate the performance of our algorithm on several datasets under various scenarios .	1	3	7	-5.7042413	5.1202707	1
339-8-28	we develop its theoretical analysis which also allows us to select near-optimal algorithm parameters .	our theoretical results can be used to obtain convergence rates of previously proposed sub-sampling based algorithms as well .	1	4	5	-4.8207483	4.410845	1
339-8-28	we demonstrate how our results apply to well-known machine learning problems .	we develop its theoretical analysis which also allows us to select near-optimal algorithm parameters .	0	6	4	3.505788	-3.2256951	0
339-8-28	lastly , we evaluate the performance of our algorithm on several datasets under various scenarios .	we develop its theoretical analysis which also allows us to select near-optimal algorithm parameters .	0	7	4	4.2283964	-3.8406925	0
339-8-28	we demonstrate how our results apply to well-known machine learning problems .	our theoretical results can be used to obtain convergence rates of previously proposed sub-sampling based algorithms as well .	0	6	5	-2.396634	2.4138608	1
339-8-28	lastly , we evaluate the performance of our algorithm on several datasets under various scenarios .	our theoretical results can be used to obtain convergence rates of previously proposed sub-sampling based algorithms as well .	0	7	5	-2.1743932	2.1877453	1
339-8-28	we demonstrate how our results apply to well-known machine learning problems .	lastly , we evaluate the performance of our algorithm on several datasets under various scenarios .	1	6	7	0.53442365	-0.25940707	0
340-5-10	recent price-of-anarchy analyses of games of complete information suggest that coarse correlated equilibria , which characterize outcomes resulting from no-regret learning dynamics , have near-optimal welfare .	this work provides two main technical results that lift this conclusion to games of incomplete information , a.k.a. , bayesian games .	1	0	1	-5.491273	5.069557	1
340-5-10	first , near-optimal welfare in bayesian games follows directly from the smoothness-based proof of near-optimal welfare in the same game when the private information is public .	recent price-of-anarchy analyses of games of complete information suggest that coarse correlated equilibria , which characterize outcomes resulting from no-regret learning dynamics , have near-optimal welfare .	0	2	0	3.153038	-2.9486861	0
340-5-10	second , no-regret learning dynamics converge to bayesian coarse correlated equilibrium in these incomplete information games .	recent price-of-anarchy analyses of games of complete information suggest that coarse correlated equilibria , which characterize outcomes resulting from no-regret learning dynamics , have near-optimal welfare .	0	3	0	4.4066052	-3.9172707	0
340-5-10	these results are enabled by interpretation of a bayesian game as a stochastic game of complete information .	recent price-of-anarchy analyses of games of complete information suggest that coarse correlated equilibria , which characterize outcomes resulting from no-regret learning dynamics , have near-optimal welfare .	0	4	0	4.6791477	-4.058301	0
340-5-10	this work provides two main technical results that lift this conclusion to games of incomplete information , a.k.a. , bayesian games .	first , near-optimal welfare in bayesian games follows directly from the smoothness-based proof of near-optimal welfare in the same game when the private information is public .	1	1	2	-5.5569835	5.1647024	1
340-5-10	second , no-regret learning dynamics converge to bayesian coarse correlated equilibrium in these incomplete information games .	this work provides two main technical results that lift this conclusion to games of incomplete information , a.k.a. , bayesian games .	0	3	1	4.55642	-4.0449677	0
340-5-10	these results are enabled by interpretation of a bayesian game as a stochastic game of complete information .	this work provides two main technical results that lift this conclusion to games of incomplete information , a.k.a. , bayesian games .	0	4	1	4.372014	-3.8928914	0
340-5-10	second , no-regret learning dynamics converge to bayesian coarse correlated equilibrium in these incomplete information games .	first , near-optimal welfare in bayesian games follows directly from the smoothness-based proof of near-optimal welfare in the same game when the private information is public .	0	3	2	5.3326397	-4.658592	0
340-5-10	first , near-optimal welfare in bayesian games follows directly from the smoothness-based proof of near-optimal welfare in the same game when the private information is public .	these results are enabled by interpretation of a bayesian game as a stochastic game of complete information .	1	2	4	-5.814917	5.20333	1
340-5-10	these results are enabled by interpretation of a bayesian game as a stochastic game of complete information .	second , no-regret learning dynamics converge to bayesian coarse correlated equilibrium in these incomplete information games .	0	4	3	3.7416155	-3.4500732	0
341-6-15	these diagrams encode persistent homology , a widely used invariant in topological data analysis .	we consider the problem of statistical computations with persistence diagrams , a summary representation of topological features in data .	0	1	0	5.484912	-4.9223137	0
341-6-15	we consider the problem of statistical computations with persistence diagrams , a summary representation of topological features in data .	while several avenues towards a statistical treatment of the diagrams have been explored recently , we follow an alternative route that is motivated by the success of methods based on the embedding of probability measures into reproducing kernel hilbert spaces .	1	0	2	-5.987316	5.1639924	1
341-6-15	in fact , a positive definite kernel on persistence diagrams has recently been proposed , connecting persistent homology to popular kernel-based learning techniques such as support vector machines .	we consider the problem of statistical computations with persistence diagrams , a summary representation of topological features in data .	0	3	0	5.4225063	-4.865477	0
341-6-15	we consider the problem of statistical computations with persistence diagrams , a summary representation of topological features in data .	however , important properties of that kernel enabling a principled use in the context of probability measure embeddings remain to be explored .	1	0	4	-5.9533634	5.186424	1
341-6-15	we consider the problem of statistical computations with persistence diagrams , a summary representation of topological features in data .	our contribution is to close this gap by proving universality of a variant of the original kernel , and to demonstrate its effective use in twosample hypothesis testing on synthetic as well as real-world data .	1	0	5	-5.9274645	5.1762705	1
341-6-15	these diagrams encode persistent homology , a widely used invariant in topological data analysis .	while several avenues towards a statistical treatment of the diagrams have been explored recently , we follow an alternative route that is motivated by the success of methods based on the embedding of probability measures into reproducing kernel hilbert spaces .	1	1	2	-0.7560091	0.99632156	1
341-6-15	these diagrams encode persistent homology , a widely used invariant in topological data analysis .	in fact , a positive definite kernel on persistence diagrams has recently been proposed , connecting persistent homology to popular kernel-based learning techniques such as support vector machines .	1	1	3	-3.3260384	3.278378	1
341-6-15	these diagrams encode persistent homology , a widely used invariant in topological data analysis .	however , important properties of that kernel enabling a principled use in the context of probability measure embeddings remain to be explored .	1	1	4	-3.2447295	3.2286348	1
341-6-15	these diagrams encode persistent homology , a widely used invariant in topological data analysis .	our contribution is to close this gap by proving universality of a variant of the original kernel , and to demonstrate its effective use in twosample hypothesis testing on synthetic as well as real-world data .	1	1	5	-5.404746	4.973743	1
341-6-15	while several avenues towards a statistical treatment of the diagrams have been explored recently , we follow an alternative route that is motivated by the success of methods based on the embedding of probability measures into reproducing kernel hilbert spaces .	in fact , a positive definite kernel on persistence diagrams has recently been proposed , connecting persistent homology to popular kernel-based learning techniques such as support vector machines .	1	2	3	3.59345	-3.1911397	0
341-6-15	while several avenues towards a statistical treatment of the diagrams have been explored recently , we follow an alternative route that is motivated by the success of methods based on the embedding of probability measures into reproducing kernel hilbert spaces .	however , important properties of that kernel enabling a principled use in the context of probability measure embeddings remain to be explored .	1	2	4	-5.5364523	5.051677	1
341-6-15	our contribution is to close this gap by proving universality of a variant of the original kernel , and to demonstrate its effective use in twosample hypothesis testing on synthetic as well as real-world data .	while several avenues towards a statistical treatment of the diagrams have been explored recently , we follow an alternative route that is motivated by the success of methods based on the embedding of probability measures into reproducing kernel hilbert spaces .	0	5	2	5.332341	-4.6994057	0
341-6-15	in fact , a positive definite kernel on persistence diagrams has recently been proposed , connecting persistent homology to popular kernel-based learning techniques such as support vector machines .	however , important properties of that kernel enabling a principled use in the context of probability measure embeddings remain to be explored .	1	3	4	-5.933152	5.2118573	1
341-6-15	in fact , a positive definite kernel on persistence diagrams has recently been proposed , connecting persistent homology to popular kernel-based learning techniques such as support vector machines .	our contribution is to close this gap by proving universality of a variant of the original kernel , and to demonstrate its effective use in twosample hypothesis testing on synthetic as well as real-world data .	1	3	5	-5.9483185	5.2024245	1
341-6-15	however , important properties of that kernel enabling a principled use in the context of probability measure embeddings remain to be explored .	our contribution is to close this gap by proving universality of a variant of the original kernel , and to demonstrate its effective use in twosample hypothesis testing on synthetic as well as real-world data .	1	4	5	-5.669195	5.1642337	1
342-7-21	the first approach is to predict what comes next in a sequence , which is a language model in nlp .	we present two approaches to use unlabeled data to improve sequence learning with recurrent networks .	0	1	0	5.4586024	-4.898518	0
342-7-21	the second approach is to use a sequence autoencoder , which reads the input sequence into a vector and predicts the input sequence again .	we present two approaches to use unlabeled data to improve sequence learning with recurrent networks .	0	2	0	5.51171	-4.9770555	0
342-7-21	these two algorithms can be used as a `` pretraining '' algorithm for a later supervised sequence learning algorithm .	we present two approaches to use unlabeled data to improve sequence learning with recurrent networks .	0	3	0	5.585584	-4.9726305	0
342-7-21	we present two approaches to use unlabeled data to improve sequence learning with recurrent networks .	in other words , the parameters obtained from the pretraining step can then be used as a starting point for other supervised training models .	1	0	4	-5.945987	5.2004924	1
342-7-21	in our experiments , we find that long short term memory recurrent networks after pretrained with the two approaches become more stable to train and generalize better .	we present two approaches to use unlabeled data to improve sequence learning with recurrent networks .	0	5	0	5.546523	-4.927348	0
342-7-21	we present two approaches to use unlabeled data to improve sequence learning with recurrent networks .	with pretraining , we were able to achieve strong performance in many classification tasks , such as text classification with imdb , dbpedia or image recognition in cifar-10 .	1	0	6	-5.95654	5.1305	1
342-7-21	the first approach is to predict what comes next in a sequence , which is a language model in nlp .	the second approach is to use a sequence autoencoder , which reads the input sequence into a vector and predicts the input sequence again .	1	1	2	-5.827795	5.2391996	1
342-7-21	the first approach is to predict what comes next in a sequence , which is a language model in nlp .	these two algorithms can be used as a `` pretraining '' algorithm for a later supervised sequence learning algorithm .	1	1	3	-4.9879665	4.5874186	1
342-7-21	in other words , the parameters obtained from the pretraining step can then be used as a starting point for other supervised training models .	the first approach is to predict what comes next in a sequence , which is a language model in nlp .	0	4	1	4.2642665	-3.9046302	0
342-7-21	in our experiments , we find that long short term memory recurrent networks after pretrained with the two approaches become more stable to train and generalize better .	the first approach is to predict what comes next in a sequence , which is a language model in nlp .	0	5	1	4.896664	-4.283413	0
342-7-21	with pretraining , we were able to achieve strong performance in many classification tasks , such as text classification with imdb , dbpedia or image recognition in cifar-10 .	the first approach is to predict what comes next in a sequence , which is a language model in nlp .	0	6	1	5.197998	-4.579448	0
342-7-21	the second approach is to use a sequence autoencoder , which reads the input sequence into a vector and predicts the input sequence again .	these two algorithms can be used as a `` pretraining '' algorithm for a later supervised sequence learning algorithm .	1	2	3	-4.028408	3.8160913	1
342-7-21	in other words , the parameters obtained from the pretraining step can then be used as a starting point for other supervised training models .	the second approach is to use a sequence autoencoder , which reads the input sequence into a vector and predicts the input sequence again .	0	4	2	3.5490236	-3.3552434	0
342-7-21	the second approach is to use a sequence autoencoder , which reads the input sequence into a vector and predicts the input sequence again .	in our experiments , we find that long short term memory recurrent networks after pretrained with the two approaches become more stable to train and generalize better .	1	2	5	-5.745896	5.02178	1
342-7-21	with pretraining , we were able to achieve strong performance in many classification tasks , such as text classification with imdb , dbpedia or image recognition in cifar-10 .	the second approach is to use a sequence autoencoder , which reads the input sequence into a vector and predicts the input sequence again .	0	6	2	4.7056136	-4.2923574	0
342-7-21	these two algorithms can be used as a `` pretraining '' algorithm for a later supervised sequence learning algorithm .	in other words , the parameters obtained from the pretraining step can then be used as a starting point for other supervised training models .	1	3	4	-5.147951	4.7014284	1
342-7-21	in our experiments , we find that long short term memory recurrent networks after pretrained with the two approaches become more stable to train and generalize better .	these two algorithms can be used as a `` pretraining '' algorithm for a later supervised sequence learning algorithm .	0	5	3	3.97802	-3.6541557	0
342-7-21	these two algorithms can be used as a `` pretraining '' algorithm for a later supervised sequence learning algorithm .	with pretraining , we were able to achieve strong performance in many classification tasks , such as text classification with imdb , dbpedia or image recognition in cifar-10 .	1	3	6	-5.8756437	5.1086097	1
342-7-21	in other words , the parameters obtained from the pretraining step can then be used as a starting point for other supervised training models .	in our experiments , we find that long short term memory recurrent networks after pretrained with the two approaches become more stable to train and generalize better .	1	4	5	-2.196666	2.2262795	1
342-7-21	in other words , the parameters obtained from the pretraining step can then be used as a starting point for other supervised training models .	with pretraining , we were able to achieve strong performance in many classification tasks , such as text classification with imdb , dbpedia or image recognition in cifar-10 .	1	4	6	-4.2760406	3.9464686	1
342-7-21	with pretraining , we were able to achieve strong performance in many classification tasks , such as text classification with imdb , dbpedia or image recognition in cifar-10 .	in our experiments , we find that long short term memory recurrent networks after pretrained with the two approaches become more stable to train and generalize better .	0	6	5	1.8815464	-1.5400432	0
343-5-10	we consider the task of building compact deep learning pipelines suitable for deployment on storage and power constrained mobile devices .	we propose a unified framework to learn a broad family of structured parameter matrices that are characterized by the notion of low displacement rank .	1	0	1	-5.7848744	5.2483883	1
343-5-10	we consider the task of building compact deep learning pipelines suitable for deployment on storage and power constrained mobile devices .	our structured transforms admit fast function and gradient evaluation , and span a rich range of parameter sharing configurations whose statistical modeling capacity can be explicitly tuned along a continuum from structured to unstructured .	1	0	2	-5.9608183	5.1989636	1
343-5-10	we consider the task of building compact deep learning pipelines suitable for deployment on storage and power constrained mobile devices .	experimental results show that these transforms can significantly accelerate inference and forward/backward passes during training , and offer superior accuracy-compactness-speed tradeoffs in comparison to a number of existing techniques .	1	0	3	-5.8945084	5.1954384	1
343-5-10	in keyword spotting applications in mobile speech recognition , our methods are much more effective than standard linear low-rank bottleneck layers and nearly retain the performance of state of the art models , while providing more than 3.5-fold compression .	we consider the task of building compact deep learning pipelines suitable for deployment on storage and power constrained mobile devices .	0	4	0	5.5504684	-4.9368715	0
343-5-10	our structured transforms admit fast function and gradient evaluation , and span a rich range of parameter sharing configurations whose statistical modeling capacity can be explicitly tuned along a continuum from structured to unstructured .	we propose a unified framework to learn a broad family of structured parameter matrices that are characterized by the notion of low displacement rank .	0	2	1	5.514726	-4.8552303	0
343-5-10	experimental results show that these transforms can significantly accelerate inference and forward/backward passes during training , and offer superior accuracy-compactness-speed tradeoffs in comparison to a number of existing techniques .	we propose a unified framework to learn a broad family of structured parameter matrices that are characterized by the notion of low displacement rank .	0	3	1	5.2276964	-4.6847267	0
343-5-10	in keyword spotting applications in mobile speech recognition , our methods are much more effective than standard linear low-rank bottleneck layers and nearly retain the performance of state of the art models , while providing more than 3.5-fold compression .	we propose a unified framework to learn a broad family of structured parameter matrices that are characterized by the notion of low displacement rank .	0	4	1	5.454276	-4.7897344	0
343-5-10	experimental results show that these transforms can significantly accelerate inference and forward/backward passes during training , and offer superior accuracy-compactness-speed tradeoffs in comparison to a number of existing techniques .	our structured transforms admit fast function and gradient evaluation , and span a rich range of parameter sharing configurations whose statistical modeling capacity can be explicitly tuned along a continuum from structured to unstructured .	0	3	2	4.88796	-4.4222875	0
343-5-10	in keyword spotting applications in mobile speech recognition , our methods are much more effective than standard linear low-rank bottleneck layers and nearly retain the performance of state of the art models , while providing more than 3.5-fold compression .	our structured transforms admit fast function and gradient evaluation , and span a rich range of parameter sharing configurations whose statistical modeling capacity can be explicitly tuned along a continuum from structured to unstructured .	0	4	2	4.337748	-3.987931	0
343-5-10	in keyword spotting applications in mobile speech recognition , our methods are much more effective than standard linear low-rank bottleneck layers and nearly retain the performance of state of the art models , while providing more than 3.5-fold compression .	experimental results show that these transforms can significantly accelerate inference and forward/backward passes during training , and offer superior accuracy-compactness-speed tradeoffs in comparison to a number of existing techniques .	0	4	3	1.4281642	-1.1075774	0
344-6-15	gibbs sampling on factor graphs is a widely used inference technique , which often produces good empirical results .	theoretical guarantees for its performance are weak : even for tree structured graphs , the mixing time of gibbs may be exponential in the number of variables .	1	0	1	-5.91875	5.2105045	1
344-6-15	gibbs sampling on factor graphs is a widely used inference technique , which often produces good empirical results .	to help understand the behavior of gibbs sampling , we introduce a new ( hyper ) graph property , called hierarchy width .	1	0	2	-5.955841	5.1034427	1
344-6-15	we show that under suitable conditions on the weights , bounded hierarchy width ensures polynomial mixing time .	gibbs sampling on factor graphs is a widely used inference technique , which often produces good empirical results .	0	3	0	5.4592485	-4.887368	0
344-6-15	our study of hierarchy width is in part motivated by a class of factor graph templates , hierarchical templates , which have bounded hierarchy width -- regardless of the data used to instantiate them .	gibbs sampling on factor graphs is a widely used inference technique , which often produces good empirical results .	0	4	0	4.8086452	-4.1472855	0
344-6-15	we demonstrate a rich application from natural language processing in which gibbs sampling provably mixes rapidly and achieves accuracy that exceeds human volunteers .	gibbs sampling on factor graphs is a widely used inference technique , which often produces good empirical results .	0	5	0	5.552022	-5.0254383	0
344-6-15	theoretical guarantees for its performance are weak : even for tree structured graphs , the mixing time of gibbs may be exponential in the number of variables .	to help understand the behavior of gibbs sampling , we introduce a new ( hyper ) graph property , called hierarchy width .	1	1	2	-3.1084435	3.1102335	1
344-6-15	theoretical guarantees for its performance are weak : even for tree structured graphs , the mixing time of gibbs may be exponential in the number of variables .	we show that under suitable conditions on the weights , bounded hierarchy width ensures polynomial mixing time .	1	1	3	-3.6559434	3.616271	1
344-6-15	theoretical guarantees for its performance are weak : even for tree structured graphs , the mixing time of gibbs may be exponential in the number of variables .	our study of hierarchy width is in part motivated by a class of factor graph templates , hierarchical templates , which have bounded hierarchy width -- regardless of the data used to instantiate them .	1	1	4	0.78778505	-0.53815925	0
344-6-15	we demonstrate a rich application from natural language processing in which gibbs sampling provably mixes rapidly and achieves accuracy that exceeds human volunteers .	theoretical guarantees for its performance are weak : even for tree structured graphs , the mixing time of gibbs may be exponential in the number of variables .	0	5	1	4.5524125	-4.113942	0
344-6-15	we show that under suitable conditions on the weights , bounded hierarchy width ensures polynomial mixing time .	to help understand the behavior of gibbs sampling , we introduce a new ( hyper ) graph property , called hierarchy width .	0	3	2	5.33321	-4.724125	0
344-6-15	to help understand the behavior of gibbs sampling , we introduce a new ( hyper ) graph property , called hierarchy width .	our study of hierarchy width is in part motivated by a class of factor graph templates , hierarchical templates , which have bounded hierarchy width -- regardless of the data used to instantiate them .	1	2	4	-5.492193	5.0365725	1
344-6-15	to help understand the behavior of gibbs sampling , we introduce a new ( hyper ) graph property , called hierarchy width .	we demonstrate a rich application from natural language processing in which gibbs sampling provably mixes rapidly and achieves accuracy that exceeds human volunteers .	1	2	5	-5.661204	5.0877743	1
344-6-15	we show that under suitable conditions on the weights , bounded hierarchy width ensures polynomial mixing time .	our study of hierarchy width is in part motivated by a class of factor graph templates , hierarchical templates , which have bounded hierarchy width -- regardless of the data used to instantiate them .	1	3	4	4.0710306	-3.6783092	0
344-6-15	we demonstrate a rich application from natural language processing in which gibbs sampling provably mixes rapidly and achieves accuracy that exceeds human volunteers .	we show that under suitable conditions on the weights , bounded hierarchy width ensures polynomial mixing time .	0	5	3	4.8979597	-4.4382915	0
344-6-15	we demonstrate a rich application from natural language processing in which gibbs sampling provably mixes rapidly and achieves accuracy that exceeds human volunteers .	our study of hierarchy width is in part motivated by a class of factor graph templates , hierarchical templates , which have bounded hierarchy width -- regardless of the data used to instantiate them .	0	5	4	5.1936073	-4.6415787	0
345-6-15	we consider the problem of recovering a low-rank tensor from its noisy observation .	previous work has shown a recovery guarantee with signal to noise ratio o ( n k/2 /2 ) for recovering a kth order rank one tensor of size n x * * * x n by recursive unfolding .	1	0	1	-5.8196487	5.221258	1
345-6-15	we consider the problem of recovering a low-rank tensor from its noisy observation .	in this paper , we first improve this bound to o ( nk/4 ) by a much simpler approach , but with a more careful analysis .	1	0	2	-5.910632	5.142019	1
345-6-15	then we propose a new norm called the subspace norm , which is based on the kronecker products of factors obtained by the proposed simple estimator .	we consider the problem of recovering a low-rank tensor from its noisy observation .	0	3	0	5.622914	-5.0662417	0
345-6-15	we consider the problem of recovering a low-rank tensor from its noisy observation .	the imposed kronecker structure allows us to show a nearly ideal o ( n + h k-1 ) bound , in which the parameter h controls the blend from the non-convex estimator to mode-wise nuclear norm minimization .	1	0	4	-5.863103	5.184022	1
345-6-15	we consider the problem of recovering a low-rank tensor from its noisy observation .	furthermore , we empirically demonstrate that the subspace norm achieves the nearly ideal denoising performance even with h = o ( 1 ) .	1	0	5	-5.879794	5.165848	1
345-6-15	in this paper , we first improve this bound to o ( nk/4 ) by a much simpler approach , but with a more careful analysis .	previous work has shown a recovery guarantee with signal to noise ratio o ( n k/2 /2 ) for recovering a kth order rank one tensor of size n x * * * x n by recursive unfolding .	0	2	1	5.3806343	-4.681003	0
345-6-15	previous work has shown a recovery guarantee with signal to noise ratio o ( n k/2 /2 ) for recovering a kth order rank one tensor of size n x * * * x n by recursive unfolding .	then we propose a new norm called the subspace norm , which is based on the kronecker products of factors obtained by the proposed simple estimator .	1	1	3	-6.0118613	5.1599054	1
345-6-15	the imposed kronecker structure allows us to show a nearly ideal o ( n + h k-1 ) bound , in which the parameter h controls the blend from the non-convex estimator to mode-wise nuclear norm minimization .	previous work has shown a recovery guarantee with signal to noise ratio o ( n k/2 /2 ) for recovering a kth order rank one tensor of size n x * * * x n by recursive unfolding .	0	4	1	5.446891	-4.7840014	0
345-6-15	previous work has shown a recovery guarantee with signal to noise ratio o ( n k/2 /2 ) for recovering a kth order rank one tensor of size n x * * * x n by recursive unfolding .	furthermore , we empirically demonstrate that the subspace norm achieves the nearly ideal denoising performance even with h = o ( 1 ) .	1	1	5	-5.995726	5.1721373	1
345-6-15	in this paper , we first improve this bound to o ( nk/4 ) by a much simpler approach , but with a more careful analysis .	then we propose a new norm called the subspace norm , which is based on the kronecker products of factors obtained by the proposed simple estimator .	1	2	3	-5.2849064	4.84398	1
345-6-15	the imposed kronecker structure allows us to show a nearly ideal o ( n + h k-1 ) bound , in which the parameter h controls the blend from the non-convex estimator to mode-wise nuclear norm minimization .	in this paper , we first improve this bound to o ( nk/4 ) by a much simpler approach , but with a more careful analysis .	0	4	2	-2.3044755	2.3987815	1
345-6-15	in this paper , we first improve this bound to o ( nk/4 ) by a much simpler approach , but with a more careful analysis .	furthermore , we empirically demonstrate that the subspace norm achieves the nearly ideal denoising performance even with h = o ( 1 ) .	1	2	5	-5.7746196	5.12131	1
345-6-15	the imposed kronecker structure allows us to show a nearly ideal o ( n + h k-1 ) bound , in which the parameter h controls the blend from the non-convex estimator to mode-wise nuclear norm minimization .	then we propose a new norm called the subspace norm , which is based on the kronecker products of factors obtained by the proposed simple estimator .	0	4	3	3.3460102	-3.0811849	0
345-6-15	furthermore , we empirically demonstrate that the subspace norm achieves the nearly ideal denoising performance even with h = o ( 1 ) .	then we propose a new norm called the subspace norm , which is based on the kronecker products of factors obtained by the proposed simple estimator .	0	5	3	4.9285765	-4.390085	0
345-6-15	furthermore , we empirically demonstrate that the subspace norm achieves the nearly ideal denoising performance even with h = o ( 1 ) .	the imposed kronecker structure allows us to show a nearly ideal o ( n + h k-1 ) bound , in which the parameter h controls the blend from the non-convex estimator to mode-wise nuclear norm minimization .	0	5	4	4.7496367	-4.230843	0
346-6-15	we consider a class of iterative stochastic policy optimization problems and analyze the resulting expected performance for each newly updated policy at each iteration .	this paper is concerned with robustness analysis of decision making under uncertainty .	0	1	0	5.66465	-5.0531282	0
346-6-15	in particular , we employ concentration-of-measure inequalities to compute future expected cost and probability of constraint violation using empirical runs .	this paper is concerned with robustness analysis of decision making under uncertainty .	0	2	0	5.6541758	-5.1208467	0
346-6-15	this paper is concerned with robustness analysis of decision making under uncertainty .	a novel inequality bound is derived that accounts for the possibly unbounded change-of-measure likelihood ratio resulting from iterative policy adaptation .	1	0	3	-5.861249	5.148837	1
346-6-15	the bound serves as a high-confidence certificate for providing future performance or safety guarantees .	this paper is concerned with robustness analysis of decision making under uncertainty .	0	4	0	5.6690426	-5.0896945	0
346-6-15	the approach is illustrated with a simple robot control scenario and initial steps towards applications to challenging aerial vehicle navigation problems are presented .	this paper is concerned with robustness analysis of decision making under uncertainty .	0	5	0	5.6306667	-5.0898275	0
346-6-15	in particular , we employ concentration-of-measure inequalities to compute future expected cost and probability of constraint violation using empirical runs .	we consider a class of iterative stochastic policy optimization problems and analyze the resulting expected performance for each newly updated policy at each iteration .	0	2	1	4.885648	-4.3534303	0
346-6-15	we consider a class of iterative stochastic policy optimization problems and analyze the resulting expected performance for each newly updated policy at each iteration .	a novel inequality bound is derived that accounts for the possibly unbounded change-of-measure likelihood ratio resulting from iterative policy adaptation .	1	1	3	-5.849905	5.191056	1
346-6-15	the bound serves as a high-confidence certificate for providing future performance or safety guarantees .	we consider a class of iterative stochastic policy optimization problems and analyze the resulting expected performance for each newly updated policy at each iteration .	0	4	1	5.0859613	-4.5365305	0
346-6-15	the approach is illustrated with a simple robot control scenario and initial steps towards applications to challenging aerial vehicle navigation problems are presented .	we consider a class of iterative stochastic policy optimization problems and analyze the resulting expected performance for each newly updated policy at each iteration .	0	5	1	5.3543515	-4.785943	0
346-6-15	in particular , we employ concentration-of-measure inequalities to compute future expected cost and probability of constraint violation using empirical runs .	a novel inequality bound is derived that accounts for the possibly unbounded change-of-measure likelihood ratio resulting from iterative policy adaptation .	1	2	3	-0.9007058	1.1637228	1
346-6-15	the bound serves as a high-confidence certificate for providing future performance or safety guarantees .	in particular , we employ concentration-of-measure inequalities to compute future expected cost and probability of constraint violation using empirical runs .	0	4	2	-2.2496648	2.3860767	1
346-6-15	in particular , we employ concentration-of-measure inequalities to compute future expected cost and probability of constraint violation using empirical runs .	the approach is illustrated with a simple robot control scenario and initial steps towards applications to challenging aerial vehicle navigation problems are presented .	1	2	5	-4.6521378	4.297414	1
346-6-15	a novel inequality bound is derived that accounts for the possibly unbounded change-of-measure likelihood ratio resulting from iterative policy adaptation .	the bound serves as a high-confidence certificate for providing future performance or safety guarantees .	1	3	4	-5.4358854	4.922075	1
346-6-15	the approach is illustrated with a simple robot control scenario and initial steps towards applications to challenging aerial vehicle navigation problems are presented .	a novel inequality bound is derived that accounts for the possibly unbounded change-of-measure likelihood ratio resulting from iterative policy adaptation .	0	5	3	4.345495	-4.0126677	0
346-6-15	the bound serves as a high-confidence certificate for providing future performance or safety guarantees .	the approach is illustrated with a simple robot control scenario and initial steps towards applications to challenging aerial vehicle navigation problems are presented .	1	4	5	-5.047317	4.6371307	1
347-8-28	in the past , gpus enabled these breakthroughs because of their greater computational speed .	deep neural networks ( dnn ) have achieved state-of-the-art results in a wide range of tasks , with the best results obtained with large training sets and large models .	0	1	0	5.1346226	-4.456549	0
347-8-28	deep neural networks ( dnn ) have achieved state-of-the-art results in a wide range of tasks , with the best results obtained with large training sets and large models .	in the future , faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices .	1	0	2	-4.464835	4.198324	1
347-8-28	deep neural networks ( dnn ) have achieved state-of-the-art results in a wide range of tasks , with the best results obtained with large training sets and large models .	as a result , there is much interest in research and development of dedicated hardware for deep learning ( dl ) .	1	0	3	-5.5016165	5.0238924	1
347-8-28	binary weights , i.e. , weights which are constrained to only two possible values ( e.g .	deep neural networks ( dnn ) have achieved state-of-the-art results in a wide range of tasks , with the best results obtained with large training sets and large models .	0	4	0	1.704831	-1.3033533	0
347-8-28	-1 or 1 ) , would bring great benefits to specialized dl hardware by replacing many multiply-accumulate operations by simple accumulations , as multipliers are the most space and powerhungry components of the digital implementation of neural networks .	deep neural networks ( dnn ) have achieved state-of-the-art results in a wide range of tasks , with the best results obtained with large training sets and large models .	0	5	0	5.3406963	-4.7371287	0
347-8-28	we introduce binaryconnect , a method which consists in training a dnn with binary weights during the forward and backward propagations , while retaining precision of the stored weights in which gradients are accumulated .	deep neural networks ( dnn ) have achieved state-of-the-art results in a wide range of tasks , with the best results obtained with large training sets and large models .	0	6	0	5.678117	-5.09284	0
347-8-28	deep neural networks ( dnn ) have achieved state-of-the-art results in a wide range of tasks , with the best results obtained with large training sets and large models .	like other dropout schemes , we show that binaryconnect acts as regularizer and we obtain near state-of-the-art results with binaryconnect on the permutation-invariant mnist , cifar-10 and svhn .	1	0	7	-6.0076246	5.1764107	1
347-8-28	in the past , gpus enabled these breakthroughs because of their greater computational speed .	in the future , faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices .	1	1	2	1.8856745	-1.5834343	0
347-8-28	as a result , there is much interest in research and development of dedicated hardware for deep learning ( dl ) .	in the past , gpus enabled these breakthroughs because of their greater computational speed .	0	3	1	-0.730658	1.0305243	1
347-8-28	binary weights , i.e. , weights which are constrained to only two possible values ( e.g .	in the past , gpus enabled these breakthroughs because of their greater computational speed .	0	4	1	-4.846384	4.4839215	1
347-8-28	-1 or 1 ) , would bring great benefits to specialized dl hardware by replacing many multiply-accumulate operations by simple accumulations , as multipliers are the most space and powerhungry components of the digital implementation of neural networks .	in the past , gpus enabled these breakthroughs because of their greater computational speed .	0	5	1	3.9947462	-3.6548362	0
347-8-28	we introduce binaryconnect , a method which consists in training a dnn with binary weights during the forward and backward propagations , while retaining precision of the stored weights in which gradients are accumulated .	in the past , gpus enabled these breakthroughs because of their greater computational speed .	0	6	1	4.23661	-3.8310921	0
347-8-28	like other dropout schemes , we show that binaryconnect acts as regularizer and we obtain near state-of-the-art results with binaryconnect on the permutation-invariant mnist , cifar-10 and svhn .	in the past , gpus enabled these breakthroughs because of their greater computational speed .	0	7	1	5.48312	-4.854254	0
347-8-28	in the future , faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices .	as a result , there is much interest in research and development of dedicated hardware for deep learning ( dl ) .	1	2	3	-2.1290991	2.2918372	1
347-8-28	in the future , faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices .	binary weights , i.e. , weights which are constrained to only two possible values ( e.g .	1	2	4	3.0845168	-2.749459	0
347-8-28	in the future , faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices .	-1 or 1 ) , would bring great benefits to specialized dl hardware by replacing many multiply-accumulate operations by simple accumulations , as multipliers are the most space and powerhungry components of the digital implementation of neural networks .	1	2	5	-5.611356	5.114683	1
347-8-28	we introduce binaryconnect , a method which consists in training a dnn with binary weights during the forward and backward propagations , while retaining precision of the stored weights in which gradients are accumulated .	in the future , faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices .	0	6	2	4.5057135	-4.129723	0
347-8-28	like other dropout schemes , we show that binaryconnect acts as regularizer and we obtain near state-of-the-art results with binaryconnect on the permutation-invariant mnist , cifar-10 and svhn .	in the future , faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices .	0	7	2	5.5699687	-4.961527	0
347-8-28	binary weights , i.e. , weights which are constrained to only two possible values ( e.g .	as a result , there is much interest in research and development of dedicated hardware for deep learning ( dl ) .	0	4	3	-4.2852077	4.001289	1
347-8-28	as a result , there is much interest in research and development of dedicated hardware for deep learning ( dl ) .	-1 or 1 ) , would bring great benefits to specialized dl hardware by replacing many multiply-accumulate operations by simple accumulations , as multipliers are the most space and powerhungry components of the digital implementation of neural networks .	1	3	5	-5.898167	5.1877193	1
347-8-28	we introduce binaryconnect , a method which consists in training a dnn with binary weights during the forward and backward propagations , while retaining precision of the stored weights in which gradients are accumulated .	as a result , there is much interest in research and development of dedicated hardware for deep learning ( dl ) .	0	6	3	3.2889578	-2.9915485	0
347-8-28	as a result , there is much interest in research and development of dedicated hardware for deep learning ( dl ) .	like other dropout schemes , we show that binaryconnect acts as regularizer and we obtain near state-of-the-art results with binaryconnect on the permutation-invariant mnist , cifar-10 and svhn .	1	3	7	-5.596466	4.990203	1
347-8-28	binary weights , i.e. , weights which are constrained to only two possible values ( e.g .	-1 or 1 ) , would bring great benefits to specialized dl hardware by replacing many multiply-accumulate operations by simple accumulations , as multipliers are the most space and powerhungry components of the digital implementation of neural networks .	1	4	5	-5.9037514	5.121639	1
347-8-28	binary weights , i.e. , weights which are constrained to only two possible values ( e.g .	we introduce binaryconnect , a method which consists in training a dnn with binary weights during the forward and backward propagations , while retaining precision of the stored weights in which gradients are accumulated .	1	4	6	-2.696588	2.7312272	1
347-8-28	binary weights , i.e. , weights which are constrained to only two possible values ( e.g .	like other dropout schemes , we show that binaryconnect acts as regularizer and we obtain near state-of-the-art results with binaryconnect on the permutation-invariant mnist , cifar-10 and svhn .	1	4	7	-5.859013	4.907222	1
347-8-28	we introduce binaryconnect , a method which consists in training a dnn with binary weights during the forward and backward propagations , while retaining precision of the stored weights in which gradients are accumulated .	-1 or 1 ) , would bring great benefits to specialized dl hardware by replacing many multiply-accumulate operations by simple accumulations , as multipliers are the most space and powerhungry components of the digital implementation of neural networks .	0	6	5	-4.512198	4.2157483	1
347-8-28	[CLS] like other dropout schemes, we show that binaryconnect acts as regularizer and we obtain near state - of - the - art results with binaryconnect on the permutation - invariant mnist, cifar - 10 and	[CLS] - 1 or 1 ), would bring great benefits to specialized dl hardware by replacing many multiply - accumulate operations by simple accumulations, as multipliers are the most space and powerhungry components of the digital implementation of neural networks.	0	7	5	2.179062	-1.9285202	0
347-8-28	like other dropout schemes , we show that binaryconnect acts as regularizer and we obtain near state-of-the-art results with binaryconnect on the permutation-invariant mnist , cifar-10 and svhn .	we introduce binaryconnect , a method which consists in training a dnn with binary weights during the forward and backward propagations , while retaining precision of the stored weights in which gradients are accumulated .	0	7	6	5.571533	-4.8753767	0
348-7-21	it is able to generate stable and realistic behaviors for a range of dynamical systems and tasks - swimming , flying , biped and quadruped walking with different body morphologies .	we present a method for training recurrent neural networks to act as near-optimal feedback controllers .	0	1	0	5.420904	-4.8435755	0
348-7-21	we present a method for training recurrent neural networks to act as near-optimal feedback controllers .	it does not require motion capture or task-specific features or state machines .	1	0	2	-5.694061	5.122297	1
348-7-21	we present a method for training recurrent neural networks to act as near-optimal feedback controllers .	the controller is a neural network , having a large number of feed-forward units that learn elaborate state-action mappings , and a small number of recurrent units that implement memory states beyond the physical system state .	1	0	3	-4.7162232	4.3225217	1
348-7-21	we present a method for training recurrent neural networks to act as near-optimal feedback controllers .	the action generated by the network is defined as velocity .	1	0	4	-5.832044	5.187791	1
348-7-21	thus the network is not learning a control policy , but rather the dynamics under an implicit policy .	we present a method for training recurrent neural networks to act as near-optimal feedback controllers .	0	5	0	5.3005123	-4.7331147	0
348-7-21	we present a method for training recurrent neural networks to act as near-optimal feedback controllers .	essential features of the method include interleaving supervised learning with trajectory optimization , injecting noise during training , training for unexpected changes in the task specification , and using the trajectory optimizer to obtain optimal feedback gains in addition to optimal actions .	1	0	6	-5.957712	5.185301	1
348-7-21	it is able to generate stable and realistic behaviors for a range of dynamical systems and tasks - swimming , flying , biped and quadruped walking with different body morphologies .	it does not require motion capture or task-specific features or state machines .	1	1	2	-1.7999539	1.9303901	1
348-7-21	the controller is a neural network , having a large number of feed-forward units that learn elaborate state-action mappings , and a small number of recurrent units that implement memory states beyond the physical system state .	it is able to generate stable and realistic behaviors for a range of dynamical systems and tasks - swimming , flying , biped and quadruped walking with different body morphologies .	0	3	1	-4.524262	4.2074885	1
348-7-21	the action generated by the network is defined as velocity .	it is able to generate stable and realistic behaviors for a range of dynamical systems and tasks - swimming , flying , biped and quadruped walking with different body morphologies .	0	4	1	-5.1696453	4.7589226	1
348-7-21	thus the network is not learning a control policy , but rather the dynamics under an implicit policy .	it is able to generate stable and realistic behaviors for a range of dynamical systems and tasks - swimming , flying , biped and quadruped walking with different body morphologies .	0	5	1	-4.335594	4.0973935	1
348-7-21	it is able to generate stable and realistic behaviors for a range of dynamical systems and tasks - swimming , flying , biped and quadruped walking with different body morphologies .	essential features of the method include interleaving supervised learning with trajectory optimization , injecting noise during training , training for unexpected changes in the task specification , and using the trajectory optimizer to obtain optimal feedback gains in addition to optimal actions .	1	1	6	-0.66350424	0.83778834	1
348-7-21	the controller is a neural network , having a large number of feed-forward units that learn elaborate state-action mappings , and a small number of recurrent units that implement memory states beyond the physical system state .	it does not require motion capture or task-specific features or state machines .	0	3	2	-4.0039225	3.7519145	1
348-7-21	the action generated by the network is defined as velocity .	it does not require motion capture or task-specific features or state machines .	0	4	2	-4.5480514	4.1892366	1
348-7-21	it does not require motion capture or task-specific features or state machines .	thus the network is not learning a control policy , but rather the dynamics under an implicit policy .	1	2	5	3.1334562	-2.987806	0
348-7-21	it does not require motion capture or task-specific features or state machines .	essential features of the method include interleaving supervised learning with trajectory optimization , injecting noise during training , training for unexpected changes in the task specification , and using the trajectory optimizer to obtain optimal feedback gains in addition to optimal actions .	1	2	6	-1.2908491	1.375982	1
348-7-21	the controller is a neural network , having a large number of feed-forward units that learn elaborate state-action mappings , and a small number of recurrent units that implement memory states beyond the physical system state .	the action generated by the network is defined as velocity .	1	3	4	-5.626359	5.107102	1
348-7-21	the controller is a neural network , having a large number of feed-forward units that learn elaborate state-action mappings , and a small number of recurrent units that implement memory states beyond the physical system state .	thus the network is not learning a control policy , but rather the dynamics under an implicit policy .	1	3	5	-5.4741	4.946315	1
348-7-21	the controller is a neural network , having a large number of feed-forward units that learn elaborate state-action mappings , and a small number of recurrent units that implement memory states beyond the physical system state .	essential features of the method include interleaving supervised learning with trajectory optimization , injecting noise during training , training for unexpected changes in the task specification , and using the trajectory optimizer to obtain optimal feedback gains in addition to optimal actions .	1	3	6	-5.5040045	4.992637	1
348-7-21	thus the network is not learning a control policy , but rather the dynamics under an implicit policy .	the action generated by the network is defined as velocity .	0	5	4	2.7100217	-2.5166621	0
348-7-21	essential features of the method include interleaving supervised learning with trajectory optimization , injecting noise during training , training for unexpected changes in the task specification , and using the trajectory optimizer to obtain optimal feedback gains in addition to optimal actions .	the action generated by the network is defined as velocity .	0	6	4	4.238743	-3.8736234	0
348-7-21	essential features of the method include interleaving supervised learning with trajectory optimization , injecting noise during training , training for unexpected changes in the task specification , and using the trajectory optimizer to obtain optimal feedback gains in addition to optimal actions .	thus the network is not learning a control policy , but rather the dynamics under an implicit policy .	0	6	5	2.5716481	-2.3903124	0
349-3-3	we show that there is a largely unexplored class of functions ( positive polymatroids ) that can define proper discrete metrics over pairs of binary vectors and that are fairly tractable to optimize over .	by exploiting submodularity , we are able to give hardness results and approximation algorithms for optimizing over such metrics .	1	0	1	-5.9999676	5.2004566	1
349-3-3	we show that there is a largely unexplored class of functions ( positive polymatroids ) that can define proper discrete metrics over pairs of binary vectors and that are fairly tractable to optimize over .	additionally , we demonstrate empirically the effectiveness of these metrics and associated algorithms on both a metric minimization task ( a form of clustering ) and also a metric maximization task ( generating diverse k-best lists ) .	1	0	2	-6.0127645	5.1804395	1
349-3-3	by exploiting submodularity , we are able to give hardness results and approximation algorithms for optimizing over such metrics .	additionally , we demonstrate empirically the effectiveness of these metrics and associated algorithms on both a metric minimization task ( a form of clustering ) and also a metric maximization task ( generating diverse k-best lists ) .	1	1	2	-5.5705094	5.034398	1
350-6-15	the algorithmic instances of our framework are universal since they can automatically adapt to the unknown holder continuity degree and constant within the dual formulation .	we propose a new primal-dual algorithmic framework for a prototypical constrained convex optimization template .	0	1	0	5.3919764	-4.7507877	0
350-6-15	they are also guaranteed to have optimal convergence rates in the objective residual and the feasibility gap for each holder smoothness degree .	we propose a new primal-dual algorithmic framework for a prototypical constrained convex optimization template .	0	2	0	5.0871897	-4.536033	0
350-6-15	in contrast to existing primal-dual algorithms , our framework avoids the proximity operator of the objective function .	we propose a new primal-dual algorithmic framework for a prototypical constrained convex optimization template .	0	3	0	5.186862	-4.5884647	0
350-6-15	we instead leverage computationally cheaper , fenchel-type operators , which are the main workhorses of the generalized conditional gradient ( gcg ) -type methods .	we propose a new primal-dual algorithmic framework for a prototypical constrained convex optimization template .	0	4	0	4.964309	-4.394313	0
350-6-15	we propose a new primal-dual algorithmic framework for a prototypical constrained convex optimization template .	in contrast to the gcg-type methods , our framework does not require the objective function to be differentiable , and can also process additional general linear inclusion constraints , while guarantees the convergence rate on the primal problem .	1	0	5	-5.8883944	5.239389	1
350-6-15	the algorithmic instances of our framework are universal since they can automatically adapt to the unknown holder continuity degree and constant within the dual formulation .	they are also guaranteed to have optimal convergence rates in the objective residual and the feasibility gap for each holder smoothness degree .	1	1	2	-0.5534514	0.77381605	1
350-6-15	the algorithmic instances of our framework are universal since they can automatically adapt to the unknown holder continuity degree and constant within the dual formulation .	in contrast to existing primal-dual algorithms , our framework avoids the proximity operator of the objective function .	1	1	3	3.6881497	-3.406282	0
350-6-15	the algorithmic instances of our framework are universal since they can automatically adapt to the unknown holder continuity degree and constant within the dual formulation .	we instead leverage computationally cheaper , fenchel-type operators , which are the main workhorses of the generalized conditional gradient ( gcg ) -type methods .	1	1	4	2.105289	-1.9381065	0
350-6-15	the algorithmic instances of our framework are universal since they can automatically adapt to the unknown holder continuity degree and constant within the dual formulation .	in contrast to the gcg-type methods , our framework does not require the objective function to be differentiable , and can also process additional general linear inclusion constraints , while guarantees the convergence rate on the primal problem .	1	1	5	-0.3444321	0.58674085	1
350-6-15	in contrast to existing primal-dual algorithms , our framework avoids the proximity operator of the objective function .	they are also guaranteed to have optimal convergence rates in the objective residual and the feasibility gap for each holder smoothness degree .	0	3	2	-4.1429796	3.9518504	1
350-6-15	they are also guaranteed to have optimal convergence rates in the objective residual and the feasibility gap for each holder smoothness degree .	we instead leverage computationally cheaper , fenchel-type operators , which are the main workhorses of the generalized conditional gradient ( gcg ) -type methods .	1	2	4	0.9302339	-0.7291151	0
350-6-15	they are also guaranteed to have optimal convergence rates in the objective residual and the feasibility gap for each holder smoothness degree .	in contrast to the gcg-type methods , our framework does not require the objective function to be differentiable , and can also process additional general linear inclusion constraints , while guarantees the convergence rate on the primal problem .	1	2	5	-1.667835	1.8420012	1
350-6-15	in contrast to existing primal-dual algorithms , our framework avoids the proximity operator of the objective function .	we instead leverage computationally cheaper , fenchel-type operators , which are the main workhorses of the generalized conditional gradient ( gcg ) -type methods .	1	3	4	-4.4160833	4.1479826	1
350-6-15	in contrast to existing primal-dual algorithms , our framework avoids the proximity operator of the objective function .	in contrast to the gcg-type methods , our framework does not require the objective function to be differentiable , and can also process additional general linear inclusion constraints , while guarantees the convergence rate on the primal problem .	1	3	5	-4.545313	4.224059	1
350-6-15	we instead leverage computationally cheaper , fenchel-type operators , which are the main workhorses of the generalized conditional gradient ( gcg ) -type methods .	in contrast to the gcg-type methods , our framework does not require the objective function to be differentiable , and can also process additional general linear inclusion constraints , while guarantees the convergence rate on the primal problem .	1	4	5	-5.8796215	5.0779886	1
351-6-15	simple decision heuristics are models of human and animal behavior that use few pieces of information -- perhaps only a single piece of information -- and integrate the pieces in simple ways , for example , by considering them sequentially , one at a time , or by giving them equal weight .	we focus on three families of heuristics : single-cue decision making , lexicographic decision making , and tallying .	1	0	1	-3.9435914	3.6951733	1
351-6-15	it is unknown how quickly these heuristics can be learned from experience .	simple decision heuristics are models of human and animal behavior that use few pieces of information -- perhaps only a single piece of information -- and integrate the pieces in simple ways , for example , by considering them sequentially , one at a time , or by giving them equal weight .	0	2	0	5.4079895	-4.7456574	0
351-6-15	simple decision heuristics are models of human and animal behavior that use few pieces of information -- perhaps only a single piece of information -- and integrate the pieces in simple ways , for example , by considering them sequentially , one at a time , or by giving them equal weight .	we show , analytically and empirically , that substantial progress in learning can be made with just a few training samples .	1	0	3	-5.9561386	5.2124434	1
351-6-15	simple decision heuristics are models of human and animal behavior that use few pieces of information -- perhaps only a single piece of information -- and integrate the pieces in simple ways , for example , by considering them sequentially , one at a time , or by giving them equal weight .	when training samples are very few , tallying performs substantially better than the alternative methods tested .	1	0	4	-5.7677402	5.1611586	1
351-6-15	our empirical analysis is the most extensive to date , employing 63 natural data sets on diverse subjects .	simple decision heuristics are models of human and animal behavior that use few pieces of information -- perhaps only a single piece of information -- and integrate the pieces in simple ways , for example , by considering them sequentially , one at a time , or by giving them equal weight .	0	5	0	5.427135	-4.837407	0
351-6-15	we focus on three families of heuristics : single-cue decision making , lexicographic decision making , and tallying .	it is unknown how quickly these heuristics can be learned from experience .	1	1	2	-1.844547	2.024858	1
351-6-15	we focus on three families of heuristics : single-cue decision making , lexicographic decision making , and tallying .	we show , analytically and empirically , that substantial progress in learning can be made with just a few training samples .	1	1	3	-5.5455723	5.0710664	1
351-6-15	when training samples are very few , tallying performs substantially better than the alternative methods tested .	we focus on three families of heuristics : single-cue decision making , lexicographic decision making , and tallying .	0	4	1	5.2601914	-4.820011	0
351-6-15	our empirical analysis is the most extensive to date , employing 63 natural data sets on diverse subjects .	we focus on three families of heuristics : single-cue decision making , lexicographic decision making , and tallying .	0	5	1	4.83052	-4.365635	0
351-6-15	we show , analytically and empirically , that substantial progress in learning can be made with just a few training samples .	it is unknown how quickly these heuristics can be learned from experience .	0	3	2	4.5724106	-4.1029067	0
351-6-15	it is unknown how quickly these heuristics can be learned from experience .	when training samples are very few , tallying performs substantially better than the alternative methods tested .	1	2	4	-2.7324457	2.7280698	1
351-6-15	our empirical analysis is the most extensive to date , employing 63 natural data sets on diverse subjects .	it is unknown how quickly these heuristics can be learned from experience .	0	5	2	4.295669	-3.9073918	0
351-6-15	when training samples are very few , tallying performs substantially better than the alternative methods tested .	we show , analytically and empirically , that substantial progress in learning can be made with just a few training samples .	0	4	3	1.9411361	-1.8168433	0
351-6-15	our empirical analysis is the most extensive to date , employing 63 natural data sets on diverse subjects .	we show , analytically and empirically , that substantial progress in learning can be made with just a few training samples .	0	5	3	3.1189048	-2.887785	0
351-6-15	our empirical analysis is the most extensive to date , employing 63 natural data sets on diverse subjects .	when training samples are very few , tallying performs substantially better than the alternative methods tested .	0	5	4	-1.9372501	2.047492	1
352-7-21	such results are rather scarce in the literature since proving them requires a large deal of technical effort and significant modifications to the standard , more intuitive algorithms that come only with guarantees that hold on expectation .	this work addresses the problem of regret minimization in non-stochastic multiarmed bandit problems , focusing on performance guarantees that hold with high probability .	0	1	0	3.9824338	-3.548271	0
352-7-21	one of these modifications is forcing the learner to sample arms from the uniform distribution at least ( t ) times over t rounds , which can adversely affect performance if many of the arms are suboptimal .	this work addresses the problem of regret minimization in non-stochastic multiarmed bandit problems , focusing on performance guarantees that hold with high probability .	0	2	0	5.409453	-4.846381	0
352-7-21	this work addresses the problem of regret minimization in non-stochastic multiarmed bandit problems , focusing on performance guarantees that hold with high probability .	while it is widely conjectured that this property is essential for proving high-probability regret bounds , we show in this paper that it is possible to achieve such strong results without this undesirable exploration component .	1	0	3	-5.8702855	5.253399	1
352-7-21	our result relies on a simple and intuitive loss-estimation strategy called implicit exploration ( ix ) that allows a remarkably clean analysis .	this work addresses the problem of regret minimization in non-stochastic multiarmed bandit problems , focusing on performance guarantees that hold with high probability .	0	4	0	5.5050135	-4.95646	0
352-7-21	to demonstrate the flexibility of our technique , we derive several improved high-probability bounds for various extensions of the standard multi-armed bandit framework .	this work addresses the problem of regret minimization in non-stochastic multiarmed bandit problems , focusing on performance guarantees that hold with high probability .	0	5	0	5.5346856	-4.9372234	0
352-7-21	this work addresses the problem of regret minimization in non-stochastic multiarmed bandit problems , focusing on performance guarantees that hold with high probability .	finally , we conduct a simple experiment that illustrates the robustness of our implicit exploration technique .	1	0	6	-5.964834	5.2009807	1
352-7-21	such results are rather scarce in the literature since proving them requires a large deal of technical effort and significant modifications to the standard , more intuitive algorithms that come only with guarantees that hold on expectation .	one of these modifications is forcing the learner to sample arms from the uniform distribution at least ( t ) times over t rounds , which can adversely affect performance if many of the arms are suboptimal .	1	1	2	-4.3967457	4.2559443	1
352-7-21	while it is widely conjectured that this property is essential for proving high-probability regret bounds , we show in this paper that it is possible to achieve such strong results without this undesirable exploration component .	such results are rather scarce in the literature since proving them requires a large deal of technical effort and significant modifications to the standard , more intuitive algorithms that come only with guarantees that hold on expectation .	0	3	1	3.898101	-3.5657923	0
352-7-21	such results are rather scarce in the literature since proving them requires a large deal of technical effort and significant modifications to the standard , more intuitive algorithms that come only with guarantees that hold on expectation .	our result relies on a simple and intuitive loss-estimation strategy called implicit exploration ( ix ) that allows a remarkably clean analysis .	1	1	4	-5.9669952	5.184806	1
352-7-21	to demonstrate the flexibility of our technique , we derive several improved high-probability bounds for various extensions of the standard multi-armed bandit framework .	such results are rather scarce in the literature since proving them requires a large deal of technical effort and significant modifications to the standard , more intuitive algorithms that come only with guarantees that hold on expectation .	0	5	1	5.4521832	-4.8642654	0
352-7-21	such results are rather scarce in the literature since proving them requires a large deal of technical effort and significant modifications to the standard , more intuitive algorithms that come only with guarantees that hold on expectation .	finally , we conduct a simple experiment that illustrates the robustness of our implicit exploration technique .	1	1	6	-5.841568	4.945455	1
352-7-21	one of these modifications is forcing the learner to sample arms from the uniform distribution at least ( t ) times over t rounds , which can adversely affect performance if many of the arms are suboptimal .	while it is widely conjectured that this property is essential for proving high-probability regret bounds , we show in this paper that it is possible to achieve such strong results without this undesirable exploration component .	1	2	3	-2.2949343	2.4279587	1
352-7-21	one of these modifications is forcing the learner to sample arms from the uniform distribution at least ( t ) times over t rounds , which can adversely affect performance if many of the arms are suboptimal .	our result relies on a simple and intuitive loss-estimation strategy called implicit exploration ( ix ) that allows a remarkably clean analysis .	1	2	4	-5.7776837	5.215789	1
352-7-21	one of these modifications is forcing the learner to sample arms from the uniform distribution at least ( t ) times over t rounds , which can adversely affect performance if many of the arms are suboptimal .	to demonstrate the flexibility of our technique , we derive several improved high-probability bounds for various extensions of the standard multi-armed bandit framework .	1	2	5	-5.9853497	5.1652374	1
352-7-21	one of these modifications is forcing the learner to sample arms from the uniform distribution at least ( t ) times over t rounds , which can adversely affect performance if many of the arms are suboptimal .	finally , we conduct a simple experiment that illustrates the robustness of our implicit exploration technique .	1	2	6	-5.926675	5.0182166	1
352-7-21	while it is widely conjectured that this property is essential for proving high-probability regret bounds , we show in this paper that it is possible to achieve such strong results without this undesirable exploration component .	our result relies on a simple and intuitive loss-estimation strategy called implicit exploration ( ix ) that allows a remarkably clean analysis .	1	3	4	-5.7590165	5.1562285	1
352-7-21	to demonstrate the flexibility of our technique , we derive several improved high-probability bounds for various extensions of the standard multi-armed bandit framework .	while it is widely conjectured that this property is essential for proving high-probability regret bounds , we show in this paper that it is possible to achieve such strong results without this undesirable exploration component .	0	5	3	4.941511	-4.3717976	0
352-7-21	while it is widely conjectured that this property is essential for proving high-probability regret bounds , we show in this paper that it is possible to achieve such strong results without this undesirable exploration component .	finally , we conduct a simple experiment that illustrates the robustness of our implicit exploration technique .	1	3	6	-5.9625683	5.1011095	1
352-7-21	to demonstrate the flexibility of our technique , we derive several improved high-probability bounds for various extensions of the standard multi-armed bandit framework .	our result relies on a simple and intuitive loss-estimation strategy called implicit exploration ( ix ) that allows a remarkably clean analysis .	0	5	4	3.8477738	-3.5418344	0
352-7-21	finally , we conduct a simple experiment that illustrates the robustness of our implicit exploration technique .	our result relies on a simple and intuitive loss-estimation strategy called implicit exploration ( ix ) that allows a remarkably clean analysis .	0	6	4	5.0906887	-4.433451	0
352-7-21	to demonstrate the flexibility of our technique , we derive several improved high-probability bounds for various extensions of the standard multi-armed bandit framework .	finally , we conduct a simple experiment that illustrates the robustness of our implicit exploration technique .	1	5	6	-4.7563295	4.371648	1
353-7-21	[CLS] we present sla ( streaming low - rank approximation ), an algorithm that is asymptotically accurate, when ksk + 1 ( m ) = o ( mn ) where sk + 1 ( m ) is the ( k + 1 ) - th largest singular value of m	in this paper , we revisit the problem of constructing a near-optimal rank k approximation of a matrix m mxn under the streaming data model where the columns of m are revealed sequentially .	0	1	0	5.4170847	-4.812705	0
353-7-21	in this paper , we revisit the problem of constructing a near-optimal rank k approximation of a matrix m mxn under the streaming data model where the columns of m are revealed sequentially .	[CLS] this means that its average mean - square error converges to 0 as m and n grow large ( k ) and m ( k ) de ( k ) - m ( k ) 2 = o ( mn ) with high probability, where m ( i. e., m f note	1	0	2	-5.8167877	5.16799	1
353-7-21	our algorithm makes one pass on the data if the columns of m are revealed in a random order , and two passes if the columns of m arrive in an arbitrary order .	in this paper , we revisit the problem of constructing a near-optimal rank k approximation of a matrix m mxn under the streaming data model where the columns of m are revealed sequentially .	0	3	0	5.654155	-5.1127696	0
353-7-21	in this paper , we revisit the problem of constructing a near-optimal rank k approximation of a matrix m mxn under the streaming data model where the columns of m are revealed sequentially .	to reduce its memory footprint and complexity , sla uses random sparsification , and samples each entry of m with a small probability .	1	0	4	-5.4415355	4.9885015	1
353-7-21	in this paper , we revisit the problem of constructing a near-optimal rank k approximation of a matrix m mxn under the streaming data model where the columns of m are revealed sequentially .	in turn , sla is memory optimal as its required memory space scales as k ( m+n ) , the dimension of its output .	1	0	5	-5.598992	5.0569725	1
353-7-21	[CLS] furthermore, sla is computationally efficient as it runs in o ( kmn ) time ( a constant number of operations is made for each observed entry of m ), which can be as small as o ( k log ( m ) 4 n ) for an appropriate choice of and if	in this paper , we revisit the problem of constructing a near-optimal rank k approximation of a matrix m mxn under the streaming data model where the columns of m are revealed sequentially .	0	6	0	5.5828657	-4.9364786	0
353-7-21	[CLS] this means that its average mean - square error converges to 0 as m and n grow large ( k ) and m ( k ) de ( k ) - m ( k ) 2 = o ( mn ) with high probability, where m (	[CLS] we present sla ( streaming low - rank approximation ), an algorithm that is asymptotically accurate, when ksk + 1 ( m ) = o ( mn ) where sk + 1 ( m ) is the ( k + 1	0	2	1	4.29714	-3.806314	0
353-7-21	we present sla ( streaming low-rank approximation ) , an algorithm that is asymptotically accurate , when ksk+1 ( m ) = o ( mn ) where sk+1 ( m ) is the ( k + 1 ) -th largest singular value of m .	our algorithm makes one pass on the data if the columns of m are revealed in a random order , and two passes if the columns of m arrive in an arbitrary order .	1	1	3	-5.8482294	5.176601	1
353-7-21	to reduce its memory footprint and complexity , sla uses random sparsification , and samples each entry of m with a small probability .	we present sla ( streaming low-rank approximation ) , an algorithm that is asymptotically accurate , when ksk+1 ( m ) = o ( mn ) where sk+1 ( m ) is the ( k + 1 ) -th largest singular value of m .	0	4	1	3.7248073	-3.3942406	0
353-7-21	we present sla ( streaming low-rank approximation ) , an algorithm that is asymptotically accurate , when ksk+1 ( m ) = o ( mn ) where sk+1 ( m ) is the ( k + 1 ) -th largest singular value of m .	in turn , sla is memory optimal as its required memory space scales as k ( m+n ) , the dimension of its output .	1	1	5	-5.0946884	4.6543765	1
353-7-21	[CLS] we present sla ( streaming low - rank approximation ), an algorithm that is asymptotically accurate, when ksk + 1 ( m ) = o ( mn ) where sk + 1 ( m ) is the ( k + 1	[CLS] furthermore, sla is computationally efficient as it runs in o ( kmn ) time ( a constant number of operations is made for each observed entry of m ), which can be as small as o ( k log ( m ) 4 n	1	1	6	-5.774854	5.1309977	1
353-7-21	our algorithm makes one pass on the data if the columns of m are revealed in a random order , and two passes if the columns of m arrive in an arbitrary order .	[CLS] this means that its average mean - square error converges to 0 as m and n grow large ( k ) and m ( k ) de ( k ) - m ( k ) 2 = o ( mn ) with high probability, where m ( i. e., m f note the output of sla and	0	3	2	-2.7164068	2.6894245	1
353-7-21	[CLS] this means that its average mean - square error converges to 0 as m and n grow large ( k ) and m ( k ) de ( k ) - m ( k ) 2 = o ( mn ) with high probability, where m ( i. e., m f note the output of sla and the optimal rank k approximation of m	to reduce its memory footprint and complexity , sla uses random sparsification , and samples each entry of m with a small probability .	1	2	4	4.2201314	-3.8331723	0
353-7-21	in turn , sla is memory optimal as its required memory space scales as k ( m+n ) , the dimension of its output .	[CLS] this means that its average mean - square error converges to 0 as m and n grow large ( k ) and m ( k ) de ( k ) - m ( k ) 2 = o ( mn ) with high probability, where m ( i. e., m f note the output of sla and the optimal rank k approximation of	0	5	2	-3.685342	3.5632474	1
353-7-21	[CLS] this means that its average mean - square error converges to 0 as m and n grow large ( k ) and m ( k ) de ( k ) - m ( k ) 2 = o ( mn ) with high probability, where m (	[CLS] furthermore, sla is computationally efficient as it runs in o ( kmn ) time ( a constant number of operations is made for each observed entry of m ), which can be as small as o ( k log ( m ) 4 n	1	2	6	-4.8536367	4.5273523	1
353-7-21	our algorithm makes one pass on the data if the columns of m are revealed in a random order , and two passes if the columns of m arrive in an arbitrary order .	to reduce its memory footprint and complexity , sla uses random sparsification , and samples each entry of m with a small probability .	1	3	4	3.422306	-3.1258354	0
353-7-21	in turn , sla is memory optimal as its required memory space scales as k ( m+n ) , the dimension of its output .	our algorithm makes one pass on the data if the columns of m are revealed in a random order , and two passes if the columns of m arrive in an arbitrary order .	0	5	3	-0.35436422	0.5977809	1
353-7-21	our algorithm makes one pass on the data if the columns of m are revealed in a random order , and two passes if the columns of m arrive in an arbitrary order .	furthermore , sla is computationally efficient as it runs in o ( kmn ) time ( a constant number of operations is made for each observed entry of m ) , which can be as small as o ( k log ( m ) 4 n ) for an appropriate choice of and if n m .	1	3	6	-4.3202047	4.0390105	1
353-7-21	to reduce its memory footprint and complexity , sla uses random sparsification , and samples each entry of m with a small probability .	in turn , sla is memory optimal as its required memory space scales as k ( m+n ) , the dimension of its output .	1	4	5	-3.6561632	3.4729776	1
353-7-21	to reduce its memory footprint and complexity , sla uses random sparsification , and samples each entry of m with a small probability .	furthermore , sla is computationally efficient as it runs in o ( kmn ) time ( a constant number of operations is made for each observed entry of m ) , which can be as small as o ( k log ( m ) 4 n ) for an appropriate choice of and if n m .	1	4	6	-5.6339836	5.0626917	1
353-7-21	in turn , sla is memory optimal as its required memory space scales as k ( m+n ) , the dimension of its output .	furthermore , sla is computationally efficient as it runs in o ( kmn ) time ( a constant number of operations is made for each observed entry of m ) , which can be as small as o ( k log ( m ) 4 n ) for an appropriate choice of and if n m .	1	5	6	-4.296837	4.0505075	1
354-5-10	we show pac learnability of influence functions for three common influence models , namely , the linear threshold ( lt ) , independent cascade ( ic ) and voter models , and present concrete sample complexity results in each case .	[CLS] our results for the lt model are based on interesting connections with neural networks ; those for the ic model are based an interpretation of the influence function as an expectation over random draw of a subgraph and use covering number arguments ; and those for the voter model are based on a reduction	1	0	1	-5.9857483	5.1486106	1
354-5-10	we show these results for the case in which the cascades are only partially observed and we do not see the time steps in which a node has been influenced .	we show pac learnability of influence functions for three common influence models , namely , the linear threshold ( lt ) , independent cascade ( ic ) and voter models , and present concrete sample complexity results in each case .	0	2	0	1.8680718	-1.6212871	0
354-5-10	we also provide efficient polynomial time learning algorithms for a setting with full observation , i.e .	we show pac learnability of influence functions for three common influence models , namely , the linear threshold ( lt ) , independent cascade ( ic ) and voter models , and present concrete sample complexity results in each case .	0	3	0	3.4593697	-3.2031083	0
354-5-10	we show pac learnability of influence functions for three common influence models , namely , the linear threshold ( lt ) , independent cascade ( ic ) and voter models , and present concrete sample complexity results in each case .	where the cascades also contain the time steps in which nodes are influenced .	1	0	4	2.4638398	-2.280574	0
354-5-10	we show these results for the case in which the cascades are only partially observed and we do not see the time steps in which a node has been influenced .	our results for the lt model are based on interesting connections with neural networks ; those for the ic model are based an interpretation of the influence function as an expectation over random draw of a subgraph and use covering number arguments ; and those for the voter model are based on a reduction to linear regression .	0	2	1	3.221727	-3.0502477	0
354-5-10	our results for the lt model are based on interesting connections with neural networks ; those for the ic model are based an interpretation of the influence function as an expectation over random draw of a subgraph and use covering number arguments ; and those for the voter model are based on a reduction to linear regression .	we also provide efficient polynomial time learning algorithms for a setting with full observation , i.e .	1	1	3	-3.2446556	3.1929574	1
354-5-10	our results for the lt model are based on interesting connections with neural networks ; those for the ic model are based an interpretation of the influence function as an expectation over random draw of a subgraph and use covering number arguments ; and those for the voter model are based on a reduction to linear regression .	where the cascades also contain the time steps in which nodes are influenced .	1	1	4	3.1074474	-2.9026942	0
354-5-10	we also provide efficient polynomial time learning algorithms for a setting with full observation , i.e .	we show these results for the case in which the cascades are only partially observed and we do not see the time steps in which a node has been influenced .	0	3	2	3.58413	-3.3307881	0
354-5-10	we show these results for the case in which the cascades are only partially observed and we do not see the time steps in which a node has been influenced .	where the cascades also contain the time steps in which nodes are influenced .	1	2	4	2.6217651	-2.47201	0
354-5-10	we also provide efficient polynomial time learning algorithms for a setting with full observation , i.e .	where the cascades also contain the time steps in which nodes are influenced .	1	3	4	-0.77262235	0.94138384	1
355-12-66	the objective is to minimize the number of experiments to discover the causal directions of all the edges in a causal graph .	we consider the problem of learning causal networks with interventions , when each intervention is limited in size under pearl 's structural equation model with independent errors ( sem-ie ) .	0	1	0	5.383373	-4.794595	0
355-12-66	we consider the problem of learning causal networks with interventions , when each intervention is limited in size under pearl 's structural equation model with independent errors ( sem-ie ) .	previous work has focused on the use of separating systems for complete graphs for this task .	1	0	2	-5.0175867	4.56481	1
355-12-66	we prove that any deterministic adaptive algorithm needs to be a separating system in order to learn complete graphs in the worst case .	we consider the problem of learning causal networks with interventions , when each intervention is limited in size under pearl 's structural equation model with independent errors ( sem-ie ) .	0	3	0	5.6386895	-5.0177174	0
355-12-66	we consider the problem of learning causal networks with interventions , when each intervention is limited in size under pearl 's structural equation model with independent errors ( sem-ie ) .	in addition , we present a novel separating system construction , whose size is close to optimal and is arguably simpler than previous work in combinatorics .	1	0	4	-5.910928	5.2024307	1
355-12-66	we also develop a novel information theoretic lower bound on the number of interventions that applies in full generality , including for randomized adaptive learning algorithms .	we consider the problem of learning causal networks with interventions , when each intervention is limited in size under pearl 's structural equation model with independent errors ( sem-ie ) .	0	5	0	5.514246	-4.895441	0
355-12-66	we consider the problem of learning causal networks with interventions , when each intervention is limited in size under pearl 's structural equation model with independent errors ( sem-ie ) .	for general chordal graphs , we derive worst case lower bounds on the number of interventions .	1	0	6	-5.83418	5.1905937	1
355-12-66	we consider the problem of learning causal networks with interventions , when each intervention is limited in size under pearl 's structural equation model with independent errors ( sem-ie ) .	building on observations about induced trees , we give a new deterministic adaptive algorithm to learn directions on any chordal skeleton completely .	1	0	7	-5.897194	5.201294	1
355-12-66	in the worst case , our achievable scheme is an -approximation algorithm where is the independence number of the graph .	we consider the problem of learning causal networks with interventions , when each intervention is limited in size under pearl 's structural equation model with independent errors ( sem-ie ) .	0	8	0	5.6223907	-5.0736856	0
355-12-66	we consider the problem of learning causal networks with interventions , when each intervention is limited in size under pearl 's structural equation model with independent errors ( sem-ie ) .	we also show that there exist graph classes for which the sufficient number of experiments is close to the lower bound .	1	0	9	-5.8719625	5.194597	1
355-12-66	in the other extreme , there are graph classes for which the required number of experiments is multiplicatively away from our lower bound .	we consider the problem of learning causal networks with interventions , when each intervention is limited in size under pearl 's structural equation model with independent errors ( sem-ie ) .	0	10	0	5.5800047	-4.965376	0
355-12-66	in simulations , our algorithm almost always performs very close to the lower bound , while the approach based on separating systems for complete graphs is significantly worse for random chordal graphs .	we consider the problem of learning causal networks with interventions , when each intervention is limited in size under pearl 's structural equation model with independent errors ( sem-ie ) .	0	11	0	5.597452	-5.0107846	0
355-12-66	previous work has focused on the use of separating systems for complete graphs for this task .	the objective is to minimize the number of experiments to discover the causal directions of all the edges in a causal graph .	0	2	1	-5.7453218	5.1552315	1
355-12-66	we prove that any deterministic adaptive algorithm needs to be a separating system in order to learn complete graphs in the worst case .	the objective is to minimize the number of experiments to discover the causal directions of all the edges in a causal graph .	0	3	1	3.3493595	-3.1380625	0
355-12-66	in addition , we present a novel separating system construction , whose size is close to optimal and is arguably simpler than previous work in combinatorics .	the objective is to minimize the number of experiments to discover the causal directions of all the edges in a causal graph .	0	4	1	4.8907967	-4.3563323	0
355-12-66	the objective is to minimize the number of experiments to discover the causal directions of all the edges in a causal graph .	we also develop a novel information theoretic lower bound on the number of interventions that applies in full generality , including for randomized adaptive learning algorithms .	1	1	5	-5.9562497	5.108487	1
355-12-66	for general chordal graphs , we derive worst case lower bounds on the number of interventions .	the objective is to minimize the number of experiments to discover the causal directions of all the edges in a causal graph .	0	6	1	4.6999636	-4.199255	0
355-12-66	the objective is to minimize the number of experiments to discover the causal directions of all the edges in a causal graph .	building on observations about induced trees , we give a new deterministic adaptive algorithm to learn directions on any chordal skeleton completely .	1	1	7	-5.648567	5.0846515	1
355-12-66	the objective is to minimize the number of experiments to discover the causal directions of all the edges in a causal graph .	in the worst case , our achievable scheme is an -approximation algorithm where is the independence number of the graph .	1	1	8	-5.8509493	5.16512	1
355-12-66	the objective is to minimize the number of experiments to discover the causal directions of all the edges in a causal graph .	we also show that there exist graph classes for which the sufficient number of experiments is close to the lower bound .	1	1	9	-5.940487	5.0797253	1
355-12-66	in the other extreme , there are graph classes for which the required number of experiments is multiplicatively away from our lower bound .	the objective is to minimize the number of experiments to discover the causal directions of all the edges in a causal graph .	0	10	1	5.1925974	-4.5245814	0
355-12-66	in simulations , our algorithm almost always performs very close to the lower bound , while the approach based on separating systems for complete graphs is significantly worse for random chordal graphs .	the objective is to minimize the number of experiments to discover the causal directions of all the edges in a causal graph .	0	11	1	4.8722134	-4.3415527	0
355-12-66	previous work has focused on the use of separating systems for complete graphs for this task .	we prove that any deterministic adaptive algorithm needs to be a separating system in order to learn complete graphs in the worst case .	1	2	3	-5.9878664	5.1931205	1
355-12-66	previous work has focused on the use of separating systems for complete graphs for this task .	in addition , we present a novel separating system construction , whose size is close to optimal and is arguably simpler than previous work in combinatorics .	1	2	4	-5.983265	5.1146135	1
355-12-66	we also develop a novel information theoretic lower bound on the number of interventions that applies in full generality , including for randomized adaptive learning algorithms .	previous work has focused on the use of separating systems for complete graphs for this task .	0	5	2	5.491181	-4.846168	0
355-12-66	previous work has focused on the use of separating systems for complete graphs for this task .	for general chordal graphs , we derive worst case lower bounds on the number of interventions .	1	2	6	-5.9628344	5.1932316	1
355-12-66	building on observations about induced trees , we give a new deterministic adaptive algorithm to learn directions on any chordal skeleton completely .	previous work has focused on the use of separating systems for complete graphs for this task .	0	7	2	5.5669594	-4.915246	0
355-12-66	previous work has focused on the use of separating systems for complete graphs for this task .	in the worst case , our achievable scheme is an -approximation algorithm where is the independence number of the graph .	1	2	8	-5.9967203	5.1800213	1
355-12-66	we also show that there exist graph classes for which the sufficient number of experiments is close to the lower bound .	previous work has focused on the use of separating systems for complete graphs for this task .	0	9	2	5.527088	-4.8893566	0
355-12-66	in the other extreme , there are graph classes for which the required number of experiments is multiplicatively away from our lower bound .	previous work has focused on the use of separating systems for complete graphs for this task .	0	10	2	5.4934974	-4.7789016	0
355-12-66	previous work has focused on the use of separating systems for complete graphs for this task .	in simulations , our algorithm almost always performs very close to the lower bound , while the approach based on separating systems for complete graphs is significantly worse for random chordal graphs .	1	2	11	-5.93267	5.184699	1
355-12-66	in addition , we present a novel separating system construction , whose size is close to optimal and is arguably simpler than previous work in combinatorics .	we prove that any deterministic adaptive algorithm needs to be a separating system in order to learn complete graphs in the worst case .	0	4	3	5.073658	-4.483703	0
355-12-66	we also develop a novel information theoretic lower bound on the number of interventions that applies in full generality , including for randomized adaptive learning algorithms .	we prove that any deterministic adaptive algorithm needs to be a separating system in order to learn complete graphs in the worst case .	0	5	3	4.6605988	-4.1756454	0
355-12-66	for general chordal graphs , we derive worst case lower bounds on the number of interventions .	we prove that any deterministic adaptive algorithm needs to be a separating system in order to learn complete graphs in the worst case .	0	6	3	2.814649	-2.607646	0
355-12-66	building on observations about induced trees , we give a new deterministic adaptive algorithm to learn directions on any chordal skeleton completely .	we prove that any deterministic adaptive algorithm needs to be a separating system in order to learn complete graphs in the worst case .	0	7	3	-0.2519083	0.51661307	1
355-12-66	in the worst case , our achievable scheme is an -approximation algorithm where is the independence number of the graph .	we prove that any deterministic adaptive algorithm needs to be a separating system in order to learn complete graphs in the worst case .	0	8	3	2.2136402	-2.077692	0
355-12-66	we prove that any deterministic adaptive algorithm needs to be a separating system in order to learn complete graphs in the worst case .	we also show that there exist graph classes for which the sufficient number of experiments is close to the lower bound .	1	3	9	-5.633901	5.0487347	1
355-12-66	in the other extreme , there are graph classes for which the required number of experiments is multiplicatively away from our lower bound .	we prove that any deterministic adaptive algorithm needs to be a separating system in order to learn complete graphs in the worst case .	0	10	3	3.7734084	-3.4869113	0
355-12-66	in simulations , our algorithm almost always performs very close to the lower bound , while the approach based on separating systems for complete graphs is significantly worse for random chordal graphs .	we prove that any deterministic adaptive algorithm needs to be a separating system in order to learn complete graphs in the worst case .	0	11	3	4.4284897	-4.014703	0
355-12-66	we also develop a novel information theoretic lower bound on the number of interventions that applies in full generality , including for randomized adaptive learning algorithms .	in addition , we present a novel separating system construction , whose size is close to optimal and is arguably simpler than previous work in combinatorics .	0	5	4	2.481411	-2.3350217	0
355-12-66	in addition , we present a novel separating system construction , whose size is close to optimal and is arguably simpler than previous work in combinatorics .	for general chordal graphs , we derive worst case lower bounds on the number of interventions .	1	4	6	3.1793306	-3.010882	0
355-12-66	building on observations about induced trees , we give a new deterministic adaptive algorithm to learn directions on any chordal skeleton completely .	in addition , we present a novel separating system construction , whose size is close to optimal and is arguably simpler than previous work in combinatorics .	0	7	4	-4.1340795	3.8591156	1
355-12-66	in addition , we present a novel separating system construction , whose size is close to optimal and is arguably simpler than previous work in combinatorics .	in the worst case , our achievable scheme is an -approximation algorithm where is the independence number of the graph .	1	4	8	2.5502107	-2.4764519	0
355-12-66	we also show that there exist graph classes for which the sufficient number of experiments is close to the lower bound .	in addition , we present a novel separating system construction , whose size is close to optimal and is arguably simpler than previous work in combinatorics .	0	9	4	2.2756011	-2.117622	0
355-12-66	in the other extreme , there are graph classes for which the required number of experiments is multiplicatively away from our lower bound .	in addition , we present a novel separating system construction , whose size is close to optimal and is arguably simpler than previous work in combinatorics .	0	10	4	-2.0139456	2.1241856	1
355-12-66	in addition , we present a novel separating system construction , whose size is close to optimal and is arguably simpler than previous work in combinatorics .	in simulations , our algorithm almost always performs very close to the lower bound , while the approach based on separating systems for complete graphs is significantly worse for random chordal graphs .	1	4	11	1.7089417	-1.6403104	0
355-12-66	for general chordal graphs , we derive worst case lower bounds on the number of interventions .	we also develop a novel information theoretic lower bound on the number of interventions that applies in full generality , including for randomized adaptive learning algorithms .	0	6	5	-5.239042	4.778057	1
355-12-66	building on observations about induced trees , we give a new deterministic adaptive algorithm to learn directions on any chordal skeleton completely .	we also develop a novel information theoretic lower bound on the number of interventions that applies in full generality , including for randomized adaptive learning algorithms .	0	7	5	-5.0504017	4.612064	1
355-12-66	in the worst case , our achievable scheme is an -approximation algorithm where is the independence number of the graph .	we also develop a novel information theoretic lower bound on the number of interventions that applies in full generality , including for randomized adaptive learning algorithms .	0	8	5	-4.5605745	4.3046703	1
355-12-66	we also show that there exist graph classes for which the sufficient number of experiments is close to the lower bound .	we also develop a novel information theoretic lower bound on the number of interventions that applies in full generality , including for randomized adaptive learning algorithms .	0	9	5	3.1463666	-2.9374785	0
355-12-66	in the other extreme , there are graph classes for which the required number of experiments is multiplicatively away from our lower bound .	we also develop a novel information theoretic lower bound on the number of interventions that applies in full generality , including for randomized adaptive learning algorithms .	0	10	5	-1.0192099	1.2313571	1
355-12-66	we also develop a novel information theoretic lower bound on the number of interventions that applies in full generality , including for randomized adaptive learning algorithms .	in simulations , our algorithm almost always performs very close to the lower bound , while the approach based on separating systems for complete graphs is significantly worse for random chordal graphs .	1	5	11	-3.4339573	3.260767	1
355-12-66	building on observations about induced trees , we give a new deterministic adaptive algorithm to learn directions on any chordal skeleton completely .	for general chordal graphs , we derive worst case lower bounds on the number of interventions .	0	7	6	-0.1366331	0.4641841	1
355-12-66	for general chordal graphs , we derive worst case lower bounds on the number of interventions .	in the worst case , our achievable scheme is an -approximation algorithm where is the independence number of the graph .	1	6	8	-3.0098193	3.054754	1
355-12-66	for general chordal graphs , we derive worst case lower bounds on the number of interventions .	we also show that there exist graph classes for which the sufficient number of experiments is close to the lower bound .	1	6	9	-5.56385	4.915154	1
355-12-66	in the other extreme , there are graph classes for which the required number of experiments is multiplicatively away from our lower bound .	for general chordal graphs , we derive worst case lower bounds on the number of interventions .	0	10	6	3.845879	-3.5384762	0
355-12-66	in simulations , our algorithm almost always performs very close to the lower bound , while the approach based on separating systems for complete graphs is significantly worse for random chordal graphs .	for general chordal graphs , we derive worst case lower bounds on the number of interventions .	0	11	6	3.7188306	-3.4380386	0
355-12-66	in the worst case , our achievable scheme is an -approximation algorithm where is the independence number of the graph .	building on observations about induced trees , we give a new deterministic adaptive algorithm to learn directions on any chordal skeleton completely .	0	8	7	-0.88898504	1.0948339	1
355-12-66	building on observations about induced trees , we give a new deterministic adaptive algorithm to learn directions on any chordal skeleton completely .	we also show that there exist graph classes for which the sufficient number of experiments is close to the lower bound .	1	7	9	-5.248151	4.7266946	1
355-12-66	building on observations about induced trees , we give a new deterministic adaptive algorithm to learn directions on any chordal skeleton completely .	in the other extreme , there are graph classes for which the required number of experiments is multiplicatively away from our lower bound .	1	7	10	-3.6886725	3.5096874	1
355-12-66	in simulations , our algorithm almost always performs very close to the lower bound , while the approach based on separating systems for complete graphs is significantly worse for random chordal graphs .	building on observations about induced trees , we give a new deterministic adaptive algorithm to learn directions on any chordal skeleton completely .	0	11	7	3.2584648	-2.9820015	0
355-12-66	in the worst case , our achievable scheme is an -approximation algorithm where is the independence number of the graph .	we also show that there exist graph classes for which the sufficient number of experiments is close to the lower bound .	1	8	9	-4.46299	4.188262	1
355-12-66	in the other extreme , there are graph classes for which the required number of experiments is multiplicatively away from our lower bound .	in the worst case , our achievable scheme is an -approximation algorithm where is the independence number of the graph .	0	10	8	1.657054	-1.587451	0
355-12-66	in simulations , our algorithm almost always performs very close to the lower bound , while the approach based on separating systems for complete graphs is significantly worse for random chordal graphs .	in the worst case , our achievable scheme is an -approximation algorithm where is the independence number of the graph .	0	11	8	1.4780352	-1.2954221	0
355-12-66	in the other extreme , there are graph classes for which the required number of experiments is multiplicatively away from our lower bound .	we also show that there exist graph classes for which the sufficient number of experiments is close to the lower bound .	0	10	9	-3.3206105	3.1376786	1
355-12-66	we also show that there exist graph classes for which the sufficient number of experiments is close to the lower bound .	in simulations , our algorithm almost always performs very close to the lower bound , while the approach based on separating systems for complete graphs is significantly worse for random chordal graphs .	1	9	11	3.2382152	-3.0732682	0
355-12-66	in the other extreme , there are graph classes for which the required number of experiments is multiplicatively away from our lower bound .	in simulations , our algorithm almost always performs very close to the lower bound , while the approach based on separating systems for complete graphs is significantly worse for random chordal graphs .	1	10	11	1.0218296	-0.750721	0
356-3-3	in particular , for a given function x f ( x ) we consider optimization when one is given access to absolute error oracles that return values in or relative error oracles that return value in , for some > 0 .	we consider the problem of optimizing convex and concave functions with access to an erroneous zeroth-order oracle .	0	1	0	4.8126583	-4.306674	0
356-3-3	we consider the problem of optimizing convex and concave functions with access to an erroneous zeroth-order oracle .	we show stark information theoretic impossibility results for minimizing convex functions and maximizing concave functions over polytopes in this model .	1	0	2	-5.9059267	5.1965914	1
356-3-3	in particular , for a given function x f ( x ) we consider optimization when one is given access to absolute error oracles that return values in or relative error oracles that return value in , for some > 0 .	we show stark information theoretic impossibility results for minimizing convex functions and maximizing concave functions over polytopes in this model .	1	1	2	-5.9686766	5.2164817	1
357-9-36	we propose a novel distribution that generalizes the multinomial distribution to enable dependencies between dimensions .	our novel distribution is based on the parametric form of the poisson mrf model but is fundamentally different because of the domain restriction to a fixed-length vector like in a multinomial where the number of trials is fixed or known .	1	0	1	-5.5624485	5.1191416	1
357-9-36	thus , we propose the fixed-length poisson mrf ( lpmrf ) distribution .	we propose a novel distribution that generalizes the multinomial distribution to enable dependencies between dimensions .	0	2	0	4.1007595	-3.6596456	0
357-9-36	we develop ais sampling methods to estimate the likelihood and log partition function ( i.e .	we propose a novel distribution that generalizes the multinomial distribution to enable dependencies between dimensions .	0	3	0	-2.8533926	2.8494096	1
357-9-36	the log normalizing constant ) , which was not developed for the poisson mrf model .	we propose a novel distribution that generalizes the multinomial distribution to enable dependencies between dimensions .	0	4	0	-2.284683	2.3920817	1
357-9-36	in addition , we propose novel mixture and topic models that use lpmrf as a base distribution and discuss the similarities and differences with previous topic models such as the recently proposed admixture of poisson mrfs .	we propose a novel distribution that generalizes the multinomial distribution to enable dependencies between dimensions .	0	5	0	4.984977	-4.3761272	0
357-9-36	we show the effectiveness of our lpmrf distribution over multinomial models by evaluating the test set perplexity on a dataset of abstracts and wikipedia .	we propose a novel distribution that generalizes the multinomial distribution to enable dependencies between dimensions .	0	6	0	5.5457	-4.881576	0
357-9-36	qualitatively , we show that the positive dependencies discovered by lpmrf are interesting and intuitive .	we propose a novel distribution that generalizes the multinomial distribution to enable dependencies between dimensions .	0	7	0	5.4487305	-4.809339	0
357-9-36	finally , we show that our algorithms are fast and have good scaling ( code available online ) .	we propose a novel distribution that generalizes the multinomial distribution to enable dependencies between dimensions .	0	8	0	5.398368	-4.7475233	0
357-9-36	our novel distribution is based on the parametric form of the poisson mrf model but is fundamentally different because of the domain restriction to a fixed-length vector like in a multinomial where the number of trials is fixed or known .	thus , we propose the fixed-length poisson mrf ( lpmrf ) distribution .	1	1	2	-0.7736894	1.0497298	1
357-9-36	our novel distribution is based on the parametric form of the poisson mrf model but is fundamentally different because of the domain restriction to a fixed-length vector like in a multinomial where the number of trials is fixed or known .	we develop ais sampling methods to estimate the likelihood and log partition function ( i.e .	1	1	3	3.779048	-3.4121702	0
357-9-36	the log normalizing constant ) , which was not developed for the poisson mrf model .	our novel distribution is based on the parametric form of the poisson mrf model but is fundamentally different because of the domain restriction to a fixed-length vector like in a multinomial where the number of trials is fixed or known .	0	4	1	-3.6212368	3.4451766	1
357-9-36	our novel distribution is based on the parametric form of the poisson mrf model but is fundamentally different because of the domain restriction to a fixed-length vector like in a multinomial where the number of trials is fixed or known .	in addition , we propose novel mixture and topic models that use lpmrf as a base distribution and discuss the similarities and differences with previous topic models such as the recently proposed admixture of poisson mrfs .	1	1	5	-5.5197496	4.935138	1
357-9-36	we show the effectiveness of our lpmrf distribution over multinomial models by evaluating the test set perplexity on a dataset of abstracts and wikipedia .	our novel distribution is based on the parametric form of the poisson mrf model but is fundamentally different because of the domain restriction to a fixed-length vector like in a multinomial where the number of trials is fixed or known .	0	6	1	5.1399803	-4.5011683	0
357-9-36	our novel distribution is based on the parametric form of the poisson mrf model but is fundamentally different because of the domain restriction to a fixed-length vector like in a multinomial where the number of trials is fixed or known .	qualitatively , we show that the positive dependencies discovered by lpmrf are interesting and intuitive .	1	1	7	-5.8797565	5.109094	1
357-9-36	our novel distribution is based on the parametric form of the poisson mrf model but is fundamentally different because of the domain restriction to a fixed-length vector like in a multinomial where the number of trials is fixed or known .	finally , we show that our algorithms are fast and have good scaling ( code available online ) .	1	1	8	-5.935746	5.0871716	1
357-9-36	thus , we propose the fixed-length poisson mrf ( lpmrf ) distribution .	we develop ais sampling methods to estimate the likelihood and log partition function ( i.e .	1	2	3	4.333655	-3.8388565	0
357-9-36	the log normalizing constant ) , which was not developed for the poisson mrf model .	thus , we propose the fixed-length poisson mrf ( lpmrf ) distribution .	0	4	2	-4.1188374	3.940864	1
357-9-36	thus , we propose the fixed-length poisson mrf ( lpmrf ) distribution .	in addition , we propose novel mixture and topic models that use lpmrf as a base distribution and discuss the similarities and differences with previous topic models such as the recently proposed admixture of poisson mrfs .	1	2	5	-5.9233627	5.0375032	1
357-9-36	thus , we propose the fixed-length poisson mrf ( lpmrf ) distribution .	we show the effectiveness of our lpmrf distribution over multinomial models by evaluating the test set perplexity on a dataset of abstracts and wikipedia .	1	2	6	-5.947981	5.061515	1
357-9-36	qualitatively , we show that the positive dependencies discovered by lpmrf are interesting and intuitive .	thus , we propose the fixed-length poisson mrf ( lpmrf ) distribution .	0	7	2	5.5465174	-4.901752	0
357-9-36	thus , we propose the fixed-length poisson mrf ( lpmrf ) distribution .	finally , we show that our algorithms are fast and have good scaling ( code available online ) .	1	2	8	-5.868588	5.0794992	1
357-9-36	the log normalizing constant ) , which was not developed for the poisson mrf model .	we develop ais sampling methods to estimate the likelihood and log partition function ( i.e .	0	4	3	1.4481633	-1.1211743	0
357-9-36	we develop ais sampling methods to estimate the likelihood and log partition function ( i.e .	in addition , we propose novel mixture and topic models that use lpmrf as a base distribution and discuss the similarities and differences with previous topic models such as the recently proposed admixture of poisson mrfs .	1	3	5	-5.908142	5.053673	1
357-9-36	we develop ais sampling methods to estimate the likelihood and log partition function ( i.e .	we show the effectiveness of our lpmrf distribution over multinomial models by evaluating the test set perplexity on a dataset of abstracts and wikipedia .	1	3	6	-5.992015	5.2136393	1
357-9-36	qualitatively , we show that the positive dependencies discovered by lpmrf are interesting and intuitive .	we develop ais sampling methods to estimate the likelihood and log partition function ( i.e .	0	7	3	5.3973103	-4.803354	0
357-9-36	we develop ais sampling methods to estimate the likelihood and log partition function ( i.e .	finally , we show that our algorithms are fast and have good scaling ( code available online ) .	1	3	8	-5.974475	5.103628	1
357-9-36	the log normalizing constant ) , which was not developed for the poisson mrf model .	in addition , we propose novel mixture and topic models that use lpmrf as a base distribution and discuss the similarities and differences with previous topic models such as the recently proposed admixture of poisson mrfs .	1	4	5	-5.7528725	5.066759	1
357-9-36	we show the effectiveness of our lpmrf distribution over multinomial models by evaluating the test set perplexity on a dataset of abstracts and wikipedia .	the log normalizing constant ) , which was not developed for the poisson mrf model .	0	6	4	5.148981	-4.5872536	0
357-9-36	qualitatively , we show that the positive dependencies discovered by lpmrf are interesting and intuitive .	the log normalizing constant ) , which was not developed for the poisson mrf model .	0	7	4	4.9229794	-4.4993057	0
357-9-36	finally , we show that our algorithms are fast and have good scaling ( code available online ) .	the log normalizing constant ) , which was not developed for the poisson mrf model .	0	8	4	5.1819625	-4.5900717	0
357-9-36	we show the effectiveness of our lpmrf distribution over multinomial models by evaluating the test set perplexity on a dataset of abstracts and wikipedia .	in addition , we propose novel mixture and topic models that use lpmrf as a base distribution and discuss the similarities and differences with previous topic models such as the recently proposed admixture of poisson mrfs .	0	6	5	2.5959463	-2.3077536	0
357-9-36	qualitatively , we show that the positive dependencies discovered by lpmrf are interesting and intuitive .	in addition , we propose novel mixture and topic models that use lpmrf as a base distribution and discuss the similarities and differences with previous topic models such as the recently proposed admixture of poisson mrfs .	0	7	5	-0.38535643	0.6350604	1
357-9-36	in addition , we propose novel mixture and topic models that use lpmrf as a base distribution and discuss the similarities and differences with previous topic models such as the recently proposed admixture of poisson mrfs .	finally , we show that our algorithms are fast and have good scaling ( code available online ) .	1	5	8	-5.040408	4.611059	1
357-9-36	qualitatively , we show that the positive dependencies discovered by lpmrf are interesting and intuitive .	we show the effectiveness of our lpmrf distribution over multinomial models by evaluating the test set perplexity on a dataset of abstracts and wikipedia .	0	7	6	2.615615	-2.4656196	0
357-9-36	finally , we show that our algorithms are fast and have good scaling ( code available online ) .	we show the effectiveness of our lpmrf distribution over multinomial models by evaluating the test set perplexity on a dataset of abstracts and wikipedia .	0	8	6	3.8060775	-3.5323172	0
357-9-36	finally , we show that our algorithms are fast and have good scaling ( code available online ) .	qualitatively , we show that the positive dependencies discovered by lpmrf are interesting and intuitive .	0	8	7	3.6540244	-3.4134996	0
358-7-21	we present a scalable bayesian multi-label learning model based on learning lowdimensional label embeddings .	our model assumes that each label vector is generated as a weighted combination of a set of topics ( each topic being a distribution over labels ) , where the combination weights ( i.e. , the embeddings ) for each label vector are conditioned on the observed feature vector .	1	0	1	-5.6927567	5.1539564	1
358-7-21	this construction , coupled with a bernoulli-poisson link function for each label of the binary label vector , leads to a model with a computational cost that scales in the number of positive labels in the label matrix .	we present a scalable bayesian multi-label learning model based on learning lowdimensional label embeddings .	0	2	0	5.2005634	-4.614622	0
358-7-21	we present a scalable bayesian multi-label learning model based on learning lowdimensional label embeddings .	this makes the model particularly appealing for real-world multi-label learning problems where the label matrix is usually very massive but highly sparse .	1	0	3	-5.9926524	5.1272664	1
358-7-21	we present a scalable bayesian multi-label learning model based on learning lowdimensional label embeddings .	using a data-augmentation strategy leads to full local conjugacy in our model , facilitating simple and very efficient gibbs sampling , as well as an expectation maximization algorithm for inference .	1	0	4	-5.90769	5.240081	1
358-7-21	we present a scalable bayesian multi-label learning model based on learning lowdimensional label embeddings .	also , predicting the label vector at test time does not require doing an inference for the label embeddings and can be done in closed form .	1	0	5	-5.5875463	5.0352654	1
358-7-21	we present a scalable bayesian multi-label learning model based on learning lowdimensional label embeddings .	we report results on several benchmark data sets , comparing our model with various state-of-the art methods .	1	0	6	-5.9994564	5.1931715	1
358-7-21	this construction , coupled with a bernoulli-poisson link function for each label of the binary label vector , leads to a model with a computational cost that scales in the number of positive labels in the label matrix .	[CLS] our model assumes that each label vector is generated as a weighted combination of a set of topics ( each topic being a distribution over labels ), where the combination weights ( i. e., the embeddings ) for each label vector are conditioned on the observed	0	2	1	3.1811504	-2.9507427	0
358-7-21	this makes the model particularly appealing for real-world multi-label learning problems where the label matrix is usually very massive but highly sparse .	our model assumes that each label vector is generated as a weighted combination of a set of topics ( each topic being a distribution over labels ) , where the combination weights ( i.e. , the embeddings ) for each label vector are conditioned on the observed feature vector .	0	3	1	5.175026	-4.4899397	0
358-7-21	using a data-augmentation strategy leads to full local conjugacy in our model , facilitating simple and very efficient gibbs sampling , as well as an expectation maximization algorithm for inference .	our model assumes that each label vector is generated as a weighted combination of a set of topics ( each topic being a distribution over labels ) , where the combination weights ( i.e. , the embeddings ) for each label vector are conditioned on the observed feature vector .	0	4	1	5.12749	-4.4537354	0
358-7-21	also , predicting the label vector at test time does not require doing an inference for the label embeddings and can be done in closed form .	our model assumes that each label vector is generated as a weighted combination of a set of topics ( each topic being a distribution over labels ) , where the combination weights ( i.e. , the embeddings ) for each label vector are conditioned on the observed feature vector .	0	5	1	4.9175034	-4.3767967	0
358-7-21	our model assumes that each label vector is generated as a weighted combination of a set of topics ( each topic being a distribution over labels ) , where the combination weights ( i.e. , the embeddings ) for each label vector are conditioned on the observed feature vector .	we report results on several benchmark data sets , comparing our model with various state-of-the art methods .	1	1	6	-5.9647727	5.0868654	1
358-7-21	this makes the model particularly appealing for real-world multi-label learning problems where the label matrix is usually very massive but highly sparse .	this construction , coupled with a bernoulli-poisson link function for each label of the binary label vector , leads to a model with a computational cost that scales in the number of positive labels in the label matrix .	0	3	2	4.814793	-4.343832	0
358-7-21	this construction , coupled with a bernoulli-poisson link function for each label of the binary label vector , leads to a model with a computational cost that scales in the number of positive labels in the label matrix .	using a data-augmentation strategy leads to full local conjugacy in our model , facilitating simple and very efficient gibbs sampling , as well as an expectation maximization algorithm for inference .	1	2	4	-5.4768233	4.9885035	1
358-7-21	also , predicting the label vector at test time does not require doing an inference for the label embeddings and can be done in closed form .	this construction , coupled with a bernoulli-poisson link function for each label of the binary label vector , leads to a model with a computational cost that scales in the number of positive labels in the label matrix .	0	5	2	3.720758	-3.4684052	0
358-7-21	we report results on several benchmark data sets , comparing our model with various state-of-the art methods .	this construction , coupled with a bernoulli-poisson link function for each label of the binary label vector , leads to a model with a computational cost that scales in the number of positive labels in the label matrix .	0	6	2	5.470644	-4.8524566	0
358-7-21	using a data-augmentation strategy leads to full local conjugacy in our model , facilitating simple and very efficient gibbs sampling , as well as an expectation maximization algorithm for inference .	this makes the model particularly appealing for real-world multi-label learning problems where the label matrix is usually very massive but highly sparse .	0	4	3	-3.8989124	3.7046244	1
358-7-21	also , predicting the label vector at test time does not require doing an inference for the label embeddings and can be done in closed form .	this makes the model particularly appealing for real-world multi-label learning problems where the label matrix is usually very massive but highly sparse .	0	5	3	-4.417871	4.1682553	1
358-7-21	we report results on several benchmark data sets , comparing our model with various state-of-the art methods .	this makes the model particularly appealing for real-world multi-label learning problems where the label matrix is usually very massive but highly sparse .	0	6	3	-2.1525083	2.1929593	1
358-7-21	using a data-augmentation strategy leads to full local conjugacy in our model , facilitating simple and very efficient gibbs sampling , as well as an expectation maximization algorithm for inference .	also , predicting the label vector at test time does not require doing an inference for the label embeddings and can be done in closed form .	1	4	5	2.482882	-2.2804868	0
358-7-21	using a data-augmentation strategy leads to full local conjugacy in our model , facilitating simple and very efficient gibbs sampling , as well as an expectation maximization algorithm for inference .	we report results on several benchmark data sets , comparing our model with various state-of-the art methods .	1	4	6	-4.8181167	4.4992194	1
358-7-21	we report results on several benchmark data sets , comparing our model with various state-of-the art methods .	also , predicting the label vector at test time does not require doing an inference for the label embeddings and can be done in closed form .	0	6	5	4.639395	-4.217582	0
359-9-36	in the blbf setting , the learner does not receive full-information feedback like in supervised learning , but observes feedback only for the actions taken by a historical policy .	this paper identifies a severe problem of the counterfactual risk estimator typically used in batch learning from logged bandit feedback ( blbf ) , and proposes the use of an alternative estimator that avoids this problem .	0	1	0	5.3581433	-4.747476	0
359-9-36	this makes blbf algorithms particularly attractive for training online systems ( e.g. , ad placement , web search , recommendation ) using their historical logs .	this paper identifies a severe problem of the counterfactual risk estimator typically used in batch learning from logged bandit feedback ( blbf ) , and proposes the use of an alternative estimator that avoids this problem .	0	2	0	5.546296	-4.9202394	0
359-9-36	the counterfactual risk minimization ( crm ) principle offers a general recipe for designing blbf algorithms .	this paper identifies a severe problem of the counterfactual risk estimator typically used in batch learning from logged bandit feedback ( blbf ) , and proposes the use of an alternative estimator that avoids this problem .	0	3	0	5.1322503	-4.568671	0
359-9-36	this paper identifies a severe problem of the counterfactual risk estimator typically used in batch learning from logged bandit feedback ( blbf ) , and proposes the use of an alternative estimator that avoids this problem .	it requires a counterfactual risk estimator , and virtually all existing works on blbf have focused on a particular unbiased estimator .	1	0	4	-4.4506273	4.125	1
359-9-36	this paper identifies a severe problem of the counterfactual risk estimator typically used in batch learning from logged bandit feedback ( blbf ) , and proposes the use of an alternative estimator that avoids this problem .	we show that this conventional estimator suffers from a propensity overfitting problem when used for learning over complex hypothesis spaces .	1	0	5	-4.8239017	4.541245	1
359-9-36	we propose to replace the risk estimator with a self-normalized estimator , showing that it neatly avoids this problem .	this paper identifies a severe problem of the counterfactual risk estimator typically used in batch learning from logged bandit feedback ( blbf ) , and proposes the use of an alternative estimator that avoids this problem .	0	6	0	5.34576	-4.7999744	0
359-9-36	this naturally gives rise to a new learning algorithm - normalized policy optimizer for exponential models ( norm-poem ) - for structured output prediction using linear rules .	this paper identifies a severe problem of the counterfactual risk estimator typically used in batch learning from logged bandit feedback ( blbf ) , and proposes the use of an alternative estimator that avoids this problem .	0	7	0	5.165208	-4.646514	0
359-9-36	we evaluate the empirical effectiveness of normpoem on several multi-label classification problems , finding that it consistently outperforms the conventional estimator .	this paper identifies a severe problem of the counterfactual risk estimator typically used in batch learning from logged bandit feedback ( blbf ) , and proposes the use of an alternative estimator that avoids this problem .	0	8	0	5.6824474	-5.0920396	0
359-9-36	in the blbf setting , the learner does not receive full-information feedback like in supervised learning , but observes feedback only for the actions taken by a historical policy .	this makes blbf algorithms particularly attractive for training online systems ( e.g. , ad placement , web search , recommendation ) using their historical logs .	1	1	2	-5.3650675	4.8982897	1
359-9-36	in the blbf setting , the learner does not receive full-information feedback like in supervised learning , but observes feedback only for the actions taken by a historical policy .	the counterfactual risk minimization ( crm ) principle offers a general recipe for designing blbf algorithms .	1	1	3	-2.8125315	2.9220831	1
359-9-36	in the blbf setting , the learner does not receive full-information feedback like in supervised learning , but observes feedback only for the actions taken by a historical policy .	it requires a counterfactual risk estimator , and virtually all existing works on blbf have focused on a particular unbiased estimator .	1	1	4	-2.4560912	2.5152133	1
359-9-36	in the blbf setting , the learner does not receive full-information feedback like in supervised learning , but observes feedback only for the actions taken by a historical policy .	we show that this conventional estimator suffers from a propensity overfitting problem when used for learning over complex hypothesis spaces .	1	1	5	-3.0893896	3.0703897	1
359-9-36	we propose to replace the risk estimator with a self-normalized estimator , showing that it neatly avoids this problem .	in the blbf setting , the learner does not receive full-information feedback like in supervised learning , but observes feedback only for the actions taken by a historical policy .	0	6	1	3.8309777	-3.5661821	0
359-9-36	in the blbf setting , the learner does not receive full-information feedback like in supervised learning , but observes feedback only for the actions taken by a historical policy .	this naturally gives rise to a new learning algorithm - normalized policy optimizer for exponential models ( norm-poem ) - for structured output prediction using linear rules .	1	1	7	-5.904707	5.1612644	1
359-9-36	in the blbf setting , the learner does not receive full-information feedback like in supervised learning , but observes feedback only for the actions taken by a historical policy .	we evaluate the empirical effectiveness of normpoem on several multi-label classification problems , finding that it consistently outperforms the conventional estimator .	1	1	8	-5.9845743	5.234448	1
359-9-36	the counterfactual risk minimization ( crm ) principle offers a general recipe for designing blbf algorithms .	this makes blbf algorithms particularly attractive for training online systems ( e.g. , ad placement , web search , recommendation ) using their historical logs .	0	3	2	-5.062759	4.6571913	1
359-9-36	this makes blbf algorithms particularly attractive for training online systems ( e.g. , ad placement , web search , recommendation ) using their historical logs .	it requires a counterfactual risk estimator , and virtually all existing works on blbf have focused on a particular unbiased estimator .	1	2	4	4.2494855	-3.7840626	0
359-9-36	we show that this conventional estimator suffers from a propensity overfitting problem when used for learning over complex hypothesis spaces .	this makes blbf algorithms particularly attractive for training online systems ( e.g. , ad placement , web search , recommendation ) using their historical logs .	0	5	2	-3.9532135	3.8454247	1
359-9-36	this makes blbf algorithms particularly attractive for training online systems ( e.g. , ad placement , web search , recommendation ) using their historical logs .	we propose to replace the risk estimator with a self-normalized estimator , showing that it neatly avoids this problem .	1	2	6	-2.3878717	2.5035539	1
359-9-36	this naturally gives rise to a new learning algorithm - normalized policy optimizer for exponential models ( norm-poem ) - for structured output prediction using linear rules .	this makes blbf algorithms particularly attractive for training online systems ( e.g. , ad placement , web search , recommendation ) using their historical logs .	0	7	2	1.1712155	-0.9823592	0
359-9-36	this makes blbf algorithms particularly attractive for training online systems ( e.g. , ad placement , web search , recommendation ) using their historical logs .	we evaluate the empirical effectiveness of normpoem on several multi-label classification problems , finding that it consistently outperforms the conventional estimator .	1	2	8	-5.6058373	5.0999227	1
359-9-36	the counterfactual risk minimization ( crm ) principle offers a general recipe for designing blbf algorithms .	it requires a counterfactual risk estimator , and virtually all existing works on blbf have focused on a particular unbiased estimator .	1	3	4	2.5014527	-2.147661	0
359-9-36	the counterfactual risk minimization ( crm ) principle offers a general recipe for designing blbf algorithms .	we show that this conventional estimator suffers from a propensity overfitting problem when used for learning over complex hypothesis spaces .	1	3	5	-5.3847246	4.951236	1
359-9-36	the counterfactual risk minimization ( crm ) principle offers a general recipe for designing blbf algorithms .	we propose to replace the risk estimator with a self-normalized estimator , showing that it neatly avoids this problem .	1	3	6	-5.910747	5.2624393	1
359-9-36	the counterfactual risk minimization ( crm ) principle offers a general recipe for designing blbf algorithms .	this naturally gives rise to a new learning algorithm - normalized policy optimizer for exponential models ( norm-poem ) - for structured output prediction using linear rules .	1	3	7	-5.923329	5.2546053	1
359-9-36	we evaluate the empirical effectiveness of normpoem on several multi-label classification problems , finding that it consistently outperforms the conventional estimator .	the counterfactual risk minimization ( crm ) principle offers a general recipe for designing blbf algorithms .	0	8	3	5.604098	-5.0356274	0
359-9-36	we show that this conventional estimator suffers from a propensity overfitting problem when used for learning over complex hypothesis spaces .	it requires a counterfactual risk estimator , and virtually all existing works on blbf have focused on a particular unbiased estimator .	0	5	4	4.856496	-4.3322763	0
359-9-36	it requires a counterfactual risk estimator , and virtually all existing works on blbf have focused on a particular unbiased estimator .	we propose to replace the risk estimator with a self-normalized estimator , showing that it neatly avoids this problem .	1	4	6	-5.9696016	5.2223063	1
359-9-36	it requires a counterfactual risk estimator , and virtually all existing works on blbf have focused on a particular unbiased estimator .	this naturally gives rise to a new learning algorithm - normalized policy optimizer for exponential models ( norm-poem ) - for structured output prediction using linear rules .	1	4	7	-5.964287	5.2111683	1
359-9-36	we evaluate the empirical effectiveness of normpoem on several multi-label classification problems , finding that it consistently outperforms the conventional estimator .	it requires a counterfactual risk estimator , and virtually all existing works on blbf have focused on a particular unbiased estimator .	0	8	4	5.6084323	-5.0176935	0
359-9-36	we propose to replace the risk estimator with a self-normalized estimator , showing that it neatly avoids this problem .	we show that this conventional estimator suffers from a propensity overfitting problem when used for learning over complex hypothesis spaces .	0	6	5	3.5190558	-3.2554526	0
359-9-36	this naturally gives rise to a new learning algorithm - normalized policy optimizer for exponential models ( norm-poem ) - for structured output prediction using linear rules .	we show that this conventional estimator suffers from a propensity overfitting problem when used for learning over complex hypothesis spaces .	0	7	5	3.9727535	-3.6404107	0
359-9-36	we evaluate the empirical effectiveness of normpoem on several multi-label classification problems , finding that it consistently outperforms the conventional estimator .	we show that this conventional estimator suffers from a propensity overfitting problem when used for learning over complex hypothesis spaces .	0	8	5	5.427127	-4.808362	0
359-9-36	this naturally gives rise to a new learning algorithm - normalized policy optimizer for exponential models ( norm-poem ) - for structured output prediction using linear rules .	we propose to replace the risk estimator with a self-normalized estimator , showing that it neatly avoids this problem .	0	7	6	2.3965712	-2.2514231	0
359-9-36	we evaluate the empirical effectiveness of normpoem on several multi-label classification problems , finding that it consistently outperforms the conventional estimator .	we propose to replace the risk estimator with a self-normalized estimator , showing that it neatly avoids this problem .	0	8	6	5.522761	-4.9264708	0
359-9-36	this naturally gives rise to a new learning algorithm - normalized policy optimizer for exponential models ( norm-poem ) - for structured output prediction using linear rules .	we evaluate the empirical effectiveness of normpoem on several multi-label classification problems , finding that it consistently outperforms the conventional estimator .	1	7	8	-5.941577	5.1837845	1
360-8-28	a key advantage of these lifted algorithms is that they have much smaller computational complexity than propositional algorithms when symmetries are present in the mln and these symmetries can be detected using lifted inference rules .	recently , there has been growing interest in lifting map inference algorithms for markov logic networks ( mlns ) .	0	1	0	5.6514215	-5.0774965	0
360-8-28	recently , there has been growing interest in lifting map inference algorithms for markov logic networks ( mlns ) .	unfortunately , lifted inference rules are sound but not complete and can often miss many symmetries .	1	0	2	-5.734188	5.207362	1
360-8-28	recently , there has been growing interest in lifting map inference algorithms for markov logic networks ( mlns ) .	this is problematic because when symmetries can not be exploited , lifted inference algorithms ground the mln , and search for solutions in the much larger propositional space .	1	0	3	-5.8591824	5.146458	1
360-8-28	recently , there has been growing interest in lifting map inference algorithms for markov logic networks ( mlns ) .	in this paper , we present a novel approach , which cleverly introduces new symmetries at the time of grounding .	1	0	4	-5.9489307	5.1480284	1
360-8-28	recently , there has been growing interest in lifting map inference algorithms for markov logic networks ( mlns ) .	our main idea is to partition the ground atoms and force the inference algorithm to treat all atoms in each part as indistinguishable .	1	0	5	-5.8640656	5.146201	1
360-8-28	we show that by systematically and carefully refining ( and growing ) the partitions , we can build advanced any-time and any-space map inference algorithms .	recently , there has been growing interest in lifting map inference algorithms for markov logic networks ( mlns ) .	0	6	0	5.601394	-5.0141354	0
360-8-28	our experiments on several real-world datasets clearly show that our new algorithm is superior to previous approaches and often finds useful symmetries in the search space that existing lifted inference rules are unable to detect .	recently , there has been growing interest in lifting map inference algorithms for markov logic networks ( mlns ) .	0	7	0	5.6543245	-5.0182657	0
360-8-28	unfortunately , lifted inference rules are sound but not complete and can often miss many symmetries .	a key advantage of these lifted algorithms is that they have much smaller computational complexity than propositional algorithms when symmetries are present in the mln and these symmetries can be detected using lifted inference rules .	0	2	1	-1.3387201	1.4823267	1
360-8-28	this is problematic because when symmetries can not be exploited , lifted inference algorithms ground the mln , and search for solutions in the much larger propositional space .	a key advantage of these lifted algorithms is that they have much smaller computational complexity than propositional algorithms when symmetries are present in the mln and these symmetries can be detected using lifted inference rules .	0	3	1	-0.083279416	0.32579234	1
360-8-28	in this paper , we present a novel approach , which cleverly introduces new symmetries at the time of grounding .	a key advantage of these lifted algorithms is that they have much smaller computational complexity than propositional algorithms when symmetries are present in the mln and these symmetries can be detected using lifted inference rules .	0	4	1	2.5864468	-2.4936295	0
360-8-28	our main idea is to partition the ground atoms and force the inference algorithm to treat all atoms in each part as indistinguishable .	a key advantage of these lifted algorithms is that they have much smaller computational complexity than propositional algorithms when symmetries are present in the mln and these symmetries can be detected using lifted inference rules .	0	5	1	-1.5812342	1.7422612	1
360-8-28	a key advantage of these lifted algorithms is that they have much smaller computational complexity than propositional algorithms when symmetries are present in the mln and these symmetries can be detected using lifted inference rules .	we show that by systematically and carefully refining ( and growing ) the partitions , we can build advanced any-time and any-space map inference algorithms .	1	1	6	-5.5902963	5.020445	1
360-8-28	a key advantage of these lifted algorithms is that they have much smaller computational complexity than propositional algorithms when symmetries are present in the mln and these symmetries can be detected using lifted inference rules .	our experiments on several real-world datasets clearly show that our new algorithm is superior to previous approaches and often finds useful symmetries in the search space that existing lifted inference rules are unable to detect .	1	1	7	-6.01896	5.137413	1
360-8-28	unfortunately , lifted inference rules are sound but not complete and can often miss many symmetries .	this is problematic because when symmetries can not be exploited , lifted inference algorithms ground the mln , and search for solutions in the much larger propositional space .	1	2	3	-2.298888	2.431524	1
360-8-28	unfortunately , lifted inference rules are sound but not complete and can often miss many symmetries .	in this paper , we present a novel approach , which cleverly introduces new symmetries at the time of grounding .	1	2	4	-5.8838935	5.208334	1
360-8-28	our main idea is to partition the ground atoms and force the inference algorithm to treat all atoms in each part as indistinguishable .	unfortunately , lifted inference rules are sound but not complete and can often miss many symmetries .	0	5	2	3.8398063	-3.574554	0
360-8-28	unfortunately , lifted inference rules are sound but not complete and can often miss many symmetries .	we show that by systematically and carefully refining ( and growing ) the partitions , we can build advanced any-time and any-space map inference algorithms .	1	2	6	-5.9619155	5.120715	1
360-8-28	unfortunately , lifted inference rules are sound but not complete and can often miss many symmetries .	our experiments on several real-world datasets clearly show that our new algorithm is superior to previous approaches and often finds useful symmetries in the search space that existing lifted inference rules are unable to detect .	1	2	7	-5.988372	5.105716	1
360-8-28	in this paper , we present a novel approach , which cleverly introduces new symmetries at the time of grounding .	this is problematic because when symmetries can not be exploited , lifted inference algorithms ground the mln , and search for solutions in the much larger propositional space .	0	4	3	4.89826	-4.369839	0
360-8-28	our main idea is to partition the ground atoms and force the inference algorithm to treat all atoms in each part as indistinguishable .	this is problematic because when symmetries can not be exploited , lifted inference algorithms ground the mln , and search for solutions in the much larger propositional space .	0	5	3	2.7443695	-2.5138953	0
360-8-28	we show that by systematically and carefully refining ( and growing ) the partitions , we can build advanced any-time and any-space map inference algorithms .	this is problematic because when symmetries can not be exploited , lifted inference algorithms ground the mln , and search for solutions in the much larger propositional space .	0	6	3	4.5009885	-4.1086006	0
360-8-28	our experiments on several real-world datasets clearly show that our new algorithm is superior to previous approaches and often finds useful symmetries in the search space that existing lifted inference rules are unable to detect .	this is problematic because when symmetries can not be exploited , lifted inference algorithms ground the mln , and search for solutions in the much larger propositional space .	0	7	3	5.498454	-4.887163	0
360-8-28	our main idea is to partition the ground atoms and force the inference algorithm to treat all atoms in each part as indistinguishable .	in this paper , we present a novel approach , which cleverly introduces new symmetries at the time of grounding .	0	5	4	3.869855	-3.5930054	0
360-8-28	in this paper , we present a novel approach , which cleverly introduces new symmetries at the time of grounding .	we show that by systematically and carefully refining ( and growing ) the partitions , we can build advanced any-time and any-space map inference algorithms .	1	4	6	-5.854121	5.0256658	1
360-8-28	our experiments on several real-world datasets clearly show that our new algorithm is superior to previous approaches and often finds useful symmetries in the search space that existing lifted inference rules are unable to detect .	in this paper , we present a novel approach , which cleverly introduces new symmetries at the time of grounding .	0	7	4	5.5973935	-4.9577866	0
360-8-28	our main idea is to partition the ground atoms and force the inference algorithm to treat all atoms in each part as indistinguishable .	we show that by systematically and carefully refining ( and growing ) the partitions , we can build advanced any-time and any-space map inference algorithms .	1	5	6	-5.9018083	5.103587	1
360-8-28	our experiments on several real-world datasets clearly show that our new algorithm is superior to previous approaches and often finds useful symmetries in the search space that existing lifted inference rules are unable to detect .	our main idea is to partition the ground atoms and force the inference algorithm to treat all atoms in each part as indistinguishable .	0	7	5	5.4582562	-4.819584	0
360-8-28	our experiments on several real-world datasets clearly show that our new algorithm is superior to previous approaches and often finds useful symmetries in the search space that existing lifted inference rules are unable to detect .	we show that by systematically and carefully refining ( and growing ) the partitions , we can build advanced any-time and any-space map inference algorithms .	0	7	6	5.054566	-4.56172	0
361-6-15	motivated by this view , we develop extensions to an existing model , and then explore the idea further in the context of data imputation - perhaps the simplest setting in which to investigate the relation between unconditional and conditional generative modelling .	we connect a broad class of generative models through their shared reliance on sequential decision making .	0	1	0	4.8527975	-4.2783804	0
361-6-15	we connect a broad class of generative models through their shared reliance on sequential decision making .	we formulate data imputation as an mdp and develop models capable of representing effective policies for it .	1	0	2	-4.9863925	4.6514883	1
361-6-15	we connect a broad class of generative models through their shared reliance on sequential decision making .	we construct the models using neural networks and train them using a form of guided policy search .	1	0	3	-5.9441233	5.211935	1
361-6-15	our models generate predictions through an iterative process of feedback and refinement .	we connect a broad class of generative models through their shared reliance on sequential decision making .	0	4	0	5.3862443	-4.755219	0
361-6-15	we show that this approach can learn effective policies for imputation problems of varying difficulty and across multiple datasets .	we connect a broad class of generative models through their shared reliance on sequential decision making .	0	5	0	5.0064535	-4.475422	0
361-6-15	we formulate data imputation as an mdp and develop models capable of representing effective policies for it .	motivated by this view , we develop extensions to an existing model , and then explore the idea further in the context of data imputation - perhaps the simplest setting in which to investigate the relation between unconditional and conditional generative modelling .	0	2	1	1.5054249	-1.1580358	0
361-6-15	motivated by this view , we develop extensions to an existing model , and then explore the idea further in the context of data imputation - perhaps the simplest setting in which to investigate the relation between unconditional and conditional generative modelling .	we construct the models using neural networks and train them using a form of guided policy search .	1	1	3	-2.4969954	2.5764863	1
361-6-15	our models generate predictions through an iterative process of feedback and refinement .	motivated by this view , we develop extensions to an existing model , and then explore the idea further in the context of data imputation - perhaps the simplest setting in which to investigate the relation between unconditional and conditional generative modelling .	0	4	1	-0.9650339	1.0952644	1
361-6-15	we show that this approach can learn effective policies for imputation problems of varying difficulty and across multiple datasets .	motivated by this view , we develop extensions to an existing model , and then explore the idea further in the context of data imputation - perhaps the simplest setting in which to investigate the relation between unconditional and conditional generative modelling .	0	5	1	3.7002783	-3.403524	0
361-6-15	we construct the models using neural networks and train them using a form of guided policy search .	we formulate data imputation as an mdp and develop models capable of representing effective policies for it .	0	3	2	5.013463	-4.4450884	0
361-6-15	our models generate predictions through an iterative process of feedback and refinement .	we formulate data imputation as an mdp and develop models capable of representing effective policies for it .	0	4	2	3.9949312	-3.5765238	0
361-6-15	we show that this approach can learn effective policies for imputation problems of varying difficulty and across multiple datasets .	we formulate data imputation as an mdp and develop models capable of representing effective policies for it .	0	5	2	4.0975285	-3.6986623	0
361-6-15	our models generate predictions through an iterative process of feedback and refinement .	we construct the models using neural networks and train them using a form of guided policy search .	0	4	3	-4.0833783	3.815061	1
361-6-15	we construct the models using neural networks and train them using a form of guided policy search .	we show that this approach can learn effective policies for imputation problems of varying difficulty and across multiple datasets .	1	3	5	-2.033192	2.1420035	1
361-6-15	we show that this approach can learn effective policies for imputation problems of varying difficulty and across multiple datasets .	our models generate predictions through an iterative process of feedback and refinement .	0	5	4	2.8972194	-2.7260506	0
362-6-15	elicitation is the study of statistics or properties which are computable via empirical risk minimization .	while several recent papers have approached the general question of which properties are elicitable , we suggest that this is the wrong question -- all properties are elicitable by first eliciting the entire distribution or data set , and thus the important question is how elicitable .	1	0	1	-5.5881257	5.107869	1
362-6-15	specifically , what is the minimum number of regression parameters needed to compute the property ?	elicitation is the study of statistics or properties which are computable via empirical risk minimization .	0	2	0	5.08731	-4.574968	0
362-6-15	elicitation is the study of statistics or properties which are computable via empirical risk minimization .	building on previous work , we introduce a new notion of elicitation complexity and lay the foundations for a calculus of elicitation .	1	0	3	-5.9639845	5.0523753	1
362-6-15	we establish several general results and techniques for proving upper and lower bounds on elicitation complexity .	elicitation is the study of statistics or properties which are computable via empirical risk minimization .	0	4	0	5.3910394	-4.7747593	0
362-6-15	elicitation is the study of statistics or properties which are computable via empirical risk minimization .	these results provide tight bounds for eliciting the bayes risk of any loss , a large class of properties which includes spectral risk measures and several new properties of interest .	1	0	5	-6.0437956	5.146	1
362-6-15	specifically , what is the minimum number of regression parameters needed to compute the property ?	while several recent papers have approached the general question of which properties are elicitable , we suggest that this is the wrong question -- all properties are elicitable by first eliciting the entire distribution or data set , and thus the important question is how elicitable .	0	2	1	4.063122	-3.6660976	0
362-6-15	while several recent papers have approached the general question of which properties are elicitable , we suggest that this is the wrong question -- all properties are elicitable by first eliciting the entire distribution or data set , and thus the important question is how elicitable .	building on previous work , we introduce a new notion of elicitation complexity and lay the foundations for a calculus of elicitation .	1	1	3	-5.990194	5.156295	1
362-6-15	we establish several general results and techniques for proving upper and lower bounds on elicitation complexity .	while several recent papers have approached the general question of which properties are elicitable , we suggest that this is the wrong question -- all properties are elicitable by first eliciting the entire distribution or data set , and thus the important question is how elicitable .	0	4	1	5.3105173	-4.708519	0
362-6-15	while several recent papers have approached the general question of which properties are elicitable , we suggest that this is the wrong question -- all properties are elicitable by first eliciting the entire distribution or data set , and thus the important question is how elicitable .	these results provide tight bounds for eliciting the bayes risk of any loss , a large class of properties which includes spectral risk measures and several new properties of interest .	1	1	5	-5.9939156	5.1851554	1
362-6-15	specifically , what is the minimum number of regression parameters needed to compute the property ?	building on previous work , we introduce a new notion of elicitation complexity and lay the foundations for a calculus of elicitation .	1	2	3	-5.3915052	4.9279323	1
362-6-15	we establish several general results and techniques for proving upper and lower bounds on elicitation complexity .	specifically , what is the minimum number of regression parameters needed to compute the property ?	0	4	2	2.6849015	-2.4179912	0
362-6-15	specifically , what is the minimum number of regression parameters needed to compute the property ?	these results provide tight bounds for eliciting the bayes risk of any loss , a large class of properties which includes spectral risk measures and several new properties of interest .	1	2	5	-5.9481325	5.1455464	1
362-6-15	we establish several general results and techniques for proving upper and lower bounds on elicitation complexity .	building on previous work , we introduce a new notion of elicitation complexity and lay the foundations for a calculus of elicitation .	0	4	3	-2.6274502	2.7476037	1
362-6-15	these results provide tight bounds for eliciting the bayes risk of any loss , a large class of properties which includes spectral risk measures and several new properties of interest .	building on previous work , we introduce a new notion of elicitation complexity and lay the foundations for a calculus of elicitation .	0	5	3	4.372834	-3.9658947	0
362-6-15	these results provide tight bounds for eliciting the bayes risk of any loss , a large class of properties which includes spectral risk measures and several new properties of interest .	we establish several general results and techniques for proving upper and lower bounds on elicitation complexity .	0	5	4	4.3512278	-3.931961	0
363-5-10	marginal map inference involves making map predictions in systems defined with latent variables or missing information .	it is significantly more difficult than pure marginalization and map tasks , for which a large class of efficient and convergent variational algorithms , such as dual decomposition , exist .	1	0	1	-5.9492354	5.1588116	1
363-5-10	marginal map inference involves making map predictions in systems defined with latent variables or missing information .	in this work , we generalize dual decomposition to a generic power sum inference task , which includes marginal map , along with pure marginalization and map , as special cases .	1	0	2	-5.848565	5.2308545	1
363-5-10	marginal map inference involves making map predictions in systems defined with latent variables or missing information .	our method is based on a block coordinate descent algorithm on a new convex decomposition bound , that is guaranteed to converge monotonically , and can be parallelized efficiently .	1	0	3	-5.995107	5.164938	1
363-5-10	marginal map inference involves making map predictions in systems defined with latent variables or missing information .	we demonstrate our approach on marginal map queries defined on real-world problems from the uai approximate inference challenge , showing that our framework is faster and more reliable than previous methods .	1	0	4	-5.9984703	5.160268	1
363-5-10	in this work , we generalize dual decomposition to a generic power sum inference task , which includes marginal map , along with pure marginalization and map , as special cases .	it is significantly more difficult than pure marginalization and map tasks , for which a large class of efficient and convergent variational algorithms , such as dual decomposition , exist .	0	2	1	4.493133	-4.0443697	0
363-5-10	it is significantly more difficult than pure marginalization and map tasks , for which a large class of efficient and convergent variational algorithms , such as dual decomposition , exist .	our method is based on a block coordinate descent algorithm on a new convex decomposition bound , that is guaranteed to converge monotonically , and can be parallelized efficiently .	1	1	3	-5.98102	5.2042236	1
363-5-10	it is significantly more difficult than pure marginalization and map tasks , for which a large class of efficient and convergent variational algorithms , such as dual decomposition , exist .	we demonstrate our approach on marginal map queries defined on real-world problems from the uai approximate inference challenge , showing that our framework is faster and more reliable than previous methods .	1	1	4	-5.9579577	5.1742773	1
363-5-10	our method is based on a block coordinate descent algorithm on a new convex decomposition bound , that is guaranteed to converge monotonically , and can be parallelized efficiently .	in this work , we generalize dual decomposition to a generic power sum inference task , which includes marginal map , along with pure marginalization and map , as special cases .	0	3	2	4.7099524	-4.2021813	0
363-5-10	we demonstrate our approach on marginal map queries defined on real-world problems from the uai approximate inference challenge , showing that our framework is faster and more reliable than previous methods .	in this work , we generalize dual decomposition to a generic power sum inference task , which includes marginal map , along with pure marginalization and map , as special cases .	0	4	2	5.3746567	-4.753312	0
363-5-10	we demonstrate our approach on marginal map queries defined on real-world problems from the uai approximate inference challenge , showing that our framework is faster and more reliable than previous methods .	our method is based on a block coordinate descent algorithm on a new convex decomposition bound , that is guaranteed to converge monotonically , and can be parallelized efficiently .	0	4	3	4.225009	-3.8545225	0
364-13-78	.	consider the binary classification problem of predicting a target variable y from a discrete feature vector x = ( x1 , .	0	1	0	5.1795015	-4.6939807	0
364-13-78	.	consider the binary classification problem of predicting a target variable y from a discrete feature vector x = ( x1 , .	0	2	0	5.1795015	-4.6939807	0
364-13-78	consider the binary classification problem of predicting a target variable y from a discrete feature vector x = ( x1 , .	, xd ) .	1	0	3	-5.634616	5.1103535	1
364-13-78	consider the binary classification problem of predicting a target variable y from a discrete feature vector x = ( x1 , .	when the probability distribution p ( x , y ) is known , the optimal classifier , leading to the minimum misclassification rate , is given by the maximum a-posteriori probability ( map ) decision rule .	1	0	4	-5.6758804	5.1460543	1
364-13-78	consider the binary classification problem of predicting a target variable y from a discrete feature vector x = ( x1 , .	however , in practice , estimating the complete joint distribution p ( x , y ) is computationally and statistically impossible for large values of d. therefore , an alternative approach is to first estimate some low order marginals of the joint probability distribution p ( x , y ) and then design the classifier based on the estimated low order marginals .	1	0	5	-5.838476	5.1828265	1
364-13-78	consider the binary classification problem of predicting a target variable y from a discrete feature vector x = ( x1 , .	this approach is also helpful when the complete training data instances are not available due to privacy concerns .	1	0	6	-5.9481254	5.156849	1
364-13-78	consider the binary classification problem of predicting a target variable y from a discrete feature vector x = ( x1 , .	in this work , we consider the problem of finding the optimum classifier based on some estimated low order marginals of ( x , y ) .	1	0	7	-5.7936106	5.1865597	1
364-13-78	we prove that for a given set of marginals , the minimum hirschfeld-gebelein-renyi ( hgr ) correlation principle introduced in leads to a randomized classification rule which is shown to have a misclassification rate no larger than twice the misclassification rate of the optimal classifier .	consider the binary classification problem of predicting a target variable y from a discrete feature vector x = ( x1 , .	0	8	0	5.6258774	-5.038023	0
364-13-78	consider the binary classification problem of predicting a target variable y from a discrete feature vector x = ( x1 , .	then , under a separability condition , it is shown that the proposed algorithm is equivalent to a randomized linear regression approach .	1	0	9	-5.8555417	5.1109686	1
364-13-78	in addition , this method naturally results in a robust feature selection method selecting a subset of features having the maximum worst case hgr correlation with the target variable .	consider the binary classification problem of predicting a target variable y from a discrete feature vector x = ( x1 , .	0	10	0	5.579918	-5.038231	0
364-13-78	our theoretical upper-bound is similar to the recent discrete chebyshev classifier ( dcc ) approach , while the proposed algorithm has significant computational advantages since it only requires solving a least square optimization problem .	consider the binary classification problem of predicting a target variable y from a discrete feature vector x = ( x1 , .	0	11	0	5.637173	-5.060019	0
364-13-78	consider the binary classification problem of predicting a target variable y from a discrete feature vector x = ( x1 , .	finally , we numerically compare our proposed algorithm with the dcc classifier and show that the proposed algorithm results in better misclassification rate over various uci data repository datasets .	1	0	12	-5.8880324	5.1217284	1
364-13-78	.	.	1	1	2	-0.43447864	0.5102596	1
364-13-78	.	, xd ) .	1	1	3	-1.5923193	1.6324306	1
364-13-78	.	when the probability distribution p ( x , y ) is known , the optimal classifier , leading to the minimum misclassification rate , is given by the maximum a-posteriori probability ( map ) decision rule .	1	1	4	3.5570388	-3.2361693	0
364-13-78	however , in practice , estimating the complete joint distribution p ( x , y ) is computationally and statistically impossible for large values of d. therefore , an alternative approach is to first estimate some low order marginals of the joint probability distribution p ( x , y ) and then design the classifier based on the estimated low order marginals .	.	0	5	1	0.85394424	-0.68961847	0
364-13-78	this approach is also helpful when the complete training data instances are not available due to privacy concerns .	.	0	6	1	1.8266063	-1.7515342	0
364-13-78	in this work , we consider the problem of finding the optimum classifier based on some estimated low order marginals of ( x , y ) .	.	0	7	1	-3.1845741	3.11756	1
364-13-78	we prove that for a given set of marginals , the minimum hirschfeld-gebelein-renyi ( hgr ) correlation principle introduced in leads to a randomized classification rule which is shown to have a misclassification rate no larger than twice the misclassification rate of the optimal classifier .	.	0	8	1	0.062071823	0.12634127	1
364-13-78	then , under a separability condition , it is shown that the proposed algorithm is equivalent to a randomized linear regression approach .	.	0	9	1	2.085814	-1.9929475	0
364-13-78	.	in addition , this method naturally results in a robust feature selection method selecting a subset of features having the maximum worst case hgr correlation with the target variable .	1	1	10	-2.3777843	2.4378629	1
364-13-78	our theoretical upper-bound is similar to the recent discrete chebyshev classifier ( dcc ) approach , while the proposed algorithm has significant computational advantages since it only requires solving a least square optimization problem .	.	0	11	1	0.74962455	-0.6637317	0
364-13-78	finally , we numerically compare our proposed algorithm with the dcc classifier and show that the proposed algorithm results in better misclassification rate over various uci data repository datasets .	.	0	12	1	1.3404973	-1.2214859	0
364-13-78	.	, xd ) .	1	2	3	-1.5923193	1.6324306	1
364-13-78	.	when the probability distribution p ( x , y ) is known , the optimal classifier , leading to the minimum misclassification rate , is given by the maximum a-posteriori probability ( map ) decision rule .	1	2	4	3.5570388	-3.2361693	0
364-13-78	.	however , in practice , estimating the complete joint distribution p ( x , y ) is computationally and statistically impossible for large values of d. therefore , an alternative approach is to first estimate some low order marginals of the joint probability distribution p ( x , y ) and then design the classifier based on the estimated low order marginals .	1	2	5	0.11775029	0.15083864	1
364-13-78	this approach is also helpful when the complete training data instances are not available due to privacy concerns .	.	0	6	2	1.8266063	-1.7515342	0
364-13-78	.	in this work , we consider the problem of finding the optimum classifier based on some estimated low order marginals of ( x , y ) .	1	2	7	3.3443813	-3.0071042	0
364-13-78	.	we prove that for a given set of marginals , the minimum hirschfeld-gebelein-renyi ( hgr ) correlation principle introduced in leads to a randomized classification rule which is shown to have a misclassification rate no larger than twice the misclassification rate of the optimal classifier .	1	2	8	0.8254219	-0.5880535	0
364-13-78	.	then , under a separability condition , it is shown that the proposed algorithm is equivalent to a randomized linear regression approach .	1	2	9	-2.2680569	2.3073745	1
364-13-78	in addition , this method naturally results in a robust feature selection method selecting a subset of features having the maximum worst case hgr correlation with the target variable .	.	0	10	2	2.2415757	-2.1606903	0
364-13-78	.	our theoretical upper-bound is similar to the recent discrete chebyshev classifier ( dcc ) approach , while the proposed algorithm has significant computational advantages since it only requires solving a least square optimization problem .	1	2	11	-2.082957	2.1425178	1
364-13-78	finally , we numerically compare our proposed algorithm with the dcc classifier and show that the proposed algorithm results in better misclassification rate over various uci data repository datasets .	.	0	12	2	1.3404973	-1.2214859	0
364-13-78	when the probability distribution p ( x , y ) is known , the optimal classifier , leading to the minimum misclassification rate , is given by the maximum a-posteriori probability ( map ) decision rule .	, xd ) .	0	4	3	-5.613592	5.0601597	1
364-13-78	however , in practice , estimating the complete joint distribution p ( x , y ) is computationally and statistically impossible for large values of d. therefore , an alternative approach is to first estimate some low order marginals of the joint probability distribution p ( x , y ) and then design the classifier based on the estimated low order marginals .	, xd ) .	0	5	3	-3.3528805	3.2603893	1
364-13-78	this approach is also helpful when the complete training data instances are not available due to privacy concerns .	, xd ) .	0	6	3	4.3592157	-4.073488	0
364-13-78	in this work , we consider the problem of finding the optimum classifier based on some estimated low order marginals of ( x , y ) .	, xd ) .	0	7	3	-4.757268	4.468171	1
364-13-78	, xd ) .	we prove that for a given set of marginals , the minimum hirschfeld-gebelein-renyi ( hgr ) correlation principle introduced in leads to a randomized classification rule which is shown to have a misclassification rate no larger than twice the misclassification rate of the optimal classifier .	1	3	8	2.7771878	-2.613597	0
364-13-78	then , under a separability condition , it is shown that the proposed algorithm is equivalent to a randomized linear regression approach .	, xd ) .	0	9	3	4.328313	-4.016942	0
364-13-78	, xd ) .	in addition , this method naturally results in a robust feature selection method selecting a subset of features having the maximum worst case hgr correlation with the target variable .	1	3	10	-5.4031677	4.870384	1
364-13-78	, xd ) .	our theoretical upper-bound is similar to the recent discrete chebyshev classifier ( dcc ) approach , while the proposed algorithm has significant computational advantages since it only requires solving a least square optimization problem .	1	3	11	-3.7160895	3.5452259	1
364-13-78	, xd ) .	finally , we numerically compare our proposed algorithm with the dcc classifier and show that the proposed algorithm results in better misclassification rate over various uci data repository datasets .	1	3	12	-5.9634094	5.146598	1
364-13-78	when the probability distribution p ( x , y ) is known , the optimal classifier , leading to the minimum misclassification rate , is given by the maximum a-posteriori probability ( map ) decision rule .	[CLS] however, in practice, estimating the complete joint distribution p ( x, y ) is computationally and statistically impossible for large values of d. therefore, an alternative approach is to first estimate some low order marginals of the joint probability distribution p ( x, y	1	4	5	-5.3093147	4.796299	1
364-13-78	when the probability distribution p ( x , y ) is known , the optimal classifier , leading to the minimum misclassification rate , is given by the maximum a-posteriori probability ( map ) decision rule .	this approach is also helpful when the complete training data instances are not available due to privacy concerns .	1	4	6	-5.9429526	5.055013	1
364-13-78	in this work , we consider the problem of finding the optimum classifier based on some estimated low order marginals of ( x , y ) .	when the probability distribution p ( x , y ) is known , the optimal classifier , leading to the minimum misclassification rate , is given by the maximum a-posteriori probability ( map ) decision rule .	0	7	4	5.177551	-4.627715	0
364-13-78	[CLS] we prove that for a given set of marginals, the minimum hirschfeld - gebelein - renyi ( hgr ) correlation principle introduced in leads to a randomized classification rule which is shown to have a misclassification rate no larger than twice the misclass	when the probability distribution p ( x , y ) is known , the optimal classifier , leading to the minimum misclassification rate , is given by the maximum a-posteriori probability ( map ) decision rule .	0	8	4	4.477504	-4.040424	0
364-13-78	when the probability distribution p ( x , y ) is known , the optimal classifier , leading to the minimum misclassification rate , is given by the maximum a-posteriori probability ( map ) decision rule .	then , under a separability condition , it is shown that the proposed algorithm is equivalent to a randomized linear regression approach .	1	4	9	-5.989441	5.0860825	1
364-13-78	in addition , this method naturally results in a robust feature selection method selecting a subset of features having the maximum worst case hgr correlation with the target variable .	when the probability distribution p ( x , y ) is known , the optimal classifier , leading to the minimum misclassification rate , is given by the maximum a-posteriori probability ( map ) decision rule .	0	10	4	5.5558205	-4.9219413	0
364-13-78	our theoretical upper-bound is similar to the recent discrete chebyshev classifier ( dcc ) approach , while the proposed algorithm has significant computational advantages since it only requires solving a least square optimization problem .	when the probability distribution p ( x , y ) is known , the optimal classifier , leading to the minimum misclassification rate , is given by the maximum a-posteriori probability ( map ) decision rule .	0	11	4	5.5240517	-4.877481	0
364-13-78	finally , we numerically compare our proposed algorithm with the dcc classifier and show that the proposed algorithm results in better misclassification rate over various uci data repository datasets .	when the probability distribution p ( x , y ) is known , the optimal classifier , leading to the minimum misclassification rate , is given by the maximum a-posteriori probability ( map ) decision rule .	0	12	4	5.615055	-4.990073	0
364-13-78	this approach is also helpful when the complete training data instances are not available due to privacy concerns .	however , in practice , estimating the complete joint distribution p ( x , y ) is computationally and statistically impossible for large values of d. therefore , an alternative approach is to first estimate some low order marginals of the joint probability distribution p ( x , y ) and then design the classifier based on the estimated low order marginals .	0	6	5	5.2021713	-4.6481013	0
364-13-78	[CLS] however, in practice, estimating the complete joint distribution p ( x, y ) is computationally and statistically impossible for large values of d. therefore, an alternative approach is to first estimate some low order marginals of the joint probability distribution p ( x, y ) and then design the classifier based on the estimated low order	in this work , we consider the problem of finding the optimum classifier based on some estimated low order marginals of ( x , y ) .	1	5	7	-3.048883	2.983496	1
364-13-78	[CLS] however, in practice, estimating the complete joint distribution p ( x, y ) is computationally and statistically impossible for large values of d. therefore, an alternative approach is to first estimate some low order marginals of the joint probability distribution	[CLS] we prove that for a given set of marginals, the minimum hirschfeld - gebelein - renyi ( hgr ) correlation principle introduced in leads to a randomized classification rule which is shown to have a misclassification rate no larger	1	5	8	-5.6935616	5.1001997	1
364-13-78	however , in practice , estimating the complete joint distribution p ( x , y ) is computationally and statistically impossible for large values of d. therefore , an alternative approach is to first estimate some low order marginals of the joint probability distribution p ( x , y ) and then design the classifier based on the estimated low order marginals .	then , under a separability condition , it is shown that the proposed algorithm is equivalent to a randomized linear regression approach .	1	5	9	-5.98464	5.1492863	1
364-13-78	[CLS] however, in practice, estimating the complete joint distribution p ( x, y ) is computationally and statistically impossible for large values of d. therefore, an alternative approach is to first estimate some low order marginals of the joint probability distribution p ( x, y ) and then design the classifier based on the estimated low	in addition , this method naturally results in a robust feature selection method selecting a subset of features having the maximum worst case hgr correlation with the target variable .	1	5	10	-5.9337416	5.202567	1
364-13-78	our theoretical upper-bound is similar to the recent discrete chebyshev classifier ( dcc ) approach , while the proposed algorithm has significant computational advantages since it only requires solving a least square optimization problem .	[CLS] however, in practice, estimating the complete joint distribution p ( x, y ) is computationally and statistically impossible for large values of d. therefore, an alternative approach is to first estimate some low order marginals of the joint probability distribution p ( x, y )	0	11	5	5.172696	-4.587406	0
364-13-78	finally , we numerically compare our proposed algorithm with the dcc classifier and show that the proposed algorithm results in better misclassification rate over various uci data repository datasets .	[CLS] however, in practice, estimating the complete joint distribution p ( x, y ) is computationally and statistically impossible for large values of d. therefore, an alternative approach is to first estimate some low order marginals of the joint probability distribution p ( x, y ) and then design the class	0	12	5	5.637115	-5.0352197	0
364-13-78	this approach is also helpful when the complete training data instances are not available due to privacy concerns .	in this work , we consider the problem of finding the optimum classifier based on some estimated low order marginals of ( x , y ) .	1	6	7	5.476509	-4.869849	0
364-13-78	we prove that for a given set of marginals , the minimum hirschfeld-gebelein-renyi ( hgr ) correlation principle introduced in leads to a randomized classification rule which is shown to have a misclassification rate no larger than twice the misclassification rate of the optimal classifier .	this approach is also helpful when the complete training data instances are not available due to privacy concerns .	0	8	6	-5.56534	4.926626	1
364-13-78	this approach is also helpful when the complete training data instances are not available due to privacy concerns .	then , under a separability condition , it is shown that the proposed algorithm is equivalent to a randomized linear regression approach .	1	6	9	3.3120003	-3.0931096	0
364-13-78	in addition , this method naturally results in a robust feature selection method selecting a subset of features having the maximum worst case hgr correlation with the target variable .	this approach is also helpful when the complete training data instances are not available due to privacy concerns .	0	10	6	-4.014826	3.7972548	1
364-13-78	our theoretical upper-bound is similar to the recent discrete chebyshev classifier ( dcc ) approach , while the proposed algorithm has significant computational advantages since it only requires solving a least square optimization problem .	this approach is also helpful when the complete training data instances are not available due to privacy concerns .	0	11	6	-4.7774496	4.399438	1
364-13-78	finally , we numerically compare our proposed algorithm with the dcc classifier and show that the proposed algorithm results in better misclassification rate over various uci data repository datasets .	this approach is also helpful when the complete training data instances are not available due to privacy concerns .	0	12	6	3.372007	-3.158599	0
364-13-78	we prove that for a given set of marginals , the minimum hirschfeld-gebelein-renyi ( hgr ) correlation principle introduced in leads to a randomized classification rule which is shown to have a misclassification rate no larger than twice the misclassification rate of the optimal classifier .	in this work , we consider the problem of finding the optimum classifier based on some estimated low order marginals of ( x , y ) .	0	8	7	4.94674	-4.393226	0
364-13-78	in this work , we consider the problem of finding the optimum classifier based on some estimated low order marginals of ( x , y ) .	then , under a separability condition , it is shown that the proposed algorithm is equivalent to a randomized linear regression approach .	1	7	9	-5.999916	5.150564	1
364-13-78	in this work , we consider the problem of finding the optimum classifier based on some estimated low order marginals of ( x , y ) .	in addition , this method naturally results in a robust feature selection method selecting a subset of features having the maximum worst case hgr correlation with the target variable .	1	7	10	-5.963876	5.1970663	1
364-13-78	in this work , we consider the problem of finding the optimum classifier based on some estimated low order marginals of ( x , y ) .	our theoretical upper-bound is similar to the recent discrete chebyshev classifier ( dcc ) approach , while the proposed algorithm has significant computational advantages since it only requires solving a least square optimization problem .	1	7	11	-6.007414	5.191846	1
364-13-78	finally , we numerically compare our proposed algorithm with the dcc classifier and show that the proposed algorithm results in better misclassification rate over various uci data repository datasets .	in this work , we consider the problem of finding the optimum classifier based on some estimated low order marginals of ( x , y ) .	0	12	7	5.5853796	-4.969963	0
364-13-78	we prove that for a given set of marginals , the minimum hirschfeld-gebelein-renyi ( hgr ) correlation principle introduced in leads to a randomized classification rule which is shown to have a misclassification rate no larger than twice the misclassification rate of the optimal classifier .	then , under a separability condition , it is shown that the proposed algorithm is equivalent to a randomized linear regression approach .	1	8	9	-5.942719	5.135619	1
364-13-78	in addition , this method naturally results in a robust feature selection method selecting a subset of features having the maximum worst case hgr correlation with the target variable .	we prove that for a given set of marginals , the minimum hirschfeld-gebelein-renyi ( hgr ) correlation principle introduced in leads to a randomized classification rule which is shown to have a misclassification rate no larger than twice the misclassification rate of the optimal classifier .	0	10	8	5.582012	-4.9704957	0
364-13-78	[CLS] we prove that for a given set of marginals, the minimum hirschfeld - gebelein - renyi ( hgr ) correlation principle introduced in leads to a randomized classification rule which is shown to have a misclassification rate no larger than twice the misclassification	our theoretical upper-bound is similar to the recent discrete chebyshev classifier ( dcc ) approach , while the proposed algorithm has significant computational advantages since it only requires solving a least square optimization problem .	1	8	11	-5.6380663	5.047889	1
364-13-78	[CLS] we prove that for a given set of marginals, the minimum hirschfeld - gebelein - renyi ( hgr ) correlation principle introduced in leads to a randomized classification rule which is shown to have a misclassification rate no larger than twice the misclassification rate of the optimal class	finally , we numerically compare our proposed algorithm with the dcc classifier and show that the proposed algorithm results in better misclassification rate over various uci data repository datasets .	1	8	12	-5.9924555	5.1785707	1
364-13-78	then , under a separability condition , it is shown that the proposed algorithm is equivalent to a randomized linear regression approach .	in addition , this method naturally results in a robust feature selection method selecting a subset of features having the maximum worst case hgr correlation with the target variable .	1	9	10	-2.260297	2.3777452	1
364-13-78	our theoretical upper-bound is similar to the recent discrete chebyshev classifier ( dcc ) approach , while the proposed algorithm has significant computational advantages since it only requires solving a least square optimization problem .	then , under a separability condition , it is shown that the proposed algorithm is equivalent to a randomized linear regression approach .	0	11	9	-2.2580097	2.3375857	1
364-13-78	then , under a separability condition , it is shown that the proposed algorithm is equivalent to a randomized linear regression approach .	finally , we numerically compare our proposed algorithm with the dcc classifier and show that the proposed algorithm results in better misclassification rate over various uci data repository datasets .	1	9	12	-5.8065753	5.115053	1
364-13-78	our theoretical upper-bound is similar to the recent discrete chebyshev classifier ( dcc ) approach , while the proposed algorithm has significant computational advantages since it only requires solving a least square optimization problem .	in addition , this method naturally results in a robust feature selection method selecting a subset of features having the maximum worst case hgr correlation with the target variable .	0	11	10	-1.5663244	1.7455031	1
364-13-78	finally , we numerically compare our proposed algorithm with the dcc classifier and show that the proposed algorithm results in better misclassification rate over various uci data repository datasets .	in addition , this method naturally results in a robust feature selection method selecting a subset of features having the maximum worst case hgr correlation with the target variable .	0	12	10	4.9316406	-4.427167	0
364-13-78	our theoretical upper-bound is similar to the recent discrete chebyshev classifier ( dcc ) approach , while the proposed algorithm has significant computational advantages since it only requires solving a least square optimization problem .	finally , we numerically compare our proposed algorithm with the dcc classifier and show that the proposed algorithm results in better misclassification rate over various uci data repository datasets .	1	11	12	-5.9908376	5.184531	1
365-5-10	the stochastic block-model ( sbm ) is a generative model for graphs with `` communities '' for which , because of its simplicity , the theoretical understanding has advanced fast in recent years .	finding communities in networks is a problem that remains difficult , in spite of the amount of attention it has recently received .	0	1	0	2.0251286	-1.522573	0
365-5-10	finding communities in networks is a problem that remains difficult , in spite of the amount of attention it has recently received .	in particular , there have been various results showing that simple versions of spectral clustering using the normalized laplacian of the graph can recover the communities almost perfectly with high probability .	1	0	2	-5.938001	5.1853933	1
365-5-10	here we show that essentially the same algorithm used for the sbm and for its extension called degree-corrected sbm , works on a wider class of block-models , which we call preference frame models , with essentially the same guarantees .	finding communities in networks is a problem that remains difficult , in spite of the amount of attention it has recently received .	0	3	0	5.619911	-4.955432	0
365-5-10	finding communities in networks is a problem that remains difficult , in spite of the amount of attention it has recently received .	moreover , the parametrization we introduce clearly exhibits the free parameters needed to specify this class of models , and results in bounds that expose with more clarity the parameters that control the recovery error in this model class .	1	0	4	-5.904714	5.140113	1
365-5-10	in particular , there have been various results showing that simple versions of spectral clustering using the normalized laplacian of the graph can recover the communities almost perfectly with high probability .	the stochastic block-model ( sbm ) is a generative model for graphs with `` communities '' for which , because of its simplicity , the theoretical understanding has advanced fast in recent years .	0	2	1	5.3762746	-4.7809057	0
365-5-10	here we show that essentially the same algorithm used for the sbm and for its extension called degree-corrected sbm , works on a wider class of block-models , which we call preference frame models , with essentially the same guarantees .	the stochastic block-model ( sbm ) is a generative model for graphs with `` communities '' for which , because of its simplicity , the theoretical understanding has advanced fast in recent years .	0	3	1	5.6603527	-5.00235	0
365-5-10	moreover , the parametrization we introduce clearly exhibits the free parameters needed to specify this class of models , and results in bounds that expose with more clarity the parameters that control the recovery error in this model class .	the stochastic block-model ( sbm ) is a generative model for graphs with `` communities '' for which , because of its simplicity , the theoretical understanding has advanced fast in recent years .	0	4	1	5.56988	-4.910279	0
365-5-10	here we show that essentially the same algorithm used for the sbm and for its extension called degree-corrected sbm , works on a wider class of block-models , which we call preference frame models , with essentially the same guarantees .	in particular , there have been various results showing that simple versions of spectral clustering using the normalized laplacian of the graph can recover the communities almost perfectly with high probability .	0	3	2	4.8292646	-4.302304	0
365-5-10	in particular , there have been various results showing that simple versions of spectral clustering using the normalized laplacian of the graph can recover the communities almost perfectly with high probability .	moreover , the parametrization we introduce clearly exhibits the free parameters needed to specify this class of models , and results in bounds that expose with more clarity the parameters that control the recovery error in this model class .	1	2	4	-5.9989986	5.2234178	1
365-5-10	moreover , the parametrization we introduce clearly exhibits the free parameters needed to specify this class of models , and results in bounds that expose with more clarity the parameters that control the recovery error in this model class .	here we show that essentially the same algorithm used for the sbm and for its extension called degree-corrected sbm , works on a wider class of block-models , which we call preference frame models , with essentially the same guarantees .	0	4	3	4.6172175	-4.1293254	0
366-6-15	using the continuity of text from books , we train an encoderdecoder model that tries to reconstruct the surrounding sentences of an encoded passage .	we describe an approach for unsupervised learning of a generic , distributed sentence encoder .	0	1	0	5.3315477	-4.7526717	0
366-6-15	we describe an approach for unsupervised learning of a generic , distributed sentence encoder .	sentences that share semantic and syntactic properties are thus mapped to similar vector representations .	1	0	2	-5.754035	5.196434	1
366-6-15	we next introduce a simple vocabulary expansion method to encode words that were not seen as part of training , allowing us to expand our vocabulary to a million words .	we describe an approach for unsupervised learning of a generic , distributed sentence encoder .	0	3	0	5.5179586	-4.8844104	0
366-6-15	we describe an approach for unsupervised learning of a generic , distributed sentence encoder .	after training our model , we extract and evaluate our vectors with linear models on 8 tasks : semantic relatedness , paraphrase detection , image-sentence ranking , question-type classification and 4 benchmark sentiment and subjectivity datasets .	1	0	4	-5.870882	5.209487	1
366-6-15	we describe an approach for unsupervised learning of a generic , distributed sentence encoder .	the end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice .	1	0	5	-5.9667187	5.1749535	1
366-6-15	using the continuity of text from books , we train an encoderdecoder model that tries to reconstruct the surrounding sentences of an encoded passage .	sentences that share semantic and syntactic properties are thus mapped to similar vector representations .	1	1	2	-2.8684022	2.9143822	1
366-6-15	using the continuity of text from books , we train an encoderdecoder model that tries to reconstruct the surrounding sentences of an encoded passage .	we next introduce a simple vocabulary expansion method to encode words that were not seen as part of training , allowing us to expand our vocabulary to a million words .	1	1	3	-5.3785	4.912325	1
366-6-15	after training our model , we extract and evaluate our vectors with linear models on 8 tasks : semantic relatedness , paraphrase detection , image-sentence ranking , question-type classification and 4 benchmark sentiment and subjectivity datasets .	using the continuity of text from books , we train an encoderdecoder model that tries to reconstruct the surrounding sentences of an encoded passage .	0	4	1	5.405788	-4.753055	0
366-6-15	the end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice .	using the continuity of text from books , we train an encoderdecoder model that tries to reconstruct the surrounding sentences of an encoded passage .	0	5	1	5.2255793	-4.590394	0
366-6-15	sentences that share semantic and syntactic properties are thus mapped to similar vector representations .	we next introduce a simple vocabulary expansion method to encode words that were not seen as part of training , allowing us to expand our vocabulary to a million words .	1	2	3	-4.2299075	4.083554	1
366-6-15	after training our model , we extract and evaluate our vectors with linear models on 8 tasks : semantic relatedness , paraphrase detection , image-sentence ranking , question-type classification and 4 benchmark sentiment and subjectivity datasets .	sentences that share semantic and syntactic properties are thus mapped to similar vector representations .	0	4	2	4.769351	-4.3333864	0
366-6-15	sentences that share semantic and syntactic properties are thus mapped to similar vector representations .	the end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice .	1	2	5	-5.7791204	5.1516423	1
366-6-15	we next introduce a simple vocabulary expansion method to encode words that were not seen as part of training , allowing us to expand our vocabulary to a million words .	after training our model , we extract and evaluate our vectors with linear models on 8 tasks : semantic relatedness , paraphrase detection , image-sentence ranking , question-type classification and 4 benchmark sentiment and subjectivity datasets .	1	3	4	-2.3012748	2.3349552	1
366-6-15	we next introduce a simple vocabulary expansion method to encode words that were not seen as part of training , allowing us to expand our vocabulary to a million words .	the end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice .	1	3	5	-4.0787625	3.7417521	1
366-6-15	after training our model , we extract and evaluate our vectors with linear models on 8 tasks : semantic relatedness , paraphrase detection , image-sentence ranking , question-type classification and 4 benchmark sentiment and subjectivity datasets .	the end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice .	1	4	5	-4.8963895	4.452385	1
367-6-15	causal structure learning from time series data is a major scientific challenge .	extant algorithms assume that measurements occur sufficiently quickly ; more precisely , they assume approximately equal system and measurement timescales .	1	0	1	-5.8215685	5.2152934	1
367-6-15	in many domains , however , measurements occur at a significantly slower rate than the underlying system changes , but the size of the timescale mismatch is often unknown .	causal structure learning from time series data is a major scientific challenge .	0	2	0	5.664448	-5.0592422	0
367-6-15	this paper develops three causal structure learning algorithms , each of which discovers all dynamic causal graphs that explain the observed measurement data , perhaps given undersampling .	causal structure learning from time series data is a major scientific challenge .	0	3	0	5.704069	-4.994136	0
367-6-15	that is , these algorithms all learn causal structure in a `` rate-agnostic '' manner : they do not assume any particular relation between the measurement and system timescales .	causal structure learning from time series data is a major scientific challenge .	0	4	0	5.6401725	-5.027424	0
367-6-15	we apply these algorithms to data from simulations to gain insight into the challenge of undersampling .	causal structure learning from time series data is a major scientific challenge .	0	5	0	5.718688	-5.1042237	0
367-6-15	extant algorithms assume that measurements occur sufficiently quickly ; more precisely , they assume approximately equal system and measurement timescales .	in many domains , however , measurements occur at a significantly slower rate than the underlying system changes , but the size of the timescale mismatch is often unknown .	1	1	2	4.4005814	-3.7968416	0
367-6-15	extant algorithms assume that measurements occur sufficiently quickly ; more precisely , they assume approximately equal system and measurement timescales .	this paper develops three causal structure learning algorithms , each of which discovers all dynamic causal graphs that explain the observed measurement data , perhaps given undersampling .	1	1	3	-3.3546705	3.1792326	1
367-6-15	that is , these algorithms all learn causal structure in a `` rate-agnostic '' manner : they do not assume any particular relation between the measurement and system timescales .	extant algorithms assume that measurements occur sufficiently quickly ; more precisely , they assume approximately equal system and measurement timescales .	0	4	1	4.2253866	-3.7543492	0
367-6-15	extant algorithms assume that measurements occur sufficiently quickly ; more precisely , they assume approximately equal system and measurement timescales .	we apply these algorithms to data from simulations to gain insight into the challenge of undersampling .	1	1	5	-5.953242	5.023349	1
367-6-15	in many domains , however , measurements occur at a significantly slower rate than the underlying system changes , but the size of the timescale mismatch is often unknown .	this paper develops three causal structure learning algorithms , each of which discovers all dynamic causal graphs that explain the observed measurement data , perhaps given undersampling .	1	2	3	-5.8162775	5.2533	1
367-6-15	that is , these algorithms all learn causal structure in a `` rate-agnostic '' manner : they do not assume any particular relation between the measurement and system timescales .	in many domains , however , measurements occur at a significantly slower rate than the underlying system changes , but the size of the timescale mismatch is often unknown .	0	4	2	4.558935	-4.0038257	0
367-6-15	we apply these algorithms to data from simulations to gain insight into the challenge of undersampling .	in many domains , however , measurements occur at a significantly slower rate than the underlying system changes , but the size of the timescale mismatch is often unknown .	0	5	2	5.572444	-4.920688	0
367-6-15	that is , these algorithms all learn causal structure in a `` rate-agnostic '' manner : they do not assume any particular relation between the measurement and system timescales .	this paper develops three causal structure learning algorithms , each of which discovers all dynamic causal graphs that explain the observed measurement data , perhaps given undersampling .	0	4	3	4.8184457	-4.249155	0
367-6-15	this paper develops three causal structure learning algorithms , each of which discovers all dynamic causal graphs that explain the observed measurement data , perhaps given undersampling .	we apply these algorithms to data from simulations to gain insight into the challenge of undersampling .	1	3	5	-5.8322988	5.2149563	1
367-6-15	that is , these algorithms all learn causal structure in a `` rate-agnostic '' manner : they do not assume any particular relation between the measurement and system timescales .	we apply these algorithms to data from simulations to gain insight into the challenge of undersampling .	1	4	5	-5.4526744	4.878141	1
368-6-15	we propose to study this problem under the optimal transport ( wasserstein ) geometry , using curves that are restricted to be geodesic segments under that metric .	given a family of probability measures in p ( x ) , the space of probability measures on a hilbert space x , our goal in this paper is to highlight one ore more curves in p ( x ) that summarize efficiently that family .	0	1	0	3.6618981	-3.2308056	0
368-6-15	we show that concepts that play a key role in euclidean pca , such as data centering or orthogonality of principal directions , find a natural equivalent in the optimal transport geometry , using wasserstein means and differential geometry .	given a family of probability measures in p ( x ) , the space of probability measures on a hilbert space x , our goal in this paper is to highlight one ore more curves in p ( x ) that summarize efficiently that family .	0	2	0	5.4447966	-4.7915273	0
368-6-15	the implementation of these ideas is , however , computationally challenging .	given a family of probability measures in p ( x ) , the space of probability measures on a hilbert space x , our goal in this paper is to highlight one ore more curves in p ( x ) that summarize efficiently that family .	0	3	0	-2.3825037	2.4057486	1
368-6-15	given a family of probability measures in p ( x ) , the space of probability measures on a hilbert space x , our goal in this paper is to highlight one ore more curves in p ( x ) that summarize efficiently that family .	to achieve scalable algorithms that can handle thousands of measures , we propose to use a relaxed definition for geodesics and regularized optimal transport distances .	1	0	4	-5.8767624	5.165476	1
368-6-15	given a family of probability measures in p ( x ) , the space of probability measures on a hilbert space x , our goal in this paper is to highlight one ore more curves in p ( x ) that summarize efficiently that family .	the interest of our approach is demonstrated on images seen either as shapes or color histograms .	1	0	5	-5.9387007	4.9967012	1
368-6-15	we show that concepts that play a key role in euclidean pca , such as data centering or orthogonality of principal directions , find a natural equivalent in the optimal transport geometry , using wasserstein means and differential geometry .	we propose to study this problem under the optimal transport ( wasserstein ) geometry , using curves that are restricted to be geodesic segments under that metric .	0	2	1	5.409692	-4.752693	0
368-6-15	the implementation of these ideas is , however , computationally challenging .	we propose to study this problem under the optimal transport ( wasserstein ) geometry , using curves that are restricted to be geodesic segments under that metric .	0	3	1	-5.0177307	4.8098497	1
368-6-15	to achieve scalable algorithms that can handle thousands of measures , we propose to use a relaxed definition for geodesics and regularized optimal transport distances .	we propose to study this problem under the optimal transport ( wasserstein ) geometry , using curves that are restricted to be geodesic segments under that metric .	0	4	1	5.184766	-4.6065545	0
368-6-15	we propose to study this problem under the optimal transport ( wasserstein ) geometry , using curves that are restricted to be geodesic segments under that metric .	the interest of our approach is demonstrated on images seen either as shapes or color histograms .	1	1	5	-5.9874287	5.0741625	1
368-6-15	we show that concepts that play a key role in euclidean pca , such as data centering or orthogonality of principal directions , find a natural equivalent in the optimal transport geometry , using wasserstein means and differential geometry .	the implementation of these ideas is , however , computationally challenging .	1	2	3	4.35428	-3.9423885	0
368-6-15	to achieve scalable algorithms that can handle thousands of measures , we propose to use a relaxed definition for geodesics and regularized optimal transport distances .	we show that concepts that play a key role in euclidean pca , such as data centering or orthogonality of principal directions , find a natural equivalent in the optimal transport geometry , using wasserstein means and differential geometry .	0	4	2	-2.7190835	2.7113798	1
368-6-15	we show that concepts that play a key role in euclidean pca , such as data centering or orthogonality of principal directions , find a natural equivalent in the optimal transport geometry , using wasserstein means and differential geometry .	the interest of our approach is demonstrated on images seen either as shapes or color histograms .	1	2	5	-5.643345	4.9761324	1
368-6-15	the implementation of these ideas is , however , computationally challenging .	to achieve scalable algorithms that can handle thousands of measures , we propose to use a relaxed definition for geodesics and regularized optimal transport distances .	1	3	4	-5.912713	5.181747	1
368-6-15	the implementation of these ideas is , however , computationally challenging .	the interest of our approach is demonstrated on images seen either as shapes or color histograms .	1	3	5	-5.984035	5.104646	1
368-6-15	the interest of our approach is demonstrated on images seen either as shapes or color histograms .	to achieve scalable algorithms that can handle thousands of measures , we propose to use a relaxed definition for geodesics and regularized optimal transport distances .	0	5	4	4.9595137	-4.396858	0
369-6-15	to this end , we propose a framework for constructing and analyzing multilabel classification metrics which reveals novel results on a parametric form for population optimal classifiers , and additional insight into the role of label correlations .	multilabel classification is rapidly developing as an important aspect of modern predictive modeling , motivating study of its theoretical aspects .	0	1	0	5.676063	-5.0275617	0
369-6-15	multilabel classification is rapidly developing as an important aspect of modern predictive modeling , motivating study of its theoretical aspects .	in particular , we show that for multilabel metrics constructed as instance- , micro- and macroaverages , the population optimal classifier can be decomposed into binary classifiers based on the marginal instance-conditional distribution of each label , with a weak association between labels via the threshold .	1	0	2	-5.855159	5.196762	1
369-6-15	multilabel classification is rapidly developing as an important aspect of modern predictive modeling , motivating study of its theoretical aspects .	thus , our analysis extends the state of the art from a few known multilabel classification metrics such as hamming loss , to a general framework applicable to many of the classification metrics in common use .	1	0	3	-5.863747	5.116103	1
369-6-15	based on the population-optimal classifier , we propose a computationally efficient and general-purpose plug-in classification algorithm , and prove its consistency with respect to the metric of interest .	multilabel classification is rapidly developing as an important aspect of modern predictive modeling , motivating study of its theoretical aspects .	0	4	0	5.7127295	-5.073646	0
369-6-15	empirical results on synthetic and benchmark datasets are supportive of our theoretical findings .	multilabel classification is rapidly developing as an important aspect of modern predictive modeling , motivating study of its theoretical aspects .	0	5	0	5.6282063	-5.0384107	0
369-6-15	[CLS] in particular, we show that for multilabel metrics constructed as instance -, micro - and macroaverages, the population optimal classifier can be decomposed into binary classifiers based on the marginal instance - conditional distribution of each label, with a weak	to this end , we propose a framework for constructing and analyzing multilabel classification metrics which reveals novel results on a parametric form for population optimal classifiers , and additional insight into the role of label correlations .	0	2	1	-5.1674027	4.4221964	1
369-6-15	to this end , we propose a framework for constructing and analyzing multilabel classification metrics which reveals novel results on a parametric form for population optimal classifiers , and additional insight into the role of label correlations .	thus , our analysis extends the state of the art from a few known multilabel classification metrics such as hamming loss , to a general framework applicable to many of the classification metrics in common use .	1	1	3	1.2445295	-1.0561442	0
369-6-15	based on the population-optimal classifier , we propose a computationally efficient and general-purpose plug-in classification algorithm , and prove its consistency with respect to the metric of interest .	to this end , we propose a framework for constructing and analyzing multilabel classification metrics which reveals novel results on a parametric form for population optimal classifiers , and additional insight into the role of label correlations .	0	4	1	-5.3578215	4.824179	1
369-6-15	to this end , we propose a framework for constructing and analyzing multilabel classification metrics which reveals novel results on a parametric form for population optimal classifiers , and additional insight into the role of label correlations .	empirical results on synthetic and benchmark datasets are supportive of our theoretical findings .	1	1	5	-4.114952	3.7854307	1
369-6-15	[CLS] in particular, we show that for multilabel metrics constructed as instance -, micro - and macroaverages, the population optimal classifier can be decomposed into binary classifiers based on the marginal instance - conditional distribution of each label, with a weak association between	thus , our analysis extends the state of the art from a few known multilabel classification metrics such as hamming loss , to a general framework applicable to many of the classification metrics in common use .	1	2	3	-5.4774632	4.884935	1
369-6-15	based on the population-optimal classifier , we propose a computationally efficient and general-purpose plug-in classification algorithm , and prove its consistency with respect to the metric of interest .	in particular , we show that for multilabel metrics constructed as instance- , micro- and macroaverages , the population optimal classifier can be decomposed into binary classifiers based on the marginal instance-conditional distribution of each label , with a weak association between labels via the threshold .	0	4	2	3.4508405	-3.1875372	0
369-6-15	empirical results on synthetic and benchmark datasets are supportive of our theoretical findings .	in particular , we show that for multilabel metrics constructed as instance- , micro- and macroaverages , the population optimal classifier can be decomposed into binary classifiers based on the marginal instance-conditional distribution of each label , with a weak association between labels via the threshold .	0	5	2	5.165592	-4.5561914	0
369-6-15	based on the population-optimal classifier , we propose a computationally efficient and general-purpose plug-in classification algorithm , and prove its consistency with respect to the metric of interest .	thus , our analysis extends the state of the art from a few known multilabel classification metrics such as hamming loss , to a general framework applicable to many of the classification metrics in common use .	0	4	3	-5.4737096	4.9371796	1
369-6-15	empirical results on synthetic and benchmark datasets are supportive of our theoretical findings .	thus , our analysis extends the state of the art from a few known multilabel classification metrics such as hamming loss , to a general framework applicable to many of the classification metrics in common use .	0	5	3	3.8880332	-3.580102	0
369-6-15	empirical results on synthetic and benchmark datasets are supportive of our theoretical findings .	based on the population-optimal classifier , we propose a computationally efficient and general-purpose plug-in classification algorithm , and prove its consistency with respect to the metric of interest .	0	5	4	5.3243113	-4.6827707	0
370-6-15	we develop parallel predictive entropy search ( ppes ) , a novel algorithm for bayesian optimization of expensive black-box objective functions .	at each iteration , ppes aims to select a batch of points which will maximize the information gain about the global maximizer of the objective .	1	0	1	-5.7784653	5.2008204	1
370-6-15	we develop parallel predictive entropy search ( ppes ) , a novel algorithm for bayesian optimization of expensive black-box objective functions .	well known strategies exist for suggesting a single evaluation point based on previous observations , while far fewer are known for selecting batches of points to evaluate in parallel .	1	0	2	-5.446845	4.9367743	1
370-6-15	we develop parallel predictive entropy search ( ppes ) , a novel algorithm for bayesian optimization of expensive black-box objective functions .	the few batch selection schemes that have been studied all resort to greedy methods to compute an optimal batch .	1	0	3	-5.769185	5.17127	1
370-6-15	to the best of our knowledge , ppes is the first nongreedy batch bayesian optimization strategy .	we develop parallel predictive entropy search ( ppes ) , a novel algorithm for bayesian optimization of expensive black-box objective functions .	0	4	0	5.5903645	-4.97895	0
370-6-15	we develop parallel predictive entropy search ( ppes ) , a novel algorithm for bayesian optimization of expensive black-box objective functions .	we demonstrate the benefit of this approach in optimization performance on both synthetic and real world applications , including problems in machine learning , rocket science and robotics .	1	0	5	-5.9917355	5.2247686	1
370-6-15	well known strategies exist for suggesting a single evaluation point based on previous observations , while far fewer are known for selecting batches of points to evaluate in parallel .	at each iteration , ppes aims to select a batch of points which will maximize the information gain about the global maximizer of the objective .	0	2	1	3.050185	-2.7793944	0
370-6-15	at each iteration , ppes aims to select a batch of points which will maximize the information gain about the global maximizer of the objective .	the few batch selection schemes that have been studied all resort to greedy methods to compute an optimal batch .	1	1	3	-5.787889	5.1822453	1
370-6-15	to the best of our knowledge , ppes is the first nongreedy batch bayesian optimization strategy .	at each iteration , ppes aims to select a batch of points which will maximize the information gain about the global maximizer of the objective .	0	4	1	5.42544	-4.7530446	0
370-6-15	we demonstrate the benefit of this approach in optimization performance on both synthetic and real world applications , including problems in machine learning , rocket science and robotics .	at each iteration , ppes aims to select a batch of points which will maximize the information gain about the global maximizer of the objective .	0	5	1	5.447731	-4.8118606	0
370-6-15	the few batch selection schemes that have been studied all resort to greedy methods to compute an optimal batch .	well known strategies exist for suggesting a single evaluation point based on previous observations , while far fewer are known for selecting batches of points to evaluate in parallel .	0	3	2	3.4266746	-3.1450257	0
370-6-15	to the best of our knowledge , ppes is the first nongreedy batch bayesian optimization strategy .	well known strategies exist for suggesting a single evaluation point based on previous observations , while far fewer are known for selecting batches of points to evaluate in parallel .	0	4	2	5.5343666	-4.9117727	0
370-6-15	we demonstrate the benefit of this approach in optimization performance on both synthetic and real world applications , including problems in machine learning , rocket science and robotics .	well known strategies exist for suggesting a single evaluation point based on previous observations , while far fewer are known for selecting batches of points to evaluate in parallel .	0	5	2	5.550915	-4.9392247	0
370-6-15	to the best of our knowledge , ppes is the first nongreedy batch bayesian optimization strategy .	the few batch selection schemes that have been studied all resort to greedy methods to compute an optimal batch .	0	4	3	4.6125975	-4.1186175	0
370-6-15	the few batch selection schemes that have been studied all resort to greedy methods to compute an optimal batch .	we demonstrate the benefit of this approach in optimization performance on both synthetic and real world applications , including problems in machine learning , rocket science and robotics .	1	3	5	-5.9514556	5.034979	1
370-6-15	we demonstrate the benefit of this approach in optimization performance on both synthetic and real world applications , including problems in machine learning , rocket science and robotics .	to the best of our knowledge , ppes is the first nongreedy batch bayesian optimization strategy .	0	5	4	3.530407	-3.256005	0
371-4-6	[CLS] as we shall see, the bandit problem we tackle requires us to address the exploration / exploitation / independence trade - off, which we do by considering the idea of a waiting arm in the new remix - ucb algorithm, a generalization of improved - uc	we study the restless bandit problem where arms are associated with stationary -mixing processes and where rewards are therefore dependent : the question that arises from this setting is that of carefully recovering some independence by `ignoring ' the values of some rewards .	0	1	0	5.3127384	-4.705156	0
371-4-6	we provide a regret analysis for this bandit strategy ; two noticeable features of remix-ucb are that i ) it reduces to the regular improved-ucb when the -mixing coefficients are all 0 , i.e .	we study the restless bandit problem where arms are associated with stationary -mixing processes and where rewards are therefore dependent : the question that arises from this setting is that of carefully recovering some independence by `ignoring ' the values of some rewards .	0	2	0	5.5483847	-4.940886	0
371-4-6	we study the restless bandit problem where arms are associated with stationary -mixing processes and where rewards are therefore dependent : the question that arises from this setting is that of carefully recovering some independence by `ignoring ' the values of some rewards .	[CLS] when the i. i. d scenario is recovered, and ii ) when ( n ) = o ( n - ), it is able to ensure a controlled regret of order ( - 2 ) / log1 / t, where encodes the distance between	1	0	3	-5.8617034	5.1949606	1
371-4-6	we provide a regret analysis for this bandit strategy ; two noticeable features of remix-ucb are that i ) it reduces to the regular improved-ucb when the -mixing coefficients are all 0 , i.e .	[CLS] as we shall see, the bandit problem we tackle requires us to address the exploration / exploitation / independence trade - off, which we do by considering the idea of a waiting arm in the new remix - ucb algorithm, a generalization of improved - ucb for	0	2	1	-3.2274776	3.057095	1
371-4-6	[CLS] as we shall see, the bandit problem we tackle requires us to address the exploration / exploitation / independence trade - off, which we do by considering the idea of a waiting arm in the new remix - ucb algorithm, a generalization of improved	[CLS] when the i. i. d scenario is recovered, and ii ) when ( n ) = o ( n - ), it is able to ensure a controlled regret of order ( - 2 ) / log1 / t, where encodes the	1	1	3	-1.6466182	1.6792804	1
371-4-6	we provide a regret analysis for this bandit strategy ; two noticeable features of remix-ucb are that i ) it reduces to the regular improved-ucb when the -mixing coefficients are all 0 , i.e .	[CLS] when the i. i. d scenario is recovered, and ii ) when ( n ) = o ( n - ), it is able to ensure a controlled regret of order ( - 2 ) / log1 / t, where encodes the distance between the best	1	2	3	-5.0603766	4.6538076	1
372-7-21	imaging neuroscience links human behavior to aspects of brain biology in everincreasing datasets .	existing neuroimaging methods typically perform either discovery of unknown neural structure or testing of neural structure associated with mental tasks .	1	0	1	-4.3731766	4.10759	1
372-7-21	however , testing hypotheses on the neural correlates underlying larger sets of mental tasks necessitates adequate representations for the observations .	imaging neuroscience links human behavior to aspects of brain biology in everincreasing datasets .	0	2	0	5.616368	-5.0482407	0
372-7-21	we therefore propose to blend representation modelling and task classification into a unified statistical learning problem .	imaging neuroscience links human behavior to aspects of brain biology in everincreasing datasets .	0	3	0	5.6193085	-5.0981293	0
372-7-21	imaging neuroscience links human behavior to aspects of brain biology in everincreasing datasets .	a multinomial logistic regression is introduced that is constrained by factored coefficients and coupled with an autoencoder .	1	0	4	-5.848076	5.1705966	1
372-7-21	we show that this approach yields more accurate and interpretable neural models of psychological tasks in a reference dataset , as well as better generalization to other datasets .	imaging neuroscience links human behavior to aspects of brain biology in everincreasing datasets .	0	5	0	5.640862	-5.0797043	0
372-7-21	keywords : brain imaging , cognitive science , semi-supervised learning , systems biology	imaging neuroscience links human behavior to aspects of brain biology in everincreasing datasets .	0	6	0	5.5073037	-5.032688	0
372-7-21	however , testing hypotheses on the neural correlates underlying larger sets of mental tasks necessitates adequate representations for the observations .	existing neuroimaging methods typically perform either discovery of unknown neural structure or testing of neural structure associated with mental tasks .	0	2	1	5.696895	-5.0339413	0
372-7-21	existing neuroimaging methods typically perform either discovery of unknown neural structure or testing of neural structure associated with mental tasks .	we therefore propose to blend representation modelling and task classification into a unified statistical learning problem .	1	1	3	-5.968688	5.135796	1
372-7-21	existing neuroimaging methods typically perform either discovery of unknown neural structure or testing of neural structure associated with mental tasks .	a multinomial logistic regression is introduced that is constrained by factored coefficients and coupled with an autoencoder .	1	1	4	-5.90341	5.1894817	1
372-7-21	we show that this approach yields more accurate and interpretable neural models of psychological tasks in a reference dataset , as well as better generalization to other datasets .	existing neuroimaging methods typically perform either discovery of unknown neural structure or testing of neural structure associated with mental tasks .	0	5	1	5.643376	-5.052945	0
372-7-21	existing neuroimaging methods typically perform either discovery of unknown neural structure or testing of neural structure associated with mental tasks .	keywords : brain imaging , cognitive science , semi-supervised learning , systems biology	1	1	6	-5.89627	5.1853485	1
372-7-21	we therefore propose to blend representation modelling and task classification into a unified statistical learning problem .	however , testing hypotheses on the neural correlates underlying larger sets of mental tasks necessitates adequate representations for the observations .	0	3	2	4.4964314	-4.06168	0
372-7-21	a multinomial logistic regression is introduced that is constrained by factored coefficients and coupled with an autoencoder .	however , testing hypotheses on the neural correlates underlying larger sets of mental tasks necessitates adequate representations for the observations .	0	4	2	3.8316367	-3.4518974	0
372-7-21	we show that this approach yields more accurate and interpretable neural models of psychological tasks in a reference dataset , as well as better generalization to other datasets .	however , testing hypotheses on the neural correlates underlying larger sets of mental tasks necessitates adequate representations for the observations .	0	5	2	4.6906447	-4.223913	0
372-7-21	keywords : brain imaging , cognitive science , semi-supervised learning , systems biology	however , testing hypotheses on the neural correlates underlying larger sets of mental tasks necessitates adequate representations for the observations .	0	6	2	5.3202233	-4.846182	0
372-7-21	a multinomial logistic regression is introduced that is constrained by factored coefficients and coupled with an autoencoder .	we therefore propose to blend representation modelling and task classification into a unified statistical learning problem .	0	4	3	3.5219626	-3.158376	0
372-7-21	we therefore propose to blend representation modelling and task classification into a unified statistical learning problem .	we show that this approach yields more accurate and interpretable neural models of psychological tasks in a reference dataset , as well as better generalization to other datasets .	1	3	5	-5.1658	4.6945024	1
372-7-21	we therefore propose to blend representation modelling and task classification into a unified statistical learning problem .	keywords : brain imaging , cognitive science , semi-supervised learning , systems biology	1	3	6	-5.7121077	4.7705693	1
372-7-21	we show that this approach yields more accurate and interpretable neural models of psychological tasks in a reference dataset , as well as better generalization to other datasets .	a multinomial logistic regression is introduced that is constrained by factored coefficients and coupled with an autoencoder .	0	5	4	4.589861	-4.226347	0
372-7-21	a multinomial logistic regression is introduced that is constrained by factored coefficients and coupled with an autoencoder .	keywords : brain imaging , cognitive science , semi-supervised learning , systems biology	1	4	6	-5.8193436	4.93658	1
372-7-21	we show that this approach yields more accurate and interpretable neural models of psychological tasks in a reference dataset , as well as better generalization to other datasets .	keywords : brain imaging , cognitive science , semi-supervised learning , systems biology	1	5	6	-5.342174	4.7709026	1
373-4-6	gaussian processes have been successful in both supervised and unsupervised machine learning tasks , but their computational complexity has constrained practical applications .	we introduce a new approximation for large-scale gaussian processes , the gaussian process random field ( gprf ) , in which local gps are coupled via pairwise potentials .	1	0	1	-5.2280703	4.9434595	1
373-4-6	gaussian processes have been successful in both supervised and unsupervised machine learning tasks , but their computational complexity has constrained practical applications .	the gprf likelihood is a simple , tractable , and parallelizeable approximation to the full gp marginal likelihood , enabling latent variable modeling and hyperparameter selection on large datasets .	1	0	2	-5.83832	5.1965113	1
373-4-6	we demonstrate its effectiveness on synthetic spatial data as well as a real-world application to seismic event location .	gaussian processes have been successful in both supervised and unsupervised machine learning tasks , but their computational complexity has constrained practical applications .	0	3	0	5.64002	-5.049659	0
373-4-6	we introduce a new approximation for large-scale gaussian processes , the gaussian process random field ( gprf ) , in which local gps are coupled via pairwise potentials .	the gprf likelihood is a simple , tractable , and parallelizeable approximation to the full gp marginal likelihood , enabling latent variable modeling and hyperparameter selection on large datasets .	1	1	2	-5.844514	5.1890144	1
373-4-6	we introduce a new approximation for large-scale gaussian processes , the gaussian process random field ( gprf ) , in which local gps are coupled via pairwise potentials .	we demonstrate its effectiveness on synthetic spatial data as well as a real-world application to seismic event location .	1	1	3	-5.985942	5.1499825	1
373-4-6	we demonstrate its effectiveness on synthetic spatial data as well as a real-world application to seismic event location .	the gprf likelihood is a simple , tractable , and parallelizeable approximation to the full gp marginal likelihood , enabling latent variable modeling and hyperparameter selection on large datasets .	0	3	2	5.067169	-4.5549793	0
374-8-28	kernel-based nonparametric statistics have been proposed for this task which make fewer assumptions on the distributions than traditional parametric approach .	detecting the emergence of an abrupt change-point is a classic problem in statistics and machine learning .	0	1	0	5.5870986	-5.0459747	0
374-8-28	detecting the emergence of an abrupt change-point is a classic problem in statistics and machine learning .	however , none of the existing kernel statistics has provided a computationally efficient way to characterize the extremal behavior of the statistic .	1	0	2	-5.8468804	5.2377586	1
374-8-28	such characterization is crucial for setting the detection threshold , to control the significance level in the offline case as well as the average run length in the online case .	detecting the emergence of an abrupt change-point is a classic problem in statistics and machine learning .	0	3	0	5.61401	-5.061033	0
374-8-28	in this paper we propose two related computationally efficient m -statistics for kernel-based change-point detection when the amount of background data is large .	detecting the emergence of an abrupt change-point is a classic problem in statistics and machine learning .	0	4	0	5.676414	-5.090566	0
374-8-28	detecting the emergence of an abrupt change-point is a classic problem in statistics and machine learning .	a novel theoretical result of the paper is the characterization of the tail probability of these statistics using a new technique based on change-ofmeasure .	1	0	5	-5.8952255	5.1259665	1
374-8-28	such characterization provides us accurate detection thresholds for both offline and online cases in computationally efficient manner , without the need to resort to the more expensive simulations such as bootstrapping .	detecting the emergence of an abrupt change-point is a classic problem in statistics and machine learning .	0	6	0	5.6013327	-5.024913	0
374-8-28	detecting the emergence of an abrupt change-point is a classic problem in statistics and machine learning .	we show that our methods perform well in both synthetic and real world data .	1	0	7	-5.9915094	5.1475797	1
374-8-28	kernel-based nonparametric statistics have been proposed for this task which make fewer assumptions on the distributions than traditional parametric approach .	however , none of the existing kernel statistics has provided a computationally efficient way to characterize the extremal behavior of the statistic .	1	1	2	-0.33291614	0.6076145	1
374-8-28	kernel-based nonparametric statistics have been proposed for this task which make fewer assumptions on the distributions than traditional parametric approach .	such characterization is crucial for setting the detection threshold , to control the significance level in the offline case as well as the average run length in the online case .	1	1	3	2.3880095	-2.2700255	0
374-8-28	in this paper we propose two related computationally efficient m -statistics for kernel-based change-point detection when the amount of background data is large .	kernel-based nonparametric statistics have been proposed for this task which make fewer assumptions on the distributions than traditional parametric approach .	0	4	1	2.362475	-2.1033616	0
374-8-28	kernel-based nonparametric statistics have been proposed for this task which make fewer assumptions on the distributions than traditional parametric approach .	a novel theoretical result of the paper is the characterization of the tail probability of these statistics using a new technique based on change-ofmeasure .	1	1	5	-5.9568186	5.101657	1
374-8-28	such characterization provides us accurate detection thresholds for both offline and online cases in computationally efficient manner , without the need to resort to the more expensive simulations such as bootstrapping .	kernel-based nonparametric statistics have been proposed for this task which make fewer assumptions on the distributions than traditional parametric approach .	0	6	1	4.5652494	-4.0518007	0
374-8-28	kernel-based nonparametric statistics have been proposed for this task which make fewer assumptions on the distributions than traditional parametric approach .	we show that our methods perform well in both synthetic and real world data .	1	1	7	-5.980574	5.0480123	1
374-8-28	however , none of the existing kernel statistics has provided a computationally efficient way to characterize the extremal behavior of the statistic .	such characterization is crucial for setting the detection threshold , to control the significance level in the offline case as well as the average run length in the online case .	1	2	3	-4.4416475	4.2149963	1
374-8-28	in this paper we propose two related computationally efficient m -statistics for kernel-based change-point detection when the amount of background data is large .	however , none of the existing kernel statistics has provided a computationally efficient way to characterize the extremal behavior of the statistic .	0	4	2	4.7510986	-4.2548833	0
374-8-28	however , none of the existing kernel statistics has provided a computationally efficient way to characterize the extremal behavior of the statistic .	a novel theoretical result of the paper is the characterization of the tail probability of these statistics using a new technique based on change-ofmeasure .	1	2	5	-5.991478	5.178042	1
374-8-28	such characterization provides us accurate detection thresholds for both offline and online cases in computationally efficient manner , without the need to resort to the more expensive simulations such as bootstrapping .	however , none of the existing kernel statistics has provided a computationally efficient way to characterize the extremal behavior of the statistic .	0	6	2	5.541833	-4.8079524	0
374-8-28	however , none of the existing kernel statistics has provided a computationally efficient way to characterize the extremal behavior of the statistic .	we show that our methods perform well in both synthetic and real world data .	1	2	7	-5.99151	5.101853	1
374-8-28	such characterization is crucial for setting the detection threshold , to control the significance level in the offline case as well as the average run length in the online case .	in this paper we propose two related computationally efficient m -statistics for kernel-based change-point detection when the amount of background data is large .	1	3	4	-3.352305	3.25586	1
374-8-28	a novel theoretical result of the paper is the characterization of the tail probability of these statistics using a new technique based on change-ofmeasure .	such characterization is crucial for setting the detection threshold , to control the significance level in the offline case as well as the average run length in the online case .	0	5	3	0.9681558	-0.80906945	0
374-8-28	such characterization provides us accurate detection thresholds for both offline and online cases in computationally efficient manner , without the need to resort to the more expensive simulations such as bootstrapping .	such characterization is crucial for setting the detection threshold , to control the significance level in the offline case as well as the average run length in the online case .	0	6	3	4.4937005	-4.0011096	0
374-8-28	we show that our methods perform well in both synthetic and real world data .	such characterization is crucial for setting the detection threshold , to control the significance level in the offline case as well as the average run length in the online case .	0	7	3	5.320389	-4.756257	0
374-8-28	a novel theoretical result of the paper is the characterization of the tail probability of these statistics using a new technique based on change-ofmeasure .	in this paper we propose two related computationally efficient m -statistics for kernel-based change-point detection when the amount of background data is large .	0	5	4	5.2574816	-4.558979	0
374-8-28	in this paper we propose two related computationally efficient m -statistics for kernel-based change-point detection when the amount of background data is large .	such characterization provides us accurate detection thresholds for both offline and online cases in computationally efficient manner , without the need to resort to the more expensive simulations such as bootstrapping .	1	4	6	-5.856187	5.208229	1
374-8-28	in this paper we propose two related computationally efficient m -statistics for kernel-based change-point detection when the amount of background data is large .	we show that our methods perform well in both synthetic and real world data .	1	4	7	-6.0256057	5.184703	1
374-8-28	such characterization provides us accurate detection thresholds for both offline and online cases in computationally efficient manner , without the need to resort to the more expensive simulations such as bootstrapping .	a novel theoretical result of the paper is the characterization of the tail probability of these statistics using a new technique based on change-ofmeasure .	0	6	5	3.9573174	-3.5382004	0
374-8-28	we show that our methods perform well in both synthetic and real world data .	a novel theoretical result of the paper is the characterization of the tail probability of these statistics using a new technique based on change-ofmeasure .	0	7	5	4.6328034	-4.1400833	0
374-8-28	we show that our methods perform well in both synthetic and real world data .	such characterization provides us accurate detection thresholds for both offline and online cases in computationally efficient manner , without the need to resort to the more expensive simulations such as bootstrapping .	0	7	6	3.0765495	-2.8996878	0
375-7-21	we propose a general framework for studying adaptive regret bounds in the online learning setting , subsuming model selection and data-dependent bounds .	given a data- or model-dependent bound we ask , `` does there exist some algorithm achieving this bound ? ''	1	0	1	-0.07412529	0.33387038	1
375-7-21	we propose a general framework for studying adaptive regret bounds in the online learning setting , subsuming model selection and data-dependent bounds .	we show that modifications to recently introduced sequential complexity measures can be used to answer this question by providing sufficient conditions under which adaptive rates can be achieved .	1	0	2	-2.439735	2.5443387	1
375-7-21	in particular each adaptive rate induces a set of so-called offset complexity measures , and obtaining small upper bounds on these quantities is sufficient to demonstrate achievability .	we propose a general framework for studying adaptive regret bounds in the online learning setting , subsuming model selection and data-dependent bounds .	0	3	0	3.2073762	-2.9509969	0
375-7-21	a cornerstone of our analysis technique is the use of one-sided tail inequalities to bound suprema of offset random processes .	we propose a general framework for studying adaptive regret bounds in the online learning setting , subsuming model selection and data-dependent bounds .	0	4	0	5.166991	-4.5380535	0
375-7-21	our framework recovers and improves a wide variety of adaptive bounds including quantile bounds , second order data-dependent bounds , and small loss bounds .	we propose a general framework for studying adaptive regret bounds in the online learning setting , subsuming model selection and data-dependent bounds .	0	5	0	5.411502	-4.8189487	0
375-7-21	we propose a general framework for studying adaptive regret bounds in the online learning setting , subsuming model selection and data-dependent bounds .	in addition we derive a new type of adaptive bound for online linear optimization based on the spectral norm , as well as a new online pac-bayes theorem .	1	0	6	-5.7545357	5.1352997	1
375-7-21	given a data- or model-dependent bound we ask , `` does there exist some algorithm achieving this bound ? ''	we show that modifications to recently introduced sequential complexity measures can be used to answer this question by providing sufficient conditions under which adaptive rates can be achieved .	1	1	2	-4.257909	4.019124	1
375-7-21	given a data- or model-dependent bound we ask , `` does there exist some algorithm achieving this bound ? ''	in particular each adaptive rate induces a set of so-called offset complexity measures , and obtaining small upper bounds on these quantities is sufficient to demonstrate achievability .	1	1	3	-5.6911473	5.1473894	1
375-7-21	a cornerstone of our analysis technique is the use of one-sided tail inequalities to bound suprema of offset random processes .	given a data- or model-dependent bound we ask , `` does there exist some algorithm achieving this bound ? ''	0	4	1	4.487752	-4.061063	0
375-7-21	our framework recovers and improves a wide variety of adaptive bounds including quantile bounds , second order data-dependent bounds , and small loss bounds .	given a data- or model-dependent bound we ask , `` does there exist some algorithm achieving this bound ? ''	0	5	1	4.7124996	-4.304896	0
375-7-21	in addition we derive a new type of adaptive bound for online linear optimization based on the spectral norm , as well as a new online pac-bayes theorem .	given a data- or model-dependent bound we ask , `` does there exist some algorithm achieving this bound ? ''	0	6	1	5.041416	-4.507588	0
375-7-21	in particular each adaptive rate induces a set of so-called offset complexity measures , and obtaining small upper bounds on these quantities is sufficient to demonstrate achievability .	we show that modifications to recently introduced sequential complexity measures can be used to answer this question by providing sufficient conditions under which adaptive rates can be achieved .	0	3	2	4.200556	-3.8466656	0
375-7-21	a cornerstone of our analysis technique is the use of one-sided tail inequalities to bound suprema of offset random processes .	we show that modifications to recently introduced sequential complexity measures can be used to answer this question by providing sufficient conditions under which adaptive rates can be achieved .	0	4	2	4.0843277	-3.753143	0
375-7-21	our framework recovers and improves a wide variety of adaptive bounds including quantile bounds , second order data-dependent bounds , and small loss bounds .	we show that modifications to recently introduced sequential complexity measures can be used to answer this question by providing sufficient conditions under which adaptive rates can be achieved .	0	5	2	4.106023	-3.7686412	0
375-7-21	in addition we derive a new type of adaptive bound for online linear optimization based on the spectral norm , as well as a new online pac-bayes theorem .	we show that modifications to recently introduced sequential complexity measures can be used to answer this question by providing sufficient conditions under which adaptive rates can be achieved .	0	6	2	4.9142046	-4.4161034	0
375-7-21	a cornerstone of our analysis technique is the use of one-sided tail inequalities to bound suprema of offset random processes .	in particular each adaptive rate induces a set of so-called offset complexity measures , and obtaining small upper bounds on these quantities is sufficient to demonstrate achievability .	0	4	3	-2.5034904	2.4967134	1
375-7-21	in particular each adaptive rate induces a set of so-called offset complexity measures , and obtaining small upper bounds on these quantities is sufficient to demonstrate achievability .	our framework recovers and improves a wide variety of adaptive bounds including quantile bounds , second order data-dependent bounds , and small loss bounds .	1	3	5	2.2416446	-2.1005661	0
375-7-21	in particular each adaptive rate induces a set of so-called offset complexity measures , and obtaining small upper bounds on these quantities is sufficient to demonstrate achievability .	in addition we derive a new type of adaptive bound for online linear optimization based on the spectral norm , as well as a new online pac-bayes theorem .	1	3	6	-3.0847056	3.0626743	1
375-7-21	our framework recovers and improves a wide variety of adaptive bounds including quantile bounds , second order data-dependent bounds , and small loss bounds .	a cornerstone of our analysis technique is the use of one-sided tail inequalities to bound suprema of offset random processes .	0	5	4	-2.1356075	2.2065988	1
375-7-21	in addition we derive a new type of adaptive bound for online linear optimization based on the spectral norm , as well as a new online pac-bayes theorem .	a cornerstone of our analysis technique is the use of one-sided tail inequalities to bound suprema of offset random processes .	0	6	4	1.5167285	-1.4087731	0
375-7-21	in addition we derive a new type of adaptive bound for online linear optimization based on the spectral norm , as well as a new online pac-bayes theorem .	our framework recovers and improves a wide variety of adaptive bounds including quantile bounds , second order data-dependent bounds , and small loss bounds .	0	6	5	3.062329	-2.9172702	0
376-5-10	our approach consists of minimizing a convex objective by approximately solving a sequence of well-chosen auxiliary problems , leading to faster convergence .	we introduce a generic scheme for accelerating first-order optimization methods in the sense of nesterov , which builds upon a new analysis of the accelerated proximal point algorithm .	0	1	0	1.2952034	-1.1019696	0
376-5-10	we introduce a generic scheme for accelerating first-order optimization methods in the sense of nesterov , which builds upon a new analysis of the accelerated proximal point algorithm .	this strategy applies to a large class of algorithms , including gradient descent , block coordinate descent , sag , saga , sdca , svrg , finito/miso , and their proximal variants .	1	0	2	-3.009921	3.0147662	1
376-5-10	for all of these methods , we provide acceleration and explicit support for non-strongly convex objectives .	we introduce a generic scheme for accelerating first-order optimization methods in the sense of nesterov , which builds upon a new analysis of the accelerated proximal point algorithm .	0	3	0	-0.2149198	0.37238717	1
376-5-10	we introduce a generic scheme for accelerating first-order optimization methods in the sense of nesterov , which builds upon a new analysis of the accelerated proximal point algorithm .	in addition to theoretical speed-up , we also show that acceleration is useful in practice , especially for ill-conditioned problems where we measure significant improvements .	1	0	4	-5.8395185	5.152656	1
376-5-10	our approach consists of minimizing a convex objective by approximately solving a sequence of well-chosen auxiliary problems , leading to faster convergence .	this strategy applies to a large class of algorithms , including gradient descent , block coordinate descent , sag , saga , sdca , svrg , finito/miso , and their proximal variants .	1	1	2	-5.6734133	5.022607	1
376-5-10	for all of these methods , we provide acceleration and explicit support for non-strongly convex objectives .	our approach consists of minimizing a convex objective by approximately solving a sequence of well-chosen auxiliary problems , leading to faster convergence .	0	3	1	-2.8819265	2.7948809	1
376-5-10	our approach consists of minimizing a convex objective by approximately solving a sequence of well-chosen auxiliary problems , leading to faster convergence .	in addition to theoretical speed-up , we also show that acceleration is useful in practice , especially for ill-conditioned problems where we measure significant improvements .	1	1	4	-5.953389	5.1553745	1
376-5-10	this strategy applies to a large class of algorithms , including gradient descent , block coordinate descent , sag , saga , sdca , svrg , finito/miso , and their proximal variants .	for all of these methods , we provide acceleration and explicit support for non-strongly convex objectives .	1	2	3	2.9268768	-2.6962078	0
376-5-10	in addition to theoretical speed-up , we also show that acceleration is useful in practice , especially for ill-conditioned problems where we measure significant improvements .	this strategy applies to a large class of algorithms , including gradient descent , block coordinate descent , sag , saga , sdca , svrg , finito/miso , and their proximal variants .	0	4	2	3.9434981	-3.6818888	0
376-5-10	in addition to theoretical speed-up , we also show that acceleration is useful in practice , especially for ill-conditioned problems where we measure significant improvements .	for all of these methods , we provide acceleration and explicit support for non-strongly convex objectives .	0	4	3	5.2501297	-4.7098684	0
377-8-28	determinantal point processes ( dpps ) are point process models that naturally encode diversity between the points of a given realization , through a positive definite kernel k. dpps possess desirable properties , such as exact sampling or analyticity of the moments , but learning the parameters of kernel k through likelihood-based inference is not straightforward .	first , the kernel that appears in the likelihood is not k , but another kernel l related to k through an often intractable spectral decomposition .	1	0	1	-5.8322773	5.2506485	1
377-8-28	this issue is typically bypassed in machine learning by directly parametrizing the kernel l , at the price of some interpretability of the model parameters .	determinantal point processes ( dpps ) are point process models that naturally encode diversity between the points of a given realization , through a positive definite kernel k. dpps possess desirable properties , such as exact sampling or analyticity of the moments , but learning the parameters of kernel k through likelihood-based inference is not straightforward .	0	2	0	5.4050903	-4.769225	0
377-8-28	determinantal point processes ( dpps ) are point process models that naturally encode diversity between the points of a given realization , through a positive definite kernel k. dpps possess desirable properties , such as exact sampling or analyticity of the moments , but learning the parameters of kernel k through likelihood-based inference is not straightforward .	we follow this approach here .	1	0	3	-5.7556453	5.1457367	1
377-8-28	[CLS] determinantal point processes ( dpps ) are point process models that naturally encode diversity between the points of a given realization, through a positive definite kernel k. dpps possess desirable properties, such as exact sampling or analyticity of the moments	[CLS] second, the likelihood has an intractable normalizing constant, which takes the form of a large determinant in the case of a dpp over a finite set of objects, and the form of a fredholm determinant in the case	1	0	4	-5.882351	5.2356668	1
377-8-28	our main contribution is to derive bounds on the likelihood of a dpp , both for finite and continuous domains .	determinantal point processes ( dpps ) are point process models that naturally encode diversity between the points of a given realization , through a positive definite kernel k. dpps possess desirable properties , such as exact sampling or analyticity of the moments , but learning the parameters of kernel k through likelihood-based inference is not straightforward .	0	5	0	5.565662	-4.950435	0
377-8-28	unlike previous work , our bounds are cheap to evaluate since they do not rely on approximating the spectrum of a large matrix or an operator .	determinantal point processes ( dpps ) are point process models that naturally encode diversity between the points of a given realization , through a positive definite kernel k. dpps possess desirable properties , such as exact sampling or analyticity of the moments , but learning the parameters of kernel k through likelihood-based inference is not straightforward .	0	6	0	5.512847	-4.889283	0
377-8-28	determinantal point processes ( dpps ) are point process models that naturally encode diversity between the points of a given realization , through a positive definite kernel k. dpps possess desirable properties , such as exact sampling or analyticity of the moments , but learning the parameters of kernel k through likelihood-based inference is not straightforward .	through usual arguments , these bounds thus yield cheap variational inference and moderately expensive exact markov chain monte carlo inference methods for dpps .	1	0	7	-5.9759603	5.1762357	1
377-8-28	this issue is typically bypassed in machine learning by directly parametrizing the kernel l , at the price of some interpretability of the model parameters .	first , the kernel that appears in the likelihood is not k , but another kernel l related to k through an often intractable spectral decomposition .	0	2	1	-3.1963146	3.134347	1
377-8-28	first , the kernel that appears in the likelihood is not k , but another kernel l related to k through an often intractable spectral decomposition .	we follow this approach here .	1	1	3	2.1295705	-1.8703338	0
377-8-28	first , the kernel that appears in the likelihood is not k , but another kernel l related to k through an often intractable spectral decomposition .	second , the likelihood has an intractable normalizing constant , which takes the form of a large determinant in the case of a dpp over a finite set of objects , and the form of a fredholm determinant in the case of a dpp over a continuous domain .	1	1	4	-5.8586364	5.218148	1
377-8-28	first , the kernel that appears in the likelihood is not k , but another kernel l related to k through an often intractable spectral decomposition .	our main contribution is to derive bounds on the likelihood of a dpp , both for finite and continuous domains .	1	1	5	-2.508316	2.6306157	1
377-8-28	unlike previous work , our bounds are cheap to evaluate since they do not rely on approximating the spectrum of a large matrix or an operator .	first , the kernel that appears in the likelihood is not k , but another kernel l related to k through an often intractable spectral decomposition .	0	6	1	3.7983413	-3.512998	0
377-8-28	through usual arguments , these bounds thus yield cheap variational inference and moderately expensive exact markov chain monte carlo inference methods for dpps .	first , the kernel that appears in the likelihood is not k , but another kernel l related to k through an often intractable spectral decomposition .	0	7	1	4.2947397	-3.9469953	0
377-8-28	we follow this approach here .	this issue is typically bypassed in machine learning by directly parametrizing the kernel l , at the price of some interpretability of the model parameters .	0	3	2	3.7627153	-3.4830892	0
377-8-28	this issue is typically bypassed in machine learning by directly parametrizing the kernel l , at the price of some interpretability of the model parameters .	second , the likelihood has an intractable normalizing constant , which takes the form of a large determinant in the case of a dpp over a finite set of objects , and the form of a fredholm determinant in the case of a dpp over a continuous domain .	1	2	4	-4.7938213	4.572768	1
377-8-28	our main contribution is to derive bounds on the likelihood of a dpp , both for finite and continuous domains .	this issue is typically bypassed in machine learning by directly parametrizing the kernel l , at the price of some interpretability of the model parameters .	0	5	2	4.7068653	-4.290305	0
377-8-28	unlike previous work , our bounds are cheap to evaluate since they do not rely on approximating the spectrum of a large matrix or an operator .	this issue is typically bypassed in machine learning by directly parametrizing the kernel l , at the price of some interpretability of the model parameters .	0	6	2	4.847866	-4.3624535	0
377-8-28	through usual arguments , these bounds thus yield cheap variational inference and moderately expensive exact markov chain monte carlo inference methods for dpps .	this issue is typically bypassed in machine learning by directly parametrizing the kernel l , at the price of some interpretability of the model parameters .	0	7	2	4.9484572	-4.444349	0
377-8-28	we follow this approach here .	second , the likelihood has an intractable normalizing constant , which takes the form of a large determinant in the case of a dpp over a finite set of objects , and the form of a fredholm determinant in the case of a dpp over a continuous domain .	1	3	4	-2.6770072	2.7274508	1
377-8-28	we follow this approach here .	our main contribution is to derive bounds on the likelihood of a dpp , both for finite and continuous domains .	1	3	5	-4.8267198	4.4484596	1
377-8-28	unlike previous work , our bounds are cheap to evaluate since they do not rely on approximating the spectrum of a large matrix or an operator .	we follow this approach here .	0	6	3	3.0185702	-2.8136652	0
377-8-28	through usual arguments , these bounds thus yield cheap variational inference and moderately expensive exact markov chain monte carlo inference methods for dpps .	we follow this approach here .	0	7	3	2.863345	-2.6548705	0
377-8-28	second , the likelihood has an intractable normalizing constant , which takes the form of a large determinant in the case of a dpp over a finite set of objects , and the form of a fredholm determinant in the case of a dpp over a continuous domain .	our main contribution is to derive bounds on the likelihood of a dpp , both for finite and continuous domains .	1	4	5	3.4832425	-3.1390078	0
377-8-28	unlike previous work , our bounds are cheap to evaluate since they do not rely on approximating the spectrum of a large matrix or an operator .	second , the likelihood has an intractable normalizing constant , which takes the form of a large determinant in the case of a dpp over a finite set of objects , and the form of a fredholm determinant in the case of a dpp over a continuous domain .	0	6	4	3.6111078	-3.3468373	0
377-8-28	through usual arguments , these bounds thus yield cheap variational inference and moderately expensive exact markov chain monte carlo inference methods for dpps .	second , the likelihood has an intractable normalizing constant , which takes the form of a large determinant in the case of a dpp over a finite set of objects , and the form of a fredholm determinant in the case of a dpp over a continuous domain .	0	7	4	4.393816	-4.0148983	0
377-8-28	our main contribution is to derive bounds on the likelihood of a dpp , both for finite and continuous domains .	unlike previous work , our bounds are cheap to evaluate since they do not rely on approximating the spectrum of a large matrix or an operator .	1	5	6	-5.563745	5.0376077	1
377-8-28	our main contribution is to derive bounds on the likelihood of a dpp , both for finite and continuous domains .	through usual arguments , these bounds thus yield cheap variational inference and moderately expensive exact markov chain monte carlo inference methods for dpps .	1	5	7	-5.625022	5.0555935	1
377-8-28	through usual arguments , these bounds thus yield cheap variational inference and moderately expensive exact markov chain monte carlo inference methods for dpps .	unlike previous work , our bounds are cheap to evaluate since they do not rely on approximating the spectrum of a large matrix or an operator .	0	7	6	2.2636006	-2.1028805	0
378-9-36	we propose a new variational inference method based on a proximal framework that uses the kullback-leibler ( kl ) divergence as the proximal term .	we make two contributions towards exploiting the geometry and structure of the variational bound .	1	0	1	-4.9046345	4.5014215	1
378-9-36	we propose a new variational inference method based on a proximal framework that uses the kullback-leibler ( kl ) divergence as the proximal term .	first , we propose a kl proximal-point algorithm and show its equivalence to variational inference with natural gradients ( e.g. , stochastic variational inference ) .	1	0	2	-5.42871	5.010067	1
378-9-36	we propose a new variational inference method based on a proximal framework that uses the kullback-leibler ( kl ) divergence as the proximal term .	second , we use the proximal framework to derive efficient variational algorithms for non-conjugate models .	1	0	3	-5.752748	5.1746964	1
378-9-36	we propose a splitting procedure to separate non-conjugate terms from conjugate ones .	we propose a new variational inference method based on a proximal framework that uses the kullback-leibler ( kl ) divergence as the proximal term .	0	4	0	-5.2051153	4.833748	1
378-9-36	we linearize the non-conjugate terms to obtain subproblems that admit a closed-form solution .	we propose a new variational inference method based on a proximal framework that uses the kullback-leibler ( kl ) divergence as the proximal term .	0	5	0	3.3931227	-3.0673456	0
378-9-36	overall , our approach converts inference in a non-conjugate model to subproblems that involve inference in well-known conjugate models .	we propose a new variational inference method based on a proximal framework that uses the kullback-leibler ( kl ) divergence as the proximal term .	0	6	0	4.7664833	-4.2030344	0
378-9-36	we propose a new variational inference method based on a proximal framework that uses the kullback-leibler ( kl ) divergence as the proximal term .	we show that our method is applicable to a wide variety of models and can result in computationally efficient algorithms .	1	0	7	-5.9713726	5.075549	1
378-9-36	applications to real-world datasets show comparable performances to existing methods .	we propose a new variational inference method based on a proximal framework that uses the kullback-leibler ( kl ) divergence as the proximal term .	0	8	0	5.4577465	-4.8381557	0
378-9-36	we make two contributions towards exploiting the geometry and structure of the variational bound .	first , we propose a kl proximal-point algorithm and show its equivalence to variational inference with natural gradients ( e.g. , stochastic variational inference ) .	1	1	2	-5.6566644	5.2281876	1
378-9-36	we make two contributions towards exploiting the geometry and structure of the variational bound .	second , we use the proximal framework to derive efficient variational algorithms for non-conjugate models .	1	1	3	-5.6906486	5.2063427	1
378-9-36	we propose a splitting procedure to separate non-conjugate terms from conjugate ones .	we make two contributions towards exploiting the geometry and structure of the variational bound .	0	4	1	-5.928273	5.1601524	1
378-9-36	we linearize the non-conjugate terms to obtain subproblems that admit a closed-form solution .	we make two contributions towards exploiting the geometry and structure of the variational bound .	0	5	1	2.4927158	-2.3070073	0
378-9-36	we make two contributions towards exploiting the geometry and structure of the variational bound .	overall , our approach converts inference in a non-conjugate model to subproblems that involve inference in well-known conjugate models .	1	1	6	-5.869535	5.183253	1
378-9-36	we show that our method is applicable to a wide variety of models and can result in computationally efficient algorithms .	we make two contributions towards exploiting the geometry and structure of the variational bound .	0	7	1	5.1063094	-4.547717	0
378-9-36	we make two contributions towards exploiting the geometry and structure of the variational bound .	applications to real-world datasets show comparable performances to existing methods .	1	1	8	-5.99212	5.1723347	1
378-9-36	second , we use the proximal framework to derive efficient variational algorithms for non-conjugate models .	first , we propose a kl proximal-point algorithm and show its equivalence to variational inference with natural gradients ( e.g. , stochastic variational inference ) .	0	3	2	5.048606	-4.4260073	0
378-9-36	we propose a splitting procedure to separate non-conjugate terms from conjugate ones .	first , we propose a kl proximal-point algorithm and show its equivalence to variational inference with natural gradients ( e.g. , stochastic variational inference ) .	0	4	2	-5.5576763	5.1161337	1
378-9-36	first , we propose a kl proximal-point algorithm and show its equivalence to variational inference with natural gradients ( e.g. , stochastic variational inference ) .	we linearize the non-conjugate terms to obtain subproblems that admit a closed-form solution .	1	2	5	-1.7565341	1.9471414	1
378-9-36	first , we propose a kl proximal-point algorithm and show its equivalence to variational inference with natural gradients ( e.g. , stochastic variational inference ) .	overall , our approach converts inference in a non-conjugate model to subproblems that involve inference in well-known conjugate models .	1	2	6	-5.9530516	5.211334	1
378-9-36	we show that our method is applicable to a wide variety of models and can result in computationally efficient algorithms .	first , we propose a kl proximal-point algorithm and show its equivalence to variational inference with natural gradients ( e.g. , stochastic variational inference ) .	0	7	2	5.217119	-4.5489974	0
378-9-36	first , we propose a kl proximal-point algorithm and show its equivalence to variational inference with natural gradients ( e.g. , stochastic variational inference ) .	applications to real-world datasets show comparable performances to existing methods .	1	2	8	-5.967121	5.2196503	1
378-9-36	second , we use the proximal framework to derive efficient variational algorithms for non-conjugate models .	we propose a splitting procedure to separate non-conjugate terms from conjugate ones .	1	3	4	4.9237995	-4.328448	0
378-9-36	second , we use the proximal framework to derive efficient variational algorithms for non-conjugate models .	we linearize the non-conjugate terms to obtain subproblems that admit a closed-form solution .	1	3	5	3.0064833	-2.774467	0
378-9-36	second , we use the proximal framework to derive efficient variational algorithms for non-conjugate models .	overall , our approach converts inference in a non-conjugate model to subproblems that involve inference in well-known conjugate models .	1	3	6	-3.3930454	3.3240037	1
378-9-36	we show that our method is applicable to a wide variety of models and can result in computationally efficient algorithms .	second , we use the proximal framework to derive efficient variational algorithms for non-conjugate models .	0	7	3	4.7968493	-4.3578434	0
378-9-36	second , we use the proximal framework to derive efficient variational algorithms for non-conjugate models .	applications to real-world datasets show comparable performances to existing methods .	1	3	8	-5.784607	5.075729	1
378-9-36	we propose a splitting procedure to separate non-conjugate terms from conjugate ones .	we linearize the non-conjugate terms to obtain subproblems that admit a closed-form solution .	1	4	5	-5.871558	5.2190623	1
378-9-36	we propose a splitting procedure to separate non-conjugate terms from conjugate ones .	overall , our approach converts inference in a non-conjugate model to subproblems that involve inference in well-known conjugate models .	1	4	6	-5.965844	5.170689	1
378-9-36	we propose a splitting procedure to separate non-conjugate terms from conjugate ones .	we show that our method is applicable to a wide variety of models and can result in computationally efficient algorithms .	1	4	7	-5.988287	5.0961523	1
378-9-36	we propose a splitting procedure to separate non-conjugate terms from conjugate ones .	applications to real-world datasets show comparable performances to existing methods .	1	4	8	-5.995971	5.1634808	1
378-9-36	overall , our approach converts inference in a non-conjugate model to subproblems that involve inference in well-known conjugate models .	we linearize the non-conjugate terms to obtain subproblems that admit a closed-form solution .	0	6	5	3.1258569	-2.908493	0
378-9-36	we linearize the non-conjugate terms to obtain subproblems that admit a closed-form solution .	we show that our method is applicable to a wide variety of models and can result in computationally efficient algorithms .	1	5	7	-5.8901143	5.0024357	1
378-9-36	we linearize the non-conjugate terms to obtain subproblems that admit a closed-form solution .	applications to real-world datasets show comparable performances to existing methods .	1	5	8	-5.97014	5.1432467	1
378-9-36	overall , our approach converts inference in a non-conjugate model to subproblems that involve inference in well-known conjugate models .	we show that our method is applicable to a wide variety of models and can result in computationally efficient algorithms .	1	6	7	-4.4391994	4.1714306	1
378-9-36	applications to real-world datasets show comparable performances to existing methods .	overall , our approach converts inference in a non-conjugate model to subproblems that involve inference in well-known conjugate models .	0	8	6	4.7980995	-4.273863	0
378-9-36	we show that our method is applicable to a wide variety of models and can result in computationally efficient algorithms .	applications to real-world datasets show comparable performances to existing methods .	1	7	8	-2.984245	2.886237	1
379-7-21	we propose a new first-order optimization algorithm to solve high-dimensional non-smooth composite minimization problems .	typical examples of such problems have an objective that decomposes into a non-smooth empirical risk part and a non-smooth regularization penalty .	1	0	1	-0.42868045	0.67726415	1
379-7-21	the proposed algorithm , called semiproximal mirror-prox , leverages the saddle point representation of one part of the objective while handling the other part of the objective via linear minimization over the domain .	we propose a new first-order optimization algorithm to solve high-dimensional non-smooth composite minimization problems .	0	2	0	5.5897255	-5.0214605	0
379-7-21	the algorithm stands in contrast with more classical proximal gradient algorithms with smoothing , which require the computation of proximal operators at each iteration and can therefore be impractical for high-dimensional problems .	we propose a new first-order optimization algorithm to solve high-dimensional non-smooth composite minimization problems .	0	3	0	5.5943093	-4.9538198	0
379-7-21	we propose a new first-order optimization algorithm to solve high-dimensional non-smooth composite minimization problems .	we establish the theoretical convergence rate of semi-proximal mirrorprox , which exhibits the optimal complexity bounds , i.e .	1	0	4	-5.8582916	5.2070923	1
379-7-21	o ( 1/2 ) , for the number of calls to linear minimization oracle .	we propose a new first-order optimization algorithm to solve high-dimensional non-smooth composite minimization problems .	0	5	0	5.3238153	-4.7502785	0
379-7-21	we present promising experimental results showing the interest of the approach in comparison to competing methods .	we propose a new first-order optimization algorithm to solve high-dimensional non-smooth composite minimization problems .	0	6	0	5.579379	-4.9517817	0
379-7-21	typical examples of such problems have an objective that decomposes into a non-smooth empirical risk part and a non-smooth regularization penalty .	the proposed algorithm , called semiproximal mirror-prox , leverages the saddle point representation of one part of the objective while handling the other part of the objective via linear minimization over the domain .	1	1	2	-5.995861	5.1532927	1
379-7-21	the algorithm stands in contrast with more classical proximal gradient algorithms with smoothing , which require the computation of proximal operators at each iteration and can therefore be impractical for high-dimensional problems .	typical examples of such problems have an objective that decomposes into a non-smooth empirical risk part and a non-smooth regularization penalty .	0	3	1	5.684999	-4.9955	0
379-7-21	we establish the theoretical convergence rate of semi-proximal mirrorprox , which exhibits the optimal complexity bounds , i.e .	typical examples of such problems have an objective that decomposes into a non-smooth empirical risk part and a non-smooth regularization penalty .	0	4	1	5.632629	-4.9622126	0
379-7-21	o ( 1/2 ) , for the number of calls to linear minimization oracle .	typical examples of such problems have an objective that decomposes into a non-smooth empirical risk part and a non-smooth regularization penalty .	0	5	1	5.5458755	-4.895748	0
379-7-21	we present promising experimental results showing the interest of the approach in comparison to competing methods .	typical examples of such problems have an objective that decomposes into a non-smooth empirical risk part and a non-smooth regularization penalty .	0	6	1	5.660506	-5.065926	0
379-7-21	the algorithm stands in contrast with more classical proximal gradient algorithms with smoothing , which require the computation of proximal operators at each iteration and can therefore be impractical for high-dimensional problems .	the proposed algorithm , called semiproximal mirror-prox , leverages the saddle point representation of one part of the objective while handling the other part of the objective via linear minimization over the domain .	0	3	2	3.5704138	-3.2333944	0
379-7-21	the proposed algorithm , called semiproximal mirror-prox , leverages the saddle point representation of one part of the objective while handling the other part of the objective via linear minimization over the domain .	we establish the theoretical convergence rate of semi-proximal mirrorprox , which exhibits the optimal complexity bounds , i.e .	1	2	4	-5.764355	5.1464725	1
379-7-21	the proposed algorithm , called semiproximal mirror-prox , leverages the saddle point representation of one part of the objective while handling the other part of the objective via linear minimization over the domain .	o ( 1/2 ) , for the number of calls to linear minimization oracle .	1	2	5	-5.5421944	4.967924	1
379-7-21	the proposed algorithm , called semiproximal mirror-prox , leverages the saddle point representation of one part of the objective while handling the other part of the objective via linear minimization over the domain .	we present promising experimental results showing the interest of the approach in comparison to competing methods .	1	2	6	-5.9551735	5.0531936	1
379-7-21	we establish the theoretical convergence rate of semi-proximal mirrorprox , which exhibits the optimal complexity bounds , i.e .	the algorithm stands in contrast with more classical proximal gradient algorithms with smoothing , which require the computation of proximal operators at each iteration and can therefore be impractical for high-dimensional problems .	0	4	3	-4.127673	3.8737288	1
379-7-21	o ( 1/2 ) , for the number of calls to linear minimization oracle .	the algorithm stands in contrast with more classical proximal gradient algorithms with smoothing , which require the computation of proximal operators at each iteration and can therefore be impractical for high-dimensional problems .	0	5	3	-3.746338	3.5525684	1
379-7-21	we present promising experimental results showing the interest of the approach in comparison to competing methods .	the algorithm stands in contrast with more classical proximal gradient algorithms with smoothing , which require the computation of proximal operators at each iteration and can therefore be impractical for high-dimensional problems .	0	6	3	4.6742773	-4.213486	0
379-7-21	we establish the theoretical convergence rate of semi-proximal mirrorprox , which exhibits the optimal complexity bounds , i.e .	o ( 1/2 ) , for the number of calls to linear minimization oracle .	1	4	5	-2.6426492	2.556879	1
379-7-21	we establish the theoretical convergence rate of semi-proximal mirrorprox , which exhibits the optimal complexity bounds , i.e .	we present promising experimental results showing the interest of the approach in comparison to competing methods .	1	4	6	-5.8820767	5.013238	1
379-7-21	we present promising experimental results showing the interest of the approach in comparison to competing methods .	o ( 1/2 ) , for the number of calls to linear minimization oracle .	0	6	5	5.0759363	-4.5750628	0
380-12-66	sparse , low-rank , etc .	consider estimating an unknown , but structured ( e.g .	0	1	0	5.230402	-4.6412363	0
380-12-66	) , signal x0 rn from a vector y rm of measurements of the form yi = gi ( ai t x0 ) , where the ai 's are the rows of a known measurement matrix a , and , g ( * ) is a ( potentially unknown ) nonlinear and random link-function .	consider estimating an unknown , but structured ( e.g .	0	2	0	5.4705205	-4.894156	0
380-12-66	consider estimating an unknown , but structured ( e.g .	such measurement functions could arise in applications where the measurement device has nonlinearities and uncertainties .	1	0	3	-5.5356307	5.007818	1
380-12-66	it could also arise by design , e.g. , gi ( x ) = sign ( x + zi ) , corresponds to noisy 1-bit quantized measurements .	consider estimating an unknown , but structured ( e.g .	0	4	0	5.4956565	-4.8744473	0
380-12-66	[CLS] motivated by the classical work of brillinger, and more recent work of plan and vershynin, we estimate x0 : = arg minx y - ax0 2 + f ( x ) via solving the generalized - lasso, i. e., x for some regularization parameter > 0 and some ( typically non - smooth ) convex regularizer f ( * ) that promotes the structure of x	consider estimating an unknown , but structured ( e.g .	0	5	0	5.5525336	-4.9823456	0
380-12-66	1 -norm , nuclear-norm , etc .	consider estimating an unknown , but structured ( e.g .	0	6	0	5.364613	-4.786232	0
380-12-66	while this approach seems to naively ignore the nonlinear function g ( * ) , both brillinger ( in the non-constrained case ) and plan and vershynin have shown that , when the entries of a are iid standard normal , this is a good estimator of x0 up to a constant of proportionality , which only depends on g ( * ) .	consider estimating an unknown , but structured ( e.g .	0	7	0	5.5129166	-4.866265	0
380-12-66	consider estimating an unknown , but structured ( e.g .	in this work , we con -x0 2 , siderably strengthen these results by obtaining explicit expressions for x for the regularized generalized-lasso , that are asymptotically precise when m and n grow large .	1	0	8	-5.969557	5.075989	1
380-12-66	a main result is that the estimation performance of the generalized lasso with non-linear measurements is asymptotically the same as one whose measurements are linear yi = ai t x0 + zi , with = eg ( ) and 2 = e ( g ( ) - ) 2 , and , standard normal .	consider estimating an unknown , but structured ( e.g .	0	9	0	5.582775	-5.001849	0
380-12-66	to the best of our knowledge , the derived expressions on the estimation performance are the first-known precise results in this context .	consider estimating an unknown , but structured ( e.g .	0	10	0	5.6467586	-5.019516	0
380-12-66	one interesting consequence of our result is that the optimal quantizer of the measurements that minimizes the estimation error of the generalized lasso is the celebrated lloyd-max quantizer .	consider estimating an unknown , but structured ( e.g .	0	11	0	5.605465	-5.028949	0
380-12-66	sparse , low-rank , etc .	) , signal x0 rn from a vector y rm of measurements of the form yi = gi ( ai t x0 ) , where the ai 's are the rows of a known measurement matrix a , and , g ( * ) is a ( potentially unknown ) nonlinear and random link-function .	1	1	2	-2.0823123	2.19281	1
380-12-66	sparse , low-rank , etc .	such measurement functions could arise in applications where the measurement device has nonlinearities and uncertainties .	1	1	3	-1.4875734	1.5950851	1
380-12-66	it could also arise by design , e.g. , gi ( x ) = sign ( x + zi ) , corresponds to noisy 1-bit quantized measurements .	sparse , low-rank , etc .	0	4	1	2.5392532	-2.3796349	0
380-12-66	[CLS] motivated by the classical work of brillinger, and more recent work of plan and vershynin, we estimate x0 : = arg minx y - ax0 2 + f ( x ) via solving the generalized - lasso, i. e., x for some regularization parameter > 0 and some ( typically non - smooth ) convex regularizer f ( * ) that promotes the structure of x0, e. g	sparse , low-rank , etc .	0	5	1	0.8399885	-0.6306575	0
380-12-66	sparse , low-rank , etc .	1 -norm , nuclear-norm , etc .	1	1	6	-1.8439143	1.8126807	1
380-12-66	while this approach seems to naively ignore the nonlinear function g ( * ) , both brillinger ( in the non-constrained case ) and plan and vershynin have shown that , when the entries of a are iid standard normal , this is a good estimator of x0 up to a constant of proportionality , which only depends on g ( * ) .	sparse , low-rank , etc .	0	7	1	3.7296684	-3.4040396	0
380-12-66	sparse , low-rank , etc .	in this work , we con -x0 2 , siderably strengthen these results by obtaining explicit expressions for x for the regularized generalized-lasso , that are asymptotically precise when m and n grow large .	1	1	8	-4.985342	4.639498	1
380-12-66	sparse , low-rank , etc .	a main result is that the estimation performance of the generalized lasso with non-linear measurements is asymptotically the same as one whose measurements are linear yi = ai t x0 + zi , with = eg ( ) and 2 = e ( g ( ) - ) 2 , and , standard normal .	1	1	9	-5.486102	4.936444	1
380-12-66	to the best of our knowledge , the derived expressions on the estimation performance are the first-known precise results in this context .	sparse , low-rank , etc .	0	10	1	4.984027	-4.4439325	0
380-12-66	one interesting consequence of our result is that the optimal quantizer of the measurements that minimizes the estimation error of the generalized lasso is the celebrated lloyd-max quantizer .	sparse , low-rank , etc .	0	11	1	4.53259	-4.135312	0
380-12-66	such measurement functions could arise in applications where the measurement device has nonlinearities and uncertainties .	) , signal x0 rn from a vector y rm of measurements of the form yi = gi ( ai t x0 ) , where the ai 's are the rows of a known measurement matrix a , and , g ( * ) is a ( potentially unknown ) nonlinear and random link-function .	0	3	2	4.00126	-3.703466	0
380-12-66	it could also arise by design , e.g. , gi ( x ) = sign ( x + zi ) , corresponds to noisy 1-bit quantized measurements .	) , signal x0 rn from a vector y rm of measurements of the form yi = gi ( ai t x0 ) , where the ai 's are the rows of a known measurement matrix a , and , g ( * ) is a ( potentially unknown ) nonlinear and random link-function .	0	4	2	2.9856508	-2.7530024	0
380-12-66	[CLS] ), signal x0 rn from a vector y rm of measurements of the form yi = gi ( ai t x0 ), where the ai's are the rows of a known measurement matrix a, and, g ( * ) is a	[CLS] motivated by the classical work of brillinger, and more recent work of plan and vershynin, we estimate x0 : = arg minx y - ax0 2 + f ( x ) via solving the generalized - lasso	1	2	5	-4.112244	3.757249	1
380-12-66	) , signal x0 rn from a vector y rm of measurements of the form yi = gi ( ai t x0 ) , where the ai 's are the rows of a known measurement matrix a , and , g ( * ) is a ( potentially unknown ) nonlinear and random link-function .	1 -norm , nuclear-norm , etc .	1	2	6	-1.7188447	1.8546293	1
380-12-66	[CLS] ), signal x0 rn from a vector y rm of measurements of the form yi = gi ( ai t x0 ), where the ai's are the rows of a known measurement matrix a, and, g ( * ) is a	[CLS] while this approach seems to naively ignore the nonlinear function g ( * ), both brillinger ( in the non - constrained case ) and plan and vershynin have shown that, when the entries of a are iid standard normal	1	2	7	-3.399477	3.22477	1
380-12-66	[CLS] ), signal x0 rn from a vector y rm of measurements of the form yi = gi ( ai t x0 ), where the ai's are the rows of a known measurement matrix a, and, g ( * ) is a (	in this work , we con -x0 2 , siderably strengthen these results by obtaining explicit expressions for x for the regularized generalized-lasso , that are asymptotically precise when m and n grow large .	1	2	8	-5.719482	4.9215307	1
380-12-66	[CLS] a main result is that the estimation performance of the generalized lasso with non - linear measurements is asymptotically the same as one whose measurements are linear yi = ai t x0 + zi, with = eg ( ) and	[CLS] ), signal x0 rn from a vector y rm of measurements of the form yi = gi ( ai t x0 ), where the ai's are the rows of a known measurement matrix a, and, g ( * ) is a	0	9	2	5.032641	-4.445825	0
380-12-66	) , signal x0 rn from a vector y rm of measurements of the form yi = gi ( ai t x0 ) , where the ai 's are the rows of a known measurement matrix a , and , g ( * ) is a ( potentially unknown ) nonlinear and random link-function .	to the best of our knowledge , the derived expressions on the estimation performance are the first-known precise results in this context .	1	2	10	-5.884758	4.960514	1
380-12-66	) , signal x0 rn from a vector y rm of measurements of the form yi = gi ( ai t x0 ) , where the ai 's are the rows of a known measurement matrix a , and , g ( * ) is a ( potentially unknown ) nonlinear and random link-function .	one interesting consequence of our result is that the optimal quantizer of the measurements that minimizes the estimation error of the generalized lasso is the celebrated lloyd-max quantizer .	1	2	11	-5.888554	4.941608	1
380-12-66	it could also arise by design , e.g. , gi ( x ) = sign ( x + zi ) , corresponds to noisy 1-bit quantized measurements .	such measurement functions could arise in applications where the measurement device has nonlinearities and uncertainties .	0	4	3	3.9377947	-3.5236313	0
380-12-66	[CLS] motivated by the classical work of brillinger, and more recent work of plan and vershynin, we estimate x0 : = arg minx y - ax0 2 + f ( x ) via solving the generalized - lasso, i. e., x for some regularization parameter > 0 and some ( typically non - smooth ) convex regularizer f ( * ) that	such measurement functions could arise in applications where the measurement device has nonlinearities and uncertainties .	0	5	3	3.8050866	-3.4705982	0
380-12-66	1 -norm , nuclear-norm , etc .	such measurement functions could arise in applications where the measurement device has nonlinearities and uncertainties .	0	6	3	1.8716468	-1.6291035	0
380-12-66	while this approach seems to naively ignore the nonlinear function g ( * ) , both brillinger ( in the non-constrained case ) and plan and vershynin have shown that , when the entries of a are iid standard normal , this is a good estimator of x0 up to a constant of proportionality , which only depends on g ( * ) .	such measurement functions could arise in applications where the measurement device has nonlinearities and uncertainties .	0	7	3	3.3555202	-3.0586448	0
380-12-66	in this work , we con -x0 2 , siderably strengthen these results by obtaining explicit expressions for x for the regularized generalized-lasso , that are asymptotically precise when m and n grow large .	such measurement functions could arise in applications where the measurement device has nonlinearities and uncertainties .	0	8	3	5.0741415	-4.4104514	0
380-12-66	such measurement functions could arise in applications where the measurement device has nonlinearities and uncertainties .	a main result is that the estimation performance of the generalized lasso with non-linear measurements is asymptotically the same as one whose measurements are linear yi = ai t x0 + zi , with = eg ( ) and 2 = e ( g ( ) - ) 2 , and , standard normal .	1	3	9	-5.9114075	5.2312856	1
380-12-66	such measurement functions could arise in applications where the measurement device has nonlinearities and uncertainties .	to the best of our knowledge , the derived expressions on the estimation performance are the first-known precise results in this context .	1	3	10	-6.0054607	5.1943226	1
380-12-66	such measurement functions could arise in applications where the measurement device has nonlinearities and uncertainties .	one interesting consequence of our result is that the optimal quantizer of the measurements that minimizes the estimation error of the generalized lasso is the celebrated lloyd-max quantizer .	1	3	11	-5.8781204	5.1047554	1
380-12-66	[CLS] motivated by the classical work of brillinger, and more recent work of plan and vershynin, we estimate x0 : = arg minx y - ax0 2 + f ( x ) via solving the generalized - lasso, i. e., x for some regularization parameter >	it could also arise by design , e.g. , gi ( x ) = sign ( x + zi ) , corresponds to noisy 1-bit quantized measurements .	0	5	4	1.6060944	-1.4126309	0
380-12-66	it could also arise by design , e.g. , gi ( x ) = sign ( x + zi ) , corresponds to noisy 1-bit quantized measurements .	1 -norm , nuclear-norm , etc .	1	4	6	1.6524568	-1.4613124	0
380-12-66	[CLS] while this approach seems to naively ignore the nonlinear function g ( * ), both brillinger ( in the non - constrained case ) and plan and vershynin have shown that, when the entries of a are iid standard normal, this is a good estimator of x0 up to	it could also arise by design , e.g. , gi ( x ) = sign ( x + zi ) , corresponds to noisy 1-bit quantized measurements .	0	7	4	-3.3219857	3.2702858	1
380-12-66	it could also arise by design , e.g. , gi ( x ) = sign ( x + zi ) , corresponds to noisy 1-bit quantized measurements .	in this work , we con -x0 2 , siderably strengthen these results by obtaining explicit expressions for x for the regularized generalized-lasso , that are asymptotically precise when m and n grow large .	1	4	8	-4.8577957	4.538037	1
380-12-66	it could also arise by design , e.g. , gi ( x ) = sign ( x + zi ) , corresponds to noisy 1-bit quantized measurements .	[CLS] a main result is that the estimation performance of the generalized lasso with non - linear measurements is asymptotically the same as one whose measurements are linear yi = ai t x0 + zi, with = eg ( ) and 2 = e ( g ( ) - ) 2, and,	1	4	9	-5.377255	4.8872194	1
380-12-66	to the best of our knowledge , the derived expressions on the estimation performance are the first-known precise results in this context .	it could also arise by design , e.g. , gi ( x ) = sign ( x + zi ) , corresponds to noisy 1-bit quantized measurements .	0	10	4	4.5399237	-4.087158	0
380-12-66	one interesting consequence of our result is that the optimal quantizer of the measurements that minimizes the estimation error of the generalized lasso is the celebrated lloyd-max quantizer .	it could also arise by design , e.g. , gi ( x ) = sign ( x + zi ) , corresponds to noisy 1-bit quantized measurements .	0	11	4	4.3347645	-3.917829	0
380-12-66	[CLS] motivated by the classical work of brillinger, and more recent work of plan and vershynin, we estimate x0 : = arg minx y - ax0 2 + f ( x ) via solving the generalized - lasso, i. e., x for some regularization parameter > 0 and some ( typically non - smooth ) convex regularizer f ( * ) that promotes the structure of x0, e	1 -norm , nuclear-norm , etc .	1	5	6	2.4833121	-2.326254	0
380-12-66	[CLS] while this approach seems to naively ignore the nonlinear function g ( * ), both brillinger ( in the non - constrained case ) and plan and vershynin have shown that, when the entries of a are iid standard normal	[CLS] motivated by the classical work of brillinger, and more recent work of plan and vershynin, we estimate x0 : = arg minx y - ax0 2 + f ( x ) via solving the generalized - lasso	0	7	5	-5.1816335	4.7905173	1
380-12-66	in this work , we con -x0 2 , siderably strengthen these results by obtaining explicit expressions for x for the regularized generalized-lasso , that are asymptotically precise when m and n grow large .	[CLS] motivated by the classical work of brillinger, and more recent work of plan and vershynin, we estimate x0 : = arg minx y - ax0 2 + f ( x ) via solving the generalized - lasso,	0	8	5	0.09007639	0.10451642	1
380-12-66	[CLS] motivated by the classical work of brillinger, and more recent work of plan and vershynin, we estimate x0 : = arg minx y - ax0 2 + f ( x ) via solving the generalized - lasso	[CLS] a main result is that the estimation performance of the generalized lasso with non - linear measurements is asymptotically the same as one whose measurements are linear yi = ai t x0 + zi, with = eg ( ) and	1	5	9	-3.7265627	3.5129335	1
380-12-66	to the best of our knowledge , the derived expressions on the estimation performance are the first-known precise results in this context .	[CLS] motivated by the classical work of brillinger, and more recent work of plan and vershynin, we estimate x0 : = arg minx y - ax0 2 + f ( x ) via solving the generalized - lasso, i. e., x for some regularization parameter > 0 and some ( typically non - smooth ) convex	0	10	5	4.911559	-4.3827615	0
380-12-66	one interesting consequence of our result is that the optimal quantizer of the measurements that minimizes the estimation error of the generalized lasso is the celebrated lloyd-max quantizer .	[CLS] motivated by the classical work of brillinger, and more recent work of plan and vershynin, we estimate x0 : = arg minx y - ax0 2 + f ( x ) via solving the generalized - lasso, i. e., x for some regularization	0	11	5	4.2104893	-3.8479922	0
380-12-66	1 -norm , nuclear-norm , etc .	while this approach seems to naively ignore the nonlinear function g ( * ) , both brillinger ( in the non-constrained case ) and plan and vershynin have shown that , when the entries of a are iid standard normal , this is a good estimator of x0 up to a constant of proportionality , which only depends on g ( * ) .	1	6	7	-0.09668694	0.42300072	1
380-12-66	1 -norm , nuclear-norm , etc .	in this work , we con -x0 2 , siderably strengthen these results by obtaining explicit expressions for x for the regularized generalized-lasso , that are asymptotically precise when m and n grow large .	1	6	8	-4.5898685	4.3033013	1
380-12-66	a main result is that the estimation performance of the generalized lasso with non-linear measurements is asymptotically the same as one whose measurements are linear yi = ai t x0 + zi , with = eg ( ) and 2 = e ( g ( ) - ) 2 , and , standard normal .	1 -norm , nuclear-norm , etc .	0	9	6	4.554141	-4.196905	0
380-12-66	1 -norm , nuclear-norm , etc .	to the best of our knowledge , the derived expressions on the estimation performance are the first-known precise results in this context .	1	6	10	-5.6152725	5.0266957	1
380-12-66	one interesting consequence of our result is that the optimal quantizer of the measurements that minimizes the estimation error of the generalized lasso is the celebrated lloyd-max quantizer .	1 -norm , nuclear-norm , etc .	0	11	6	4.075325	-3.8074028	0
380-12-66	[CLS] while this approach seems to naively ignore the nonlinear function g ( * ), both brillinger ( in the non - constrained case ) and plan and vershynin have shown that, when the entries of a are iid standard normal,	in this work , we con -x0 2 , siderably strengthen these results by obtaining explicit expressions for x for the regularized generalized-lasso , that are asymptotically precise when m and n grow large .	1	7	8	-5.9227514	5.162673	1
380-12-66	[CLS] a main result is that the estimation performance of the generalized lasso with non - linear measurements is asymptotically the same as one whose measurements are linear yi = ai t x0 + zi, with = eg ( ) and	[CLS] while this approach seems to naively ignore the nonlinear function g ( * ), both brillinger ( in the non - constrained case ) and plan and vershynin have shown that, when the entries of a are iid standard normal	0	9	7	4.2050557	-3.732644	0
380-12-66	[CLS] while this approach seems to naively ignore the nonlinear function g ( * ), both brillinger ( in the non - constrained case ) and plan and vershynin have shown that, when the entries of a are iid standard normal, this is a good estimator of x0 up to a constant of proportionality, which only depends on	to the best of our knowledge , the derived expressions on the estimation performance are the first-known precise results in this context .	1	7	10	-5.9794254	5.153037	1
380-12-66	[CLS] while this approach seems to naively ignore the nonlinear function g ( * ), both brillinger ( in the non - constrained case ) and plan and vershynin have shown that, when the entries of a are iid standard normal, this is a good estimator of x0	one interesting consequence of our result is that the optimal quantizer of the measurements that minimizes the estimation error of the generalized lasso is the celebrated lloyd-max quantizer .	1	7	11	-5.929132	5.09554	1
380-12-66	in this work , we con -x0 2 , siderably strengthen these results by obtaining explicit expressions for x for the regularized generalized-lasso , that are asymptotically precise when m and n grow large .	[CLS] a main result is that the estimation performance of the generalized lasso with non - linear measurements is asymptotically the same as one whose measurements are linear yi = ai t x0 + zi, with = eg ( ) and 2	1	8	9	-4.0502224	3.8258667	1
380-12-66	in this work , we con -x0 2 , siderably strengthen these results by obtaining explicit expressions for x for the regularized generalized-lasso , that are asymptotically precise when m and n grow large .	to the best of our knowledge , the derived expressions on the estimation performance are the first-known precise results in this context .	1	8	10	-5.6783466	5.072711	1
380-12-66	in this work , we con -x0 2 , siderably strengthen these results by obtaining explicit expressions for x for the regularized generalized-lasso , that are asymptotically precise when m and n grow large .	one interesting consequence of our result is that the optimal quantizer of the measurements that minimizes the estimation error of the generalized lasso is the celebrated lloyd-max quantizer .	1	8	11	-3.977192	3.7722273	1
380-12-66	to the best of our knowledge , the derived expressions on the estimation performance are the first-known precise results in this context .	a main result is that the estimation performance of the generalized lasso with non-linear measurements is asymptotically the same as one whose measurements are linear yi = ai t x0 + zi , with = eg ( ) and 2 = e ( g ( ) - ) 2 , and , standard normal .	0	10	9	3.9115136	-3.6355557	0
380-12-66	[CLS] a main result is that the estimation performance of the generalized lasso with non - linear measurements is asymptotically the same as one whose measurements are linear yi = ai t x0 + zi, with = eg ( ) and 2 = e ( g ( ) - ) 2,	one interesting consequence of our result is that the optimal quantizer of the measurements that minimizes the estimation error of the generalized lasso is the celebrated lloyd-max quantizer .	1	9	11	-2.4847763	2.5545552	1
380-12-66	one interesting consequence of our result is that the optimal quantizer of the measurements that minimizes the estimation error of the generalized lasso is the celebrated lloyd-max quantizer .	to the best of our knowledge , the derived expressions on the estimation performance are the first-known precise results in this context .	0	11	10	-2.5990453	2.69036	1
381-9-36	a fundamental problem in the analysis of such networks is to properly define the similarity or dissimilarity between any two vertices .	large unweighted directed graphs are commonly used to capture relations between entities .	0	1	0	4.707453	-3.9023783	0
381-9-36	large unweighted directed graphs are commonly used to capture relations between entities .	despite the significance of this problem , statistical characterization of the proposed metrics has been limited .	1	0	2	-5.987299	5.1250577	1
381-9-36	large unweighted directed graphs are commonly used to capture relations between entities .	we introduce and develop a class of techniques for analyzing random walks on graphs using stochastic calculus .	1	0	3	-5.95714	5.177028	1
381-9-36	using these techniques we generalize results on the degeneracy of hitting times and analyze a metric based on the laplace transformed hitting time ( ltht ) .	large unweighted directed graphs are commonly used to capture relations between entities .	0	4	0	5.7440157	-5.10233	0
381-9-36	the metric serves as a natural , provably well-behaved alternative to the expected hitting time .	large unweighted directed graphs are commonly used to capture relations between entities .	0	5	0	5.730275	-5.0820875	0
381-9-36	large unweighted directed graphs are commonly used to capture relations between entities .	we establish a general correspondence between hitting times of the brownian motion and analogous hitting times on the graph .	1	0	6	-5.9227934	5.165761	1
381-9-36	large unweighted directed graphs are commonly used to capture relations between entities .	we show that the ltht is consistent with respect to the underlying metric of a geometric graph , preserves clustering tendency , and remains robust against random addition of non-geometric edges .	1	0	7	-5.8842816	5.179117	1
381-9-36	large unweighted directed graphs are commonly used to capture relations between entities .	tests on simulated and real-world data show that the ltht matches theoretical predictions and outperforms alternatives .	1	0	8	-5.881528	5.174092	1
381-9-36	a fundamental problem in the analysis of such networks is to properly define the similarity or dissimilarity between any two vertices .	despite the significance of this problem , statistical characterization of the proposed metrics has been limited .	1	1	2	-5.950516	5.169391	1
381-9-36	we introduce and develop a class of techniques for analyzing random walks on graphs using stochastic calculus .	a fundamental problem in the analysis of such networks is to properly define the similarity or dissimilarity between any two vertices .	0	3	1	4.313839	-3.5637903	0
381-9-36	a fundamental problem in the analysis of such networks is to properly define the similarity or dissimilarity between any two vertices .	using these techniques we generalize results on the degeneracy of hitting times and analyze a metric based on the laplace transformed hitting time ( ltht ) .	1	1	4	-5.9619417	5.1460056	1
381-9-36	a fundamental problem in the analysis of such networks is to properly define the similarity or dissimilarity between any two vertices .	the metric serves as a natural , provably well-behaved alternative to the expected hitting time .	1	1	5	-5.937363	5.188404	1
381-9-36	we establish a general correspondence between hitting times of the brownian motion and analogous hitting times on the graph .	a fundamental problem in the analysis of such networks is to properly define the similarity or dissimilarity between any two vertices .	0	6	1	5.647555	-5.046724	0
381-9-36	a fundamental problem in the analysis of such networks is to properly define the similarity or dissimilarity between any two vertices .	we show that the ltht is consistent with respect to the underlying metric of a geometric graph , preserves clustering tendency , and remains robust against random addition of non-geometric edges .	1	1	7	-5.8708014	5.171036	1
381-9-36	tests on simulated and real-world data show that the ltht matches theoretical predictions and outperforms alternatives .	a fundamental problem in the analysis of such networks is to properly define the similarity or dissimilarity between any two vertices .	0	8	1	5.726764	-5.1889496	0
381-9-36	we introduce and develop a class of techniques for analyzing random walks on graphs using stochastic calculus .	despite the significance of this problem , statistical characterization of the proposed metrics has been limited .	0	3	2	-2.83783	2.9541385	1
381-9-36	despite the significance of this problem , statistical characterization of the proposed metrics has been limited .	using these techniques we generalize results on the degeneracy of hitting times and analyze a metric based on the laplace transformed hitting time ( ltht ) .	1	2	4	-5.879118	5.19569	1
381-9-36	the metric serves as a natural , provably well-behaved alternative to the expected hitting time .	despite the significance of this problem , statistical characterization of the proposed metrics has been limited .	0	5	2	1.587155	-1.1850914	0
381-9-36	we establish a general correspondence between hitting times of the brownian motion and analogous hitting times on the graph .	despite the significance of this problem , statistical characterization of the proposed metrics has been limited .	0	6	2	4.776077	-4.332342	0
381-9-36	we show that the ltht is consistent with respect to the underlying metric of a geometric graph , preserves clustering tendency , and remains robust against random addition of non-geometric edges .	despite the significance of this problem , statistical characterization of the proposed metrics has been limited .	0	7	2	4.2796535	-3.8280287	0
381-9-36	tests on simulated and real-world data show that the ltht matches theoretical predictions and outperforms alternatives .	despite the significance of this problem , statistical characterization of the proposed metrics has been limited .	0	8	2	5.6196814	-5.0657563	0
381-9-36	we introduce and develop a class of techniques for analyzing random walks on graphs using stochastic calculus .	using these techniques we generalize results on the degeneracy of hitting times and analyze a metric based on the laplace transformed hitting time ( ltht ) .	1	3	4	-6.0150156	5.17952	1
381-9-36	the metric serves as a natural , provably well-behaved alternative to the expected hitting time .	we introduce and develop a class of techniques for analyzing random walks on graphs using stochastic calculus .	0	5	3	5.1051435	-4.560426	0
381-9-36	we introduce and develop a class of techniques for analyzing random walks on graphs using stochastic calculus .	we establish a general correspondence between hitting times of the brownian motion and analogous hitting times on the graph .	1	3	6	-5.89781	5.2185946	1
381-9-36	we show that the ltht is consistent with respect to the underlying metric of a geometric graph , preserves clustering tendency , and remains robust against random addition of non-geometric edges .	we introduce and develop a class of techniques for analyzing random walks on graphs using stochastic calculus .	0	7	3	5.5417585	-4.8826256	0
381-9-36	we introduce and develop a class of techniques for analyzing random walks on graphs using stochastic calculus .	tests on simulated and real-world data show that the ltht matches theoretical predictions and outperforms alternatives .	1	3	8	-5.970706	5.1888328	1
381-9-36	using these techniques we generalize results on the degeneracy of hitting times and analyze a metric based on the laplace transformed hitting time ( ltht ) .	the metric serves as a natural , provably well-behaved alternative to the expected hitting time .	1	4	5	-5.2458467	4.821686	1
381-9-36	using these techniques we generalize results on the degeneracy of hitting times and analyze a metric based on the laplace transformed hitting time ( ltht ) .	we establish a general correspondence between hitting times of the brownian motion and analogous hitting times on the graph .	1	4	6	2.3806348	-2.1089416	0
381-9-36	using these techniques we generalize results on the degeneracy of hitting times and analyze a metric based on the laplace transformed hitting time ( ltht ) .	we show that the ltht is consistent with respect to the underlying metric of a geometric graph , preserves clustering tendency , and remains robust against random addition of non-geometric edges .	1	4	7	-5.8789616	5.209979	1
381-9-36	tests on simulated and real-world data show that the ltht matches theoretical predictions and outperforms alternatives .	using these techniques we generalize results on the degeneracy of hitting times and analyze a metric based on the laplace transformed hitting time ( ltht ) .	0	8	4	5.6119046	-5.025983	0
381-9-36	the metric serves as a natural , provably well-behaved alternative to the expected hitting time .	we establish a general correspondence between hitting times of the brownian motion and analogous hitting times on the graph .	1	5	6	-1.8551811	2.096219	1
381-9-36	the metric serves as a natural , provably well-behaved alternative to the expected hitting time .	we show that the ltht is consistent with respect to the underlying metric of a geometric graph , preserves clustering tendency , and remains robust against random addition of non-geometric edges .	1	5	7	-2.2919717	2.336815	1
381-9-36	tests on simulated and real-world data show that the ltht matches theoretical predictions and outperforms alternatives .	the metric serves as a natural , provably well-behaved alternative to the expected hitting time .	0	8	5	5.323641	-4.816004	0
381-9-36	we establish a general correspondence between hitting times of the brownian motion and analogous hitting times on the graph .	we show that the ltht is consistent with respect to the underlying metric of a geometric graph , preserves clustering tendency , and remains robust against random addition of non-geometric edges .	1	6	7	-3.1273503	3.0752091	1
381-9-36	tests on simulated and real-world data show that the ltht matches theoretical predictions and outperforms alternatives .	we establish a general correspondence between hitting times of the brownian motion and analogous hitting times on the graph .	0	8	6	5.2713404	-4.77679	0
381-9-36	we show that the ltht is consistent with respect to the underlying metric of a geometric graph , preserves clustering tendency , and remains robust against random addition of non-geometric edges .	tests on simulated and real-world data show that the ltht matches theoretical predictions and outperforms alternatives .	1	7	8	-4.9502172	4.521962	1
382-6-15	one simple approach to this is to use online monte carlo methods , such as sgld ( stochastic gradient langevin dynamics ) .	we consider the problem of bayesian parameter estimation for deep neural networks , which is important in problem settings where we may have little data , and/ or where we need accurate posterior predictive densities p ( y|x , d ) , e.g. , for applications involving bandits or active learning .	0	1	0	5.3956604	-4.8354073	0
382-6-15	unfortunately , such a method needs to store many copies of the parameters ( which wastes memory ) , and needs to make predictions using many versions of the model ( which wastes time ) .	[CLS] we consider the problem of bayesian parameter estimation for deep neural networks, which is important in problem settings where we may have little data, and / or where we need accurate posterior predictive densities p ( y | x, d ), e. g., for applications involving bandits or active	0	2	0	4.7006297	-4.177181	0
382-6-15	we consider the problem of bayesian parameter estimation for deep neural networks , which is important in problem settings where we may have little data , and/ or where we need accurate posterior predictive densities p ( y|x , d ) , e.g. , for applications involving bandits or active learning .	we describe a method for `` distilling '' a monte carlo approximation to the posterior predictive density into a more compact form , namely a single deep neural network .	1	0	3	-5.858653	5.254881	1
382-6-15	we compare to two very recent approaches to bayesian neural networks , namely an approach based on expectation propagation and an approach based on variational bayes .	we consider the problem of bayesian parameter estimation for deep neural networks , which is important in problem settings where we may have little data , and/ or where we need accurate posterior predictive densities p ( y|x , d ) , e.g. , for applications involving bandits or active learning .	0	4	0	5.5679135	-4.986711	0
382-6-15	our method performs better than both of these , is much simpler to implement , and uses less computation at test time .	we consider the problem of bayesian parameter estimation for deep neural networks , which is important in problem settings where we may have little data , and/ or where we need accurate posterior predictive densities p ( y|x , d ) , e.g. , for applications involving bandits or active learning .	0	5	0	5.4194384	-4.919281	0
382-6-15	unfortunately , such a method needs to store many copies of the parameters ( which wastes memory ) , and needs to make predictions using many versions of the model ( which wastes time ) .	one simple approach to this is to use online monte carlo methods , such as sgld ( stochastic gradient langevin dynamics ) .	0	2	1	-3.1017294	3.1173224	1
382-6-15	we describe a method for `` distilling '' a monte carlo approximation to the posterior predictive density into a more compact form , namely a single deep neural network .	one simple approach to this is to use online monte carlo methods , such as sgld ( stochastic gradient langevin dynamics ) .	0	3	1	2.617797	-2.2609606	0
382-6-15	one simple approach to this is to use online monte carlo methods , such as sgld ( stochastic gradient langevin dynamics ) .	we compare to two very recent approaches to bayesian neural networks , namely an approach based on expectation propagation and an approach based on variational bayes .	1	1	4	-5.9007835	5.1927776	1
382-6-15	our method performs better than both of these , is much simpler to implement , and uses less computation at test time .	one simple approach to this is to use online monte carlo methods , such as sgld ( stochastic gradient langevin dynamics ) .	0	5	1	5.331957	-4.660448	0
382-6-15	unfortunately , such a method needs to store many copies of the parameters ( which wastes memory ) , and needs to make predictions using many versions of the model ( which wastes time ) .	we describe a method for `` distilling '' a monte carlo approximation to the posterior predictive density into a more compact form , namely a single deep neural network .	1	2	3	-5.7837267	5.1862054	1
382-6-15	unfortunately , such a method needs to store many copies of the parameters ( which wastes memory ) , and needs to make predictions using many versions of the model ( which wastes time ) .	we compare to two very recent approaches to bayesian neural networks , namely an approach based on expectation propagation and an approach based on variational bayes .	1	2	4	-5.9648504	5.182594	1
382-6-15	our method performs better than both of these , is much simpler to implement , and uses less computation at test time .	unfortunately , such a method needs to store many copies of the parameters ( which wastes memory ) , and needs to make predictions using many versions of the model ( which wastes time ) .	0	5	2	5.5137987	-4.895928	0
382-6-15	we compare to two very recent approaches to bayesian neural networks , namely an approach based on expectation propagation and an approach based on variational bayes .	we describe a method for `` distilling '' a monte carlo approximation to the posterior predictive density into a more compact form , namely a single deep neural network .	0	4	3	5.2305117	-4.6001225	0
382-6-15	our method performs better than both of these , is much simpler to implement , and uses less computation at test time .	we describe a method for `` distilling '' a monte carlo approximation to the posterior predictive density into a more compact form , namely a single deep neural network .	0	5	3	5.468994	-4.8418484	0
382-6-15	our method performs better than both of these , is much simpler to implement , and uses less computation at test time .	we compare to two very recent approaches to bayesian neural networks , namely an approach based on expectation propagation and an approach based on variational bayes .	0	5	4	-1.9816527	2.061976	1
383-8-28	side information has been considered in several matrix completion applications , and has been empirically shown to be useful in many cases .	we study the matrix completion problem with side information .	0	1	0	2.7485733	-2.2870874	0
383-8-28	we study the matrix completion problem with side information .	recently , researchers studied the effect of side information for matrix completion from a theoretical viewpoint , showing that sample complexity can be significantly reduced given completely clean features .	1	0	2	-5.154533	4.6607637	1
383-8-28	however , since in reality most given features are noisy or only weakly informative , the development of a model to handle a general feature set , and investigation of how much noisy features can help matrix recovery , remains an important issue .	we study the matrix completion problem with side information .	0	3	0	2.671378	-2.2500768	0
383-8-28	in this paper , we propose a novel model that balances between features and observations simultaneously in order to leverage feature information yet be robust to feature noise .	we study the matrix completion problem with side information .	0	4	0	4.407454	-3.9370995	0
383-8-28	we study the matrix completion problem with side information .	moreover , we study the effect of general features in theory and show that by using our model , the sample complexity can be lower than matrix completion as long as features are sufficiently informative .	1	0	5	-5.8867083	5.0664277	1
383-8-28	this result provides a theoretical insight into the usefulness of general side information .	we study the matrix completion problem with side information .	0	6	0	5.516382	-4.9609756	0
383-8-28	finally , we consider synthetic data and two applications -- relationship prediction and semisupervised clustering -- and show that our model outperforms other methods for matrix completion that use features both in theory and practice .	we study the matrix completion problem with side information .	0	7	0	5.4917502	-4.944806	0
383-8-28	side information has been considered in several matrix completion applications , and has been empirically shown to be useful in many cases .	recently , researchers studied the effect of side information for matrix completion from a theoretical viewpoint , showing that sample complexity can be significantly reduced given completely clean features .	1	1	2	-1.4680313	1.7259904	1
383-8-28	side information has been considered in several matrix completion applications , and has been empirically shown to be useful in many cases .	however , since in reality most given features are noisy or only weakly informative , the development of a model to handle a general feature set , and investigation of how much noisy features can help matrix recovery , remains an important issue .	1	1	3	-2.2034993	2.3356996	1
383-8-28	in this paper , we propose a novel model that balances between features and observations simultaneously in order to leverage feature information yet be robust to feature noise .	side information has been considered in several matrix completion applications , and has been empirically shown to be useful in many cases .	0	4	1	4.6799436	-4.203486	0
383-8-28	moreover , we study the effect of general features in theory and show that by using our model , the sample complexity can be lower than matrix completion as long as features are sufficiently informative .	side information has been considered in several matrix completion applications , and has been empirically shown to be useful in many cases .	0	5	1	5.3687563	-4.751214	0
383-8-28	side information has been considered in several matrix completion applications , and has been empirically shown to be useful in many cases .	this result provides a theoretical insight into the usefulness of general side information .	1	1	6	-5.984148	5.192872	1
383-8-28	side information has been considered in several matrix completion applications , and has been empirically shown to be useful in many cases .	finally , we consider synthetic data and two applications -- relationship prediction and semisupervised clustering -- and show that our model outperforms other methods for matrix completion that use features both in theory and practice .	1	1	7	-5.994704	5.2214065	1
383-8-28	however , since in reality most given features are noisy or only weakly informative , the development of a model to handle a general feature set , and investigation of how much noisy features can help matrix recovery , remains an important issue .	recently , researchers studied the effect of side information for matrix completion from a theoretical viewpoint , showing that sample complexity can be significantly reduced given completely clean features .	0	3	2	2.9405322	-2.4831724	0
383-8-28	recently , researchers studied the effect of side information for matrix completion from a theoretical viewpoint , showing that sample complexity can be significantly reduced given completely clean features .	in this paper , we propose a novel model that balances between features and observations simultaneously in order to leverage feature information yet be robust to feature noise .	1	2	4	-6.002084	5.1996408	1
383-8-28	recently , researchers studied the effect of side information for matrix completion from a theoretical viewpoint , showing that sample complexity can be significantly reduced given completely clean features .	moreover , we study the effect of general features in theory and show that by using our model , the sample complexity can be lower than matrix completion as long as features are sufficiently informative .	1	2	5	-5.990769	5.1270523	1
383-8-28	this result provides a theoretical insight into the usefulness of general side information .	recently , researchers studied the effect of side information for matrix completion from a theoretical viewpoint , showing that sample complexity can be significantly reduced given completely clean features .	0	6	2	5.5156136	-4.896624	0
383-8-28	recently , researchers studied the effect of side information for matrix completion from a theoretical viewpoint , showing that sample complexity can be significantly reduced given completely clean features .	finally , we consider synthetic data and two applications -- relationship prediction and semisupervised clustering -- and show that our model outperforms other methods for matrix completion that use features both in theory and practice .	1	2	7	-5.948668	5.1851997	1
383-8-28	in this paper , we propose a novel model that balances between features and observations simultaneously in order to leverage feature information yet be robust to feature noise .	however , since in reality most given features are noisy or only weakly informative , the development of a model to handle a general feature set , and investigation of how much noisy features can help matrix recovery , remains an important issue .	0	4	3	5.383195	-4.8377266	0
383-8-28	however , since in reality most given features are noisy or only weakly informative , the development of a model to handle a general feature set , and investigation of how much noisy features can help matrix recovery , remains an important issue .	moreover , we study the effect of general features in theory and show that by using our model , the sample complexity can be lower than matrix completion as long as features are sufficiently informative .	1	3	5	-5.939134	5.1955986	1
383-8-28	however , since in reality most given features are noisy or only weakly informative , the development of a model to handle a general feature set , and investigation of how much noisy features can help matrix recovery , remains an important issue .	this result provides a theoretical insight into the usefulness of general side information .	1	3	6	-5.9790688	5.1250143	1
383-8-28	finally , we consider synthetic data and two applications -- relationship prediction and semisupervised clustering -- and show that our model outperforms other methods for matrix completion that use features both in theory and practice .	however , since in reality most given features are noisy or only weakly informative , the development of a model to handle a general feature set , and investigation of how much noisy features can help matrix recovery , remains an important issue .	0	7	3	5.5552526	-4.9772825	0
383-8-28	moreover , we study the effect of general features in theory and show that by using our model , the sample complexity can be lower than matrix completion as long as features are sufficiently informative .	in this paper , we propose a novel model that balances between features and observations simultaneously in order to leverage feature information yet be robust to feature noise .	0	5	4	5.542578	-4.8718557	0
383-8-28	this result provides a theoretical insight into the usefulness of general side information .	in this paper , we propose a novel model that balances between features and observations simultaneously in order to leverage feature information yet be robust to feature noise .	0	6	4	5.463193	-4.762236	0
383-8-28	in this paper , we propose a novel model that balances between features and observations simultaneously in order to leverage feature information yet be robust to feature noise .	finally , we consider synthetic data and two applications -- relationship prediction and semisupervised clustering -- and show that our model outperforms other methods for matrix completion that use features both in theory and practice .	1	4	7	-5.9560623	5.2001514	1
383-8-28	this result provides a theoretical insight into the usefulness of general side information .	moreover , we study the effect of general features in theory and show that by using our model , the sample complexity can be lower than matrix completion as long as features are sufficiently informative .	0	6	5	1.9200993	-1.775521	0
383-8-28	moreover , we study the effect of general features in theory and show that by using our model , the sample complexity can be lower than matrix completion as long as features are sufficiently informative .	finally , we consider synthetic data and two applications -- relationship prediction and semisupervised clustering -- and show that our model outperforms other methods for matrix completion that use features both in theory and practice .	1	5	7	-3.8432984	3.5936117	1
383-8-28	finally , we consider synthetic data and two applications -- relationship prediction and semisupervised clustering -- and show that our model outperforms other methods for matrix completion that use features both in theory and practice .	this result provides a theoretical insight into the usefulness of general side information .	0	7	6	3.1030388	-2.9606338	0
384-5-10	for example , nucleotides in a dna sequence , children 's names in a given state and year , and text documents are all commonly modeled with multinomial distributions .	many practical modeling problems involve discrete data that are best represented as draws from multinomial or categorical distributions .	0	1	0	5.318472	-4.5704083	0
384-5-10	many practical modeling problems involve discrete data that are best represented as draws from multinomial or categorical distributions .	in all of these cases , we expect some form of dependency between the draws : the nucleotide at one position in the dna strand may depend on the preceding nucleotides , children 's names are highly correlated from year to year , and topics in text may be correlated and dynamic .	1	0	2	-5.961891	5.044094	1
384-5-10	many practical modeling problems involve discrete data that are best represented as draws from multinomial or categorical distributions .	these dependencies are not naturally captured by the typical dirichlet-multinomial formulation .	1	0	3	-5.9460425	5.14733	1
384-5-10	here , we leverage a logistic stick-breaking representation and recent innovations in polya-gamma augmentation to reformulate the multinomial distribution in terms of latent variables with jointly gaussian likelihoods , enabling us to take advantage of a host of bayesian inference techniques for gaussian models with minimal overhead .	many practical modeling problems involve discrete data that are best represented as draws from multinomial or categorical distributions .	0	4	0	5.711379	-5.065179	0
384-5-10	in all of these cases , we expect some form of dependency between the draws : the nucleotide at one position in the dna strand may depend on the preceding nucleotides , children 's names are highly correlated from year to year , and topics in text may be correlated and dynamic .	for example , nucleotides in a dna sequence , children 's names in a given state and year , and text documents are all commonly modeled with multinomial distributions .	0	2	1	5.4608936	-4.7170186	0
384-5-10	these dependencies are not naturally captured by the typical dirichlet-multinomial formulation .	for example , nucleotides in a dna sequence , children 's names in a given state and year , and text documents are all commonly modeled with multinomial distributions .	0	3	1	5.289743	-4.661724	0
384-5-10	[CLS] here, we leverage a logistic stick - breaking representation and recent innovations in polya - gamma augmentation to reformulate the multinomial distribution in terms of latent variables with jointly gaussian likelihoods, enabling us to take advantage of a host of bayesian inference techniques for ga	for example , nucleotides in a dna sequence , children 's names in a given state and year , and text documents are all commonly modeled with multinomial distributions .	0	4	1	5.5558157	-4.870879	0
384-5-10	these dependencies are not naturally captured by the typical dirichlet-multinomial formulation .	in all of these cases , we expect some form of dependency between the draws : the nucleotide at one position in the dna strand may depend on the preceding nucleotides , children 's names are highly correlated from year to year , and topics in text may be correlated and dynamic .	0	3	2	1.9029317	-1.7202762	0
384-5-10	[CLS] here, we leverage a logistic stick - breaking representation and recent innovations in polya - gamma augmentation to reformulate the multinomial distribution in terms of latent variables with jointly gaussian likelihoods, enabling us to take	[CLS] in all of these cases, we expect some form of dependency between the draws : the nucleotide at one position in the dna strand may depend on the preceding nucleotides, children's names are highly correlated from year to year,	0	4	2	-2.6817005	2.6011236	1
384-5-10	here , we leverage a logistic stick-breaking representation and recent innovations in polya-gamma augmentation to reformulate the multinomial distribution in terms of latent variables with jointly gaussian likelihoods , enabling us to take advantage of a host of bayesian inference techniques for gaussian models with minimal overhead .	these dependencies are not naturally captured by the typical dirichlet-multinomial formulation .	0	4	3	4.3543406	-3.980158	0
385-9-36	our goal is to deploy a high-accuracy system starting with zero training examples .	we consider an on-the-job setting , where as inputs arrive , we use real-time crowdsourcing to resolve uncertainty where needed and output our prediction when confident .	1	0	1	-1.4152031	1.6916678	1
385-9-36	our goal is to deploy a high-accuracy system starting with zero training examples .	as the model improves over time , the reliance on crowdsourcing queries decreases .	1	0	2	-4.216497	3.906478	1
385-9-36	our goal is to deploy a high-accuracy system starting with zero training examples .	we cast our setting as a stochastic game based on bayesian decision theory , which allows us to balance latency , cost , and accuracy objectives in a principled way .	1	0	3	-3.2563329	3.2244864	1
385-9-36	our goal is to deploy a high-accuracy system starting with zero training examples .	computing the optimal policy is intractable , so we develop an approximation based on monte carlo tree search .	1	0	4	-5.490938	5.030776	1
385-9-36	our goal is to deploy a high-accuracy system starting with zero training examples .	we tested our approach on three datasets -- named-entity recognition , sentiment classification , and image classification .	1	0	5	-5.8811374	5.2081046	1
385-9-36	on the ner task we obtained more than an order of magnitude reduction in cost compared to full human annotation , while boosting performance relative to the expert provided labels .	our goal is to deploy a high-accuracy system starting with zero training examples .	0	6	0	5.0349565	-4.4949875	0
385-9-36	our goal is to deploy a high-accuracy system starting with zero training examples .	we also achieve a 8 % f1 improvement over having a single human label the whole set , and a 28 % f1 improvement over online learning .	1	0	7	-5.7612967	5.160327	1
385-9-36	`` poor is the pupil who does not surpass his master . '' - leonardo da vinci	our goal is to deploy a high-accuracy system starting with zero training examples .	0	8	0	-3.1506262	3.091545	1
385-9-36	as the model improves over time , the reliance on crowdsourcing queries decreases .	we consider an on-the-job setting , where as inputs arrive , we use real-time crowdsourcing to resolve uncertainty where needed and output our prediction when confident .	0	2	1	2.830029	-2.585145	0
385-9-36	we cast our setting as a stochastic game based on bayesian decision theory , which allows us to balance latency , cost , and accuracy objectives in a principled way .	we consider an on-the-job setting , where as inputs arrive , we use real-time crowdsourcing to resolve uncertainty where needed and output our prediction when confident .	0	3	1	3.3840344	-3.0431035	0
385-9-36	computing the optimal policy is intractable , so we develop an approximation based on monte carlo tree search .	we consider an on-the-job setting , where as inputs arrive , we use real-time crowdsourcing to resolve uncertainty where needed and output our prediction when confident .	0	4	1	3.957932	-3.5885541	0
385-9-36	we tested our approach on three datasets -- named-entity recognition , sentiment classification , and image classification .	we consider an on-the-job setting , where as inputs arrive , we use real-time crowdsourcing to resolve uncertainty where needed and output our prediction when confident .	0	5	1	5.3485637	-4.808408	0
385-9-36	on the ner task we obtained more than an order of magnitude reduction in cost compared to full human annotation , while boosting performance relative to the expert provided labels .	we consider an on-the-job setting , where as inputs arrive , we use real-time crowdsourcing to resolve uncertainty where needed and output our prediction when confident .	0	6	1	4.888227	-4.4352903	0
385-9-36	we consider an on-the-job setting , where as inputs arrive , we use real-time crowdsourcing to resolve uncertainty where needed and output our prediction when confident .	we also achieve a 8 % f1 improvement over having a single human label the whole set , and a 28 % f1 improvement over online learning .	1	1	7	-5.5960884	5.0812645	1
385-9-36	`` poor is the pupil who does not surpass his master . '' - leonardo da vinci	we consider an on-the-job setting , where as inputs arrive , we use real-time crowdsourcing to resolve uncertainty where needed and output our prediction when confident .	0	8	1	-3.395427	3.302848	1
385-9-36	we cast our setting as a stochastic game based on bayesian decision theory , which allows us to balance latency , cost , and accuracy objectives in a principled way .	as the model improves over time , the reliance on crowdsourcing queries decreases .	0	3	2	-3.5572364	3.4295583	1
385-9-36	computing the optimal policy is intractable , so we develop an approximation based on monte carlo tree search .	as the model improves over time , the reliance on crowdsourcing queries decreases .	0	4	2	1.5463784	-1.4015746	0
385-9-36	we tested our approach on three datasets -- named-entity recognition , sentiment classification , and image classification .	as the model improves over time , the reliance on crowdsourcing queries decreases .	0	5	2	4.565957	-4.1095676	0
385-9-36	on the ner task we obtained more than an order of magnitude reduction in cost compared to full human annotation , while boosting performance relative to the expert provided labels .	as the model improves over time , the reliance on crowdsourcing queries decreases .	0	6	2	3.6962268	-3.4221847	0
385-9-36	as the model improves over time , the reliance on crowdsourcing queries decreases .	we also achieve a 8 % f1 improvement over having a single human label the whole set , and a 28 % f1 improvement over online learning .	1	2	7	-5.4942913	4.998077	1
385-9-36	as the model improves over time , the reliance on crowdsourcing queries decreases .	`` poor is the pupil who does not surpass his master . '' - leonardo da vinci	1	2	8	3.468423	-3.2294593	0
385-9-36	we cast our setting as a stochastic game based on bayesian decision theory , which allows us to balance latency , cost , and accuracy objectives in a principled way .	computing the optimal policy is intractable , so we develop an approximation based on monte carlo tree search .	1	3	4	-4.473977	4.130895	1
385-9-36	we cast our setting as a stochastic game based on bayesian decision theory , which allows us to balance latency , cost , and accuracy objectives in a principled way .	we tested our approach on three datasets -- named-entity recognition , sentiment classification , and image classification .	1	3	5	-5.7930202	5.1674604	1
385-9-36	we cast our setting as a stochastic game based on bayesian decision theory , which allows us to balance latency , cost , and accuracy objectives in a principled way .	on the ner task we obtained more than an order of magnitude reduction in cost compared to full human annotation , while boosting performance relative to the expert provided labels .	1	3	6	-5.651082	5.0936594	1
385-9-36	we also achieve a 8 % f1 improvement over having a single human label the whole set , and a 28 % f1 improvement over online learning .	we cast our setting as a stochastic game based on bayesian decision theory , which allows us to balance latency , cost , and accuracy objectives in a principled way .	0	7	3	5.111357	-4.5480356	0
385-9-36	`` poor is the pupil who does not surpass his master . '' - leonardo da vinci	we cast our setting as a stochastic game based on bayesian decision theory , which allows us to balance latency , cost , and accuracy objectives in a principled way .	0	8	3	-5.548012	5.0111217	1
385-9-36	computing the optimal policy is intractable , so we develop an approximation based on monte carlo tree search .	we tested our approach on three datasets -- named-entity recognition , sentiment classification , and image classification .	1	4	5	-5.145176	4.8163958	1
385-9-36	on the ner task we obtained more than an order of magnitude reduction in cost compared to full human annotation , while boosting performance relative to the expert provided labels .	computing the optimal policy is intractable , so we develop an approximation based on monte carlo tree search .	0	6	4	4.10147	-3.8170757	0
385-9-36	computing the optimal policy is intractable , so we develop an approximation based on monte carlo tree search .	we also achieve a 8 % f1 improvement over having a single human label the whole set , and a 28 % f1 improvement over online learning .	1	4	7	-5.421175	4.9771843	1
385-9-36	`` poor is the pupil who does not surpass his master . '' - leonardo da vinci	computing the optimal policy is intractable , so we develop an approximation based on monte carlo tree search .	0	8	4	-5.7026615	5.165311	1
385-9-36	we tested our approach on three datasets -- named-entity recognition , sentiment classification , and image classification .	on the ner task we obtained more than an order of magnitude reduction in cost compared to full human annotation , while boosting performance relative to the expert provided labels .	1	5	6	-4.108637	3.8570871	1
385-9-36	we also achieve a 8 % f1 improvement over having a single human label the whole set , and a 28 % f1 improvement over online learning .	we tested our approach on three datasets -- named-entity recognition , sentiment classification , and image classification .	0	7	5	3.9270124	-3.6169596	0
385-9-36	`` poor is the pupil who does not surpass his master . '' - leonardo da vinci	we tested our approach on three datasets -- named-entity recognition , sentiment classification , and image classification .	0	8	5	-5.914032	5.155361	1
385-9-36	on the ner task we obtained more than an order of magnitude reduction in cost compared to full human annotation , while boosting performance relative to the expert provided labels .	we also achieve a 8 % f1 improvement over having a single human label the whole set , and a 28 % f1 improvement over online learning .	1	6	7	-2.6351657	2.538554	1
385-9-36	on the ner task we obtained more than an order of magnitude reduction in cost compared to full human annotation , while boosting performance relative to the expert provided labels .	`` poor is the pupil who does not surpass his master . '' - leonardo da vinci	1	6	8	5.4027886	-4.909574	0
385-9-36	`` poor is the pupil who does not surpass his master . '' - leonardo da vinci	we also achieve a 8 % f1 improvement over having a single human label the whole set , and a 28 % f1 improvement over online learning .	0	8	7	-5.9692817	5.2052975	1
386-5-10	we are interested in calibration for structured prediction problems such as speech recognition , optical character recognition , and medical diagnosis .	in user-facing applications , displaying calibrated confidence measures -- probabilities that correspond to true frequency -- can be as important as obtaining high accuracy .	0	1	0	-5.908026	5.2016068	1
386-5-10	structured prediction presents new challenges for calibration : the output space is large , and users may issue many types of probability queries ( e.g. , marginals ) on the structured output .	in user-facing applications , displaying calibrated confidence measures -- probabilities that correspond to true frequency -- can be as important as obtaining high accuracy .	0	2	0	-5.878685	5.168179	1
386-5-10	we extend the notion of calibration so as to handle various subtleties pertaining to the structured setting , and then provide a simple recalibration method that trains a binary classifier to predict probabilities of interest .	in user-facing applications , displaying calibrated confidence measures -- probabilities that correspond to true frequency -- can be as important as obtaining high accuracy .	0	3	0	4.736354	-4.2002287	0
386-5-10	in user-facing applications , displaying calibrated confidence measures -- probabilities that correspond to true frequency -- can be as important as obtaining high accuracy .	we explore a range of features appropriate for structured recalibration , and demonstrate their efficacy on three real-world datasets .	1	0	4	-5.7227488	5.2172637	1
386-5-10	we are interested in calibration for structured prediction problems such as speech recognition , optical character recognition , and medical diagnosis .	structured prediction presents new challenges for calibration : the output space is large , and users may issue many types of probability queries ( e.g. , marginals ) on the structured output .	1	1	2	-4.1513247	3.9022965	1
386-5-10	we are interested in calibration for structured prediction problems such as speech recognition , optical character recognition , and medical diagnosis .	we extend the notion of calibration so as to handle various subtleties pertaining to the structured setting , and then provide a simple recalibration method that trains a binary classifier to predict probabilities of interest .	1	1	3	-5.9022627	5.1996217	1
386-5-10	we are interested in calibration for structured prediction problems such as speech recognition , optical character recognition , and medical diagnosis .	we explore a range of features appropriate for structured recalibration , and demonstrate their efficacy on three real-world datasets .	1	1	4	-5.920172	5.2812395	1
386-5-10	we extend the notion of calibration so as to handle various subtleties pertaining to the structured setting , and then provide a simple recalibration method that trains a binary classifier to predict probabilities of interest .	structured prediction presents new challenges for calibration : the output space is large , and users may issue many types of probability queries ( e.g. , marginals ) on the structured output .	0	3	2	5.4318247	-4.7706947	0
386-5-10	structured prediction presents new challenges for calibration : the output space is large , and users may issue many types of probability queries ( e.g. , marginals ) on the structured output .	we explore a range of features appropriate for structured recalibration , and demonstrate their efficacy on three real-world datasets .	1	2	4	-6.0211825	5.123411	1
386-5-10	we explore a range of features appropriate for structured recalibration , and demonstrate their efficacy on three real-world datasets .	we extend the notion of calibration so as to handle various subtleties pertaining to the structured setting , and then provide a simple recalibration method that trains a binary classifier to predict probabilities of interest .	0	4	3	4.4873047	-4.047008	0
387-7-21	although it can approximate a complex many-to-one function well when a large amount of training data is provided , it is still challenging to model complex structured output representations that effectively perform probabilistic inference and make diverse predictions .	supervised deep learning has been successfully applied to many recognition problems .	0	1	0	5.3996	-4.8428216	0
387-7-21	supervised deep learning has been successfully applied to many recognition problems .	in this work , we develop a deep conditional generative model for structured output prediction using gaussian latent variables .	1	0	2	-5.954244	5.2058163	1
387-7-21	supervised deep learning has been successfully applied to many recognition problems .	the model is trained efficiently in the framework of stochastic gradient variational bayes , and allows for fast prediction using stochastic feed-forward inference .	1	0	3	-5.864596	5.2021604	1
387-7-21	supervised deep learning has been successfully applied to many recognition problems .	in addition , we provide novel strategies to build robust structured prediction algorithms , such as input noise-injection and multi-scale prediction objective at training .	1	0	4	-5.9188175	5.158993	1
387-7-21	supervised deep learning has been successfully applied to many recognition problems .	in experiments , we demonstrate the effectiveness of our proposed algorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic structured output predictions using stochastic inference .	1	0	5	-5.883565	5.16985	1
387-7-21	furthermore , the proposed training methods are complimentary , which leads to strong pixel-level object segmentation and semantic labeling performance on caltech-ucsd birds 200 and the subset of labeled faces in the wild dataset .	supervised deep learning has been successfully applied to many recognition problems .	0	6	0	5.5585985	-4.9892654	0
387-7-21	although it can approximate a complex many-to-one function well when a large amount of training data is provided , it is still challenging to model complex structured output representations that effectively perform probabilistic inference and make diverse predictions .	in this work , we develop a deep conditional generative model for structured output prediction using gaussian latent variables .	1	1	2	-5.9046936	5.2103314	1
387-7-21	although it can approximate a complex many-to-one function well when a large amount of training data is provided , it is still challenging to model complex structured output representations that effectively perform probabilistic inference and make diverse predictions .	the model is trained efficiently in the framework of stochastic gradient variational bayes , and allows for fast prediction using stochastic feed-forward inference .	1	1	3	-5.906973	5.2360015	1
387-7-21	in addition , we provide novel strategies to build robust structured prediction algorithms , such as input noise-injection and multi-scale prediction objective at training .	although it can approximate a complex many-to-one function well when a large amount of training data is provided , it is still challenging to model complex structured output representations that effectively perform probabilistic inference and make diverse predictions .	0	4	1	5.576043	-4.9554267	0
387-7-21	in experiments , we demonstrate the effectiveness of our proposed algorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic structured output predictions using stochastic inference .	although it can approximate a complex many-to-one function well when a large amount of training data is provided , it is still challenging to model complex structured output representations that effectively perform probabilistic inference and make diverse predictions .	0	5	1	5.636532	-5.0201893	0
387-7-21	although it can approximate a complex many-to-one function well when a large amount of training data is provided , it is still challenging to model complex structured output representations that effectively perform probabilistic inference and make diverse predictions .	furthermore , the proposed training methods are complimentary , which leads to strong pixel-level object segmentation and semantic labeling performance on caltech-ucsd birds 200 and the subset of labeled faces in the wild dataset .	1	1	6	-5.956529	5.180093	1
387-7-21	in this work , we develop a deep conditional generative model for structured output prediction using gaussian latent variables .	the model is trained efficiently in the framework of stochastic gradient variational bayes , and allows for fast prediction using stochastic feed-forward inference .	1	2	3	-5.899188	5.1794243	1
387-7-21	in this work , we develop a deep conditional generative model for structured output prediction using gaussian latent variables .	in addition , we provide novel strategies to build robust structured prediction algorithms , such as input noise-injection and multi-scale prediction objective at training .	1	2	4	-5.8926373	5.0299506	1
387-7-21	in experiments , we demonstrate the effectiveness of our proposed algorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic structured output predictions using stochastic inference .	in this work , we develop a deep conditional generative model for structured output prediction using gaussian latent variables .	0	5	2	5.577515	-4.953092	0
387-7-21	in this work , we develop a deep conditional generative model for structured output prediction using gaussian latent variables .	furthermore , the proposed training methods are complimentary , which leads to strong pixel-level object segmentation and semantic labeling performance on caltech-ucsd birds 200 and the subset of labeled faces in the wild dataset .	1	2	6	-5.967537	5.087715	1
387-7-21	in addition , we provide novel strategies to build robust structured prediction algorithms , such as input noise-injection and multi-scale prediction objective at training .	the model is trained efficiently in the framework of stochastic gradient variational bayes , and allows for fast prediction using stochastic feed-forward inference .	0	4	3	4.4832807	-4.092798	0
387-7-21	the model is trained efficiently in the framework of stochastic gradient variational bayes , and allows for fast prediction using stochastic feed-forward inference .	in experiments , we demonstrate the effectiveness of our proposed algorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic structured output predictions using stochastic inference .	1	3	5	-5.7555356	5.124861	1
387-7-21	the model is trained efficiently in the framework of stochastic gradient variational bayes , and allows for fast prediction using stochastic feed-forward inference .	furthermore , the proposed training methods are complimentary , which leads to strong pixel-level object segmentation and semantic labeling performance on caltech-ucsd birds 200 and the subset of labeled faces in the wild dataset .	1	3	6	-5.8669066	5.11667	1
387-7-21	in experiments , we demonstrate the effectiveness of our proposed algorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic structured output predictions using stochastic inference .	in addition , we provide novel strategies to build robust structured prediction algorithms , such as input noise-injection and multi-scale prediction objective at training .	0	5	4	2.976239	-2.7815742	0
387-7-21	in addition , we provide novel strategies to build robust structured prediction algorithms , such as input noise-injection and multi-scale prediction objective at training .	furthermore , the proposed training methods are complimentary , which leads to strong pixel-level object segmentation and semantic labeling performance on caltech-ucsd birds 200 and the subset of labeled faces in the wild dataset .	1	4	6	-3.7586687	3.5454917	1
387-7-21	furthermore , the proposed training methods are complimentary , which leads to strong pixel-level object segmentation and semantic labeling performance on caltech-ucsd birds 200 and the subset of labeled faces in the wild dataset .	in experiments , we demonstrate the effectiveness of our proposed algorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic structured output predictions using stochastic inference .	0	6	5	3.2310603	-3.0206156	0
388-7-21	however , most recommendation algorithms do not explicitly take into account the temporal behavior and the recurrent activities of users .	by making personalized suggestions , a recommender system is playing a crucial role in improving the engagement of users in modern web-services .	0	1	0	4.914811	-4.374839	0
388-7-21	two central but less explored questions are how to recommend the most desirable item at the right moment , and how to predict the next returning time of a user to a service .	by making personalized suggestions , a recommender system is playing a crucial role in improving the engagement of users in modern web-services .	0	2	0	5.1799936	-4.5692344	0
388-7-21	by making personalized suggestions , a recommender system is playing a crucial role in improving the engagement of users in modern web-services .	to address these questions , we propose a novel framework which connects self-exciting point processes and low-rank models to capture the recurrent temporal patterns in a large collection of user-item consumption pairs .	1	0	3	-5.879595	5.219718	1
388-7-21	we show that the parameters of the model can be estimated via a convex optimization , and furthermore , we develop an efficient algorithm that maintains o ( 1/ ) convergence rate , scales up to problems with millions of user-item pairs and hundreds of millions of temporal events .	by making personalized suggestions , a recommender system is playing a crucial role in improving the engagement of users in modern web-services .	0	4	0	5.586036	-4.9596667	0
388-7-21	compared to other state-of-the-arts in both synthetic and real datasets , our model achieves superb predictive performance in the two time-sensitive recommendation tasks .	by making personalized suggestions , a recommender system is playing a crucial role in improving the engagement of users in modern web-services .	0	5	0	5.6091356	-5.028082	0
388-7-21	finally , we point out that our formulation can incorporate other extra context information of users , such as profile , textual and spatial features .	by making personalized suggestions , a recommender system is playing a crucial role in improving the engagement of users in modern web-services .	0	6	0	5.630698	-5.051152	0
388-7-21	however , most recommendation algorithms do not explicitly take into account the temporal behavior and the recurrent activities of users .	two central but less explored questions are how to recommend the most desirable item at the right moment , and how to predict the next returning time of a user to a service .	1	1	2	0.14278029	0.007342711	0
388-7-21	however , most recommendation algorithms do not explicitly take into account the temporal behavior and the recurrent activities of users .	to address these questions , we propose a novel framework which connects self-exciting point processes and low-rank models to capture the recurrent temporal patterns in a large collection of user-item consumption pairs .	1	1	3	-5.9417906	5.162838	1
388-7-21	however , most recommendation algorithms do not explicitly take into account the temporal behavior and the recurrent activities of users .	we show that the parameters of the model can be estimated via a convex optimization , and furthermore , we develop an efficient algorithm that maintains o ( 1/ ) convergence rate , scales up to problems with millions of user-item pairs and hundreds of millions of temporal events .	1	1	4	-5.901355	5.181859	1
388-7-21	compared to other state-of-the-arts in both synthetic and real datasets , our model achieves superb predictive performance in the two time-sensitive recommendation tasks .	however , most recommendation algorithms do not explicitly take into account the temporal behavior and the recurrent activities of users .	0	5	1	5.560573	-4.881383	0
388-7-21	finally , we point out that our formulation can incorporate other extra context information of users , such as profile , textual and spatial features .	however , most recommendation algorithms do not explicitly take into account the temporal behavior and the recurrent activities of users .	0	6	1	5.5171633	-4.900807	0
388-7-21	to address these questions , we propose a novel framework which connects self-exciting point processes and low-rank models to capture the recurrent temporal patterns in a large collection of user-item consumption pairs .	two central but less explored questions are how to recommend the most desirable item at the right moment , and how to predict the next returning time of a user to a service .	0	3	2	5.0365496	-4.3967023	0
388-7-21	two central but less explored questions are how to recommend the most desirable item at the right moment , and how to predict the next returning time of a user to a service .	we show that the parameters of the model can be estimated via a convex optimization , and furthermore , we develop an efficient algorithm that maintains o ( 1/ ) convergence rate , scales up to problems with millions of user-item pairs and hundreds of millions of temporal events .	1	2	4	-5.968336	5.2401342	1
388-7-21	compared to other state-of-the-arts in both synthetic and real datasets , our model achieves superb predictive performance in the two time-sensitive recommendation tasks .	two central but less explored questions are how to recommend the most desirable item at the right moment , and how to predict the next returning time of a user to a service .	0	5	2	5.518156	-4.90858	0
388-7-21	two central but less explored questions are how to recommend the most desirable item at the right moment , and how to predict the next returning time of a user to a service .	finally , we point out that our formulation can incorporate other extra context information of users , such as profile , textual and spatial features .	1	2	6	-5.966357	5.1064615	1
388-7-21	we show that the parameters of the model can be estimated via a convex optimization , and furthermore , we develop an efficient algorithm that maintains o ( 1/ ) convergence rate , scales up to problems with millions of user-item pairs and hundreds of millions of temporal events .	to address these questions , we propose a novel framework which connects self-exciting point processes and low-rank models to capture the recurrent temporal patterns in a large collection of user-item consumption pairs .	0	4	3	5.5301824	-4.8789067	0
388-7-21	to address these questions , we propose a novel framework which connects self-exciting point processes and low-rank models to capture the recurrent temporal patterns in a large collection of user-item consumption pairs .	compared to other state-of-the-arts in both synthetic and real datasets , our model achieves superb predictive performance in the two time-sensitive recommendation tasks .	1	3	5	-5.9341984	5.196802	1
388-7-21	to address these questions , we propose a novel framework which connects self-exciting point processes and low-rank models to capture the recurrent temporal patterns in a large collection of user-item consumption pairs .	finally , we point out that our formulation can incorporate other extra context information of users , such as profile , textual and spatial features .	1	3	6	-5.9874716	5.111197	1
388-7-21	compared to other state-of-the-arts in both synthetic and real datasets , our model achieves superb predictive performance in the two time-sensitive recommendation tasks .	we show that the parameters of the model can be estimated via a convex optimization , and furthermore , we develop an efficient algorithm that maintains o ( 1/ ) convergence rate , scales up to problems with millions of user-item pairs and hundreds of millions of temporal events .	0	5	4	3.1706262	-2.9646146	0
388-7-21	finally , we point out that our formulation can incorporate other extra context information of users , such as profile , textual and spatial features .	we show that the parameters of the model can be estimated via a convex optimization , and furthermore , we develop an efficient algorithm that maintains o ( 1/ ) convergence rate , scales up to problems with millions of user-item pairs and hundreds of millions of temporal events .	0	6	4	3.0766964	-2.9016485	0
388-7-21	finally , we point out that our formulation can incorporate other extra context information of users , such as profile , textual and spatial features .	compared to other state-of-the-arts in both synthetic and real datasets , our model achieves superb predictive performance in the two time-sensitive recommendation tasks .	0	6	5	-0.6378238	0.7964632	1
389-7-21	[CLS] we introduce the gaussian process convolution model ( gpcm ), a two - stage nonparametric generative procedure to model stationary signals as the convolution between a continuous - time white - noise process and a continuous - time linear filter drawn	the gpcm is a continuous-time nonparametricwindow moving average process and , conditionally , is itself a gaussian process with a nonparametric kernel defined in a probabilistic fashion .	1	0	1	-5.665102	5.1729236	1
389-7-21	we introduce the gaussian process convolution model ( gpcm ) , a two-stage nonparametric generative procedure to model stationary signals as the convolution between a continuous-time white-noise process and a continuous-time linear filter drawn from gaussian process .	the generative model can be equivalently considered in the frequency domain , where the power spectral density of the signal is specified using a gaussian process .	1	0	2	-5.934279	5.2249746	1
389-7-21	[CLS] we introduce the gaussian process convolution model ( gpcm ), a two - stage nonparametric generative procedure to model stationary signals as the convolution between a continuous - time white - noise process and a continuous - time linear	one of the main contributions of the paper is to develop a novel variational freeenergy approach based on inter-domain inducing variables that efficiently learns the continuous-time linear filter and infers the driving white-noise process .	1	0	3	-5.811115	5.162409	1
389-7-21	we introduce the gaussian process convolution model ( gpcm ) , a two-stage nonparametric generative procedure to model stationary signals as the convolution between a continuous-time white-noise process and a continuous-time linear filter drawn from gaussian process .	in turn , this scheme provides closed-form probabilistic estimates of the covariance kernel and the noise-free signal both in denoising and prediction scenarios .	1	0	4	-5.9117804	5.190406	1
389-7-21	[CLS] we introduce the gaussian process convolution model ( gpcm ), a two - stage nonparametric generative procedure to model stationary signals as the convolution between a continuous - time white - noise process and a continuous - time linear filter drawn from gaussian process	additionally , the variational inference procedure provides closed-form expressions for the approximate posterior of the spectral density given the observed data , leading to new bayesian nonparametric approaches to spectrum estimation .	1	0	5	-5.9164534	5.2122498	1
389-7-21	the proposed gpcm is validated using synthetic and real-world signals .	we introduce the gaussian process convolution model ( gpcm ) , a two-stage nonparametric generative procedure to model stationary signals as the convolution between a continuous-time white-noise process and a continuous-time linear filter drawn from gaussian process .	0	6	0	5.61854	-5.0646057	0
389-7-21	the generative model can be equivalently considered in the frequency domain , where the power spectral density of the signal is specified using a gaussian process .	the gpcm is a continuous-time nonparametricwindow moving average process and , conditionally , is itself a gaussian process with a nonparametric kernel defined in a probabilistic fashion .	0	2	1	4.02145	-3.680973	0
389-7-21	one of the main contributions of the paper is to develop a novel variational freeenergy approach based on inter-domain inducing variables that efficiently learns the continuous-time linear filter and infers the driving white-noise process .	the gpcm is a continuous-time nonparametricwindow moving average process and , conditionally , is itself a gaussian process with a nonparametric kernel defined in a probabilistic fashion .	0	3	1	2.255669	-2.0059907	0
389-7-21	in turn , this scheme provides closed-form probabilistic estimates of the covariance kernel and the noise-free signal both in denoising and prediction scenarios .	the gpcm is a continuous-time nonparametricwindow moving average process and , conditionally , is itself a gaussian process with a nonparametric kernel defined in a probabilistic fashion .	0	4	1	4.894367	-4.364715	0
389-7-21	the gpcm is a continuous-time nonparametricwindow moving average process and , conditionally , is itself a gaussian process with a nonparametric kernel defined in a probabilistic fashion .	additionally , the variational inference procedure provides closed-form expressions for the approximate posterior of the spectral density given the observed data , leading to new bayesian nonparametric approaches to spectrum estimation .	1	1	5	-5.85723	5.1387243	1
389-7-21	the proposed gpcm is validated using synthetic and real-world signals .	the gpcm is a continuous-time nonparametricwindow moving average process and , conditionally , is itself a gaussian process with a nonparametric kernel defined in a probabilistic fashion .	0	6	1	5.259935	-4.7045937	0
389-7-21	the generative model can be equivalently considered in the frequency domain , where the power spectral density of the signal is specified using a gaussian process .	one of the main contributions of the paper is to develop a novel variational freeenergy approach based on inter-domain inducing variables that efficiently learns the continuous-time linear filter and infers the driving white-noise process .	1	2	3	-0.5059166	0.8082227	1
389-7-21	the generative model can be equivalently considered in the frequency domain , where the power spectral density of the signal is specified using a gaussian process .	in turn , this scheme provides closed-form probabilistic estimates of the covariance kernel and the noise-free signal both in denoising and prediction scenarios .	1	2	4	-3.0607662	3.0088992	1
389-7-21	additionally , the variational inference procedure provides closed-form expressions for the approximate posterior of the spectral density given the observed data , leading to new bayesian nonparametric approaches to spectrum estimation .	the generative model can be equivalently considered in the frequency domain , where the power spectral density of the signal is specified using a gaussian process .	0	5	2	3.6464596	-3.439068	0
389-7-21	the generative model can be equivalently considered in the frequency domain , where the power spectral density of the signal is specified using a gaussian process .	the proposed gpcm is validated using synthetic and real-world signals .	1	2	6	-5.410329	4.864505	1
389-7-21	one of the main contributions of the paper is to develop a novel variational freeenergy approach based on inter-domain inducing variables that efficiently learns the continuous-time linear filter and infers the driving white-noise process .	in turn , this scheme provides closed-form probabilistic estimates of the covariance kernel and the noise-free signal both in denoising and prediction scenarios .	1	3	4	-2.2433345	2.3768137	1
389-7-21	additionally , the variational inference procedure provides closed-form expressions for the approximate posterior of the spectral density given the observed data , leading to new bayesian nonparametric approaches to spectrum estimation .	one of the main contributions of the paper is to develop a novel variational freeenergy approach based on inter-domain inducing variables that efficiently learns the continuous-time linear filter and infers the driving white-noise process .	0	5	3	2.102408	-1.9321893	0
389-7-21	the proposed gpcm is validated using synthetic and real-world signals .	one of the main contributions of the paper is to develop a novel variational freeenergy approach based on inter-domain inducing variables that efficiently learns the continuous-time linear filter and infers the driving white-noise process .	0	6	3	4.6265974	-4.069537	0
389-7-21	in turn , this scheme provides closed-form probabilistic estimates of the covariance kernel and the noise-free signal both in denoising and prediction scenarios .	additionally , the variational inference procedure provides closed-form expressions for the approximate posterior of the spectral density given the observed data , leading to new bayesian nonparametric approaches to spectrum estimation .	1	4	5	-3.118011	3.1060886	1
389-7-21	the proposed gpcm is validated using synthetic and real-world signals .	in turn , this scheme provides closed-form probabilistic estimates of the covariance kernel and the noise-free signal both in denoising and prediction scenarios .	0	6	4	4.3019247	-3.956117	0
389-7-21	the proposed gpcm is validated using synthetic and real-world signals .	additionally , the variational inference procedure provides closed-form expressions for the approximate posterior of the spectral density given the observed data , leading to new bayesian nonparametric approaches to spectrum estimation .	0	6	5	4.1324472	-3.7985368	0
390-4-6	the participants may simply hold data points they wish to sell , or may have more sophisticated information ; either way , they are incentivized to participate as long as they believe their data points are representative or their information will improve the mechanism 's future prediction on a test set .	we propose a mechanism for purchasing information from a sequence of participants .	0	1	0	5.1496716	-4.5154834	0
390-4-6	we propose a mechanism for purchasing information from a sequence of participants .	the mechanism , which draws on the principles of prediction markets , has a bounded budget and minimizes generalization error for bregman divergence loss functions .	1	0	2	-5.8254724	5.241679	1
390-4-6	we propose a mechanism for purchasing information from a sequence of participants .	we then show how to modify this mechanism to preserve the privacy of participants ' information : at any given time , the current prices and predictions of the mechanism reveal almost no information about any one participant , yet in total over all participants , information is accurately aggregated .	1	0	3	-5.9649267	5.2166796	1
390-4-6	the mechanism , which draws on the principles of prediction markets , has a bounded budget and minimizes generalization error for bregman divergence loss functions .	the participants may simply hold data points they wish to sell , or may have more sophisticated information ; either way , they are incentivized to participate as long as they believe their data points are representative or their information will improve the mechanism 's future prediction on a test set .	0	2	1	-2.1106799	2.1500194	1
390-4-6	[CLS] the participants may simply hold data points they wish to sell, or may have more sophisticated information ; either way, they are incentivized to participate as long as they believe their data points are representative or their information will improve the mechanism's	[CLS] we then show how to modify this mechanism to preserve the privacy of participants'information : at any given time, the current prices and predictions of the mechanism reveal almost no information about any one participant, yet in total over all participants, information is accurately	1	1	3	-2.2555113	2.362422	1
390-4-6	we then show how to modify this mechanism to preserve the privacy of participants ' information : at any given time , the current prices and predictions of the mechanism reveal almost no information about any one participant , yet in total over all participants , information is accurately aggregated .	the mechanism , which draws on the principles of prediction markets , has a bounded budget and minimizes generalization error for bregman divergence loss functions .	0	3	2	2.929018	-2.6956158	0
391-6-15	computational complexity of these rules is highly dependent on the choice of the constraint language they operate on and therefore coming up with the right kind of representation is critical to the success of lifted inference .	lifted inference rules exploit symmetries for fast reasoning in statistical relational models .	0	1	0	4.8100834	-4.263874	0
391-6-15	in this paper , we propose a new constraint language , called setineq , which allows subset , equality and inequality constraints , to represent substitutions over the variables in the theory .	lifted inference rules exploit symmetries for fast reasoning in statistical relational models .	0	2	0	4.5731497	-4.112282	0
391-6-15	our constraint formulation is strictly more expressive than existing representations , yet easy to operate on .	lifted inference rules exploit symmetries for fast reasoning in statistical relational models .	0	3	0	2.7259648	-2.5961695	0
391-6-15	lifted inference rules exploit symmetries for fast reasoning in statistical relational models .	we reformulate the three main lifting rules : decomposer , generalized binomial and the recently proposed single occurrence for map inference , to work with our constraint representation .	1	0	4	-5.640266	5.1305547	1
391-6-15	experiments on benchmark mlns for exact and sampling based inference demonstrate the effectiveness of our approach over several other existing techniques .	lifted inference rules exploit symmetries for fast reasoning in statistical relational models .	0	5	0	5.4223485	-4.827755	0
391-6-15	in this paper , we propose a new constraint language , called setineq , which allows subset , equality and inequality constraints , to represent substitutions over the variables in the theory .	computational complexity of these rules is highly dependent on the choice of the constraint language they operate on and therefore coming up with the right kind of representation is critical to the success of lifted inference .	0	2	1	4.4834013	-4.055577	0
391-6-15	our constraint formulation is strictly more expressive than existing representations , yet easy to operate on .	computational complexity of these rules is highly dependent on the choice of the constraint language they operate on and therefore coming up with the right kind of representation is critical to the success of lifted inference .	0	3	1	4.881627	-4.38192	0
391-6-15	we reformulate the three main lifting rules : decomposer , generalized binomial and the recently proposed single occurrence for map inference , to work with our constraint representation .	computational complexity of these rules is highly dependent on the choice of the constraint language they operate on and therefore coming up with the right kind of representation is critical to the success of lifted inference .	0	4	1	4.548831	-4.13736	0
391-6-15	experiments on benchmark mlns for exact and sampling based inference demonstrate the effectiveness of our approach over several other existing techniques .	computational complexity of these rules is highly dependent on the choice of the constraint language they operate on and therefore coming up with the right kind of representation is critical to the success of lifted inference .	0	5	1	5.5960007	-4.969509	0
391-6-15	in this paper , we propose a new constraint language , called setineq , which allows subset , equality and inequality constraints , to represent substitutions over the variables in the theory .	our constraint formulation is strictly more expressive than existing representations , yet easy to operate on .	1	2	3	-5.8043547	5.1780457	1
391-6-15	in this paper , we propose a new constraint language , called setineq , which allows subset , equality and inequality constraints , to represent substitutions over the variables in the theory .	we reformulate the three main lifting rules : decomposer , generalized binomial and the recently proposed single occurrence for map inference , to work with our constraint representation .	1	2	4	-5.906527	5.1972914	1
391-6-15	experiments on benchmark mlns for exact and sampling based inference demonstrate the effectiveness of our approach over several other existing techniques .	in this paper , we propose a new constraint language , called setineq , which allows subset , equality and inequality constraints , to represent substitutions over the variables in the theory .	0	5	2	5.5711384	-4.898523	0
391-6-15	our constraint formulation is strictly more expressive than existing representations , yet easy to operate on .	we reformulate the three main lifting rules : decomposer , generalized binomial and the recently proposed single occurrence for map inference , to work with our constraint representation .	1	3	4	-1.0312399	1.2181163	1
391-6-15	experiments on benchmark mlns for exact and sampling based inference demonstrate the effectiveness of our approach over several other existing techniques .	our constraint formulation is strictly more expressive than existing representations , yet easy to operate on .	0	5	3	5.234342	-4.5640564	0
391-6-15	we reformulate the three main lifting rules : decomposer , generalized binomial and the recently proposed single occurrence for map inference , to work with our constraint representation .	experiments on benchmark mlns for exact and sampling based inference demonstrate the effectiveness of our approach over several other existing techniques .	1	4	5	-5.956775	5.192452	1
392-6-15	in a variety of problems originating in supervised , unsupervised , and reinforcement learning , the loss function is defined by an expectation over a collection of random variables , which might be part of a probabilistic model or the external world .	estimating the gradient of this loss function , using samples , lies at the core of gradient-based learning algorithms for these problems .	1	0	1	-5.7627196	5.224635	1
392-6-15	[CLS] in a variety of problems originating in supervised, unsupervised, and reinforcement learning, the loss function is defined by an expectation over a collection of random variables, which might be part of a probabilistic model or the external world	[CLS] we introduce the formalism of stochastic computation graphs - - directed acyclic graphs that include both deterministic functions and conditional probability distributions - - and describe how to easily and automatically derive an unbiased estimator of the	1	0	2	-5.3228836	4.9301276	1
392-6-15	the resulting algorithm for computing the gradient estimator is a simple modification of the standard backpropagation algorithm .	in a variety of problems originating in supervised , unsupervised , and reinforcement learning , the loss function is defined by an expectation over a collection of random variables , which might be part of a probabilistic model or the external world .	0	3	0	5.653246	-5.124305	0
392-6-15	the generic scheme we propose unifies estimators derived in variety of prior work , along with variance-reduction techniques therein .	in a variety of problems originating in supervised , unsupervised , and reinforcement learning , the loss function is defined by an expectation over a collection of random variables , which might be part of a probabilistic model or the external world .	0	4	0	5.700627	-5.1319857	0
392-6-15	in a variety of problems originating in supervised , unsupervised , and reinforcement learning , the loss function is defined by an expectation over a collection of random variables , which might be part of a probabilistic model or the external world .	it could assist researchers in developing intricate models involving a combination of stochastic and deterministic operations , enabling , for example , attention , memory , and control actions .	1	0	5	-5.8739095	5.1489325	1
392-6-15	we introduce the formalism of stochastic computation graphs -- directed acyclic graphs that include both deterministic functions and conditional probability distributions -- and describe how to easily and automatically derive an unbiased estimator of the loss function 's gradient .	estimating the gradient of this loss function , using samples , lies at the core of gradient-based learning algorithms for these problems .	0	2	1	4.006597	-3.4172897	0
392-6-15	estimating the gradient of this loss function , using samples , lies at the core of gradient-based learning algorithms for these problems .	the resulting algorithm for computing the gradient estimator is a simple modification of the standard backpropagation algorithm .	1	1	3	-5.97857	5.128504	1
392-6-15	estimating the gradient of this loss function , using samples , lies at the core of gradient-based learning algorithms for these problems .	the generic scheme we propose unifies estimators derived in variety of prior work , along with variance-reduction techniques therein .	1	1	4	-6.024333	5.148141	1
392-6-15	estimating the gradient of this loss function , using samples , lies at the core of gradient-based learning algorithms for these problems .	it could assist researchers in developing intricate models involving a combination of stochastic and deterministic operations , enabling , for example , attention , memory , and control actions .	1	1	5	-5.6579304	5.0245476	1
392-6-15	the resulting algorithm for computing the gradient estimator is a simple modification of the standard backpropagation algorithm .	we introduce the formalism of stochastic computation graphs -- directed acyclic graphs that include both deterministic functions and conditional probability distributions -- and describe how to easily and automatically derive an unbiased estimator of the loss function 's gradient .	0	3	2	5.51313	-4.939111	0
392-6-15	we introduce the formalism of stochastic computation graphs -- directed acyclic graphs that include both deterministic functions and conditional probability distributions -- and describe how to easily and automatically derive an unbiased estimator of the loss function 's gradient .	the generic scheme we propose unifies estimators derived in variety of prior work , along with variance-reduction techniques therein .	1	2	4	-5.9836354	5.196177	1
392-6-15	it could assist researchers in developing intricate models involving a combination of stochastic and deterministic operations , enabling , for example , attention , memory , and control actions .	we introduce the formalism of stochastic computation graphs -- directed acyclic graphs that include both deterministic functions and conditional probability distributions -- and describe how to easily and automatically derive an unbiased estimator of the loss function 's gradient .	0	5	2	3.711021	-3.3625119	0
392-6-15	the generic scheme we propose unifies estimators derived in variety of prior work , along with variance-reduction techniques therein .	the resulting algorithm for computing the gradient estimator is a simple modification of the standard backpropagation algorithm .	0	4	3	-1.3833731	1.5738412	1
392-6-15	the resulting algorithm for computing the gradient estimator is a simple modification of the standard backpropagation algorithm .	it could assist researchers in developing intricate models involving a combination of stochastic and deterministic operations , enabling , for example , attention , memory , and control actions .	1	3	5	-2.6181753	2.619083	1
392-6-15	it could assist researchers in developing intricate models involving a combination of stochastic and deterministic operations , enabling , for example , attention , memory , and control actions .	the generic scheme we propose unifies estimators derived in variety of prior work , along with variance-reduction techniques therein .	0	5	4	-1.6827917	1.6660192	1
393-11-55	stochastic search algorithms are general black-box optimizers .	due to their ease of use and their generality , they have recently also gained a lot of attention in operations research , machine learning and policy search .	1	0	1	-5.5142817	5.0808687	1
393-11-55	yet , these algorithms require a lot of evaluations of the objective , scale poorly with the problem dimension , are affected by highly noisy objective functions and may converge prematurely .	stochastic search algorithms are general black-box optimizers .	0	2	0	5.6804214	-5.0354977	0
393-11-55	to alleviate these problems , we introduce a new surrogate-based stochastic search approach .	stochastic search algorithms are general black-box optimizers .	0	3	0	5.3204813	-4.70751	0
393-11-55	stochastic search algorithms are general black-box optimizers .	we learn simple , quadratic surrogate models of the objective function .	1	0	4	-5.9848423	5.146924	1
393-11-55	stochastic search algorithms are general black-box optimizers .	as the quality of such a quadratic approximation is limited , we do not greedily exploit the learned models .	1	0	5	-6.0127125	5.171589	1
393-11-55	stochastic search algorithms are general black-box optimizers .	the algorithm can be misled by an inaccurate optimum introduced by the surrogate .	1	0	6	-5.88858	5.111312	1
393-11-55	instead , we use information theoretic constraints to bound the `distance ' between the new and old data distribution while maximizing the objective function .	stochastic search algorithms are general black-box optimizers .	0	7	0	5.582924	-4.9840775	0
393-11-55	additionally the new method is able to sustain the exploration of the search distribution to avoid premature convergence .	stochastic search algorithms are general black-box optimizers .	0	8	0	5.6428156	-5.0879	0
393-11-55	we compare our method with state of art black-box optimization methods on standard uni-modal and multi-modal optimization functions , on simulated planar robot tasks and a complex robot ball throwing task .	stochastic search algorithms are general black-box optimizers .	0	9	0	5.61637	-4.9793005	0
393-11-55	the proposed method considerably outperforms the existing approaches .	stochastic search algorithms are general black-box optimizers .	0	10	0	5.6161084	-5.0043464	0
393-11-55	due to their ease of use and their generality , they have recently also gained a lot of attention in operations research , machine learning and policy search .	yet , these algorithms require a lot of evaluations of the objective , scale poorly with the problem dimension , are affected by highly noisy objective functions and may converge prematurely .	1	1	2	-5.812102	5.295487	1
393-11-55	due to their ease of use and their generality , they have recently also gained a lot of attention in operations research , machine learning and policy search .	to alleviate these problems , we introduce a new surrogate-based stochastic search approach .	1	1	3	-5.988004	5.233778	1
393-11-55	we learn simple , quadratic surrogate models of the objective function .	due to their ease of use and their generality , they have recently also gained a lot of attention in operations research , machine learning and policy search .	0	4	1	5.703271	-5.1235485	0
393-11-55	as the quality of such a quadratic approximation is limited , we do not greedily exploit the learned models .	due to their ease of use and their generality , they have recently also gained a lot of attention in operations research , machine learning and policy search .	0	5	1	5.485055	-4.919614	0
393-11-55	the algorithm can be misled by an inaccurate optimum introduced by the surrogate .	due to their ease of use and their generality , they have recently also gained a lot of attention in operations research , machine learning and policy search .	0	6	1	5.6543097	-5.0934296	0
393-11-55	instead , we use information theoretic constraints to bound the `distance ' between the new and old data distribution while maximizing the objective function .	due to their ease of use and their generality , they have recently also gained a lot of attention in operations research , machine learning and policy search .	0	7	1	5.666569	-5.082435	0
393-11-55	due to their ease of use and their generality , they have recently also gained a lot of attention in operations research , machine learning and policy search .	additionally the new method is able to sustain the exploration of the search distribution to avoid premature convergence .	1	1	8	-5.903244	5.1756086	1
393-11-55	we compare our method with state of art black-box optimization methods on standard uni-modal and multi-modal optimization functions , on simulated planar robot tasks and a complex robot ball throwing task .	due to their ease of use and their generality , they have recently also gained a lot of attention in operations research , machine learning and policy search .	0	9	1	5.6582584	-5.085278	0
393-11-55	due to their ease of use and their generality , they have recently also gained a lot of attention in operations research , machine learning and policy search .	the proposed method considerably outperforms the existing approaches .	1	1	10	-5.9168215	5.229252	1
393-11-55	yet , these algorithms require a lot of evaluations of the objective , scale poorly with the problem dimension , are affected by highly noisy objective functions and may converge prematurely .	to alleviate these problems , we introduce a new surrogate-based stochastic search approach .	1	2	3	-3.5825982	3.4479766	1
393-11-55	we learn simple , quadratic surrogate models of the objective function .	yet , these algorithms require a lot of evaluations of the objective , scale poorly with the problem dimension , are affected by highly noisy objective functions and may converge prematurely .	0	4	2	4.488802	-4.0874157	0
393-11-55	as the quality of such a quadratic approximation is limited , we do not greedily exploit the learned models .	yet , these algorithms require a lot of evaluations of the objective , scale poorly with the problem dimension , are affected by highly noisy objective functions and may converge prematurely .	0	5	2	0.36237884	-0.14263743	0
393-11-55	the algorithm can be misled by an inaccurate optimum introduced by the surrogate .	yet , these algorithms require a lot of evaluations of the objective , scale poorly with the problem dimension , are affected by highly noisy objective functions and may converge prematurely .	0	6	2	3.868153	-3.582919	0
393-11-55	yet , these algorithms require a lot of evaluations of the objective , scale poorly with the problem dimension , are affected by highly noisy objective functions and may converge prematurely .	instead , we use information theoretic constraints to bound the `distance ' between the new and old data distribution while maximizing the objective function .	1	2	7	-3.625222	3.5299435	1
393-11-55	additionally the new method is able to sustain the exploration of the search distribution to avoid premature convergence .	yet , these algorithms require a lot of evaluations of the objective , scale poorly with the problem dimension , are affected by highly noisy objective functions and may converge prematurely .	0	8	2	5.3015304	-4.7847586	0
393-11-55	we compare our method with state of art black-box optimization methods on standard uni-modal and multi-modal optimization functions , on simulated planar robot tasks and a complex robot ball throwing task .	yet , these algorithms require a lot of evaluations of the objective , scale poorly with the problem dimension , are affected by highly noisy objective functions and may converge prematurely .	0	9	2	5.441743	-4.9361234	0
393-11-55	the proposed method considerably outperforms the existing approaches .	yet , these algorithms require a lot of evaluations of the objective , scale poorly with the problem dimension , are affected by highly noisy objective functions and may converge prematurely .	0	10	2	5.1509094	-4.625733	0
393-11-55	to alleviate these problems , we introduce a new surrogate-based stochastic search approach .	we learn simple , quadratic surrogate models of the objective function .	1	3	4	-5.281213	4.921031	1
393-11-55	as the quality of such a quadratic approximation is limited , we do not greedily exploit the learned models .	to alleviate these problems , we introduce a new surrogate-based stochastic search approach .	0	5	3	-1.8265688	1.9263252	1
393-11-55	the algorithm can be misled by an inaccurate optimum introduced by the surrogate .	to alleviate these problems , we introduce a new surrogate-based stochastic search approach .	0	6	3	3.1120808	-2.8463447	0
393-11-55	instead , we use information theoretic constraints to bound the `distance ' between the new and old data distribution while maximizing the objective function .	to alleviate these problems , we introduce a new surrogate-based stochastic search approach .	0	7	3	3.848411	-3.443408	0
393-11-55	additionally the new method is able to sustain the exploration of the search distribution to avoid premature convergence .	to alleviate these problems , we introduce a new surrogate-based stochastic search approach .	0	8	3	5.5636463	-4.9525423	0
393-11-55	we compare our method with state of art black-box optimization methods on standard uni-modal and multi-modal optimization functions , on simulated planar robot tasks and a complex robot ball throwing task .	to alleviate these problems , we introduce a new surrogate-based stochastic search approach .	0	9	3	5.629753	-5.005496	0
393-11-55	to alleviate these problems , we introduce a new surrogate-based stochastic search approach .	the proposed method considerably outperforms the existing approaches .	1	3	10	-6.001985	5.179796	1
393-11-55	we learn simple , quadratic surrogate models of the objective function .	as the quality of such a quadratic approximation is limited , we do not greedily exploit the learned models .	1	4	5	-2.4315376	2.4804904	1
393-11-55	the algorithm can be misled by an inaccurate optimum introduced by the surrogate .	we learn simple , quadratic surrogate models of the objective function .	0	6	4	1.9102726	-1.6863638	0
393-11-55	we learn simple , quadratic surrogate models of the objective function .	instead , we use information theoretic constraints to bound the `distance ' between the new and old data distribution while maximizing the objective function .	1	4	7	3.4518414	-3.156211	0
393-11-55	additionally the new method is able to sustain the exploration of the search distribution to avoid premature convergence .	we learn simple , quadratic surrogate models of the objective function .	0	8	4	5.0388145	-4.5609517	0
393-11-55	we learn simple , quadratic surrogate models of the objective function .	we compare our method with state of art black-box optimization methods on standard uni-modal and multi-modal optimization functions , on simulated planar robot tasks and a complex robot ball throwing task .	1	4	9	-5.8657517	5.1468077	1
393-11-55	we learn simple , quadratic surrogate models of the objective function .	the proposed method considerably outperforms the existing approaches .	1	4	10	-5.9224987	5.0673304	1
393-11-55	the algorithm can be misled by an inaccurate optimum introduced by the surrogate .	as the quality of such a quadratic approximation is limited , we do not greedily exploit the learned models .	0	6	5	1.9203993	-1.8141185	0
393-11-55	instead , we use information theoretic constraints to bound the `distance ' between the new and old data distribution while maximizing the objective function .	as the quality of such a quadratic approximation is limited , we do not greedily exploit the learned models .	0	7	5	0.20884195	-0.025313623	0
393-11-55	additionally the new method is able to sustain the exploration of the search distribution to avoid premature convergence .	as the quality of such a quadratic approximation is limited , we do not greedily exploit the learned models .	0	8	5	5.0492435	-4.4635534	0
393-11-55	as the quality of such a quadratic approximation is limited , we do not greedily exploit the learned models .	we compare our method with state of art black-box optimization methods on standard uni-modal and multi-modal optimization functions , on simulated planar robot tasks and a complex robot ball throwing task .	1	5	9	-5.9892583	5.1224747	1
393-11-55	the proposed method considerably outperforms the existing approaches .	as the quality of such a quadratic approximation is limited , we do not greedily exploit the learned models .	0	10	5	4.9148483	-4.3575115	0
393-11-55	instead , we use information theoretic constraints to bound the `distance ' between the new and old data distribution while maximizing the objective function .	the algorithm can be misled by an inaccurate optimum introduced by the surrogate .	0	7	6	-4.5745425	4.3001785	1
393-11-55	additionally the new method is able to sustain the exploration of the search distribution to avoid premature convergence .	the algorithm can be misled by an inaccurate optimum introduced by the surrogate .	0	8	6	4.1171417	-3.8070662	0
393-11-55	we compare our method with state of art black-box optimization methods on standard uni-modal and multi-modal optimization functions , on simulated planar robot tasks and a complex robot ball throwing task .	the algorithm can be misled by an inaccurate optimum introduced by the surrogate .	0	9	6	4.9877615	-4.4533987	0
393-11-55	the proposed method considerably outperforms the existing approaches .	the algorithm can be misled by an inaccurate optimum introduced by the surrogate .	0	10	6	4.2277617	-3.8903375	0
393-11-55	additionally the new method is able to sustain the exploration of the search distribution to avoid premature convergence .	instead , we use information theoretic constraints to bound the `distance ' between the new and old data distribution while maximizing the objective function .	0	8	7	5.2663126	-4.6824636	0
393-11-55	instead , we use information theoretic constraints to bound the `distance ' between the new and old data distribution while maximizing the objective function .	we compare our method with state of art black-box optimization methods on standard uni-modal and multi-modal optimization functions , on simulated planar robot tasks and a complex robot ball throwing task .	1	7	9	-5.978029	5.138783	1
393-11-55	the proposed method considerably outperforms the existing approaches .	instead , we use information theoretic constraints to bound the `distance ' between the new and old data distribution while maximizing the objective function .	0	10	7	4.994951	-4.513221	0
393-11-55	additionally the new method is able to sustain the exploration of the search distribution to avoid premature convergence .	we compare our method with state of art black-box optimization methods on standard uni-modal and multi-modal optimization functions , on simulated planar robot tasks and a complex robot ball throwing task .	1	8	9	-1.5866565	1.7490056	1
393-11-55	additionally the new method is able to sustain the exploration of the search distribution to avoid premature convergence .	the proposed method considerably outperforms the existing approaches .	1	8	10	-1.4137962	1.4483316	1
393-11-55	the proposed method considerably outperforms the existing approaches .	we compare our method with state of art black-box optimization methods on standard uni-modal and multi-modal optimization functions , on simulated planar robot tasks and a complex robot ball throwing task .	0	10	9	1.5591533	-1.4129903	0
394-4-6	we combine supervised learning with unsupervised learning in deep neural networks .	the proposed model is trained to simultaneously minimize the sum of supervised and unsupervised cost functions by backpropagation , avoiding the need for layer-wise pre-training .	1	0	1	-5.842206	5.1851454	1
394-4-6	we combine supervised learning with unsupervised learning in deep neural networks .	our work builds on top of the ladder network proposed by valpola which we extend by combining the model with supervision .	1	0	2	-5.0603456	4.6869497	1
394-4-6	we combine supervised learning with unsupervised learning in deep neural networks .	we show that the resulting model reaches state-of-the-art performance in semi-supervised mnist and cifar-10 classification in addition to permutationinvariant mnist classification with all labels .	1	0	3	-5.9622803	5.182063	1
394-4-6	the proposed model is trained to simultaneously minimize the sum of supervised and unsupervised cost functions by backpropagation , avoiding the need for layer-wise pre-training .	our work builds on top of the ladder network proposed by valpola which we extend by combining the model with supervision .	1	1	2	2.9083643	-2.5924616	0
394-4-6	we show that the resulting model reaches state-of-the-art performance in semi-supervised mnist and cifar-10 classification in addition to permutationinvariant mnist classification with all labels .	the proposed model is trained to simultaneously minimize the sum of supervised and unsupervised cost functions by backpropagation , avoiding the need for layer-wise pre-training .	0	3	1	4.682347	-4.202748	0
394-4-6	our work builds on top of the ladder network proposed by valpola which we extend by combining the model with supervision .	we show that the resulting model reaches state-of-the-art performance in semi-supervised mnist and cifar-10 classification in addition to permutationinvariant mnist classification with all labels .	1	2	3	-5.3886633	4.9024324	1
395-5-10	a key bottleneck in structured output prediction is the need for inference during training and testing , usually requiring some form of dynamic programming .	rather than using approximate inference or tailoring a specialized inference method for a particular structure -- standard responses to the scaling challenge -- we propose to embed prediction constraints directly into the learned representation .	1	0	1	-5.92438	5.256372	1
395-5-10	a key bottleneck in structured output prediction is the need for inference during training and testing , usually requiring some form of dynamic programming .	by eliminating the need for explicit inference a more scalable approach to structured output prediction can be achieved , particularly at test time .	1	0	2	-5.9604187	5.0650682	1
395-5-10	a key bottleneck in structured output prediction is the need for inference during training and testing , usually requiring some form of dynamic programming .	we demonstrate the idea for multi-label prediction under subsumption and mutual exclusion constraints , where a relationship to maximum margin structured output prediction can be established .	1	0	3	-5.9249434	5.2432556	1
395-5-10	experiments demonstrate that the benefits of structured output training can still be realized even after inference has been eliminated .	a key bottleneck in structured output prediction is the need for inference during training and testing , usually requiring some form of dynamic programming .	0	4	0	5.692564	-5.1236305	0
395-5-10	by eliminating the need for explicit inference a more scalable approach to structured output prediction can be achieved , particularly at test time .	rather than using approximate inference or tailoring a specialized inference method for a particular structure -- standard responses to the scaling challenge -- we propose to embed prediction constraints directly into the learned representation .	0	2	1	2.4419627	-2.2499456	0
395-5-10	rather than using approximate inference or tailoring a specialized inference method for a particular structure -- standard responses to the scaling challenge -- we propose to embed prediction constraints directly into the learned representation .	we demonstrate the idea for multi-label prediction under subsumption and mutual exclusion constraints , where a relationship to maximum margin structured output prediction can be established .	1	1	3	-0.7804973	1.0431728	1
395-5-10	experiments demonstrate that the benefits of structured output training can still be realized even after inference has been eliminated .	rather than using approximate inference or tailoring a specialized inference method for a particular structure -- standard responses to the scaling challenge -- we propose to embed prediction constraints directly into the learned representation .	0	4	1	5.422879	-4.8864145	0
395-5-10	we demonstrate the idea for multi-label prediction under subsumption and mutual exclusion constraints , where a relationship to maximum margin structured output prediction can be established .	by eliminating the need for explicit inference a more scalable approach to structured output prediction can be achieved , particularly at test time .	0	3	2	-3.6626036	3.590304	1
395-5-10	by eliminating the need for explicit inference a more scalable approach to structured output prediction can be achieved , particularly at test time .	experiments demonstrate that the benefits of structured output training can still be realized even after inference has been eliminated .	1	2	4	-5.5650573	5.001884	1
395-5-10	experiments demonstrate that the benefits of structured output training can still be realized even after inference has been eliminated .	we demonstrate the idea for multi-label prediction under subsumption and mutual exclusion constraints , where a relationship to maximum margin structured output prediction can be established .	0	4	3	5.6195	-5.0712824	0
396-6-15	our method uses copulas to augment the families of distributions used in mean-field and structured approximations .	we develop a general variational inference method that preserves dependency among the latent variables .	0	1	0	2.5358114	-2.2325659	0
396-6-15	we develop a general variational inference method that preserves dependency among the latent variables .	copulas model the dependency that is not captured by the original variational distribution , and thus the augmented variational family guarantees better approximations to the posterior .	1	0	2	-1.6793038	1.7432144	1
396-6-15	with stochastic optimization , inference on the augmented distribution is scalable .	we develop a general variational inference method that preserves dependency among the latent variables .	0	3	0	-3.4110255	3.4110918	1
396-6-15	furthermore , our strategy is generic : it can be applied to any inference procedure that currently uses the mean-field or structured approach .	we develop a general variational inference method that preserves dependency among the latent variables .	0	4	0	4.684952	-4.1566386	0
396-6-15	copula variational inference has many advantages : it reduces bias ; it is less sensitive to local optima ; it is less sensitive to hyperparameters ; and it helps characterize and interpret the dependency among the latent variables .	we develop a general variational inference method that preserves dependency among the latent variables .	0	5	0	-2.509253	2.52762	1
396-6-15	our method uses copulas to augment the families of distributions used in mean-field and structured approximations .	copulas model the dependency that is not captured by the original variational distribution , and thus the augmented variational family guarantees better approximations to the posterior .	1	1	2	-5.0295854	4.6529503	1
396-6-15	with stochastic optimization , inference on the augmented distribution is scalable .	our method uses copulas to augment the families of distributions used in mean-field and structured approximations .	0	3	1	2.691208	-2.4738097	0
396-6-15	furthermore , our strategy is generic : it can be applied to any inference procedure that currently uses the mean-field or structured approach .	our method uses copulas to augment the families of distributions used in mean-field and structured approximations .	0	4	1	5.115442	-4.4801292	0
396-6-15	copula variational inference has many advantages : it reduces bias ; it is less sensitive to local optima ; it is less sensitive to hyperparameters ; and it helps characterize and interpret the dependency among the latent variables .	our method uses copulas to augment the families of distributions used in mean-field and structured approximations .	0	5	1	3.1830964	-2.9303625	0
396-6-15	copulas model the dependency that is not captured by the original variational distribution , and thus the augmented variational family guarantees better approximations to the posterior .	with stochastic optimization , inference on the augmented distribution is scalable .	1	2	3	-2.6377397	2.556059	1
396-6-15	copulas model the dependency that is not captured by the original variational distribution , and thus the augmented variational family guarantees better approximations to the posterior .	furthermore , our strategy is generic : it can be applied to any inference procedure that currently uses the mean-field or structured approach .	1	2	4	-5.9433813	5.0768147	1
396-6-15	copulas model the dependency that is not captured by the original variational distribution , and thus the augmented variational family guarantees better approximations to the posterior .	copula variational inference has many advantages : it reduces bias ; it is less sensitive to local optima ; it is less sensitive to hyperparameters ; and it helps characterize and interpret the dependency among the latent variables .	1	2	5	-1.641019	1.7960333	1
396-6-15	with stochastic optimization , inference on the augmented distribution is scalable .	furthermore , our strategy is generic : it can be applied to any inference procedure that currently uses the mean-field or structured approach .	1	3	4	-5.9147725	5.0793076	1
396-6-15	copula variational inference has many advantages : it reduces bias ; it is less sensitive to local optima ; it is less sensitive to hyperparameters ; and it helps characterize and interpret the dependency among the latent variables .	with stochastic optimization , inference on the augmented distribution is scalable .	0	5	3	2.3894017	-2.234003	0
396-6-15	copula variational inference has many advantages : it reduces bias ; it is less sensitive to local optima ; it is less sensitive to hyperparameters ; and it helps characterize and interpret the dependency among the latent variables .	furthermore , our strategy is generic : it can be applied to any inference procedure that currently uses the mean-field or structured approach .	0	5	4	-5.960144	5.2201176	1
397-10-45	an important computation for reconstruction is the detection of neuronal boundaries .	efforts to automate the reconstruction of neural circuits from 3d electron microscopic ( em ) brain images are critical for the field of connectomics .	0	1	0	4.1557617	-3.424717	0
397-10-45	efforts to automate the reconstruction of neural circuits from 3d electron microscopic ( em ) brain images are critical for the field of connectomics .	images acquired by serial section em , a leading 3d em technique , are highly anisotropic , with inferior quality along the third dimension .	1	0	2	-5.8291893	5.15777	1
397-10-45	efforts to automate the reconstruction of neural circuits from 3d electron microscopic ( em ) brain images are critical for the field of connectomics .	for such images , the 2d maxpooling convolutional network has set the standard for performance at boundary detection .	1	0	3	-5.845131	5.210511	1
397-10-45	efforts to automate the reconstruction of neural circuits from 3d electron microscopic ( em ) brain images are critical for the field of connectomics .	here we achieve a substantial gain in accuracy through three innovations .	1	0	4	-5.994971	5.1400094	1
397-10-45	efforts to automate the reconstruction of neural circuits from 3d electron microscopic ( em ) brain images are critical for the field of connectomics .	following the trend towards deeper networks for object recognition , we use a much deeper network than previously employed for boundary detection .	1	0	5	-5.9237304	5.176532	1
397-10-45	efforts to automate the reconstruction of neural circuits from 3d electron microscopic ( em ) brain images are critical for the field of connectomics .	second , we incorporate 3d as well as 2d filters , to enable computations that use 3d context .	1	0	6	-5.906638	5.200068	1
397-10-45	efforts to automate the reconstruction of neural circuits from 3d electron microscopic ( em ) brain images are critical for the field of connectomics .	finally , we adopt a recursively trained architecture in which a first network generates a preliminary boundary map that is provided as input along with the original image to a second network that generates a final boundary map .	1	0	7	-5.8716483	5.1602516	1
397-10-45	backpropagation training is accelerated by znn , a new implementation of 3d convolutional networks that uses multicore cpu parallelism for speed .	efforts to automate the reconstruction of neural circuits from 3d electron microscopic ( em ) brain images are critical for the field of connectomics .	0	8	0	5.5960236	-5.016663	0
397-10-45	our hybrid 2d3d architecture could be more generally applicable to other types of anisotropic 3d images , including video , and our recursive framework for any image labeling problem .	efforts to automate the reconstruction of neural circuits from 3d electron microscopic ( em ) brain images are critical for the field of connectomics .	0	9	0	5.5611362	-5.002232	0
397-10-45	an important computation for reconstruction is the detection of neuronal boundaries .	images acquired by serial section em , a leading 3d em technique , are highly anisotropic , with inferior quality along the third dimension .	1	1	2	-5.840868	5.2030087	1
397-10-45	for such images , the 2d maxpooling convolutional network has set the standard for performance at boundary detection .	an important computation for reconstruction is the detection of neuronal boundaries .	0	3	1	5.3792114	-4.902938	0
397-10-45	an important computation for reconstruction is the detection of neuronal boundaries .	here we achieve a substantial gain in accuracy through three innovations .	1	1	4	-5.961915	5.062789	1
397-10-45	following the trend towards deeper networks for object recognition , we use a much deeper network than previously employed for boundary detection .	an important computation for reconstruction is the detection of neuronal boundaries .	0	5	1	5.5339837	-4.923794	0
397-10-45	second , we incorporate 3d as well as 2d filters , to enable computations that use 3d context .	an important computation for reconstruction is the detection of neuronal boundaries .	0	6	1	5.512427	-4.888638	0
397-10-45	an important computation for reconstruction is the detection of neuronal boundaries .	finally , we adopt a recursively trained architecture in which a first network generates a preliminary boundary map that is provided as input along with the original image to a second network that generates a final boundary map .	1	1	7	-5.9391675	5.118472	1
397-10-45	backpropagation training is accelerated by znn , a new implementation of 3d convolutional networks that uses multicore cpu parallelism for speed .	an important computation for reconstruction is the detection of neuronal boundaries .	0	8	1	5.5032883	-4.951321	0
397-10-45	an important computation for reconstruction is the detection of neuronal boundaries .	our hybrid 2d3d architecture could be more generally applicable to other types of anisotropic 3d images , including video , and our recursive framework for any image labeling problem .	1	1	9	-5.9553127	5.0524025	1
397-10-45	for such images , the 2d maxpooling convolutional network has set the standard for performance at boundary detection .	images acquired by serial section em , a leading 3d em technique , are highly anisotropic , with inferior quality along the third dimension .	0	3	2	-3.7213702	3.6188211	1
397-10-45	here we achieve a substantial gain in accuracy through three innovations .	images acquired by serial section em , a leading 3d em technique , are highly anisotropic , with inferior quality along the third dimension .	0	4	2	4.0923247	-3.706862	0
397-10-45	images acquired by serial section em , a leading 3d em technique , are highly anisotropic , with inferior quality along the third dimension .	following the trend towards deeper networks for object recognition , we use a much deeper network than previously employed for boundary detection .	1	2	5	-5.4618073	5.0092516	1
397-10-45	second , we incorporate 3d as well as 2d filters , to enable computations that use 3d context .	images acquired by serial section em , a leading 3d em technique , are highly anisotropic , with inferior quality along the third dimension .	0	6	2	4.656129	-4.1844077	0
397-10-45	finally , we adopt a recursively trained architecture in which a first network generates a preliminary boundary map that is provided as input along with the original image to a second network that generates a final boundary map .	images acquired by serial section em , a leading 3d em technique , are highly anisotropic , with inferior quality along the third dimension .	0	7	2	5.261554	-4.6596775	0
397-10-45	backpropagation training is accelerated by znn , a new implementation of 3d convolutional networks that uses multicore cpu parallelism for speed .	images acquired by serial section em , a leading 3d em technique , are highly anisotropic , with inferior quality along the third dimension .	0	8	2	4.86844	-4.349041	0
397-10-45	images acquired by serial section em , a leading 3d em technique , are highly anisotropic , with inferior quality along the third dimension .	our hybrid 2d3d architecture could be more generally applicable to other types of anisotropic 3d images , including video , and our recursive framework for any image labeling problem .	1	2	9	-5.7844534	4.854208	1
397-10-45	for such images , the 2d maxpooling convolutional network has set the standard for performance at boundary detection .	here we achieve a substantial gain in accuracy through three innovations .	1	3	4	-5.9220657	5.032078	1
397-10-45	for such images , the 2d maxpooling convolutional network has set the standard for performance at boundary detection .	following the trend towards deeper networks for object recognition , we use a much deeper network than previously employed for boundary detection .	1	3	5	-5.958119	5.2526608	1
397-10-45	second , we incorporate 3d as well as 2d filters , to enable computations that use 3d context .	for such images , the 2d maxpooling convolutional network has set the standard for performance at boundary detection .	0	6	3	5.461159	-4.8261	0
397-10-45	finally , we adopt a recursively trained architecture in which a first network generates a preliminary boundary map that is provided as input along with the original image to a second network that generates a final boundary map .	for such images , the 2d maxpooling convolutional network has set the standard for performance at boundary detection .	0	7	3	5.5493884	-4.926962	0
397-10-45	for such images , the 2d maxpooling convolutional network has set the standard for performance at boundary detection .	backpropagation training is accelerated by znn , a new implementation of 3d convolutional networks that uses multicore cpu parallelism for speed .	1	3	8	-5.92317	5.212943	1
397-10-45	our hybrid 2d3d architecture could be more generally applicable to other types of anisotropic 3d images , including video , and our recursive framework for any image labeling problem .	for such images , the 2d maxpooling convolutional network has set the standard for performance at boundary detection .	0	9	3	5.295964	-4.6479206	0
397-10-45	following the trend towards deeper networks for object recognition , we use a much deeper network than previously employed for boundary detection .	here we achieve a substantial gain in accuracy through three innovations .	0	5	4	2.3265328	-2.2527144	0
397-10-45	second , we incorporate 3d as well as 2d filters , to enable computations that use 3d context .	here we achieve a substantial gain in accuracy through three innovations .	0	6	4	4.489281	-4.1089334	0
397-10-45	finally , we adopt a recursively trained architecture in which a first network generates a preliminary boundary map that is provided as input along with the original image to a second network that generates a final boundary map .	here we achieve a substantial gain in accuracy through three innovations .	0	7	4	5.3217883	-4.750602	0
397-10-45	here we achieve a substantial gain in accuracy through three innovations .	backpropagation training is accelerated by znn , a new implementation of 3d convolutional networks that uses multicore cpu parallelism for speed .	1	4	8	-5.158003	4.78155	1
397-10-45	our hybrid 2d3d architecture could be more generally applicable to other types of anisotropic 3d images , including video , and our recursive framework for any image labeling problem .	here we achieve a substantial gain in accuracy through three innovations .	0	9	4	4.788717	-4.3274255	0
397-10-45	following the trend towards deeper networks for object recognition , we use a much deeper network than previously employed for boundary detection .	second , we incorporate 3d as well as 2d filters , to enable computations that use 3d context .	1	5	6	-1.9066348	2.0184853	1
397-10-45	finally , we adopt a recursively trained architecture in which a first network generates a preliminary boundary map that is provided as input along with the original image to a second network that generates a final boundary map .	following the trend towards deeper networks for object recognition , we use a much deeper network than previously employed for boundary detection .	0	7	5	5.0160246	-4.440062	0
397-10-45	backpropagation training is accelerated by znn , a new implementation of 3d convolutional networks that uses multicore cpu parallelism for speed .	following the trend towards deeper networks for object recognition , we use a much deeper network than previously employed for boundary detection .	0	8	5	4.2211914	-3.89146	0
397-10-45	following the trend towards deeper networks for object recognition , we use a much deeper network than previously employed for boundary detection .	our hybrid 2d3d architecture could be more generally applicable to other types of anisotropic 3d images , including video , and our recursive framework for any image labeling problem .	1	5	9	-5.5267363	4.889733	1
397-10-45	second , we incorporate 3d as well as 2d filters , to enable computations that use 3d context .	finally , we adopt a recursively trained architecture in which a first network generates a preliminary boundary map that is provided as input along with the original image to a second network that generates a final boundary map .	1	6	7	-5.470348	5.0082016	1
397-10-45	backpropagation training is accelerated by znn , a new implementation of 3d convolutional networks that uses multicore cpu parallelism for speed .	second , we incorporate 3d as well as 2d filters , to enable computations that use 3d context .	0	8	6	4.4198236	-4.03511	0
397-10-45	second , we incorporate 3d as well as 2d filters , to enable computations that use 3d context .	our hybrid 2d3d architecture could be more generally applicable to other types of anisotropic 3d images , including video , and our recursive framework for any image labeling problem .	1	6	9	-5.5203586	4.9819093	1
397-10-45	backpropagation training is accelerated by znn , a new implementation of 3d convolutional networks that uses multicore cpu parallelism for speed .	finally , we adopt a recursively trained architecture in which a first network generates a preliminary boundary map that is provided as input along with the original image to a second network that generates a final boundary map .	0	8	7	-2.4677796	2.5393972	1
397-10-45	finally , we adopt a recursively trained architecture in which a first network generates a preliminary boundary map that is provided as input along with the original image to a second network that generates a final boundary map .	our hybrid 2d3d architecture could be more generally applicable to other types of anisotropic 3d images , including video , and our recursive framework for any image labeling problem .	1	7	9	-4.8607907	4.4471583	1
397-10-45	backpropagation training is accelerated by znn , a new implementation of 3d convolutional networks that uses multicore cpu parallelism for speed .	our hybrid 2d3d architecture could be more generally applicable to other types of anisotropic 3d images , including video , and our recursive framework for any image labeling problem .	1	8	9	-4.911434	4.4828796	1
398-5-10	in this paper , we consider the more general setting of regularized empirical risk minimization ( erm ) when data can not fit into memory .	in past few years , several techniques have been proposed for training of linear support vector machine ( svm ) in limited-memory setting , where a dual blockcoordinate descent ( dual-bcd ) method was used to balance cost spent on i/o and computation .	0	1	0	5.358001	-4.762932	0
398-5-10	in past few years , several techniques have been proposed for training of linear support vector machine ( svm ) in limited-memory setting , where a dual blockcoordinate descent ( dual-bcd ) method was used to balance cost spent on i/o and computation .	in particular , we generalize the existing block minimization framework based on strong duality and augmented lagrangian technique to achieve global convergence for general convex erm .	1	0	2	-5.9444714	5.2053614	1
398-5-10	[CLS] in past few years, several techniques have been proposed for training of linear support vector machine ( svm ) in limited - memory setting, where a dual blockcoordinate descent ( dual - bcd ) method was used to balance cost spent on i / o and computation	the block minimization framework is flexible in the sense that , given a solver working under sufficient memory , one can integrate it with the framework to obtain a solver globally convergent under limited-memory condition .	1	0	3	-5.5357842	5.103911	1
398-5-10	[CLS] in past few years, several techniques have been proposed for training of linear support vector machine ( svm ) in limited - memory setting, where a dual blockcoordinate descent ( dual - bcd ) method was used to balance cost spent on	[CLS] we conduct experiments on l1 - regularized classification and regression problems to corroborate our convergence theory and compare the proposed framework to algorithms adopted from online and distributed settings, which shows superiority of the proposed approach on data of size ten times larger	1	0	4	-5.91741	5.211782	1
398-5-10	in particular , we generalize the existing block minimization framework based on strong duality and augmented lagrangian technique to achieve global convergence for general convex erm .	in this paper , we consider the more general setting of regularized empirical risk minimization ( erm ) when data can not fit into memory .	0	2	1	5.604621	-4.901001	0
398-5-10	in this paper , we consider the more general setting of regularized empirical risk minimization ( erm ) when data can not fit into memory .	the block minimization framework is flexible in the sense that , given a solver working under sufficient memory , one can integrate it with the framework to obtain a solver globally convergent under limited-memory condition .	1	1	3	-5.338819	4.899967	1
398-5-10	in this paper , we consider the more general setting of regularized empirical risk minimization ( erm ) when data can not fit into memory .	we conduct experiments on l1-regularized classification and regression problems to corroborate our convergence theory and compare the proposed framework to algorithms adopted from online and distributed settings , which shows superiority of the proposed approach on data of size ten times larger than the memory capacity .	1	1	4	-5.993158	5.186694	1
398-5-10	the block minimization framework is flexible in the sense that , given a solver working under sufficient memory , one can integrate it with the framework to obtain a solver globally convergent under limited-memory condition .	in particular , we generalize the existing block minimization framework based on strong duality and augmented lagrangian technique to achieve global convergence for general convex erm .	0	3	2	-3.2225697	3.139266	1
398-5-10	we conduct experiments on l1-regularized classification and regression problems to corroborate our convergence theory and compare the proposed framework to algorithms adopted from online and distributed settings , which shows superiority of the proposed approach on data of size ten times larger than the memory capacity .	in particular , we generalize the existing block minimization framework based on strong duality and augmented lagrangian technique to achieve global convergence for general convex erm .	0	4	2	5.0198355	-4.4238505	0
398-5-10	the block minimization framework is flexible in the sense that , given a solver working under sufficient memory , one can integrate it with the framework to obtain a solver globally convergent under limited-memory condition .	we conduct experiments on l1-regularized classification and regression problems to corroborate our convergence theory and compare the proposed framework to algorithms adopted from online and distributed settings , which shows superiority of the proposed approach on data of size ten times larger than the memory capacity .	1	3	4	-5.81438	5.1895657	1
399-7-21	this fundamental question has received tremendous attention in statistics , focusing primarily on asymptotic analysis , as well as in information theory and theoretical computer science , where the emphasis has been on small sample size and computational complexity .	given samples from an unknown discrete distribution p , is it possible to distinguish whether p belongs to some class of distributions c versus p being far from every distribution in c ?	0	1	0	4.676402	-4.0817995	0
399-7-21	given samples from an unknown discrete distribution p , is it possible to distinguish whether p belongs to some class of distributions c versus p being far from every distribution in c ?	nevertheless , even for basic properties of discrete distributions such as monotonicity , independence , logconcavity , unimodality , and monotone-hazard rate , the optimal sample complexity is unknown .	1	0	2	-5.856389	5.226688	1
399-7-21	we provide a general approach via which we obtain sample-optimal and computationally efficient testers for all these distribution families .	given samples from an unknown discrete distribution p , is it possible to distinguish whether p belongs to some class of distributions c versus p being far from every distribution in c ?	0	3	0	5.70476	-5.045067	0
399-7-21	given samples from an unknown discrete distribution p , is it possible to distinguish whether p belongs to some class of distributions c versus p being far from every distribution in c ?	at the core of our approach is an algorithm which solves the following problem : given samples from an unknown distribution p , and a known distribution q , are p and q close in 2 -distance , or far in total variation distance ?	1	0	4	-5.812941	5.239526	1
399-7-21	given samples from an unknown discrete distribution p , is it possible to distinguish whether p belongs to some class of distributions c versus p being far from every distribution in c ?	the optimality of our testers is established by providing matching lower bounds , up to constant factors .	1	0	5	-6.009352	5.1809487	1
399-7-21	finally , a necessary building block for our testers and an important byproduct of our work are the first known computationally efficient proper learners for discrete log-concave , monotone hazard rate distributions .	given samples from an unknown discrete distribution p , is it possible to distinguish whether p belongs to some class of distributions c versus p being far from every distribution in c ?	0	6	0	5.553187	-4.9621687	0
399-7-21	this fundamental question has received tremendous attention in statistics , focusing primarily on asymptotic analysis , as well as in information theory and theoretical computer science , where the emphasis has been on small sample size and computational complexity .	nevertheless , even for basic properties of discrete distributions such as monotonicity , independence , logconcavity , unimodality , and monotone-hazard rate , the optimal sample complexity is unknown .	1	1	2	-5.4910407	5.059965	1
399-7-21	we provide a general approach via which we obtain sample-optimal and computationally efficient testers for all these distribution families .	this fundamental question has received tremendous attention in statistics , focusing primarily on asymptotic analysis , as well as in information theory and theoretical computer science , where the emphasis has been on small sample size and computational complexity .	0	3	1	5.5090218	-4.868538	0
399-7-21	at the core of our approach is an algorithm which solves the following problem : given samples from an unknown distribution p , and a known distribution q , are p and q close in 2 -distance , or far in total variation distance ?	this fundamental question has received tremendous attention in statistics , focusing primarily on asymptotic analysis , as well as in information theory and theoretical computer science , where the emphasis has been on small sample size and computational complexity .	0	4	1	4.924494	-4.4119444	0
399-7-21	this fundamental question has received tremendous attention in statistics , focusing primarily on asymptotic analysis , as well as in information theory and theoretical computer science , where the emphasis has been on small sample size and computational complexity .	the optimality of our testers is established by providing matching lower bounds , up to constant factors .	1	1	5	-5.9761634	5.254773	1
399-7-21	this fundamental question has received tremendous attention in statistics , focusing primarily on asymptotic analysis , as well as in information theory and theoretical computer science , where the emphasis has been on small sample size and computational complexity .	finally , a necessary building block for our testers and an important byproduct of our work are the first known computationally efficient proper learners for discrete log-concave , monotone hazard rate distributions .	1	1	6	-5.9603543	5.1730633	1
399-7-21	we provide a general approach via which we obtain sample-optimal and computationally efficient testers for all these distribution families .	nevertheless , even for basic properties of discrete distributions such as monotonicity , independence , logconcavity , unimodality , and monotone-hazard rate , the optimal sample complexity is unknown .	0	3	2	4.612526	-4.067835	0
399-7-21	at the core of our approach is an algorithm which solves the following problem : given samples from an unknown distribution p , and a known distribution q , are p and q close in 2 -distance , or far in total variation distance ?	nevertheless , even for basic properties of discrete distributions such as monotonicity , independence , logconcavity , unimodality , and monotone-hazard rate , the optimal sample complexity is unknown .	0	4	2	-1.2300913	1.5011672	1
399-7-21	nevertheless , even for basic properties of discrete distributions such as monotonicity , independence , logconcavity , unimodality , and monotone-hazard rate , the optimal sample complexity is unknown .	the optimality of our testers is established by providing matching lower bounds , up to constant factors .	1	2	5	-5.1700754	4.8460765	1
399-7-21	nevertheless , even for basic properties of discrete distributions such as monotonicity , independence , logconcavity , unimodality , and monotone-hazard rate , the optimal sample complexity is unknown .	finally , a necessary building block for our testers and an important byproduct of our work are the first known computationally efficient proper learners for discrete log-concave , monotone hazard rate distributions .	1	2	6	-5.784378	4.8750763	1
399-7-21	we provide a general approach via which we obtain sample-optimal and computationally efficient testers for all these distribution families .	at the core of our approach is an algorithm which solves the following problem : given samples from an unknown distribution p , and a known distribution q , are p and q close in 2 -distance , or far in total variation distance ?	1	3	4	3.4374928	-3.2307394	0
399-7-21	we provide a general approach via which we obtain sample-optimal and computationally efficient testers for all these distribution families .	the optimality of our testers is established by providing matching lower bounds , up to constant factors .	1	3	5	-2.8953755	2.8396845	1
399-7-21	we provide a general approach via which we obtain sample-optimal and computationally efficient testers for all these distribution families .	finally , a necessary building block for our testers and an important byproduct of our work are the first known computationally efficient proper learners for discrete log-concave , monotone hazard rate distributions .	1	3	6	-5.9118047	5.0906878	1
399-7-21	at the core of our approach is an algorithm which solves the following problem : given samples from an unknown distribution p , and a known distribution q , are p and q close in 2 -distance , or far in total variation distance ?	the optimality of our testers is established by providing matching lower bounds , up to constant factors .	1	4	5	-5.4625797	5.0106144	1
399-7-21	finally , a necessary building block for our testers and an important byproduct of our work are the first known computationally efficient proper learners for discrete log-concave , monotone hazard rate distributions .	at the core of our approach is an algorithm which solves the following problem : given samples from an unknown distribution p , and a known distribution q , are p and q close in 2 -distance , or far in total variation distance ?	0	6	4	5.2704053	-4.6578927	0
399-7-21	finally , a necessary building block for our testers and an important byproduct of our work are the first known computationally efficient proper learners for discrete log-concave , monotone hazard rate distributions .	the optimality of our testers is established by providing matching lower bounds , up to constant factors .	0	6	5	5.032711	-4.4645863	0
400-7-21	the continuous-time hidden markov model ( ct-hmm ) is an attractive approach to modeling disease progression due to its ability to describe noisy observations arriving irregularly in time .	however , the lack of an efficient parameter learning algorithm for ct-hmm restricts its use to very small models or requires unrealistic constraints on the state transitions .	1	0	1	-5.8841553	5.176761	1
400-7-21	in this paper , we present the first complete characterization of efficient em-based learning methods for ct-hmm models .	the continuous-time hidden markov model ( ct-hmm ) is an attractive approach to modeling disease progression due to its ability to describe noisy observations arriving irregularly in time .	0	2	0	5.727916	-5.080779	0
400-7-21	we demonstrate that the learning problem consists of two challenges : the estimation of posterior state probabilities and the computation of end-state conditioned statistics .	the continuous-time hidden markov model ( ct-hmm ) is an attractive approach to modeling disease progression due to its ability to describe noisy observations arriving irregularly in time .	0	3	0	5.3856883	-4.8396864	0
400-7-21	the continuous-time hidden markov model ( ct-hmm ) is an attractive approach to modeling disease progression due to its ability to describe noisy observations arriving irregularly in time .	we solve the first challenge by reformulating the estimation problem in terms of an equivalent discrete time-inhomogeneous hidden markov model .	1	0	4	-5.938924	5.260453	1
400-7-21	the continuous-time hidden markov model ( ct-hmm ) is an attractive approach to modeling disease progression due to its ability to describe noisy observations arriving irregularly in time .	the second challenge is addressed by adapting three approaches from the continuous time markov chain literature to the ct-hmm domain .	1	0	5	-5.988636	5.1917048	1
400-7-21	we demonstrate the use of ct-hmms with more than 100 states to visualize and predict disease progression using a glaucoma dataset and an alzheimer 's disease dataset .	the continuous-time hidden markov model ( ct-hmm ) is an attractive approach to modeling disease progression due to its ability to describe noisy observations arriving irregularly in time .	0	6	0	5.6345787	-5.01208	0
400-7-21	however , the lack of an efficient parameter learning algorithm for ct-hmm restricts its use to very small models or requires unrealistic constraints on the state transitions .	in this paper , we present the first complete characterization of efficient em-based learning methods for ct-hmm models .	1	1	2	-5.9028006	5.2115593	1
400-7-21	we demonstrate that the learning problem consists of two challenges : the estimation of posterior state probabilities and the computation of end-state conditioned statistics .	however , the lack of an efficient parameter learning algorithm for ct-hmm restricts its use to very small models or requires unrealistic constraints on the state transitions .	0	3	1	3.9077866	-3.6103334	0
400-7-21	we solve the first challenge by reformulating the estimation problem in terms of an equivalent discrete time-inhomogeneous hidden markov model .	however , the lack of an efficient parameter learning algorithm for ct-hmm restricts its use to very small models or requires unrealistic constraints on the state transitions .	0	4	1	4.50403	-4.0665746	0
400-7-21	the second challenge is addressed by adapting three approaches from the continuous time markov chain literature to the ct-hmm domain .	however , the lack of an efficient parameter learning algorithm for ct-hmm restricts its use to very small models or requires unrealistic constraints on the state transitions .	0	5	1	3.4069066	-3.2048745	0
400-7-21	we demonstrate the use of ct-hmms with more than 100 states to visualize and predict disease progression using a glaucoma dataset and an alzheimer 's disease dataset .	however , the lack of an efficient parameter learning algorithm for ct-hmm restricts its use to very small models or requires unrealistic constraints on the state transitions .	0	6	1	5.484301	-4.8545465	0
400-7-21	in this paper , we present the first complete characterization of efficient em-based learning methods for ct-hmm models .	we demonstrate that the learning problem consists of two challenges : the estimation of posterior state probabilities and the computation of end-state conditioned statistics .	1	2	3	-5.428547	4.9547386	1
400-7-21	in this paper , we present the first complete characterization of efficient em-based learning methods for ct-hmm models .	we solve the first challenge by reformulating the estimation problem in terms of an equivalent discrete time-inhomogeneous hidden markov model .	1	2	4	-5.142605	4.7215486	1
400-7-21	in this paper , we present the first complete characterization of efficient em-based learning methods for ct-hmm models .	the second challenge is addressed by adapting three approaches from the continuous time markov chain literature to the ct-hmm domain .	1	2	5	-3.2358994	3.1953409	1
400-7-21	in this paper , we present the first complete characterization of efficient em-based learning methods for ct-hmm models .	we demonstrate the use of ct-hmms with more than 100 states to visualize and predict disease progression using a glaucoma dataset and an alzheimer 's disease dataset .	1	2	6	-5.780517	5.2058697	1
400-7-21	we solve the first challenge by reformulating the estimation problem in terms of an equivalent discrete time-inhomogeneous hidden markov model .	we demonstrate that the learning problem consists of two challenges : the estimation of posterior state probabilities and the computation of end-state conditioned statistics .	0	4	3	3.9073396	-3.6272411	0
400-7-21	we demonstrate that the learning problem consists of two challenges : the estimation of posterior state probabilities and the computation of end-state conditioned statistics .	the second challenge is addressed by adapting three approaches from the continuous time markov chain literature to the ct-hmm domain .	1	3	5	-5.9529867	5.21498	1
400-7-21	we demonstrate that the learning problem consists of two challenges : the estimation of posterior state probabilities and the computation of end-state conditioned statistics .	we demonstrate the use of ct-hmms with more than 100 states to visualize and predict disease progression using a glaucoma dataset and an alzheimer 's disease dataset .	1	3	6	-4.856512	4.5394373	1
400-7-21	we solve the first challenge by reformulating the estimation problem in terms of an equivalent discrete time-inhomogeneous hidden markov model .	the second challenge is addressed by adapting three approaches from the continuous time markov chain literature to the ct-hmm domain .	1	4	5	-4.9206266	4.6282315	1
400-7-21	we solve the first challenge by reformulating the estimation problem in terms of an equivalent discrete time-inhomogeneous hidden markov model .	we demonstrate the use of ct-hmms with more than 100 states to visualize and predict disease progression using a glaucoma dataset and an alzheimer 's disease dataset .	1	4	6	-5.6211786	5.071377	1
400-7-21	the second challenge is addressed by adapting three approaches from the continuous time markov chain literature to the ct-hmm domain .	we demonstrate the use of ct-hmms with more than 100 states to visualize and predict disease progression using a glaucoma dataset and an alzheimer 's disease dataset .	1	5	6	-5.783481	5.0973134	1
401-7-21	we propose an original particle-based implementation of the loopy belief propagation ( lpb ) algorithm for pairwise markov random fields ( mrf ) on a continuous state space .	the algorithm constructs adaptively efficient proposal distributions approximating the local beliefs at each note of the mrf .	1	0	1	-5.839704	5.1752033	1
401-7-21	we propose an original particle-based implementation of the loopy belief propagation ( lpb ) algorithm for pairwise markov random fields ( mrf ) on a continuous state space .	this is achieved by considering proposal distributions in the exponential family whose parameters are updated iterately in an expectation propagation ( ep ) framework .	1	0	2	-5.821625	5.1519184	1
401-7-21	we propose an original particle-based implementation of the loopy belief propagation ( lpb ) algorithm for pairwise markov random fields ( mrf ) on a continuous state space .	the proposed particle scheme provides consistent estimation of the lbp marginals as the number of particles increases .	1	0	3	-5.9331646	5.1905775	1
401-7-21	we demonstrate that it provides more accurate results than the particle belief propagation ( pbp ) algorithm of at a fraction of the computational cost and is additionally more robust empirically .	we propose an original particle-based implementation of the loopy belief propagation ( lpb ) algorithm for pairwise markov random fields ( mrf ) on a continuous state space .	0	4	0	5.5413094	-4.8436284	0
401-7-21	the computational complexity of our algorithm at each iteration is quadratic in the number of particles .	we propose an original particle-based implementation of the loopy belief propagation ( lpb ) algorithm for pairwise markov random fields ( mrf ) on a continuous state space .	0	5	0	5.6012535	-4.953357	0
401-7-21	we propose an original particle-based implementation of the loopy belief propagation ( lpb ) algorithm for pairwise markov random fields ( mrf ) on a continuous state space .	we also propose an accelerated implementation with sub-quadratic computational complexity which still provides consistent estimates of the loopy bp marginal distributions and performs almost as well as the original procedure .	1	0	6	-6.0077963	5.1985826	1
401-7-21	this is achieved by considering proposal distributions in the exponential family whose parameters are updated iterately in an expectation propagation ( ep ) framework .	the algorithm constructs adaptively efficient proposal distributions approximating the local beliefs at each note of the mrf .	0	2	1	-5.0973377	4.7211657	1
401-7-21	the algorithm constructs adaptively efficient proposal distributions approximating the local beliefs at each note of the mrf .	the proposed particle scheme provides consistent estimation of the lbp marginals as the number of particles increases .	1	1	3	-4.152585	3.8587708	1
401-7-21	we demonstrate that it provides more accurate results than the particle belief propagation ( pbp ) algorithm of at a fraction of the computational cost and is additionally more robust empirically .	the algorithm constructs adaptively efficient proposal distributions approximating the local beliefs at each note of the mrf .	0	4	1	4.916505	-4.436096	0
401-7-21	the computational complexity of our algorithm at each iteration is quadratic in the number of particles .	the algorithm constructs adaptively efficient proposal distributions approximating the local beliefs at each note of the mrf .	0	5	1	4.237834	-3.8866808	0
401-7-21	we also propose an accelerated implementation with sub-quadratic computational complexity which still provides consistent estimates of the loopy bp marginal distributions and performs almost as well as the original procedure .	the algorithm constructs adaptively efficient proposal distributions approximating the local beliefs at each note of the mrf .	0	6	1	4.9108505	-4.4175024	0
401-7-21	this is achieved by considering proposal distributions in the exponential family whose parameters are updated iterately in an expectation propagation ( ep ) framework .	the proposed particle scheme provides consistent estimation of the lbp marginals as the number of particles increases .	1	2	3	-5.702861	5.0649447	1
401-7-21	this is achieved by considering proposal distributions in the exponential family whose parameters are updated iterately in an expectation propagation ( ep ) framework .	we demonstrate that it provides more accurate results than the particle belief propagation ( pbp ) algorithm of at a fraction of the computational cost and is additionally more robust empirically .	1	2	4	-5.8939466	4.9860625	1
401-7-21	the computational complexity of our algorithm at each iteration is quadratic in the number of particles .	this is achieved by considering proposal distributions in the exponential family whose parameters are updated iterately in an expectation propagation ( ep ) framework .	0	5	2	4.3357677	-3.999723	0
401-7-21	this is achieved by considering proposal distributions in the exponential family whose parameters are updated iterately in an expectation propagation ( ep ) framework .	we also propose an accelerated implementation with sub-quadratic computational complexity which still provides consistent estimates of the loopy bp marginal distributions and performs almost as well as the original procedure .	1	2	6	-5.9436646	5.0688086	1
401-7-21	the proposed particle scheme provides consistent estimation of the lbp marginals as the number of particles increases .	we demonstrate that it provides more accurate results than the particle belief propagation ( pbp ) algorithm of at a fraction of the computational cost and is additionally more robust empirically .	1	3	4	-5.1010275	4.6830997	1
401-7-21	the proposed particle scheme provides consistent estimation of the lbp marginals as the number of particles increases .	the computational complexity of our algorithm at each iteration is quadratic in the number of particles .	1	3	5	1.4569194	-1.2726771	0
401-7-21	we also propose an accelerated implementation with sub-quadratic computational complexity which still provides consistent estimates of the loopy bp marginal distributions and performs almost as well as the original procedure .	the proposed particle scheme provides consistent estimation of the lbp marginals as the number of particles increases .	0	6	3	3.8857617	-3.6243978	0
401-7-21	the computational complexity of our algorithm at each iteration is quadratic in the number of particles .	we demonstrate that it provides more accurate results than the particle belief propagation ( pbp ) algorithm of at a fraction of the computational cost and is additionally more robust empirically .	0	5	4	-4.41868	4.176347	1
401-7-21	we demonstrate that it provides more accurate results than the particle belief propagation ( pbp ) algorithm of at a fraction of the computational cost and is additionally more robust empirically .	we also propose an accelerated implementation with sub-quadratic computational complexity which still provides consistent estimates of the loopy bp marginal distributions and performs almost as well as the original procedure .	1	4	6	1.58594	-1.245683	0
401-7-21	the computational complexity of our algorithm at each iteration is quadratic in the number of particles .	we also propose an accelerated implementation with sub-quadratic computational complexity which still provides consistent estimates of the loopy bp marginal distributions and performs almost as well as the original procedure .	1	5	6	-5.5837774	5.016448	1
402-5-10	in many statistical problems , a more coarse-grained model may be suitable for population-level behaviour , whereas a more detailed model is appropriate for accurate modelling of individual behaviour .	this raises the question of how to integrate both types of models .	1	0	1	-4.245599	4.0867662	1
402-5-10	methods such as posterior regularization follow the idea of generalized moment matching , in that they allow matching expectations between two models , but sometimes both models are most conveniently expressed as latent variable models .	in many statistical problems , a more coarse-grained model may be suitable for population-level behaviour , whereas a more detailed model is appropriate for accurate modelling of individual behaviour .	0	2	0	-2.860617	2.9689722	1
402-5-10	in many statistical problems , a more coarse-grained model may be suitable for population-level behaviour , whereas a more detailed model is appropriate for accurate modelling of individual behaviour .	we propose latent bayesian melding , which is motivated by averaging the distributions over populations statistics of both the individual-level and the population-level models under a logarithmic opinion pool framework .	1	0	3	-5.482237	5.12142	1
402-5-10	in many statistical problems , a more coarse-grained model may be suitable for population-level behaviour , whereas a more detailed model is appropriate for accurate modelling of individual behaviour .	in a case study on electricity disaggregation , which is a type of singlechannel blind source separation problem , we show that latent bayesian melding leads to significantly more accurate predictions than an approach based solely on generalized moment matching .	1	0	4	-5.979151	5.254794	1
402-5-10	this raises the question of how to integrate both types of models .	methods such as posterior regularization follow the idea of generalized moment matching , in that they allow matching expectations between two models , but sometimes both models are most conveniently expressed as latent variable models .	1	1	2	3.985144	-3.5723684	0
402-5-10	this raises the question of how to integrate both types of models .	we propose latent bayesian melding , which is motivated by averaging the distributions over populations statistics of both the individual-level and the population-level models under a logarithmic opinion pool framework .	1	1	3	-5.1440334	4.8899503	1
402-5-10	this raises the question of how to integrate both types of models .	in a case study on electricity disaggregation , which is a type of singlechannel blind source separation problem , we show that latent bayesian melding leads to significantly more accurate predictions than an approach based solely on generalized moment matching .	1	1	4	-5.9875264	5.1319656	1
402-5-10	we propose latent bayesian melding , which is motivated by averaging the distributions over populations statistics of both the individual-level and the population-level models under a logarithmic opinion pool framework .	methods such as posterior regularization follow the idea of generalized moment matching , in that they allow matching expectations between two models , but sometimes both models are most conveniently expressed as latent variable models .	0	3	2	5.02162	-4.4040604	0
402-5-10	in a case study on electricity disaggregation , which is a type of singlechannel blind source separation problem , we show that latent bayesian melding leads to significantly more accurate predictions than an approach based solely on generalized moment matching .	methods such as posterior regularization follow the idea of generalized moment matching , in that they allow matching expectations between two models , but sometimes both models are most conveniently expressed as latent variable models .	0	4	2	5.4132986	-4.7252836	0
402-5-10	we propose latent bayesian melding , which is motivated by averaging the distributions over populations statistics of both the individual-level and the population-level models under a logarithmic opinion pool framework .	in a case study on electricity disaggregation , which is a type of singlechannel blind source separation problem , we show that latent bayesian melding leads to significantly more accurate predictions than an approach based solely on generalized moment matching .	1	3	4	-5.9810176	5.1256094	1
